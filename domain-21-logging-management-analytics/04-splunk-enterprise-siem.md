# Splunkä¼ä¸šçº§æ—¥å¿—åˆ†æä¸å®‰å…¨æ™ºèƒ½å¹³å°æ·±åº¦å®è·µ

> **æ–‡æ¡£å®šä½**: ä¼ä¸šçº§æ—¥å¿—åˆ†æã€å®‰å…¨ä¿¡æ¯å’Œäº‹ä»¶ç®¡ç†(SIEM)å¹³å° | **æ›´æ–°æ—¶é—´**: 2026-02-07
> 
> æœ¬æ–‡æ¡£æ·±å…¥è§£æSplunkåœ¨ä¼ä¸šç¯å¢ƒä¸­çš„å®Œæ•´æ—¥å¿—åˆ†æå’Œå®‰å…¨æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼Œæ¶µç›–æ•°æ®æ‘„å…¥ã€å®æ—¶åˆ†æã€æœºå™¨å­¦ä¹ ã€å®‰å…¨ç›‘æ§ç­‰æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸ºæ„å»ºä¼ä¸šçº§æ•°æ®æ´å¯Ÿå’Œå¨èƒæ£€æµ‹å¹³å°æä¾›ä¸“ä¸šæŒ‡å¯¼ã€‚

## ğŸ“‹ æ–‡æ¡£ç›®å½•

- [æ¶æ„æ¦‚è¿°](#æ¶æ„æ¦‚è¿°)
- [æ ¸å¿ƒç»„ä»¶æ·±åº¦è§£æ](#æ ¸å¿ƒç»„ä»¶æ·±åº¦è§£æ)
- [ä¼ä¸šçº§éƒ¨ç½²æ¶æ„](#ä¼ä¸šçº§éƒ¨ç½²æ¶æ„)
- [æ•°æ®æ‘„å…¥ä¸å¤„ç†](#æ•°æ®æ‘„å…¥ä¸å¤„ç†)
- [å®æ—¶æœç´¢ä¸åˆ†æ](#å®æ—¶æœç´¢ä¸åˆ†æ)
- [æœºå™¨å­¦ä¹ ä¸AIèƒ½åŠ›](#æœºå™¨å­¦ä¹ ä¸aièƒ½åŠ›)
- [å®‰å…¨ä¿¡æ¯ä¸äº‹ä»¶ç®¡ç†](#å®‰å…¨ä¿¡æ¯ä¸äº‹ä»¶ç®¡ç†)
- [å¯è§†åŒ–ä¸æŠ¥è¡¨](#å¯è§†åŒ–ä¸æŠ¥è¡¨)
- [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
- [æœ€ä½³å®è·µæ€»ç»“](#æœ€ä½³å®è·µæ€»ç»“)

---

## æ¶æ„æ¦‚è¿°

### Splunkå¹³å°æ¶æ„

```yaml
# Splunkä¼ä¸šçº§æ—¥å¿—åˆ†æå¹³å°æ•´ä½“æ¶æ„
splunk_platform:
  æ•°æ®æ‘„å…¥å±‚:
    universal_forwarder: è½»é‡çº§æ•°æ®è½¬å‘å™¨
    heavy_forwarder: é‡é‡çº§æ•°æ®å¤„ç†è½¬å‘å™¨
    syslog_inputs: ç³»ç»Ÿæ—¥å¿—è¾“å…¥
    network_inputs: ç½‘ç»œè®¾å¤‡æ—¥å¿—è¾“å…¥
    api_inputs: åº”ç”¨ç¨‹åºAPIè¾“å…¥
    
  æ•°æ®å¤„ç†å±‚:
    parsing_queue: æ•°æ®è§£æé˜Ÿåˆ—
    transformation_engine: æ•°æ®è½¬æ¢å¼•æ“
    field_extraction: å­—æ®µæŠ½å–å¤„ç†å™¨
    event_typing: äº‹ä»¶åˆ†ç±»å¼•æ“
    
  å­˜å‚¨ç´¢å¼•å±‚:
    indexer_cluster: ç´¢å¼•å™¨é›†ç¾¤
    search_head_cluster: æœç´¢å¤´é›†ç¾¤
    kv_store: é”®å€¼å­˜å‚¨
    smartstore: æ™ºèƒ½å­˜å‚¨æ¶æ„
    
  åˆ†æå±•ç¤ºå±‚:
    splunk_web: Webç”¨æˆ·ç•Œé¢
    dashboards: äº¤äº’å¼ä»ªè¡¨æ¿
    alerts: å®æ—¶å‘Šè­¦ç³»ç»Ÿ
    reports: è‡ªåŠ¨åŒ–æŠ¥è¡¨ç”Ÿæˆ
```

### æ ¸å¿ƒä»·å€¼ä¸»å¼ 

**ç»Ÿä¸€æ•°æ®å¹³å°**
- å•ä¸€å¹³å°å¤„ç†æœºå™¨æ•°æ®ã€æ—¥å¿—ã€æŒ‡æ ‡ã€äº‹ä»¶ç­‰å¤šç§æ•°æ®ç±»å‹
- ç»Ÿä¸€çš„æœç´¢è¯­è¨€(SPL)å’Œåˆ†ææ¥å£
- è·¨é¢†åŸŸæ•°æ®å…³è”åˆ†æå’Œå¨èƒæƒ…æŠ¥æ•´åˆ
- æ”¯æŒPBçº§æ•°æ®çš„å®æ—¶æœç´¢å’Œåˆ†æ

**æ™ºèƒ½åˆ†æèƒ½åŠ›**
- å†…ç½®æœºå™¨å­¦ä¹ ç®—æ³•å’Œç»Ÿè®¡åˆ†æåŠŸèƒ½
- å¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹æ€§åˆ†æ
- è‡ªç„¶è¯­è¨€å¤„ç†å’Œè¯­ä¹‰åˆ†æ
- è‡ªåŠ¨åŒ–æ¨¡å¼è¯†åˆ«å’Œå…³è”åˆ†æ

**ä¼ä¸šçº§å®‰å…¨**
- ç¬¦åˆSOC 2ã€ISO 27001ç­‰å®‰å…¨æ ‡å‡†
- å¤šå±‚æ¬¡è®¿é—®æ§åˆ¶å’Œå®¡è®¡æ—¥å¿—
- æ•°æ®åŠ å¯†å’Œéšç§ä¿æŠ¤
- é«˜å¯ç”¨æ¶æ„å’Œç¾å¤‡èƒ½åŠ›

---

## æ ¸å¿ƒç»„ä»¶æ·±åº¦è§£æ

### Indexeré›†ç¾¤æ¶æ„

#### é›†ç¾¤éƒ¨ç½²é…ç½®

```ini
# Splunk Indexeré›†ç¾¤é…ç½®
[clustering]
mode = master
pass4SymmKey = $7$xxxxxxx...
cluster_label = enterprise-splunk-cluster

[replication_factor]
search_factor = 2
replication_factor = 3

[master_uri]
master_uri = https://splunk-master:8089

[sslConfig]
enableSplunkdSSL = true
sslRootCAPath = $SPLUNK_HOME/etc/auth/cacert.pem
serverCert = $SPLUNK_HOME/etc/auth/server.pem

# IndexerèŠ‚ç‚¹é…ç½®
[indexer1]
serverName = splunk-indexer-01
mgmtHostPort = splunk-indexer-01:8089
site = site1

[indexer2]
serverName = splunk-indexer-02
mgmtHostPort = splunk-indexer-02:8089
site = site2

[indexer3]
serverName = splunk-indexer-03
mgmtHostPort = splunk-indexer-03:8089
site = site3
```

#### ç´¢å¼•ä¼˜åŒ–é…ç½®

```ini
# ç´¢å¼•æ€§èƒ½ä¼˜åŒ–é…ç½®
[volume:hot]
path = /opt/splunk/var/lib/splunk
maxVolumeDataSizeMB = 100000

[volume:warm]
path = /data/splunk/warm
maxVolumeDataSizeMB = 1000000

[volume:cold]
path = /archive/splunk/cold
maxVolumeDataSizeMB = 5000000

[index]
homePath = volume:hot/$index_name/db
coldPath = volume:cold/$index_name/colddb
thawedPath = $SPLUNK_DB/$index_name/thaweddb
frozenTimePeriodInSecs = 2592000
maxHotBuckets = 10
maxWarmDBCount = 300
maxDataSize = auto
frozenTimePeriodInSecs = 7776000
```

### Search Headé›†ç¾¤é…ç½®

#### è´Ÿè½½å‡è¡¡é…ç½®

```xml
<!-- Search Headè´Ÿè½½å‡è¡¡é…ç½® -->
<Proxy balancer://splunk_searchheads>
    BalancerMember https://splunk-sh1:8000 route=sh1
    BalancerMember https://splunk-sh2:8000 route=sh2
    BalancerMember https://splunk-sh3:8000 route=sh3
    
    ProxySet lbmethod=byrequests
    ProxySet stickysession=JSESSIONID|jsessionid
    ProxySet nofailover=On
    ProxySet timeout=30
</Proxy>

<Location />
    ProxyPass balancer://splunk_searchheads/
    ProxyPassReverse balancer://splunk_searchheads/
</Location>
```

#### æœç´¢ä¼˜åŒ–é…ç½®

```ini
# Search Headæ€§èƒ½ä¼˜åŒ–
[search]
max_searches_per_cpu = 2
base_max_searches = 6
max_rt_search_multiplier = 2
realtime_buffer = 10000
indexed_realtime = 1
indexed_realtime_use_indextime = 1

[diskUsage]
minFreeSpace = 5000
pollingFrequency = 30

[clustering]
multisite = true
available_sites = site1,site2,site3
site_replication_factor = origin:2,total:3
site_search_factor = origin:1,total:2
```

---

## ä¼ä¸šçº§éƒ¨ç½²æ¶æ„

### é«˜å¯ç”¨éƒ¨ç½²æ–¹æ¡ˆ

#### Kuberneteséƒ¨ç½²æ¶æ„

```yaml
# Splunk Kuberneteséƒ¨ç½²é…ç½®
apiVersion: v1
kind: Namespace
metadata:
  name: splunk-enterprise

---
# Indexer StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: splunk-indexer
  namespace: splunk-enterprise
spec:
  serviceName: splunk-indexer-headless
  replicas: 6
  selector:
    matchLabels:
      app: splunk-indexer
  template:
    metadata:
      labels:
        app: splunk-indexer
        tier: indexer
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - splunk-indexer
                topologyKey: kubernetes.io/hostname
                
      containers:
        - name: splunk
          image: splunk/splunk:9.0.4
          env:
            - name: SPLUNK_START_ARGS
              value: "--accept-license"
            - name: SPLUNK_CLUSTER_MASTER_URL
              value: "splunk-master.splunk-enterprise.svc.cluster.local"
            - name: SPLUNK_ROLE
              value: "splunk_indexer"
            - name: SPLUNK_INDEXER_URL
              value: "splunk-indexer-0.splunk-indexer-headless,splunk-indexer-1.splunk-indexer-headless"
              
          ports:
            - containerPort: 8089
              name: mgmt
            - containerPort: 9997
              name: receiving
            
          readinessProbe:
            exec:
              command:
                - /sbin/checkstate.sh
            initialDelaySeconds: 300
            periodSeconds: 30
            
          livenessProbe:
            exec:
              command:
                - /sbin/checkstate.sh
            initialDelaySeconds: 300
            periodSeconds: 60
            
          resources:
            requests:
              memory: "8Gi"
              cpu: "2"
            limits:
              memory: "16Gi"
              cpu: "4"
              
          volumeMounts:
            - name: splunk-var
              mountPath: /opt/splunk/var
            - name: splunk-etc
              mountPath: /opt/splunk/etc
              
  volumeClaimTemplates:
    - metadata:
        name: splunk-var
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 500Gi
    - metadata:
        name: splunk-etc
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 50Gi
```

#### ç½‘ç»œå®‰å…¨é…ç½®

```yaml
# ç½‘ç»œç­–ç•¥é…ç½®
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: splunk-network-policy
  namespace: splunk-enterprise
spec:
  podSelector:
    matchLabels:
      app: splunk-indexer
  policyTypes:
    - Ingress
    - Egress
    
  ingress:
    # å…è®¸Forwarderè¿æ¥
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
      ports:
        - protocol: TCP
          port: 9997
          
    # å…è®¸Search Headè¿æ¥
    - from:
        - podSelector:
            matchLabels:
              app: splunk-search-head
      ports:
        - protocol: TCP
          port: 8089
          
    # å…è®¸é›†ç¾¤å†…éƒ¨é€šä¿¡
    - from:
        - podSelector:
            matchLabels:
              app: splunk-indexer
      ports:
        - protocol: TCP
          port: 8089
        - protocol: TCP
          port: 9997
          
  egress:
    # å…è®¸DNSæŸ¥è¯¢
    - to:
        - namespaceSelector:
            matchLabels:
              name: kube-system
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
          
    # å…è®¸å¤–éƒ¨æ•°æ®æºè¿æ¥
    - to:
        - ipBlock:
            cidr: 10.0.0.0/8
      ports:
        - protocol: TCP
          port: 514  # Syslog
        - protocol: TCP
          port: 1514 # Secure Syslog
```

### å®‰å…¨åŠ å›ºé…ç½®

#### è®¿é—®æ§åˆ¶ç­–ç•¥

```ini
# Splunkå®‰å…¨é…ç½®
[authentication]
authType = SAML
saml_idpUrl = https://sso.company.com/idp/profile/SAML2/Redirect/SSO
saml_entityId = https://splunk.company.com/saml/login
saml_certPath = /opt/splunk/etc/auth/saml/samlCert.pem
saml_signAuthnRequest = true

[roleMap_SAML]
admin = SplunkAdmin
power = SplunkPowerUser
user = SplunkUser

[authorization]
forceCookieSecure = true
rest_sslVerifyServerCert = true
allowHttpFrameAncestors = false

[sslConfig]
sslVersions = tls1.2
cipherSuite = ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256
sslKeysfile = $SPLUNK_HOME/etc/auth/server.pem
sslRootCAPath = $SPLUNK_HOME/etc/auth/cacert.pem
requireClientCert = false
```

---

## æ•°æ®æ‘„å…¥ä¸å¤„ç†

### Universal Forwarderé…ç½®

#### é«˜çº§æ•°æ®æ”¶é›†é…ç½®

```ini
# Universal Forwarder inputs.conf
[default]
host = $decideOnStartup

[monitor:///var/log/application/]
disabled = false
index = application-logs
sourcetype = app_log
crcSalt = <SOURCE>
ignoreOlderThan = 5d
followTail = true
whitelist = \.(log|out)$
blacklist = \.(tmp|swp)$

[monitor:///var/log/security/]
disabled = false
index = security-logs
sourcetype = linux_secure
crcSalt = <SOURCE>

[script:///opt/scripts/system_metrics.sh]
disabled = false
index = system-metrics
interval = 60
sourcetype = script_metrics

[tcp://:9997]
disabled = false
connection_host = dns

[udp://:514]
disabled = false
connection_host = ip

# é«˜çº§å¤„ç†é…ç½®
[host]
separator = -
regex = ^([^-]+)-(.+)$
dest = host_segment

[datetime]
TZ = Asia/Shanghai
MAX_TIMESTAMP_LOOKAHEAD = 32
```

#### æ•°æ®é¢„å¤„ç†å’Œä¸°å¯Œ

```python
# Pythonè„šæœ¬è¿›è¡Œæ•°æ®é¢„å¤„ç†
import sys
import json
import re
from datetime import datetime

def preprocess_log_line(line):
    """é¢„å¤„ç†æ—¥å¿—è¡Œ"""
    try:
        # è§£æJSONæ ¼å¼æ—¥å¿—
        if line.strip().startswith('{'):
            log_data = json.loads(line)
            
            # æ ‡å‡†åŒ–æ—¶é—´æˆ³
            if 'timestamp' in log_data:
                dt = datetime.fromisoformat(log_data['timestamp'].replace('Z', '+00:00'))
                log_data['timestamp'] = dt.strftime('%Y-%m-%d %H:%M:%S')
            
            # æå–å…³é”®å­—æ®µ
            enriched_data = {
                'host': log_data.get('hostname', 'unknown'),
                'source': log_data.get('source', 'application'),
                'level': log_data.get('level', 'INFO'),
                'message': log_data.get('message', ''),
                'user_id': log_data.get('user_id', ''),
                'session_id': log_data.get('session_id', ''),
                'processing_time': log_data.get('duration_ms', 0),
                '_raw': line.strip()
            }
            
            # æ·»åŠ è®¡ç®—å­—æ®µ
            if 'error' in enriched_data['message'].lower():
                enriched_data['error_flag'] = 1
            else:
                enriched_data['error_flag'] = 0
                
            return json.dumps(enriched_data)
            
        else:
            # å¤„ç†æ–‡æœ¬æ—¥å¿—
            parts = line.split('|')
            if len(parts) >= 4:
                return json.dumps({
                    'timestamp': parts[0].strip(),
                    'level': parts[1].strip(),
                    'source': parts[2].strip(),
                    'message': '|'.join(parts[3:]).strip(),
                    '_raw': line.strip()
                })
            else:
                return line
                
    except Exception as e:
        return json.dumps({
            'error': str(e),
            'original_line': line,
            '_raw': line
        })

if __name__ == "__main__":
    for line in sys.stdin:
        processed_line = preprocess_log_line(line)
        print(processed_line)
```

### Heavy Forwarderæ•°æ®å¤„ç†

```ini
# Heavy Forwarder props.conf
[source::Syslog_Network]
TRANSFORMS-set_index = set_network_index
TRANSFORMS-anonymize_ip = anonymize_src_ip, anonymize_dst_ip
REPORT-network_fields = extract_network_fields

[source::Application_Logs]
TRANSFORMS-set_index = set_app_index
TRANSFORMS-enrich_data = enrich_user_session
REPORT-app_fields = extract_app_fields

# Heavy Forwarder transforms.conf
[set_network_index]
REGEX = .
FORMAT = index::network-logs
DEST_KEY = _MetaData:Index

[set_app_index]
REGEX = .
FORMAT = index::application-logs
DEST_KEY = _MetaData:Index

[anonymize_src_ip]
SOURCE_KEY = _raw
REGEX = (SRC=)(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})
FORMAT = $1XXX.XXX.XXX.XXX
DEST_KEY = _raw

[extract_network_fields]
SOURCE_KEY = _raw
REGEX = (\w+)=(?:"([^"]*)"|(\S+))
FORMAT = $1::$2$3
REPEAT_MATCH = true

[enrich_user_session]
SOURCE_KEY = _raw
REGEX = user_id=(\w+)
LOOKUP = user_lookup user_id OUTPUT user_name, department, role
```

---

## å®æ—¶æœç´¢ä¸åˆ†æ

### SPLæœç´¢è¯­è¨€é«˜çº§åº”ç”¨

#### å¤æ‚æ•°æ®åˆ†ææŸ¥è¯¢

```spl
# ç”¨æˆ·è¡Œä¸ºåˆ†æ
index=application-logs sourcetype=app_log 
| eval user_id=coalesce(user_id, uid)
| eval session_id=coalesce(session_id, sid)
| stats 
    count as total_actions,
    avg(processing_time) as avg_response_time,
    dc(session_id) as unique_sessions,
    earliest(_time) as first_action,
    latest(_time) as last_action
  by user_id, date_mday, date_hour
| eval session_duration=last_action-first_action
| where total_actions > 10 AND avg_response_time > 1000
| sort - avg_response_time
| head 100

# å¼‚å¸¸æ£€æµ‹åˆ†æ
index=security-logs sourcetype=linux_secure failed_password=yes
| eval src_ip=mvindex(split(_raw, " "), -1)
| iplocation src_ip
| geostats latfield=lat longfield=lon count by Country
| where count > 50
| sort - count

# ä¸šåŠ¡æŒ‡æ ‡å…³è”åˆ†æ
index=application-logs sourcetype=transaction_log status=*
| eval success=if(status=="SUCCESS", 1, 0)
| eval failure=if(status=="FAILURE", 1, 0)
| timechart 
    span=1h 
    sum(success) as successful_transactions,
    sum(failure) as failed_transactions,
    avg(response_time) as avg_response_time
| eval success_rate=successful_transactions/(successful_transactions+failed_transactions)*100
| where success_rate < 95
```

#### æœºå™¨å­¦ä¹ æ¨¡å‹åº”ç”¨

```spl
# å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒ
| inputlookup transactions.csv 
| fit DensityFunctionDetection response_time, transaction_amount, user_risk_score
| outputfit anomalymodel

# å®æ—¶å¼‚å¸¸æ£€æµ‹
index=application-logs sourcetype=transaction_log
| apply anomalymodel
| where is_anomaly=1
| table _time, user_id, transaction_amount, response_time, anomaly_score

# é¢„æµ‹æ€§åˆ†æ
index=system-metrics sourcetype=cpu_usage
| predict cpu_utilization algorithm=LLP future_timespan=24
| timechart span=1h avg(cpu_utilization) as actual, lower95 as lower_bound, upper95 as upper_bound, predicted(cpu_utilization) as forecast
| where _time > relative_time(now(), "-1d@d")
```

---

## æœºå™¨å­¦ä¹ ä¸AIèƒ½åŠ›

### å†…ç½®MLç®—æ³•åº”ç”¨

#### å¼‚å¸¸æ£€æµ‹é…ç½®

```ini
# ML Toolkitå¼‚å¸¸æ£€æµ‹é…ç½®
[anomaly_detection_config]
algorithm = DensityFunctionDetection
fields = response_time, error_rate, throughput
training_window = 30d
detection_window = 1h
threshold = 0.95

[model_parameters]
normalization = zscore
smoothing = exponential
seasonality = daily,weekly
```

#### é¢„æµ‹æ¨¡å‹é…ç½®

```python
# Pythoné¢„æµ‹æ¨¡å‹é…ç½®
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import joblib

class ResourcePredictor:
    def __init__(self):
        self.model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        self.scaler = StandardScaler()
        
    def train(self, training_data):
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        X = training_data[['cpu_usage', 'memory_usage', 'network_io', 'disk_io']]
        y = training_data['future_load']
        
        X_scaled = self.scaler.fit_transform(X)
        self.model.fit(X_scaled, y)
        
        # ä¿å­˜æ¨¡å‹
        joblib.dump(self.model, '/opt/splunk/etc/apps/ml_models/resource_predictor.pkl')
        joblib.dump(self.scaler, '/opt/splunk/etc/apps/ml_models/scaler.pkl')
        
    def predict(self, current_metrics):
        """é¢„æµ‹æœªæ¥èµ„æºä½¿ç”¨"""
        X_scaled = self.scaler.transform([current_metrics])
        prediction = self.model.predict(X_scaled)[0]
        return prediction

# åœ¨Splunkä¸­æ³¨å†Œè‡ªå®šä¹‰å‘½ä»¤
if __name__ == "__main__":
    import sys
    predictor = ResourcePredictor()
    
    # ä»stdinè¯»å–æ•°æ®
    input_data = sys.stdin.read()
    # å¤„ç†å’Œé¢„æµ‹é€»è¾‘
    # ...
```

---

## å®‰å…¨ä¿¡æ¯ä¸äº‹ä»¶ç®¡ç†

### SIEMè§„åˆ™é…ç½®

#### å¨èƒæ£€æµ‹è§„åˆ™

```xml
<!-- XMLæ ¼å¼çš„å¨èƒæ£€æµ‹è§„åˆ™ -->
<threat_hunting_rule>
    <name>Lateral Movement Detection</name>
    <description>Detect unusual authentication patterns indicating lateral movement</description>
    <enabled>true</enabled>
    <severity>high</severity>
    
    <search>
        <![CDATA[
        index=security-logs sourcetype=windows_security EventCode=4624 OR EventCode=4625
        | eval success=if(EventCode=4624, 1, 0)
        | eval failure=if(EventCode=4625, 1, 0)
        | stats 
            count as total_logons,
            sum(success) as successful_logons,
            sum(failure) as failed_logons,
            dc(host) as unique_hosts,
            dc(user) as unique_users
          by user, _time
        | where unique_hosts > 10 AND failed_logons > 5
        | lookup user_behavior_baseline user OUTPUT baseline_unique_hosts, baseline_failed_attempts
        | eval anomaly_score=(unique_hosts/baseline_unique_hosts) * (failed_logons/baseline_failed_attempts)
        | where anomaly_score > 2.0
        ]]>
    </search>
    
    <cron_schedule>*/15 * * * *</cron_schedule>
    <earliest_time>-1h@h</earliest_time>
    <latest_time>now</latest_time>
    
    <actions>
        <action type="alert">
            <threshold>1</threshold>
            <suppression>
                <field>user</field>
                <period>1h</period>
            </suppression>
        </action>
        <action type="notable_event">
            <title>Potential Lateral Movement Detected</title>
            <urgency>critical</urgency>
            <owner>security_team</owner>
        </action>
    </actions>
</threat_hunting_rule>
```

#### è¡Œä¸ºåŸºçº¿å»ºç«‹

```spl
# ç”¨æˆ·è¡Œä¸ºåŸºçº¿å»ºç«‹
index=application-logs sourcetype=user_activity
| bucket _time span=1d
| stats 
    avg(actions_per_day) as baseline_daily_actions,
    avg(session_duration) as baseline_session_duration,
    avg(login_hours) as baseline_login_hours,
    stdev(actions_per_day) as std_actions,
    stdev(session_duration) as std_duration
  by user_id
| outputlookup user_behavior_baselines.csv

# å®æ—¶è¡Œä¸ºå¯¹æ¯”
index=application-logs sourcetype=user_activity
| lookup user_behavior_baselines user_id
| eval 
    anomaly_actions = abs(actions_today-baseline_daily_actions)/std_actions,
    anomaly_duration = abs(session_minutes-baseline_session_duration)/std_duration
| where anomaly_actions > 3 OR anomaly_duration > 3
| table _time, user_id, actions_today, session_minutes, anomaly_actions, anomaly_duration
```

---

## å¯è§†åŒ–ä¸æŠ¥è¡¨

### ä»ªè¡¨æ¿é…ç½®

#### äº¤äº’å¼ä»ªè¡¨æ¿

```xml
<!-- Dashboard XMLé…ç½® -->
<form theme="dark">
  <label>Security Operations Center Dashboard</label>
  <fieldset submitButton="false"></fieldset>
  
  <row>
    <panel>
      <single>
        <title>Total Security Events</title>
        <search>
          <query>index=security-logs | stats count</query>
          <earliest>-24h@h</earliest>
          <latest>now</latest>
        </search>
        <option name="colorBy">value</option>
        <option name="colorMode">block</option>
        <option name="drilldown">none</option>
        <option name="numberPrecision">0</option>
        <option name="rangeColors">["0x53a051","0x0877a6","0xf8be34","0xf1813f","0xdc4e41"]</option>
      </single>
    </panel>
    
    <panel>
      <chart>
        <title>Security Events by Type</title>
        <search>
          <query>index=security-logs | timechart count by event_type</query>
          <earliest>-7d@h</earliest>
          <latest>now</latest>
        </search>
        <option name="charting.chart">area</option>
        <option name="charting.drilldown">none</option>
      </chart>
    </panel>
  </row>
  
  <row>
    <panel>
      <table>
        <title>Top Threat Sources</title>
        <search>
          <query>index=security-logs threat_level=high | top limit=10 src_ip | iplocation src_ip | table src_ip, Country, count</query>
          <earliest>-24h@h</earliest>
          <latest>now</latest>
        </search>
        <option name="drilldown">cell</option>
      </table>
    </panel>
  </row>
</form>
```

#### è‡ªåŠ¨åŒ–æŠ¥è¡¨ç”Ÿæˆ

```python
# è‡ªåŠ¨åŒ–æŠ¥è¡¨ç”Ÿæˆè„šæœ¬
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
import pandas as pd

class ReportGenerator:
    def __init__(self, smtp_config):
        self.smtp_server = smtp_config['server']
        self.smtp_port = smtp_config['port']
        self.username = smtp_config['username']
        self.password = smtp_config['password']
        
    def generate_weekly_report(self):
        """ç”Ÿæˆå‘¨æŠ¥"""
        # æ‰§è¡ŒSplunkæœç´¢è·å–æ•°æ®
        search_query = """
        index=security-logs 
        | timechart span=1d count by event_severity
        | addtotals fieldname=total_events
        | eval week_number=strftime(_time, "%Y-W%V")
        """
        
        # å¤„ç†æ•°æ®ç”ŸæˆæŠ¥è¡¨
        df = self.execute_splunk_search(search_query)
        summary_stats = df.describe()
        
        # ç”ŸæˆHTMLæŠ¥è¡¨
        html_report = self.create_html_report(df, summary_stats)
        
        # å‘é€é‚®ä»¶
        self.send_report(html_report, 'Weekly Security Report')
        
    def create_html_report(self, data_df, stats_df):
        """åˆ›å»ºHTMLæ ¼å¼æŠ¥è¡¨"""
        html_template = """
        <html>
        <head>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .header { background-color: #4CAF50; color: white; padding: 20px; text-align: center; }
                .section { margin: 20px 0; }
                table { border-collapse: collapse; width: 100%; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
                .chart { margin: 20px 0; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Weekly Security Operations Report</h1>
                <p>Generated on {date}</p>
            </div>
            
            <div class="section">
                <h2>Executive Summary</h2>
                <p>Total Security Events: {total_events}</p>
                <p>Critical Incidents: {critical_incidents}</p>
                <p>Average Response Time: {avg_response_time} minutes</p>
            </div>
            
            <div class="section">
                <h2>Event Distribution by Severity</h2>
                {severity_table}
            </div>
            
            <div class="section">
                <h2>Trend Analysis</h2>
                <img src="cid:trend_chart" alt="Trend Chart">
            </div>
        </body>
        </html>
        """
        
        return html_template.format(
            date=pd.Timestamp.now().strftime('%Y-%m-%d'),
            total_events=data_df['total_events'].sum(),
            critical_incidents=data_df.get('critical', pd.Series([0])).sum(),
            avg_response_time=stats_df.get('response_time', pd.Series([0])).mean(),
            severity_table=data_df.to_html(classes='table', escape=False)
        )

# è°ƒåº¦é…ç½®
if __name__ == "__main__":
    config = {
        'server': 'smtp.company.com',
        'port': 587,
        'username': 'reports@company.com',
        'password': 'secure_password'
    }
    
    reporter = ReportGenerator(config)
    reporter.generate_weekly_report()
```

---

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### ç´¢å¼•ä¼˜åŒ–

#### SmartStoreé…ç½®

```ini
# SmartStoreé…ç½®
[smartstore]
disabled = false

[volume:remote_store]
storageType = remote
path = s3://splunk-smartstore-bucket
remote.s3.access_key = xxxxxxxx
remote.s3.secret_key = yyyyyyyy
remote.s3.endpoint = https://s3.cn-north-1.amazonaws.com.cn

[index]
remotePath = volume:remote_store/%index%
cachePath = $SPLUNK_DB/%index%/cache
maxCacheSize = 100000
minHotIdleSecsBeforeForceUpload = 300
```

#### æœç´¢æ€§èƒ½ä¼˜åŒ–

```spl
# æ€§èƒ½ä¼˜åŒ–çš„æœç´¢ç¤ºä¾‹
| tstats 
    count as event_count,
    avg(response_time) as avg_response,
    max(error_code) as max_error
  from datamodel=Application_State.Application_Event
  where Application_State.Application_Event.app="webapp" 
    AND _time > relative_time(now(), "-1d")
  by host, sourcetype, _time
| fields host, sourcetype, _time, event_count, avg_response, max_error
| where avg_response > 1000 OR max_error > 0
```

### é›†ç¾¤æ€§èƒ½ç›‘æ§

```bash
#!/bin/bash
# Splunké›†ç¾¤æ€§èƒ½ç›‘æ§è„šæœ¬

# æ£€æŸ¥Indexeré›†ç¾¤çŠ¶æ€
check_indexer_cluster() {
    curl -k -u admin:password \
        https://splunk-master:8089/services/cluster/master/info \
        | grep -E "(replication_factor|search_factor|cluster_label)"
}

# æ£€æŸ¥Search Headè´Ÿè½½
check_search_head_load() {
    curl -k -u admin:password \
        https://splunk-sh1:8089/services/server/status/performance \
        | jq '.entry[].content | {cpu_usage, memory_usage, search_load}'
}

# æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ
check_disk_usage() {
    df -h | grep -E "(splunk|data)" | awk '{print $5 " " $6}'
}

# æ£€æŸ¥ç´¢å¼•å¤§å°
check_index_sizes() {
    splunk cmd splunkd rest \
        /services/data/indexes \
        -auth admin:password \
        | grep -E "(title|totalSizeMB)" \
        | paste - - \
        | awk '{print $2 " " $4}'
}

# æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
main() {
    echo "=== Splunké›†ç¾¤å¥åº·æ£€æŸ¥ ==="
    echo "Indexeré›†ç¾¤çŠ¶æ€:"
    check_indexer_cluster
    
    echo -e "\nSearch Headè´Ÿè½½:"
    check_search_head_load
    
    echo -e "\nç£ç›˜ä½¿ç”¨æƒ…å†µ:"
    check_disk_usage
    
    echo -e "\nç´¢å¼•å¤§å°:"
    check_index_sizes
}

main
```

---

## æœ€ä½³å®è·µæ€»ç»“

### éƒ¨ç½²æ¶æ„å»ºè®®

```yaml
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
production_recommendations:
  cluster_sizing:
    indexers: 6-12 nodes
    search_heads: 3-5 nodes
    masters: 3 nodes (cluster master, deployer, license master)
    
  hardware_requirements:
    indexers:
      cpu: 16-32 cores
      memory: 128-256GB
      storage: 2-4TB SSD per node
      
    search_heads:
      cpu: 8-16 cores
      memory: 64-128GB
      storage: 500GB SSD
      
  network_configuration:
    bandwidth: 10Gbps between nodes
    latency: < 2ms within cluster
    mtu: 9000 (jumbo frames)
    
  backup_strategy:
    configuration_backup: daily to git repository
    data_backup: weekly snapshots to remote storage
    disaster_recovery: cross-region replication
```

### ç›‘æ§å’Œç»´æŠ¤

#### æ—¥å¸¸è¿ç»´æ£€æŸ¥æ¸…å•

- [ ] é›†ç¾¤å¥åº·çŠ¶æ€æ£€æŸ¥
- [ ] ç´¢å¼•å™¨æ•°æ®æ‘„å…¥é€Ÿç‡ç›‘æ§
- [ ] Search Headæœç´¢æ€§èƒ½åˆ†æ
- [ ] ç£ç›˜ç©ºé—´ä½¿ç”¨æƒ…å†µè·Ÿè¸ª
- [ ] Licenseä½¿ç”¨æƒ…å†µå®¡æŸ¥
- [ ] å®‰å…¨é…ç½®åˆè§„æ€§æ£€æŸ¥
- [ ] å¤‡ä»½å®Œæ•´æ€§éªŒè¯
- [ ] ç”¨æˆ·è®¿é—®æƒé™å®¡è®¡

é€šè¿‡ä»¥ä¸Šå…¨é¢çš„Splunkä¼ä¸šçº§æ—¥å¿—åˆ†æå’Œå®‰å…¨æ™ºèƒ½å¹³å°å®è·µï¼Œå¯ä»¥æ„å»ºå¼ºå¤§çš„æ•°æ®æ´å¯Ÿå’Œå¨èƒæ£€æµ‹èƒ½åŠ›ã€‚
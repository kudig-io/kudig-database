# 04-ä¼ä¸šçº§ç›‘æ§ä½“ç³»

> **é€‚ç”¨èŒƒå›´**: Kubernetes v1.25-v1.32 | **ç»´æŠ¤çŠ¶æ€**: ğŸ”§ æŒç»­æ›´æ–°ä¸­ | **ä¸“å®¶çº§åˆ«**: â­â­â­â­â­

## ğŸ“‹ æ¦‚è¿°

ä¼ä¸šçº§ç›‘æ§ä½“ç³»æ˜¯ä¿éšœKubernetesç”Ÿäº§ç¯å¢ƒç¨³å®šè¿è¡Œçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å®Œæ•´çš„ç›‘æ§æ¶æ„è®¾è®¡ã€ç»„ä»¶é€‰å‹å’Œæœ€ä½³å®è·µã€‚

## ğŸ—ï¸ ç›‘æ§æ¶æ„è®¾è®¡

### ä¸‰å±‚ç›‘æ§æ¶æ„

#### 1. åŸºç¡€è®¾æ–½å±‚ç›‘æ§
```yaml
# Node Exporter DaemonSeté…ç½®
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      hostNetwork: true
      hostPID: true
      containers:
      - name: node-exporter
        image: quay.io/prometheus/node-exporter:v1.5.0
        args:
        - --web.listen-address=:9100
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+)($|/)
        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
        ports:
        - containerPort: 9100
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
```

#### 2. Kubernetesç»„ä»¶ç›‘æ§
```yaml
# kube-state-metricsé…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: kube-state-metrics
  replicas: 2
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      containers:
      - name: kube-state-metrics
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.7.0
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 8081
          name: telemetry
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8081
          initialDelaySeconds: 5
          timeoutSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  namespace: monitoring
  labels:
    app: kube-state-metrics
spec:
  ports:
  - name: http-metrics
    port: 8080
    targetPort: http-metrics
  - name: telemetry
    port: 8081
    targetPort: telemetry
  selector:
    app: kube-state-metrics
```

#### 3. åº”ç”¨å±‚ç›‘æ§
```yaml
# åº”ç”¨ç›‘æ§Sidecaræ¨¡å¼
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitored-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: myapp:latest
        ports:
        - containerPort: 8080
        env:
        - name: METRICS_ENABLED
          value: "true"
```

## ğŸ“Š Prometheusç›‘æ§æ ˆ

### æ ¸å¿ƒç»„ä»¶é…ç½®

#### 1. Prometheus Serveré…ç½®
```yaml
# Prometheusé…ç½®æ–‡ä»¶
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: production
    region: us-west-2

rule_files:
  - "rules/alerts.yml"
  - "rules/recording.yml"

alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - alertmanager.monitoring.svc:9093

scrape_configs:
  # KubernetesèŠ‚ç‚¹ç›‘æ§
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
    - role: node
    relabel_configs:
    - source_labels: [__address__]
      regex: '(.*):10250'
      target_label: __address__
      replacement: '${1}:9100'
    - target_label: __scheme__
      replacement: http

  # Kubernetes Podsç›‘æ§
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
    - role: pod
    relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      target_label: __metrics_path__
      regex: (.+)
    - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
      action: replace
      regex: ([^:]+)(?::\d+)?;(\d+)
      replacement: $1:$2
      target_label: __address__

  # kube-state-metrics
  - job_name: 'kube-state-metrics'
    static_configs:
    - targets: ['kube-state-metrics:8080']
```

#### 2. é•¿æœŸå­˜å‚¨é…ç½®
```yaml
# Thanos Sidecaré…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-thanos
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-thanos
  template:
    metadata:
      labels:
        app: prometheus-thanos
    spec:
      containers:
      - name: thanos-sidecar
        image: quay.io/thanos/thanos:v0.30.0
        args:
        - sidecar
        - --http-address=0.0.0.0:10902
        - --grpc-address=0.0.0.0:10901
        - --prometheus.url=http://localhost:9090
        - --objstore.config-file=/etc/thanos/objstore.yml
        - --tsdb.path=/prometheus
        ports:
        - name: http
          containerPort: 10902
        - name: grpc
          containerPort: 10901
        volumeMounts:
        - name: prometheus-storage
          mountPath: /prometheus
        - name: thanos-config
          mountPath: /etc/thanos
      volumes:
      - name: prometheus-storage
        persistentVolumeClaim:
          claimName: prometheus-pvc
      - name: thanos-config
        configMap:
          name: thanos-objstore-config
```

### å‘Šè­¦è§„åˆ™é…ç½®

#### 1. æ ¸å¿ƒå‘Šè­¦è§„åˆ™
```yaml
# æ ¸å¿ƒå‘Šè­¦è§„åˆ™
groups:
- name: kubernetes.rules
  rules:
  # èŠ‚ç‚¹ç›¸å…³å‘Šè­¦
  - alert: NodeDown
    expr: up{job="kubernetes-nodes"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Node {{ $labels.instance }} is down"
      description: "Node has been down for more than 5 minutes"

  - alert: NodeCPUHigh
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage on node {{ $labels.instance }}"
      description: "CPU usage is above 85% for more than 10 minutes"

  # Podç›¸å…³å‘Šè­¦
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 0
    for: 15m
    labels:
      severity: critical
    annotations:
      summary: "Pod {{ $labels.pod }} is crash looping"
      description: "Pod is restarting more than 5 times per hour"

  - alert: PodPending
    expr: kube_pod_status_phase{phase="Pending"} == 1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.pod }} is pending"
      description: "Pod has been in Pending state for more than 10 minutes"
```

## ğŸ¨ Grafanaå¯è§†åŒ–

### æ ¸å¿ƒä»ªè¡¨æ¿é…ç½®

#### 1. é›†ç¾¤æ¦‚è§ˆä»ªè¡¨æ¿
```json
{
  "dashboard": {
    "title": "Kubernetes Cluster Overview",
    "panels": [
      {
        "title": "Cluster Health Status",
        "type": "stat",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "sum(up{job=\"kubernetes-nodes\"})",
            "legendFormat": "Nodes Up"
          },
          {
            "expr": "count(kube_pod_info)",
            "legendFormat": "Total Pods"
          },
          {
            "expr": "sum(kube_deployment_status_replicas_available)",
            "legendFormat": "Available Deployments"
          }
        ]
      },
      {
        "title": "Resource Utilization",
        "type": "graph",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "100 * sum(kube_pod_container_resource_requests{resource=\"cpu\"}) / sum(kube_node_status_allocatable{resource=\"cpu\"})",
            "legendFormat": "CPU Requested %"
          },
          {
            "expr": "100 * sum(kube_pod_container_resource_limits{resource=\"cpu\"}) / sum(kube_node_status_allocatable{resource=\"cpu\"})",
            "legendFormat": "CPU Limits %"
          }
        ]
      }
    ]
  }
}
```

#### 2. åº”ç”¨æ€§èƒ½ä»ªè¡¨æ¿
```json
{
  "dashboard": {
    "title": "Application Performance",
    "panels": [
      {
        "title": "HTTP Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (app)",
            "legendFormat": "{{app}}"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (app) / sum(rate(http_requests_total[5m])) by (app) * 100",
            "legendFormat": "{{app}} Error %"
          }
        ]
      },
      {
        "title": "Latency Distribution",
        "type": "heatmap",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "p95 Latency"
          }
        ]
      }
    ]
  }
}
```

## ğŸš¨ Alertmanagerå‘Šè­¦ç®¡ç†

### å‘Šè­¦è·¯ç”±é…ç½®

#### 1. å¤šçº§å‘Šè­¦è·¯ç”±
```yaml
# Alertmanageré…ç½®
global:
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts'
  smtp_auth_password: 'password'

route:
  group_by: ['alertname', 'cluster']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 3h
  receiver: 'default-receiver'
  
  routes:
  - match:
      severity: critical
    receiver: 'pagerduty'
    group_wait: 10s
    repeat_interval: 1h
    
  - match:
      severity: warning
    receiver: 'slack-warning'
    group_wait: 1m
    
  - match:
      team: sre
    receiver: 'sre-team'
    routes:
    - match:
        service: database
      receiver: 'db-team'

receivers:
- name: 'default-receiver'
  email_configs:
  - to: 'team@example.com'
    send_resolved: true

- name: 'pagerduty'
  pagerduty_configs:
  - service_key: 'YOUR_PAGERDUTY_KEY'
    send_resolved: true

- name: 'slack-warning'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts-warning'
    send_resolved: true
    title: '{{ template "slack.warning.title" . }}'
    text: '{{ template "slack.warning.text" . }}'
```

#### 2. å‘Šè­¦æŠ‘åˆ¶è§„åˆ™
```yaml
# å‘Šè­¦æŠ‘åˆ¶é…ç½®
inhibit_rules:
- source_match:
    alertname: 'NodeDown'
  target_match:
    alertname: 'ServiceDown'
  equal: ['instance']

- source_match:
    alertname: 'ClusterDown'
  target_match_re:
    alertname: '.*'
  equal: ['cluster']
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### ç›‘æ§ç³»ç»Ÿè°ƒä¼˜

#### 1. Prometheusæ€§èƒ½ä¼˜åŒ–
```yaml
# Prometheuså­˜å‚¨ä¼˜åŒ–é…ç½®
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prometheus
spec:
  template:
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:v2.40.0
        args:
        - --storage.tsdb.retention.time=30d
        - --storage.tsdb.retention.size=50GB
        - --storage.tsdb.wal-compression
        - --web.enable-lifecycle
        - --web.enable-admin-api
        - --query.max-concurrency=20
        - --query.timeout=2m
        resources:
          requests:
            cpu: 2
            memory: 8Gi
          limits:
            cpu: 4
            memory: 16Gi
```

#### 2. æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥
```yaml
# Recordingè§„åˆ™ä¼˜åŒ–
groups:
- name: recording.rules
  rules:
  # é¢„è®¡ç®—é«˜é¢‘æŸ¥è¯¢
  - record: job:node_cpu_utilization:avg5m
    expr: avg by(job) (rate(node_cpu_seconds_total{mode!="idle"}[5m]))
    
  - record: cluster:memory_utilization:ratio
    expr: sum(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / sum(node_memory_MemTotal_bytes)
    
  # èšåˆé™é‡‡æ ·
  - record: instance:network_bytes:rate1m
    expr: rate(node_network_receive_bytes_total[1m]) + rate(node_network_transmit_bytes_total[1m])
```

## ğŸ”§ å®æ–½æ£€æŸ¥æ¸…å•

### ç›‘æ§ä½“ç³»å»ºè®¾
- [ ] è®¾è®¡å®Œæ•´çš„ä¸‰å±‚ç›‘æ§æ¶æ„
- [ ] éƒ¨ç½²æ ¸å¿ƒç›‘æ§ç»„ä»¶(Prometheusã€Grafanaã€Alertmanager)
- [ ] é…ç½®åŸºç¡€è®¾æ–½å±‚ç›‘æ§(Node Exporterã€kube-state-metrics)
- [ ] å®ç°åº”ç”¨å±‚ç›‘æ§é›†æˆ
- [ ] å»ºç«‹å®Œå–„çš„å‘Šè­¦è§„åˆ™ä½“ç³»
- [ ] é…ç½®å¤šæ¸ é“å‘Šè­¦é€šçŸ¥

### æ€§èƒ½ä¸å¯é æ€§
- [ ] ä¼˜åŒ–Prometheuså­˜å‚¨å’ŒæŸ¥è¯¢æ€§èƒ½
- [ ] å®æ–½ç›‘æ§æ•°æ®é•¿æœŸå­˜å‚¨æ–¹æ¡ˆ
- [ ] é…ç½®ç›‘æ§ç³»ç»Ÿçš„é«˜å¯ç”¨éƒ¨ç½²
- [ ] å»ºç«‹ç›‘æ§æ•°æ®å¤‡ä»½å’Œæ¢å¤æœºåˆ¶
- [ ] å®æ–½ç›‘æ§ç³»ç»Ÿå®¹é‡è§„åˆ’
- [ ] å®šæœŸå®¡æŸ¥å’Œä¼˜åŒ–å‘Šè­¦è§„åˆ™

### è¿è¥ç»´æŠ¤
- [ ] å»ºç«‹ç›‘æ§ä»ªè¡¨æ¿æ ‡å‡†åŒ–æ¨¡æ¿
- [ ] å®æ–½ç›‘æ§æ•°æ®è´¨é‡ç›‘æ§
- [ ] å»ºç«‹å‘Šè­¦å“åº”å’Œå¤„ç†æµç¨‹
- [ ] å®šæœŸè¿›è¡Œç›‘æ§ç³»ç»Ÿå¥åº·æ£€æŸ¥
- [ ] ç»´æŠ¤ç›‘æ§æ–‡æ¡£å’Œæ“ä½œæ‰‹å†Œ
- [ ] æŒç»­æ”¹è¿›ç›‘æ§è¦†ç›–èŒƒå›´

---

*æœ¬æ–‡æ¡£ä¸ºä¼ä¸šçº§Kubernetesç›‘æ§ä½“ç³»æä¾›å…¨é¢çš„æŠ€æœ¯æŒ‡å¯¼å’Œå®æ–½æ¡†æ¶*
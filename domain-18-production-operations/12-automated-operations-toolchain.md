# 12-è‡ªåŠ¨åŒ–è¿ç»´å·¥å…·é“¾

> **é€‚ç”¨èŒƒå›´**: Kubernetes v1.25-v1.32 | **ç»´æŠ¤çŠ¶æ€**: ğŸ”§ æŒç»­æ›´æ–°ä¸­ | **ä¸“å®¶çº§åˆ«**: â­â­â­â­â­

## ğŸ“‹ æ¦‚è¿°

è‡ªåŠ¨åŒ–è¿ç»´å·¥å…·é“¾æ˜¯æå‡è¿ç»´æ•ˆç‡å’Œç³»ç»Ÿå¯é æ€§çš„å…³é”®ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»Kubernetesç¯å¢ƒä¸‹çš„è‡ªåŠ¨åŒ–è¿ç»´å·¥å…·å’Œæœ€ä½³å®è·µã€‚

## ğŸ› ï¸ æ ¸å¿ƒå·¥å…·ç»„ä»¶

### åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–

#### 1. Ansibleè¿ç»´å‰§æœ¬
```yaml
# KubernetesèŠ‚ç‚¹åˆå§‹åŒ–å‰§æœ¬
---
- name: Initialize Kubernetes Nodes
  hosts: k8s_nodes
  become: yes
  vars:
    kubernetes_version: "1.28.2"
    container_runtime: "containerd"
    pod_network_cidr: "10.244.0.0/16"
  
  tasks:
  - name: Install container runtime
    apt:
      name: "{{ container_runtime }}"
      state: present
    when: ansible_os_family == "Debian"
  
  - name: Configure containerd
    copy:
      src: files/containerd-config.toml
      dest: /etc/containerd/config.toml
      owner: root
      group: root
      mode: '0644'
  
  - name: Install Kubernetes components
    apt:
      name:
        - kubelet={{ kubernetes_version }}-00
        - kubeadm={{ kubernetes_version }}-00
        - kubectl={{ kubernetes_version }}-00
      state: present
      update_cache: yes
  
  - name: Hold Kubernetes packages
    dpkg_selections:
      name: "{{ item }}"
      selection: hold
    loop:
      - kubelet
      - kubeadm
      - kubectl
  
  - name: Enable and start services
    systemd:
      name: "{{ item }}"
      enabled: yes
      state: started
    loop:
      - containerd
      - kubelet
```

#### 2. èŠ‚ç‚¹å¥åº·æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# èŠ‚ç‚¹å¥åº·æ£€æŸ¥è„šæœ¬

NODE_NAME=$(hostname)
CHECK_TIME=$(date -Iseconds)
HEALTH_STATUS="healthy"

# æ£€æŸ¥å…³é”®æœåŠ¡çŠ¶æ€
check_services() {
    local services=("kubelet" "containerd" "docker")
    for service in "${services[@]}"; do
        if ! systemctl is-active --quiet "$service"; then
            echo "Service $service is not running"
            HEALTH_STATUS="unhealthy"
        fi
    done
}

# æ£€æŸ¥ç£ç›˜ç©ºé—´
check_disk_space() {
    local threshold=85
    local usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
    
    if [ "$usage" -gt "$threshold" ]; then
        echo "Disk usage is ${usage}% (threshold: ${threshold}%)"
        HEALTH_STATUS="warning"
    fi
}

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
check_memory() {
    local threshold=90
    local usage=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
    
    if [ "$usage" -gt "$threshold" ]; then
        echo "Memory usage is ${usage}% (threshold: ${threshold}%)"
        HEALTH_STATUS="warning"
    fi
}

# æ£€æŸ¥KubernetesèŠ‚ç‚¹çŠ¶æ€
check_k8s_node() {
    if ! kubectl get node "$NODE_NAME" -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep -q "True"; then
        echo "Kubernetes node is not ready"
        HEALTH_STATUS="unhealthy"
    fi
}

# æ‰§è¡Œæ£€æŸ¥
check_services
check_disk_space
check_memory
check_k8s_node

# ä¸ŠæŠ¥å¥åº·çŠ¶æ€
report_health() {
    local payload=$(jq -n \
        --arg node "$NODE_NAME" \
        --arg status "$HEALTH_STATUS" \
        --arg time "$CHECK_TIME" \
        '{
            node: $node,
            status: $status,
            timestamp: $time,
            checks: {
                services: "passed",
                disk_space: "passed",
                memory: "passed",
                k8s_node: "passed"
            }
        }')
    
    curl -X POST -H "Content-Type: application/json" \
        -d "$payload" \
        "http://monitoring-server/health"
}

report_health
```

### åº”ç”¨éƒ¨ç½²è‡ªåŠ¨åŒ–

#### 1. Helméƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# è‡ªåŠ¨åŒ–Helméƒ¨ç½²è„šæœ¬

set -e

APP_NAME="$1"
NAMESPACE="$2"
VALUES_FILE="$3"
CHART_REPO="$4"
CHART_NAME="$5"
CHART_VERSION="$6"

# éªŒè¯å‚æ•°
validate_params() {
    if [[ -z "$APP_NAME" || -z "$NAMESPACE" || -z "$VALUES_FILE" ]]; then
        echo "Usage: $0 <app-name> <namespace> <values-file> [chart-repo] [chart-name] [chart-version]"
        exit 1
    fi
    
    if [[ ! -f "$VALUES_FILE" ]]; then
        echo "Values file $VALUES_FILE not found"
        exit 1
    fi
}

# åˆå§‹åŒ–Helm
init_helm() {
    echo "Initializing Helm..."
    helm repo add stable https://charts.helm.sh/stable
    if [[ -n "$CHART_REPO" ]]; then
        helm repo add custom "$CHART_REPO"
    fi
    helm repo update
}

# éƒ¨ç½²åº”ç”¨
deploy_application() {
    echo "Deploying $APP_NAME to namespace $NAMESPACE..."
    
    # åˆ›å»ºå‘½åç©ºé—´
    kubectl create namespace "$NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -
    
    # éƒ¨ç½²åº”ç”¨
    local helm_args=(
        "--namespace" "$NAMESPACE"
        "--values" "$VALUES_FILE"
        "--timeout" "10m"
        "--wait"
    )
    
    if [[ -n "$CHART_VERSION" ]]; then
        helm_args+=("--version" "$CHART_VERSION")
    fi
    
    if [[ -n "$CHART_NAME" && -n "$CHART_REPO" ]]; then
        helm upgrade --install "$APP_NAME" "$CHART_REPO/$CHART_NAME" "${helm_args[@]}"
    else
        helm upgrade --install "$APP_NAME" "./charts/$APP_NAME" "${helm_args[@]}"
    fi
}

# éªŒè¯éƒ¨ç½²
verify_deployment() {
    echo "Verifying deployment..."
    
    # ç­‰å¾…Podå°±ç»ª
    kubectl wait --for=condition=ready pod -l app.kubernetes.io/name="$APP_NAME" \
        --namespace "$NAMESPACE" --timeout=300s
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    if kubectl get svc -l app.kubernetes.io/name="$APP_NAME" --namespace "$NAMESPACE" >/dev/null 2>&1; then
        echo "Service is available"
    else
        echo "Warning: No service found for $APP_NAME"
    fi
    
    # è¿è¡Œå¥åº·æ£€æŸ¥
    if [[ -f "scripts/health-check-$APP_NAME.sh" ]]; then
        bash "scripts/health-check-$APP_NAME.sh" "$NAMESPACE"
    fi
}

# å›æ»šæœºåˆ¶
rollback_on_failure() {
    echo "Deployment failed, rolling back..."
    helm rollback "$APP_NAME" --namespace "$NAMESPACE"
    exit 1
}

# ä¸»æ‰§è¡Œæµç¨‹
main() {
    trap rollback_on_failure ERR
    
    validate_params
    init_helm
    deploy_application
    verify_deployment
    
    echo "Deployment completed successfully!"
}

main "$@"
```

#### 2. è“ç»¿éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# è“ç»¿éƒ¨ç½²è‡ªåŠ¨åŒ–è„šæœ¬

set -e

APP_NAME="$1"
NAMESPACE="$2"
NEW_VERSION="$3"

# éƒ¨ç½²æ–°ç‰ˆæœ¬ï¼ˆç»¿è‰²ç¯å¢ƒï¼‰
deploy_green() {
    echo "Deploying new version to green environment..."
    
    helm upgrade --install "${APP_NAME}-green" "./charts/$APP_NAME" \
        --namespace "$NAMESPACE" \
        --set image.tag="$NEW_VERSION" \
        --set service.name="${APP_NAME}-green" \
        --set ingress.hosts[0].host="${APP_NAME}-green.example.com" \
        --timeout 10m \
        --wait
    
    # éªŒè¯ç»¿è‰²ç¯å¢ƒ
    kubectl wait --for=condition=available deployment/"${APP_NAME}-green" \
        --namespace "$NAMESPACE" --timeout=300s
}

# æµé‡åˆ‡æ¢
switch_traffic() {
    echo "Switching traffic to green environment..."
    
    # æ›´æ–°ä¸»æœåŠ¡æŒ‡å‘ç»¿è‰²éƒ¨ç½²
    kubectl patch service "$APP_NAME" \
        -p '{"spec":{"selector":{"app.kubernetes.io/name":"'"$APP_NAME"'","version":"green"}}}' \
        --namespace "$NAMESPACE"
    
    # ç­‰å¾…æµé‡åˆ‡æ¢å®Œæˆ
    sleep 30
    
    # éªŒè¯æµé‡åˆ‡æ¢
    if curl -f "http://${APP_NAME}.example.com/health" >/dev/null 2>&1; then
        echo "Traffic successfully switched to green environment"
    else
        echo "Health check failed after traffic switch"
        exit 1
    fi
}

# æ¸…ç†è“è‰²ç¯å¢ƒ
cleanup_blue() {
    echo "Cleaning up blue environment..."
    
    helm uninstall "${APP_NAME}-blue" --namespace "$NAMESPACE" || true
}

# å›æ»šå‡½æ•°
rollback() {
    echo "Rolling back to blue environment..."
    
    # æ¢å¤æœåŠ¡æŒ‡å‘è“è‰²ç¯å¢ƒ
    kubectl patch service "$APP_NAME" \
        -p '{"spec":{"selector":{"app.kubernetes.io/name":"'"$APP_NAME"'","version":"blue"}}}' \
        --namespace "$NAMESPACE"
    
    # é‡æ–°éƒ¨ç½²è“è‰²ç¯å¢ƒ
    helm upgrade --install "${APP_NAME}-blue" "./charts/$APP_NAME" \
        --namespace "$NAMESPACE" \
        --timeout 5m \
        --wait
    
    exit 1
}

# ä¸»æ‰§è¡Œæµç¨‹
main() {
    trap rollback ERR
    
    deploy_green
    switch_traffic
    cleanup_blue
    
    echo "Blue-green deployment completed successfully!"
}

main "$@"
```

## ğŸ¤– æ™ºèƒ½è¿ç»´å·¥å…·

### è‡ªæ„ˆç³»ç»Ÿ

#### 1. è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œæ¢å¤
```python
#!/usr/bin/env python3
# è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œæ¢å¤ç³»ç»Ÿ

import asyncio
import aiohttp
import json
import logging
from datetime import datetime, timedelta
from kubernetes import client, config
from kubernetes.client.rest import ApiException

class AutoHealingSystem:
    def __init__(self):
        config.load_kube_config()
        self.apps_v1 = client.AppsV1Api()
        self.core_v1 = client.CoreV1Api()
        self.monitoring_url = "http://prometheus:9090/api/v1/query"
        self.logger = self.setup_logger()
    
    def setup_logger(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    async def check_pod_health(self, namespace, deployment_name):
        """æ£€æŸ¥Podå¥åº·çŠ¶æ€"""
        try:
            deployment = self.apps_v1.read_namespaced_deployment(
                deployment_name, namespace
            )
            
            # æ£€æŸ¥å‰¯æœ¬çŠ¶æ€
            if (deployment.status.ready_replicas != deployment.status.replicas or
                deployment.status.unavailable_replicas > 0):
                return False, "Replica mismatch"
            
            # æ£€æŸ¥PodçŠ¶æ€
            pods = self.core_v1.list_namespaced_pod(
                namespace, 
                label_selector=f"app={deployment_name}"
            )
            
            for pod in pods.items:
                if pod.status.phase not in ['Running', 'Succeeded']:
                    return False, f"Pod {pod.metadata.name} in {pod.status.phase} state"
                
                # æ£€æŸ¥å®¹å™¨é‡å¯æ¬¡æ•°
                for container_status in pod.status.container_statuses or []:
                    if container_status.restart_count > 5:
                        return False, f"Container {container_status.name} restarted {container_status.restart_count} times"
            
            return True, "Healthy"
            
        except ApiException as e:
            return False, f"API Error: {e}"
    
    async def get_error_metrics(self, app_name):
        """è·å–åº”ç”¨é”™è¯¯æŒ‡æ ‡"""
        query = f'rate(http_requests_total{{app="{app_name}",status=~"5.."}}[5m])'
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                self.monitoring_url, 
                params={'query': query}
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    if data['data']['result']:
                        return float(data['data']['result'][0]['value'][1])
                return 0.0
    
    async def restart_deployment(self, namespace, deployment_name):
        """é‡å¯Deployment"""
        try:
            # è§¦å‘æ»šåŠ¨æ›´æ–°
            deployment = self.apps_v1.read_namespaced_deployment(
                deployment_name, namespace
            )
            
            # æ·»åŠ æ—¶é—´æˆ³æ³¨è§£è§¦å‘æ›´æ–°
            if deployment.spec.template.metadata.annotations is None:
                deployment.spec.template.metadata.annotations = {}
            
            deployment.spec.template.metadata.annotations['kubectl.kubernetes.io/restartedAt'] = \
                datetime.now().isoformat()
            
            self.apps_v1.patch_namespaced_deployment(
                deployment_name, namespace, deployment
            )
            
            self.logger.info(f"Restarted deployment {deployment_name} in {namespace}")
            return True
            
        except ApiException as e:
            self.logger.error(f"Failed to restart deployment: {e}")
            return False
    
    async def heal_application(self, namespace, app_name):
        """æ²»æ„ˆåº”ç”¨ç¨‹åº"""
        # æ£€æŸ¥å¥åº·çŠ¶æ€
        is_healthy, reason = await self.check_pod_health(namespace, app_name)
        
        if is_healthy:
            # æ£€æŸ¥é”™è¯¯ç‡
            error_rate = await self.get_error_metrics(app_name)
            
            if error_rate > 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
                self.logger.warning(f"High error rate ({error_rate:.2%}) for {app_name}")
                await self.restart_deployment(namespace, app_name)
        else:
            self.logger.warning(f"Unhealthy application {app_name}: {reason}")
            await self.restart_deployment(namespace, app_name)
    
    async def run_continuous_healing(self):
        """æŒç»­è¿è¡Œè‡ªæ„ˆç³»ç»Ÿ"""
        while True:
            try:
                # è·å–æ‰€æœ‰åº”ç”¨åˆ—è¡¨
                namespaces = self.core_v1.list_namespace()
                
                for ns in namespaces.items:
                    if ns.metadata.name in ['kube-system', 'monitoring']:
                        continue
                    
                    deployments = self.apps_v1.list_namespaced_deployment(ns.metadata.name)
                    
                    for deployment in deployments.items:
                        app_name = deployment.metadata.name
                        await self.heal_application(ns.metadata.name, app_name)
                
                # ç­‰å¾…ä¸‹ä¸€è½®æ£€æŸ¥
                await asyncio.sleep(300)  # 5åˆ†é’Ÿ
                
            except Exception as e:
                self.logger.error(f"Error in healing cycle: {e}")
                await asyncio.sleep(60)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    healer = AutoHealingSystem()
    await healer.run_continuous_healing()

if __name__ == "__main__":
    asyncio.run(main())
```

### å®¹é‡è§„åˆ’å·¥å…·

#### 1. èµ„æºé¢„æµ‹å’Œè§„åˆ’
```python
#!/usr/bin/env python3
# å®¹å™¨èµ„æºé¢„æµ‹å·¥å…·

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class ResourcePredictor:
    def __init__(self):
        self.cpu_model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.memory_model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        
    def prepare_features(self, metrics_data):
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        df = pd.DataFrame(metrics_data)
        
        # æ—¶é—´ç‰¹å¾
        df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
        df['dayofweek'] = pd.to_datetime(df['timestamp']).dt.dayofweek
        df['month'] = pd.to_datetime(df['timestamp']).dt.month
        
        # æ»åç‰¹å¾
        for lag in [1, 2, 3, 6, 12, 24]:
            df[f'cpu_lag_{lag}h'] = df['cpu_usage'].shift(lag)
            df[f'memory_lag_{lag}h'] = df['memory_usage'].shift(lag)
        
        # æ»šåŠ¨çª—å£ç»Ÿè®¡
        windows = [3, 6, 12, 24]
        for window in windows:
            df[f'cpu_mean_{window}h'] = df['cpu_usage'].rolling(window=window).mean()
            df[f'cpu_std_{window}h'] = df['cpu_usage'].rolling(window=window).std()
            df[f'memory_mean_{window}h'] = df['memory_usage'].rolling(window=window).mean()
            df[f'memory_std_{window}h'] = df['memory_usage'].rolling(window=window).std()
        
        return df.dropna()
    
    def train_models(self, training_data):
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        df = self.prepare_features(training_data)
        
        feature_columns = [col for col in df.columns 
                          if col not in ['timestamp', 'cpu_usage', 'memory_usage']]
        
        X = df[feature_columns]
        y_cpu = df['cpu_usage']
        y_memory = df['memory_usage']
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler.fit_transform(X)
        
        # è®­ç»ƒæ¨¡å‹
        self.cpu_model.fit(X_scaled, y_cpu)
        self.memory_model.fit(X_scaled, y_memory)
        
        print(f"Model trained on {len(df)} samples")
        print(f"Feature importance (CPU): {self.cpu_model.feature_importances_[:5]}")
    
    def predict_resources(self, future_timestamps):
        """é¢„æµ‹æœªæ¥èµ„æºéœ€æ±‚"""
        # æ„é€ æœªæ¥æ—¶é—´ç‰¹å¾
        future_dates = pd.to_datetime(future_timestamps)
        future_df = pd.DataFrame({
            'timestamp': future_timestamps,
            'hour': future_dates.hour,
            'dayofweek': future_dates.dayofweek,
            'month': future_dates.month
        })
        
        # æ·»åŠ æ»åç‰¹å¾å ä½ç¬¦
        for lag in [1, 2, 3, 6, 12, 24]:
            future_df[f'cpu_lag_{lag}h'] = 0.5  # ä½¿ç”¨å¹³å‡å€¼å¡«å……
            future_df[f'memory_lag_{lag}h'] = 0.5
        
        # æ·»åŠ æ»šåŠ¨ç»Ÿè®¡å ä½ç¬¦
        windows = [3, 6, 12, 24]
        for window in windows:
            future_df[f'cpu_mean_{window}h'] = 0.5
            future_df[f'cpu_std_{window}h'] = 0.1
            future_df[f'memory_mean_{window}h'] = 0.5
            future_df[f'memory_std_{window}h'] = 0.1
        
        feature_columns = [col for col in future_df.columns if col != 'timestamp']
        X_future = self.scaler.transform(future_df[feature_columns])
        
        cpu_predictions = self.cpu_model.predict(X_future)
        memory_predictions = self.memory_model.predict(X_future)
        
        return {
            'timestamps': future_timestamps,
            'cpu_predictions': cpu_predictions,
            'memory_predictions': memory_predictions
        }
    
    def generate_capacity_plan(self, predictions, current_capacity, growth_factor=1.2):
        """ç”Ÿæˆå®¹é‡è§„åˆ’å»ºè®®"""
        max_cpu = np.max(predictions['cpu_predictions'])
        max_memory = np.max(predictions['memory_predictions'])
        
        recommended_cpu = max_cpu * growth_factor
        recommended_memory = max_memory * growth_factor
        
        plan = {
            'current_capacity': current_capacity,
            'predicted_peak': {
                'cpu': max_cpu,
                'memory': max_memory
            },
            'recommended_capacity': {
                'cpu': recommended_cpu,
                'memory': recommended_memory
            },
            'scaling_required': {
                'cpu': recommended_cpu > current_capacity['cpu'],
                'memory': recommended_memory > current_capacity['memory']
            }
        }
        
        return plan

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡æ‹Ÿå†å²æ•°æ®
    dates = pd.date_range('2024-01-01', periods=168, freq='H')  # ä¸€å‘¨æ•°æ®
    training_data = {
        'timestamp': dates,
        'cpu_usage': np.random.normal(0.6, 0.15, 168),  # 60%å¹³å‡CPUä½¿ç”¨ç‡
        'memory_usage': np.random.normal(0.5, 0.12, 168)  # 50%å¹³å‡å†…å­˜ä½¿ç”¨ç‡
    }
    
    # è®­ç»ƒæ¨¡å‹
    predictor = ResourcePredictor()
    predictor.train_models(training_data)
    
    # é¢„æµ‹æœªæ¥ä¸€å‘¨
    future_dates = pd.date_range('2024-01-08', periods=168, freq='H')
    predictions = predictor.predict_resources(future_dates)
    
    # ç”Ÿæˆå®¹é‡è§„åˆ’
    current_capacity = {'cpu': 1.0, 'memory': 1.0}  # å½“å‰å®¹é‡100%
    capacity_plan = predictor.generate_capacity_plan(predictions, current_capacity)
    
    print("Capacity Planning Results:")
    print(f"Current Capacity: {capacity_plan['current_capacity']}")
    print(f"Predicted Peak: {capacity_plan['predicted_peak']}")
    print(f"Recommended Capacity: {capacity_plan['recommended_capacity']}")
    print(f"Scaling Required: {capacity_plan['scaling_required']}")
```

## ğŸ“Š ç›‘æ§å‘Šè­¦ç³»ç»Ÿ

### æ™ºèƒ½å‘Šè­¦èšåˆ

#### 1. å‘Šè­¦å»é‡å’Œå…³è”
```python
#!/usr/bin/env python3
# æ™ºèƒ½å‘Šè­¦å¤„ç†ç³»ç»Ÿ

import asyncio
import json
from collections import defaultdict
from datetime import datetime, timedelta
import logging

class SmartAlertManager:
    def __init__(self):
        self.alert_history = defaultdict(list)
        self.correlation_rules = self.load_correlation_rules()
        self.logger = self.setup_logger()
    
    def setup_logger(self):
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(__name__)
    
    def load_correlation_rules(self):
        """åŠ è½½å‘Šè­¦å…³è”è§„åˆ™"""
        return {
            'node_failure_cascade': {
                'patterns': [
                    'NodeNotReady',
                    'PodEvicted',
                    'ServiceUnavailable'
                ],
                'time_window': 300  # 5åˆ†é’Ÿçª—å£
            },
            'resource_exhaustion': {
                'patterns': [
                    'HighMemoryUsage',
                    'HighCPULoad',
                    'PodPending'
                ],
                'time_window': 600  # 10åˆ†é’Ÿçª—å£
            }
        }
    
    def calculate_alert_severity(self, alert):
        """è®¡ç®—å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""
        severity_weights = {
            'critical': 10,
            'warning': 5,
            'info': 1
        }
        
        base_severity = severity_weights.get(alert.get('severity', 'info'), 1)
        
        # è€ƒè™‘å‘Šè­¦é¢‘ç‡
        recent_alerts = self.get_recent_alerts(
            alert['alertname'], 
            timedelta(minutes=30)
        )
        
        frequency_factor = min(len(recent_alerts) / 5.0, 2.0)  # æœ€å¤š2å€æƒé‡
        
        # è€ƒè™‘å½±å“èŒƒå›´
        affected_pods = alert.get('affected_pods', 1)
        scope_factor = min(affected_pods / 10.0, 3.0)  # æœ€å¤š3å€æƒé‡
        
        return base_severity * frequency_factor * scope_factor
    
    def get_recent_alerts(self, alert_name, time_window):
        """è·å–è¿‘æœŸç›¸åŒç±»å‹çš„å‘Šè­¦"""
        cutoff_time = datetime.now() - time_window
        recent_alerts = []
        
        for alert in self.alert_history[alert_name]:
            if alert['timestamp'] > cutoff_time:
                recent_alerts.append(alert)
        
        return recent_alerts
    
    def detect_correlated_alerts(self, new_alert):
        """æ£€æµ‹å…³è”å‘Šè­¦"""
        correlations = []
        
        for rule_name, rule in self.correlation_rules.items():
            pattern_matches = 0
            
            for pattern in rule['patterns']:
                recent_alerts = self.get_recent_alerts(
                    pattern, 
                    timedelta(seconds=rule['time_window'])
                )
                
                if recent_alerts:
                    pattern_matches += 1
            
            # å¦‚æœåŒ¹é…è¶³å¤Ÿå¤šçš„æ¨¡å¼ï¼Œåˆ™è®¤ä¸ºå­˜åœ¨å…³è”
            if pattern_matches >= len(rule['patterns']) * 0.6:  # 60%åŒ¹é…ç‡
                correlations.append({
                    'rule': rule_name,
                    'confidence': pattern_matches / len(rule['patterns']),
                    'related_alerts': self.get_related_alerts(rule['patterns'], rule['time_window'])
                })
        
        return correlations
    
    def get_related_alerts(self, patterns, time_window):
        """è·å–ç›¸å…³å‘Šè­¦"""
        related = []
        cutoff_time = datetime.now() - timedelta(seconds=time_window)
        
        for pattern in patterns:
            for alert in self.alert_history[pattern]:
                if alert['timestamp'] > cutoff_time:
                    related.append(alert)
        
        return related
    
    def suppress_duplicate_alerts(self, new_alert):
        """æŠ‘åˆ¶é‡å¤å‘Šè­¦"""
        alert_name = new_alert['alertname']
        recent_alerts = self.get_recent_alerts(alert_name, timedelta(minutes=15))
        
        if not recent_alerts:
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å‘Šè­¦
        for recent_alert in recent_alerts:
            if (abs(new_alert.get('value', 0) - recent_alert.get('value', 0)) < 0.1 and
                new_alert.get('labels') == recent_alert.get('labels')):
                return True
        
        return False
    
    async def process_alert(self, alert_data):
        """å¤„ç†æ–°å‘Šè­¦"""
        alert = json.loads(alert_data)
        alert['timestamp'] = datetime.now()
        
        # è®°å½•å‘Šè­¦å†å²
        self.alert_history[alert['alertname']].append(alert)
        
        # æ£€æŸ¥é‡å¤å‘Šè­¦
        if self.suppress_duplicate_alerts(alert):
            self.logger.info(f"Suppressed duplicate alert: {alert['alertname']}")
            return
        
        # è®¡ç®—ä¸¥é‡ç¨‹åº¦
        severity_score = self.calculate_alert_severity(alert)
        alert['severity_score'] = severity_score
        
        # æ£€æµ‹å…³è”å‘Šè­¦
        correlations = self.detect_correlated_alerts(alert)
        alert['correlations'] = correlations
        
        # æ ¹æ®ä¸¥é‡ç¨‹åº¦å†³å®šå¤„ç†æ–¹å¼
        if severity_score > 50:
            await self.handle_critical_alert(alert)
        elif severity_score > 20:
            await self.handle_warning_alert(alert)
        else:
            await self.handle_info_alert(alert)
        
        self.logger.info(f"Processed alert: {alert['alertname']} (Severity: {severity_score})")
    
    async def handle_critical_alert(self, alert):
        """å¤„ç†ä¸¥é‡å‘Šè­¦"""
        self.logger.critical(f"CRITICAL ALERT: {alert}")
        
        # å‘é€ç´§æ€¥é€šçŸ¥
        await self.send_emergency_notification(alert)
        
        # è§¦å‘è‡ªåŠ¨ä¿®å¤
        if alert['alertname'] in ['NodeNotReady', 'PodCrashLooping']:
            await self.trigger_auto_repair(alert)
    
    async def handle_warning_alert(self, alert):
        """å¤„ç†è­¦å‘Šå‘Šè­¦"""
        self.logger.warning(f"WARNING ALERT: {alert}")
        
        # å‘é€é€šçŸ¥ç»™ç›¸å…³å›¢é˜Ÿ
        await self.send_team_notification(alert)
        
        # è®°å½•ç”¨äºè¶‹åŠ¿åˆ†æ
        await self.record_for_analysis(alert)
    
    async def handle_info_alert(self, alert):
        """å¤„ç†ä¿¡æ¯å‘Šè­¦"""
        self.logger.info(f"INFO ALERT: {alert}")
        
        # è®°å½•ç”¨äºç»Ÿè®¡åˆ†æ
        await self.record_for_analysis(alert)
    
    async def send_emergency_notification(self, alert):
        """å‘é€ç´§æ€¥é€šçŸ¥"""
        notification = {
            'type': 'emergency',
            'alert': alert,
            'timestamp': datetime.now().isoformat(),
            'recipients': ['sre-team', 'oncall-engineer']
        }
        
        # è¿™é‡Œå¯ä»¥é›†æˆå…·ä½“çš„é€šçŸ¥ç³»ç»Ÿ
        print(f"EMERGENCY NOTIFICATION: {json.dumps(notification, indent=2)}")
    
    async def send_team_notification(self, alert):
        """å‘é€å›¢é˜Ÿé€šçŸ¥"""
        notification = {
            'type': 'team',
            'alert': alert,
            'timestamp': datetime.now().isoformat(),
            'recipients': ['dev-team']
        }
        
        print(f"TEAM NOTIFICATION: {json.dumps(notification, indent=2)}")
    
    async def trigger_auto_repair(self, alert):
        """è§¦å‘è‡ªåŠ¨ä¿®å¤"""
        repair_actions = {
            'NodeNotReady': self.restart_node_components,
            'PodCrashLooping': self.restart_pod
        }
        
        action = repair_actions.get(alert['alertname'])
        if action:
            await action(alert)
    
    async def restart_node_components(self, alert):
        """é‡å¯èŠ‚ç‚¹ç»„ä»¶"""
        node_name = alert.get('labels', {}).get('node')
        if node_name:
            self.logger.info(f"Restarting components on node: {node_name}")
            # æ‰§è¡Œé‡å¯å‘½ä»¤
            # await self.execute_command(f"kubectl drain {node_name}")
            # await self.execute_command(f"kubectl uncordon {node_name}")
    
    async def restart_pod(self, alert):
        """é‡å¯Pod"""
        namespace = alert.get('labels', {}).get('namespace')
        pod_name = alert.get('labels', {}).get('pod')
        
        if namespace and pod_name:
            self.logger.info(f"Restarting pod: {namespace}/{pod_name}")
            # await self.execute_command(f"kubectl delete pod {pod_name} -n {namespace}")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    alert_manager = SmartAlertManager()
    
    # æ¨¡æ‹Ÿæ¥æ”¶å‘Šè­¦
    test_alerts = [
        '{"alertname": "HighMemoryUsage", "severity": "warning", "value": 85}',
        '{"alertname": "NodeNotReady", "severity": "critical", "labels": {"node": "node-1"}}',
        '{"alertname": "PodEvicted", "severity": "warning", "labels": {"namespace": "default", "pod": "app-1"}}'
    ]
    
    for alert_json in test_alerts:
        await alert_manager.process_alert(alert_json)
        await asyncio.sleep(1)

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ”§ å®æ–½æ£€æŸ¥æ¸…å•

### è‡ªåŠ¨åŒ–å·¥å…·éƒ¨ç½²
- [ ] éƒ¨ç½²åŸºç¡€è®¾æ–½è‡ªåŠ¨åŒ–å·¥å…·(Ansible/Terraform)
- [ ] é…ç½®åº”ç”¨éƒ¨ç½²è‡ªåŠ¨åŒ–è„šæœ¬
- [ ] å®æ–½æ™ºèƒ½æ•…éšœæ£€æµ‹å’Œè‡ªæ„ˆç³»ç»Ÿ
- [ ] éƒ¨ç½²å®¹é‡è§„åˆ’å’Œé¢„æµ‹å·¥å…·
- [ ] å»ºç«‹æ™ºèƒ½å‘Šè­¦å¤„ç†æœºåˆ¶
- [ ] é…ç½®ç›‘æ§å’Œæ—¥å¿—æ”¶é›†è‡ªåŠ¨åŒ–

### è¿ç»´æµç¨‹ä¼˜åŒ–
- [ ] å®æ–½æ ‡å‡†åŒ–è¿ç»´æ“ä½œæµç¨‹
- [ ] å»ºç«‹è‡ªåŠ¨åŒ–æµ‹è¯•å’ŒéªŒè¯æœºåˆ¶
- [ ] é…ç½®å˜æ›´ç®¡ç†å’Œå®¡æ‰¹æµç¨‹
- [ ] å®æ–½å›æ»šå’Œç¾éš¾æ¢å¤æœºåˆ¶
- [ ] å»ºç«‹è¿ç»´çŸ¥è¯†åº“å’Œæ–‡æ¡£
- [ ] é…ç½®è¿ç»´äººå‘˜åŸ¹è®­è®¡åˆ’

### ç³»ç»Ÿå¯é æ€§ä¿éšœ
- [ ] å®æ–½å¤šå±‚æ¬¡ç›‘æ§å‘Šè­¦ä½“ç³»
- [ ] é…ç½®è‡ªåŠ¨åŒ–æ•…éšœè½¬ç§»æœºåˆ¶
- [ ] å»ºç«‹æ€§èƒ½åŸºå‡†å’Œå®¹é‡è§„åˆ’
- [ ] å®æ–½å®‰å…¨åˆè§„è‡ªåŠ¨åŒ–æ£€æŸ¥
- [ ] é…ç½®å¤‡ä»½å’Œæ¢å¤è‡ªåŠ¨åŒ–
- [ ] å»ºç«‹æŒç»­æ”¹è¿›åé¦ˆæœºåˆ¶

---

*æœ¬æ–‡æ¡£ä¸ºä¼ä¸šçº§è‡ªåŠ¨åŒ–è¿ç»´å·¥å…·é“¾å»ºè®¾æä¾›å®Œæ•´çš„æŠ€æœ¯æ–¹æ¡ˆå’Œæœ€ä½³å®è·µæŒ‡å¯¼*
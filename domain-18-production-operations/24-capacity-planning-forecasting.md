# 24. å®¹é‡è§„åˆ’ä¸é¢„æµ‹ (Capacity Planning & Forecasting)

> **é€‚ç”¨èŒƒå›´**: Kubernetes v1.25-v1.32 | **æ›´æ–°æ—¶é—´**: 2024å¹´ | **é¢„è®¡é˜…è¯»æ—¶é—´**: 50åˆ†é’Ÿ

## ğŸ“‹ ç« èŠ‚æ¦‚è§ˆ

æœ¬ç« èŠ‚æ·±å…¥æ¢è®¨Kubernetesç¯å¢ƒä¸‹çš„å®¹é‡è§„åˆ’æ–¹æ³•è®ºï¼ŒåŒ…æ‹¬èµ„æºéœ€æ±‚é¢„æµ‹ã€æ‰©å®¹ç­–ç•¥åˆ¶å®šã€å®¹é‡ä¼˜åŒ–å’Œæˆæœ¬æ•ˆç›Šåˆ†æç­‰æ ¸å¿ƒå†…å®¹ã€‚

---

## 1. å®¹é‡è§„åˆ’åŸºç¡€ç†è®º

### 1.1 å®¹é‡è§„åˆ’æ ¸å¿ƒæ¦‚å¿µ

#### å®¹é‡è§„åˆ’å®šä¹‰ä¸ç›®æ ‡
```yaml
å®¹é‡è§„åˆ’æ ¸å¿ƒè¦ç´ :
  èµ„æºç»´åº¦:
    è®¡ç®—èµ„æº: CPUã€å†…å­˜ã€GPU
    å­˜å‚¨èµ„æº: æŒä¹…åŒ–å­˜å‚¨ã€ä¸´æ—¶å­˜å‚¨
    ç½‘ç»œèµ„æº: å¸¦å®½ã€è¿æ¥æ•°
    äººåŠ›èµ„æº: è¿ç»´äººå‘˜ã€å¼€å‘äººå‘˜
    
  æ—¶é—´ç»´åº¦:
    çŸ­æœŸè§„åˆ’: 3-6ä¸ªæœˆ
    ä¸­æœŸè§„åˆ’: 6-12ä¸ªæœˆ
    é•¿æœŸè§„åˆ’: 1-3å¹´
    
  è§„åˆ’ç›®æ ‡:
    - ç¡®ä¿ä¸šåŠ¡è¿ç»­æ€§
    - ä¼˜åŒ–æˆæœ¬æ•ˆç›Šæ¯”
    - æ”¯æŒä¸šåŠ¡å¢é•¿éœ€æ±‚
    - é¢„é˜²æ€§èƒ½ç“¶é¢ˆ
```

#### å®¹é‡è§„åˆ’ç”Ÿå‘½å‘¨æœŸ
```mermaid
graph LR
    A[éœ€æ±‚åˆ†æ] --> B[ç°çŠ¶è¯„ä¼°]
    B --> C[è¶‹åŠ¿é¢„æµ‹]
    C --> D[æ–¹æ¡ˆè®¾è®¡]
    D --> E[å®æ–½éƒ¨ç½²]
    E --> F[æ•ˆæœç›‘æ§]
    F --> G[æŒç»­ä¼˜åŒ–]
    G --> A
```

### 1.2 Kubernetesèµ„æºæ¨¡å‹

#### èµ„æºè¯·æ±‚ä¸é™åˆ¶
```yaml
# èµ„æºé…ç½®ç¤ºä¾‹
apiVersion: v1
kind: Pod
metadata:
  name: capacity-demo
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "100m"      # è¯·æ±‚100æ¯«æ ¸
        memory: "128Mi"  # è¯·æ±‚128MBå†…å­˜
      limits:
        cpu: "200m"      # é™åˆ¶200æ¯«æ ¸
        memory: "256Mi"  # é™åˆ¶256MBå†…å­˜
```

#### èµ„æºè®¡é‡å•ä½è¯´æ˜
```bash
# CPUå•ä½æ¢ç®—
1 Core = 1000 milli cores (m)
1 Core = 1000000 micro cores (u)

# å†…å­˜å•ä½æ¢ç®—
1 Ki = 1024 bytes
1 Mi = 1024 Ki = 1,048,576 bytes
1 Gi = 1024 Mi = 1,073,741,824 bytes
```

---

## 2. ç°çŠ¶è¯„ä¼°ä¸æ•°æ®åˆ†æ

### 2.1 é›†ç¾¤èµ„æºç°çŠ¶åˆ†æ

#### é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µæ”¶é›†
```bash
#!/bin/bash
# cluster-capacity-analyzer.sh

echo "=== Kubernetesé›†ç¾¤å®¹é‡åˆ†ææŠ¥å‘Š ==="
DATE=$(date '+%Y-%m-%d %H:%M:%S')
echo "åˆ†ææ—¶é—´: ${DATE}"

# 1. èŠ‚ç‚¹èµ„æºç»Ÿè®¡
echo -e "\n--- èŠ‚ç‚¹èµ„æºé…ç½® ---"
kubectl get nodes -o jsonpath='{
  "æ€»èŠ‚ç‚¹æ•°": "{range .items[*]}{.metadata.name}{"\n"}{end}",
  "CPUæ€»é‡": "{range .items[*]}{.status.capacity.cpu}{"\n"}{end}",
  "å†…å­˜æ€»é‡": "{range .items[*]}{.status.capacity.memory}{"\n"}{end}"
}' | jq '.'

# 2. Podèµ„æºåˆ†é…æƒ…å†µ
echo -e "\n--- Podèµ„æºåˆ†é… ---"
kubectl get pods --all-namespaces -o jsonpath='{
  "Podæ€»æ•°": "{range .items[*]}{.metadata.name}{"\n"}{end}",
  "CPUè¯·æ±‚æ€»é‡": "{range .items[*].spec.containers[*]}{.resources.requests.cpu}{"\n"}{end}",
  "å†…å­˜è¯·æ±‚æ€»é‡": "{range .items[*].spec.containers[*]}{.resources.requests.memory}{"\n"}{end}"
}' | jq '.'

# 3. èµ„æºä½¿ç”¨ç‡ç»Ÿè®¡
echo -e "\n--- å®é™…èµ„æºä½¿ç”¨ç‡ ---"
kubectl top nodes
kubectl top pods --all-namespaces

# 4. å­˜å‚¨ä½¿ç”¨æƒ…å†µ
echo -e "\n--- å­˜å‚¨èµ„æºä½¿ç”¨ ---"
kubectl get pv -o jsonpath='{
  "PVæ€»æ•°": "{range .items[*]}{.metadata.name}{"\n"}{end}",
  "æ€»å­˜å‚¨å®¹é‡": "{range .items[*]}{.spec.capacity.storage}{"\n"}{end}"
}' | jq '.'
```

#### èµ„æºä½¿ç”¨ç‡å¯è§†åŒ–è„šæœ¬
```python
#!/usr/bin/env python3
# resource-visualizer.py

import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime, timedelta
import subprocess
import json

def collect_resource_metrics():
    """æ”¶é›†èµ„æºä½¿ç”¨æ•°æ®"""
    # æ‰§è¡Œkubectlå‘½ä»¤è·å–æ•°æ®
    result = subprocess.run([
        'kubectl', 'top', 'nodes', '-o', 'json'
    ], capture_output=True, text=True)
    
    data = json.loads(result.stdout)
    metrics = []
    
    for item in data['rows']:
        metrics.append({
            'node': item['metadata']['name'],
            'cpu_usage': float(item['metrics']['cpu']['usage']),
            'memory_usage': float(item['metrics']['memory']['usage'])
        })
    
    return metrics

def plot_resource_utilization(metrics):
    """ç»˜åˆ¶èµ„æºä½¿ç”¨å›¾è¡¨"""
    nodes = [m['node'] for m in metrics]
    cpu_usage = [m['cpu_usage'] for m in metrics]
    memory_usage = [m['memory_usage'] for m in metrics]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # CPUä½¿ç”¨ç‡å›¾è¡¨
    bars1 = ax1.bar(nodes, cpu_usage, color='skyblue')
    ax1.set_title('CPUä½¿ç”¨ç‡ (%)')
    ax1.set_ylabel('ä½¿ç”¨ç‡ (%)')
    ax1.tick_params(axis='x', rotation=45)
    
    # å†…å­˜ä½¿ç”¨ç‡å›¾è¡¨
    bars2 = ax2.bar(nodes, memory_usage, color='lightcoral')
    ax2.set_title('å†…å­˜ä½¿ç”¨ç‡ (%)')
    ax2.set_ylabel('ä½¿ç”¨ç‡ (%)')
    ax2.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        ax1.annotate(f'{height:.1f}%',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')
    
    for bar in bars2:
        height = bar.get_height()
        ax2.annotate(f'{height:.1f}%',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('/tmp/resource_utilization.png', dpi=300, bbox_inches='tight')
    print("èµ„æºä½¿ç”¨å›¾è¡¨å·²ä¿å­˜åˆ°: /tmp/resource_utilization.png")

# æ‰§è¡Œåˆ†æ
if __name__ == "__main__":
    metrics = collect_resource_metrics()
    plot_resource_utilization(metrics)
```

### 2.2 å†å²æ•°æ®åˆ†æ

#### èµ„æºä½¿ç”¨å†å²æ•°æ®æ”¶é›†
```bash
#!/bin/bash
# historical-data-collector.sh

# æ”¶é›†è¿‡å»30å¤©çš„å†å²æ•°æ®
END_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
START_TIME=$(date -u -d "30 days ago" +"%Y-%m-%dT%H:%M:%SZ")

# ä»Prometheusæ”¶é›†æ•°æ®
curl -G "http://prometheus-server:9090/api/v1/query_range" \
  --data-urlencode "query=rate(container_cpu_usage_seconds_total[5m])" \
  --data-urlencode "start=${START_TIME}" \
  --data-urlencode "end=${END_TIME}" \
  --data-urlencode "step=1h" > /tmp/cpu_usage_history.json

curl -G "http://prometheus-server:9090/api/v1/query_range" \
  --data-urlencode "query=container_memory_working_set_bytes" \
  --data-urlencode "start=${START_TIME}" \
  --data-urlencode "end=${END_TIME}" \
  --data-urlencode "step=1h" > /tmp/memory_usage_history.json
```

#### è¶‹åŠ¿åˆ†æè„šæœ¬
```python
#!/usr/bin/env python3
# trend-analyzer.py

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import json

class CapacityTrendAnalyzer:
    def __init__(self, data_file):
        self.data = pd.read_json(data_file)
        self.model = LinearRegression()
    
    def analyze_cpu_trend(self):
        """åˆ†æCPUä½¿ç”¨è¶‹åŠ¿"""
        # æå–CPUä½¿ç”¨æ•°æ®
        timestamps = []
        cpu_values = []
        
        for result in self.data['data']['result']:
            for value in result['values']:
                timestamp = datetime.fromtimestamp(int(value[0]))
                cpu_percent = float(value[1]) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                timestamps.append(timestamp)
                cpu_values.append(cpu_percent)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame({
            'timestamp': timestamps,
            'cpu_usage': cpu_values
        })
        
        # æŒ‰æ—¥æœŸèšåˆ
        daily_avg = df.groupby(df['timestamp'].dt.date)['cpu_usage'].mean()
        
        # çº¿æ€§å›å½’åˆ†æ
        X = np.array(range(len(daily_avg))).reshape(-1, 1)
        y = daily_avg.values
        
        self.model.fit(X, y)
        trend_slope = self.model.coef_[0]
        
        # é¢„æµ‹æœªæ¥30å¤©
        future_days = np.array(range(len(daily_avg), len(daily_avg) + 30)).reshape(-1, 1)
        future_predictions = self.model.predict(future_days)
        
        return {
            'current_avg': daily_avg.iloc[-1],
            'trend_slope': trend_slope,
            'predictions': future_predictions,
            'daily_data': daily_avg
        }
    
    def plot_trend_analysis(self, analysis_result):
        """ç»˜åˆ¶è¶‹åŠ¿åˆ†æå›¾è¡¨"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # å†å²æ•°æ®å›¾è¡¨
        dates = list(analysis_result['daily_data'].index)
        values = analysis_result['daily_data'].values
        
        ax1.plot(dates, values, marker='o', linewidth=2, markersize=4)
        ax1.set_title('CPUä½¿ç”¨ç‡å†å²è¶‹åŠ¿')
        ax1.set_ylabel('CPUä½¿ç”¨ç‡ (%)')
        ax1.grid(True, alpha=0.3)
        ax1.tick_params(axis='x', rotation=45)
        
        # è¶‹åŠ¿é¢„æµ‹å›¾è¡¨
        future_dates = [dates[-1] + timedelta(days=i) for i in range(1, 31)]
        ax2.plot(dates[-30:], values[-30:], 'b-', label='å†å²æ•°æ®', marker='o')
        ax2.plot(future_dates, analysis_result['predictions'], 'r--', 
                label='è¶‹åŠ¿é¢„æµ‹', marker='s')
        ax2.axhline(y=80, color='orange', linestyle=':', 
                   label='é¢„è­¦é˜ˆå€¼ (80%)')
        ax2.set_title('CPUä½¿ç”¨ç‡è¶‹åŠ¿é¢„æµ‹ (æœªæ¥30å¤©)')
        ax2.set_ylabel('CPUä½¿ç”¨ç‡ (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('/tmp/capacity_trend_analysis.png', dpi=300, bbox_inches='tight')

# ä½¿ç”¨ç¤ºä¾‹
analyzer = CapacityTrendAnalyzer('/tmp/cpu_usage_history.json')
result = analyzer.analyze_cpu_trend()
analyzer.plot_trend_analysis(result)

print(f"å½“å‰å¹³å‡CPUä½¿ç”¨ç‡: {result['current_avg']:.2f}%")
print(f"è¶‹åŠ¿æ–œç‡: {result['trend_slope']:.4f}%/å¤©")
print(f"30å¤©åé¢„æµ‹: {result['predictions'][-1]:.2f}%")
```

---

## 3. éœ€æ±‚é¢„æµ‹æ–¹æ³•

### 3.1 ç»Ÿè®¡å­¦é¢„æµ‹æ–¹æ³•

#### æ—¶é—´åºåˆ—åˆ†æ
```python
#!/usr/bin/env python3
# time-series-forecast.py

import pandas as pd
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

class ResourceForecaster:
    def __init__(self, historical_data):
        self.data = pd.read_csv(historical_data, parse_dates=['timestamp'])
        self.data.set_index('timestamp', inplace=True)
    
    def decompose_series(self):
        """åˆ†è§£æ—¶é—´åºåˆ—"""
        decomposition = seasonal_decompose(
            self.data['cpu_usage'], 
            model='additive', 
            period=24  # å‡è®¾24å°æ—¶å‘¨æœŸ
        )
        
        fig, axes = plt.subplots(4, 1, figsize=(12, 10))
        decomposition.observed.plot(ax=axes[0], title='åŸå§‹æ•°æ®')
        decomposition.trend.plot(ax=axes[1], title='è¶‹åŠ¿')
        decomposition.seasonal.plot(ax=axes[2], title='å­£èŠ‚æ€§')
        decomposition.resid.plot(ax=axes[3], title='æ®‹å·®')
        
        plt.tight_layout()
        plt.savefig('/tmp/time_series_decomposition.png')
        
        return decomposition
    
    def arima_forecast(self, periods=30):
        """ARIMAé¢„æµ‹"""
        # æ‹ŸåˆARIMAæ¨¡å‹
        model = ARIMA(self.data['cpu_usage'], order=(1,1,1))
        fitted_model = model.fit()
        
        # é¢„æµ‹æœªæ¥periodsæœŸ
        forecast = fitted_model.forecast(steps=periods)
        confidence_intervals = fitted_model.get_forecast(steps=periods).conf_int()
        
        # åˆ›å»ºé¢„æµ‹æ—¶é—´è½´
        last_date = self.data.index[-1]
        forecast_dates = [last_date + timedelta(hours=i) for i in range(1, periods+1)]
        
        # ç»˜åˆ¶é¢„æµ‹ç»“æœ
        plt.figure(figsize=(12, 6))
        plt.plot(self.data.index[-168:], self.data['cpu_usage'][-168:], 
                label='å†å²æ•°æ®', linewidth=2)
        plt.plot(forecast_dates, forecast, 'r--', label='ARIMAé¢„æµ‹', linewidth=2)
        plt.fill_between(forecast_dates, 
                        confidence_intervals['lower cpu_usage'],
                        confidence_intervals['upper cpu_usage'],
                        alpha=0.3, label='95%ç½®ä¿¡åŒºé—´')
        
        plt.title('CPUä½¿ç”¨ç‡ARIMAé¢„æµ‹')
        plt.xlabel('æ—¶é—´')
        plt.ylabel('CPUä½¿ç”¨ç‡ (%)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig('/tmp/arima_forecast.png')
        
        return {
            'forecast': forecast,
            'confidence_intervals': confidence_intervals,
            'forecast_dates': forecast_dates
        }

# ä½¿ç”¨ç¤ºä¾‹
forecaster = ResourceForecaster('/tmp/historical_cpu_data.csv')
decomposition = forecaster.decompose_series()
forecast_result = forecaster.arima_forecast(periods=168)  # é¢„æµ‹ä¸€å‘¨
```

### 3.2 æœºå™¨å­¦ä¹ é¢„æµ‹æ–¹æ³•

#### åŸºäºç‰¹å¾çš„é¢„æµ‹æ¨¡å‹
```python
#!/usr/bin/env python3
# ml-capacity-predictor.py

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import joblib
from datetime import datetime, timedelta

class MLResourcePredictor:
    def __init__(self):
        self.model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
    
    def prepare_features(self, data):
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        df = data.copy()
        
        # æ—¶é—´ç‰¹å¾
        df['hour'] = df.index.hour
        df['day_of_week'] = df.index.dayofweek
        df['day_of_month'] = df.index.day
        df['month'] = df.index.month
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        
        # æ»åç‰¹å¾
        for lag in [1, 2, 3, 24, 168]:  # 1h, 2h, 3h, 24h, 168h
            df[f'cpu_lag_{lag}'] = df['cpu_usage'].shift(lag)
        
        # æ»šåŠ¨çª—å£ç»Ÿè®¡
        windows = [3, 6, 12, 24]
        for window in windows:
            df[f'cpu_mean_{window}h'] = df['cpu_usage'].rolling(window=window).mean()
            df[f'cpu_std_{window}h'] = df['cpu_usage'].rolling(window=window).std()
            df[f'cpu_max_{window}h'] = df['cpu_usage'].rolling(window=window).max()
        
        # åˆ é™¤å«æœ‰NaNçš„è¡Œ
        df = df.dropna()
        
        return df
    
    def train_model(self, training_data):
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        # å‡†å¤‡ç‰¹å¾
        feature_df = self.prepare_features(training_data)
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
        feature_columns = [col for col in feature_df.columns if col != 'cpu_usage']
        X = feature_df[feature_columns]
        y = feature_df['cpu_usage']
        
        # åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(X_train, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        y_pred = self.model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        
        print(f"æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.2f}%")
        print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.2f}%")
        
        # ä¿å­˜æ¨¡å‹
        joblib.dump(self.model, '/tmp/resource_predictor_model.pkl')
        
        return {
            'mae': mae,
            'rmse': rmse,
            'feature_importance': dict(zip(feature_columns, self.model.feature_importances_))
        }
    
    def predict_future(self, recent_data, hours_ahead=168):
        """é¢„æµ‹æœªæ¥èµ„æºä½¿ç”¨"""
        # è·å–æœ€è¿‘çš„æ•°æ®ç‚¹ç”¨äºé¢„æµ‹
        latest_data = recent_data.tail(168).copy()  # è·å–æœ€è¿‘ä¸€å‘¨æ•°æ®
        prediction_start = latest_data.index[-1] + timedelta(hours=1)
        
        predictions = []
        current_data = latest_data.copy()
        
        for i in range(hours_ahead):
            # å‡†å¤‡å½“å‰æ—¶é—´ç‚¹çš„ç‰¹å¾
            current_time = prediction_start + timedelta(hours=i)
            feature_row = self._create_feature_row(current_data, current_time)
            
            # é¢„æµ‹
            pred = self.model.predict([feature_row])[0]
            predictions.append(pred)
            
            # å°†é¢„æµ‹ç»“æœæ·»åŠ åˆ°æ•°æ®ä¸­ç”¨äºåç»­é¢„æµ‹
            new_row = pd.DataFrame({'cpu_usage': [pred]}, index=[current_time])
            current_data = pd.concat([current_data, new_row])
        
        return predictions
    
    def _create_feature_row(self, data, timestamp):
        """ä¸ºç‰¹å®šæ—¶é—´ç‚¹åˆ›å»ºç‰¹å¾å‘é‡"""
        hour = timestamp.hour
        day_of_week = timestamp.dayofweek
        day_of_month = timestamp.day
        month = timestamp.month
        is_weekend = 1 if day_of_week >= 5 else 0
        
        # è·å–æ»åå€¼
        lag_values = []
        for lag in [1, 2, 3, 24, 168]:
            if len(data) >= lag:
                lag_value = data['cpu_usage'].iloc[-lag]
            else:
                lag_value = data['cpu_usage'].mean()  # å¦‚æœæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨å¹³å‡å€¼
            lag_values.append(lag_value)
        
        # è·å–æ»šåŠ¨ç»Ÿè®¡
        rolling_stats = []
        windows = [3, 6, 12, 24]
        for window in windows:
            if len(data) >= window:
                mean_val = data['cpu_usage'].tail(window).mean()
                std_val = data['cpu_usage'].tail(window).std()
                max_val = data['cpu_usage'].tail(window).max()
            else:
                mean_val = data['cpu_usage'].mean()
                std_val = data['cpu_usage'].std()
                max_val = data['cpu_usage'].max()
            
            rolling_stats.extend([mean_val, std_val, max_val])
        
        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        feature_vector = [hour, day_of_week, day_of_month, month, is_weekend]
        feature_vector.extend(lag_values)
        feature_vector.extend(rolling_stats)
        
        return feature_vector

# ä½¿ç”¨ç¤ºä¾‹
predictor = MLResourcePredictor()
training_data = pd.read_csv('/tmp/training_data.csv', parse_dates=['timestamp'])
training_data.set_index('timestamp', inplace=True)

# è®­ç»ƒæ¨¡å‹
evaluation = predictor.train_model(training_data)

# é¢„æµ‹æœªæ¥ä¸€å‘¨
future_predictions = predictor.predict_future(training_data, hours_ahead=168)
```

---

## 4. å®¹é‡è§„åˆ’ç­–ç•¥

### 4.1 æ‰©å®¹ç­–ç•¥åˆ¶å®š

#### åŸºäºé˜ˆå€¼çš„è‡ªåŠ¨æ‰©å®¹
```yaml
# hpa-capacity-planning.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: capacity-aware-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # 70%ä½¿ç”¨ç‡è§¦å‘æ‰©å®¹
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # 80%ä½¿ç”¨ç‡è§¦å‘æ‰©å®¹
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100  # æ¯æ¬¡æœ€å¤šæ‰©å®¹100%
        periodSeconds: 60
      - type: Pods
        value: 5    # æ¯æ¬¡æœ€å¤šå¢åŠ 5ä¸ªPod
        periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10   # æ¯æ¬¡æœ€å¤šç¼©å®¹10%
        periodSeconds: 120
```

#### é›†ç¾¤è‡ªåŠ¨æ‰©å®¹é…ç½®
```yaml
# cluster-autoscaler-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-config
  namespace: kube-system
data:
  cluster-autoscaler.yaml: |
    ---
    expander: least-waste
    scale-down-enabled: true
    scale-down-delay-after-add: 10m
    scale-down-unneeded-time: 10m
    scale-down-utilization-threshold: 0.5
    max-node-provision-time: 15m
    cores-total: 0:1000
    memory-total: 0:3000Gi
    gpu-total: 0:100
    
    # å®¹é‡è§„åˆ’ç›¸å…³é…ç½®
    max-empty-bulk-delete: 10
    max-graceful-termination-sec: 600
    max-total-unready-percentage: 45
    ok-total-unready-count: 3
```

### 4.2 èµ„æºé¢„ç•™ç­–ç•¥

#### èŠ‚ç‚¹èµ„æºé¢„ç•™é…ç½®
```yaml
# kubelet-config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
kubeReserved:
  cpu: "500m"
  memory: "1Gi"
  ephemeral-storage: "10Gi"
systemReserved:
  cpu: "500m"
  memory: "1Gi"
  ephemeral-storage: "5Gi"
evictionHard:
  memory.available: "500Mi"
  nodefs.available: "10%"
  nodefs.inodesFree: "5%"
  imagefs.available: "15%"
```

#### å‘½åç©ºé—´èµ„æºé…é¢
```yaml
# namespace-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: capacity-planning-quota
  namespace: production
spec:
  hard:
    # è®¡ç®—èµ„æºé…é¢
    requests.cpu: "100"
    requests.memory: "200Gi"
    limits.cpu: "200"
    limits.memory: "400Gi"
    
    # å­˜å‚¨èµ„æºé…é¢
    requests.storage: "10Ti"
    persistentvolumeclaims: "1000"
    
    # å¯¹è±¡æ•°é‡é…é¢
    pods: "10000"
    services: "500"
    secrets: "1000"
    configmaps: "1000"
```

---

## 5. æˆæœ¬æ•ˆç›Šåˆ†æ

### 5.1 æˆæœ¬è®¡ç®—æ¨¡å‹

#### èµ„æºæˆæœ¬è®¡ç®—å™¨
```python
#!/usr/bin/env python3
# cost-calculator.py

class ResourceCostCalculator:
    def __init__(self, pricing_config):
        self.pricing = pricing_config
    
    def calculate_node_cost(self, node_spec):
        """è®¡ç®—èŠ‚ç‚¹æˆæœ¬"""
        hourly_rate = self.pricing['node_types'][node_spec['type']]['hourly_rate']
        monthly_hours = 730  # å¹³å‡æ¯æœˆå°æ—¶æ•°
        
        base_cost = hourly_rate * monthly_hours
        
        # å­˜å‚¨æˆæœ¬
        storage_cost = node_spec['storage_gb'] * self.pricing['storage_per_gb_month']
        
        # ç½‘ç»œæˆæœ¬
        network_cost = node_spec['bandwidth_gb'] * self.pricing['network_per_gb']
        
        total_cost = base_cost + storage_cost + network_cost
        
        return {
            'base_cost': base_cost,
            'storage_cost': storage_cost,
            'network_cost': network_cost,
            'total_cost': total_cost
        }
    
    def calculate_cluster_cost(self, cluster_nodes):
        """è®¡ç®—é›†ç¾¤æ€»æˆæœ¬"""
        total_costs = {
            'compute': 0,
            'storage': 0,
            'network': 0,
            'total': 0
        }
        
        for node in cluster_nodes:
            node_cost = self.calculate_node_cost(node)
            total_costs['compute'] += node_cost['base_cost']
            total_costs['storage'] += node_cost['storage_cost']
            total_costs['network'] += node_cost['network_cost']
            total_costs['total'] += node_cost['total_cost']
        
        return total_costs
    
    def optimize_capacity(self, current_usage, growth_rate=0.2):
        """å®¹é‡ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # CPUåˆ©ç”¨ç‡ä¼˜åŒ–
        if current_usage['cpu_utilization'] < 30:
            recommendations.append({
                'type': 'rightsizing',
                'action': 'é™ä½å®ä¾‹è§„æ ¼',
                'savings': 'é¢„è®¡èŠ‚çœ30-50%æˆæœ¬'
            })
        
        # å†…å­˜åˆ©ç”¨ç‡ä¼˜åŒ–
        if current_usage['memory_utilization'] < 40:
            recommendations.append({
                'type': 'memory_optimization',
                'action': 'è°ƒæ•´å†…å­˜åˆ†é…',
                'savings': 'é¢„è®¡èŠ‚çœ20-40%å†…å­˜æˆæœ¬'
            })
        
        # å­˜å‚¨ä¼˜åŒ–
        if current_usage['storage_utilization'] < 50:
            recommendations.append({
                'type': 'storage_optimization',
                'action': 'æ¸…ç†æ— ç”¨æ•°æ®ï¼Œä½¿ç”¨æ›´ä¾¿å®œçš„å­˜å‚¨ç±»åˆ«',
                'savings': 'é¢„è®¡èŠ‚çœ25-60%å­˜å‚¨æˆæœ¬'
            })
        
        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
pricing_config = {
    'node_types': {
        't3.medium': {'hourly_rate': 0.0416},
        't3.large': {'hourly_rate': 0.0832},
        'm5.xlarge': {'hourly_rate': 0.192}
    },
    'storage_per_gb_month': 0.10,
    'network_per_gb': 0.01
}

calculator = ResourceCostCalculator(pricing_config)

cluster_nodes = [
    {
        'type': 't3.medium',
        'count': 10,
        'storage_gb': 100,
        'bandwidth_gb': 1000
    },
    {
        'type': 'm5.xlarge',
        'count': 3,
        'storage_gb': 500,
        'bandwidth_gb': 5000
    }
]

costs = calculator.calculate_cluster_cost(cluster_nodes)
recommendations = calculator.optimize_capacity({
    'cpu_utilization': 25,
    'memory_utilization': 35,
    'storage_utilization': 45
})

print("é›†ç¾¤æœˆåº¦æˆæœ¬åˆ†æ:")
print(f"è®¡ç®—èµ„æº: ${costs['compute']:.2f}")
print(f"å­˜å‚¨èµ„æº: ${costs['storage']:.2f}")
print(f"ç½‘ç»œèµ„æº: ${costs['network']:.2f}")
print(f"æ€»æˆæœ¬: ${costs['total']:.2f}")

print("\nä¼˜åŒ–å»ºè®®:")
for rec in recommendations:
    print(f"- {rec['action']}: {rec['savings']}")
```

### 5.2 ROIè®¡ç®—ä¸æŠ•èµ„å›æŠ¥åˆ†æ

#### å®¹é‡æŠ•èµ„å›æŠ¥è®¡ç®—å™¨
```python
#!/usr/bin/env python3
# roi-calculator.py

class ROICalculator:
    def __init__(self):
        pass
    
    def calculate_capacity_investment_roi(self, scenario):
        """
        è®¡ç®—å®¹é‡æŠ•èµ„çš„ROI
        scenario: {
            'current_capacity': å½“å‰å®¹é‡é…ç½®,
            'proposed_capacity': å»ºè®®å®¹é‡é…ç½®,
            'growth_projection': ä¸šåŠ¡å¢é•¿é¢„æµ‹,
            'implementation_cost': å®æ–½æˆæœ¬,
            'operational_savings': è¿è¥èŠ‚çœ,
            'timeline_months': æ—¶é—´å‘¨æœŸ
        }
        """
        # è®¡ç®—ç›´æ¥æˆæœ¬å·®å¼‚
        current_monthly_cost = scenario['current_capacity']['monthly_cost']
        proposed_monthly_cost = scenario['proposed_capacity']['monthly_cost']
        
        monthly_savings = current_monthly_cost - proposed_monthly_cost
        
        # è®¡ç®—å®æ–½æˆæœ¬
        implementation_cost = scenario['implementation_cost']
        
        # è®¡ç®—ç´¯è®¡æ”¶ç›Š
        timeline = scenario['timeline_months']
        cumulative_savings = monthly_savings * timeline
        
        # è®¡ç®—ROI
        roi_percentage = (cumulative_savings - implementation_cost) / implementation_cost * 100
        
        # è®¡ç®—å›æ”¶æœŸ
        payback_period = implementation_cost / monthly_savings if monthly_savings > 0 else float('inf')
        
        # è€ƒè™‘ä¸šåŠ¡å¢é•¿çš„å½±å“
        growth_impact = self._calculate_growth_impact(
            scenario['growth_projection'],
            scenario['proposed_capacity']
        )
        
        return {
            'monthly_savings': monthly_savings,
            'cumulative_savings': cumulative_savings,
            'implementation_cost': implementation_cost,
            'roi_percentage': roi_percentage,
            'payback_period_months': payback_period,
            'growth_impact': growth_impact,
            'net_present_value': self._calculate_npv(cumulative_savings, implementation_cost)
        }
    
    def _calculate_growth_impact(self, growth_projection, capacity_config):
        """è®¡ç®—ä¸šåŠ¡å¢é•¿å¯¹å®¹é‡éœ€æ±‚çš„å½±å“"""
        # ç®€åŒ–çš„å¢é•¿æ¨¡å‹
        baseline_capacity = capacity_config['baseline_resources']
        projected_capacity = {}
        
        for resource_type, current_amount in baseline_capacity.items():
            growth_factor = (1 + growth_projection['annual_growth_rate']) ** (growth_projection['years'] / 12)
            projected_capacity[resource_type] = current_amount * growth_factor
        
        return {
            'current_capacity': baseline_capacity,
            'projected_capacity': projected_capacity,
            'additional_capacity_needed': {
                k: projected_capacity[k] - baseline_capacity[k] 
                for k in baseline_capacity.keys()
            }
        }
    
    def _calculate_npv(self, future_cash_flows, initial_investment, discount_rate=0.1):
        """è®¡ç®—å‡€ç°å€¼"""
        npv = -initial_investment
        monthly_discount_rate = (1 + discount_rate) ** (1/12) - 1
        
        for month in range(1, 37):  # 3å¹´é¢„æµ‹
            discounted_cf = future_cash_flows / ((1 + monthly_discount_rate) ** month)
            npv += discounted_cf
            
        return npv

# ä½¿ç”¨ç¤ºä¾‹
roi_calc = ROICalculator()

scenario = {
    'current_capacity': {
        'monthly_cost': 15000,
        'baseline_resources': {
            'cpu_cores': 200,
            'memory_gb': 800,
            'storage_tb': 50
        }
    },
    'proposed_capacity': {
        'monthly_cost': 12000,
        'baseline_resources': {
            'cpu_cores': 180,
            'memory_gb': 750,
            'storage_tb': 45
        }
    },
    'growth_projection': {
        'annual_growth_rate': 0.3,  # 30%å¹´å¢é•¿ç‡
        'years': 2
    },
    'implementation_cost': 50000,  # ä¸€æ¬¡æ€§å®æ–½æˆæœ¬
    'timeline_months': 36
}

results = roi_calc.calculate_capacity_investment_roi(scenario)

print("å®¹é‡æŠ•èµ„ROIåˆ†æ:")
print(f"æœˆåº¦èŠ‚çœ: ${results['monthly_savings']:,.2f}")
print(f"3å¹´ç´¯è®¡èŠ‚çœ: ${results['cumulative_savings']:,.2f}")
print(f"å®æ–½æˆæœ¬: ${results['implementation_cost']:,.2f}")
print(f"æŠ•èµ„å›æŠ¥ç‡: {results['roi_percentage']:.1f}%")
print(f"æŠ•èµ„å›æ”¶æœŸ: {results['payback_period_months']:.1f} ä¸ªæœˆ")
print(f"å‡€ç°å€¼(NPV): ${results['net_present_value']:,.2f}")
```

---

## 6. å®æ–½ä¸ç›‘æ§

### 6.1 å®¹é‡è§„åˆ’å®æ–½æ¡†æ¶

#### å®¹é‡è§„åˆ’å®æ–½è·¯çº¿å›¾
```yaml
# capacity-planning-roadmap.yaml
capacity_planning_phases:
  phase_1_assessment:
    duration: "1-2 months"
    objectives:
      - å®Œæˆç°æœ‰èµ„æºç›˜ç‚¹
      - å»ºç«‹ç›‘æ§ä½“ç³»
      - æ”¶é›†å†å²æ•°æ®
    deliverables:
      - èµ„æºä½¿ç”¨ç°çŠ¶æŠ¥å‘Š
      - ç›‘æ§ä»ªè¡¨æ¿
      - æ•°æ®æ”¶é›†ç®¡é“
    
  phase_2_modeling:
    duration: "2-3 months"
    objectives:
      - å»ºç«‹é¢„æµ‹æ¨¡å‹
      - éªŒè¯æ¨¡å‹å‡†ç¡®æ€§
      - åˆ¶å®šå®¹é‡ç­–ç•¥
    deliverables:
      - é¢„æµ‹æ¨¡å‹
      - å®¹é‡è§„åˆ’ç­–ç•¥æ–‡æ¡£
      - è‡ªåŠ¨åŒ–å·¥å…·
  
  phase_3_implementation:
    duration: "3-6 months"
    objectives:
      - éƒ¨ç½²è‡ªåŠ¨åŒ–å·¥å…·
      - å®æ–½å®¹é‡ç­–ç•¥
      - å»ºç«‹é¢„è­¦æœºåˆ¶
    deliverables:
      - è‡ªåŠ¨åŒ–æ‰©å®¹ç³»ç»Ÿ
      - å®¹é‡é¢„è­¦ç³»ç»Ÿ
      - æ“ä½œæ‰‹å†Œ
  
  phase_4_optimization:
    duration: "æŒç»­è¿›è¡Œ"
    objectives:
      - æŒç»­ç›‘æ§ä¼˜åŒ–
      - å®šæœŸå›é¡¾è°ƒæ•´
      - æˆæœ¬æ•ˆç›Šåˆ†æ
    deliverables:
      - æœˆåº¦ä¼˜åŒ–æŠ¥å‘Š
      - æˆæœ¬åˆ†ææŠ¥å‘Š
      - æ”¹è¿›å»ºè®®
```

### 6.2 ç›‘æ§ä¸å‘Šè­¦ä½“ç³»

#### å®¹é‡ç›¸å…³ç›‘æ§æŒ‡æ ‡
```yaml
# capacity-monitoring-rules.yaml
groups:
- name: capacity.planning
  rules:
  # èµ„æºä½¿ç”¨ç‡å‘Šè­¦
  - alert: HighCPUUtilization
    expr: avg(rate(container_cpu_usage_seconds_total[5m])) by (node) > 0.8
    for: 10m
    labels:
      severity: warning
      category: capacity
    annotations:
      summary: "èŠ‚ç‚¹CPUä½¿ç”¨ç‡è¿‡é«˜"
      description: "èŠ‚ç‚¹ {{ $labels.node }} CPUä½¿ç”¨ç‡è¶…è¿‡80%"

  - alert: HighMemoryUtilization
    expr: avg(container_memory_working_set_bytes/container_memory_limit_bytes) by (node) > 0.85
    for: 10m
    labels:
      severity: warning
      category: capacity
    annotations:
      summary: "èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "èŠ‚ç‚¹ {{ $labels.node }} å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡85%"

  # å®¹é‡é¢„è­¦æŒ‡æ ‡
  - alert: StorageCapacityLow
    expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.2
    for: 5m
    labels:
      severity: warning
      category: capacity
    annotations:
      summary: "å­˜å‚¨å®¹é‡ä¸è¶³"
      description: "å­˜å‚¨å· {{ $labels.persistentvolumeclaim }} å¯ç”¨ç©ºé—´ä½äº20%"

  # é¢„æµ‹æ€§å‘Šè­¦
  - alert: PredictedCapacityExhaustion
    expr: predict_linear(kube_pod_container_resource_requests{resource="cpu"}[1d], 7*24*3600) > kube_node_status_allocatable{resource="cpu"}
    for: 1h
    labels:
      severity: critical
      category: capacity
    annotations:
      summary: "é¢„æµ‹CPUå®¹é‡å³å°†è€—å°½"
      description: "é¢„æµ‹7å¤©åèŠ‚ç‚¹ {{ $labels.node }} CPUèµ„æºå°†ä¸è¶³"
```

#### å®¹é‡è§„åˆ’ä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "title": "å®¹é‡è§„åˆ’ç›‘æ§é¢æ¿",
    "panels": [
      {
        "title": "é›†ç¾¤èµ„æºä½¿ç”¨æ¦‚è§ˆ",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(kube_pod_container_resource_requests{resource=\"cpu\"}) / sum(kube_node_status_allocatable{resource=\"cpu\"}) * 100",
            "legendFormat": "CPUä½¿ç”¨ç‡"
          },
          {
            "expr": "sum(kube_pod_container_resource_requests{resource=\"memory\"}) / sum(kube_node_status_allocatable{resource=\"memory\"}) * 100",
            "legendFormat": "å†…å­˜ä½¿ç”¨ç‡"
          }
        ]
      },
      {
        "title": "å®¹é‡é¢„æµ‹è¶‹åŠ¿",
        "type": "graph",
        "targets": [
          {
            "expr": "predict_linear(kube_pod_container_resource_requests{resource=\"cpu\"}[7d], 30*24*3600)",
            "legendFormat": "CPUé¢„æµ‹ (30å¤©)"
          },
          {
            "expr": "sum(kube_node_status_allocatable{resource=\"cpu\"})",
            "legendFormat": "CPUæ€»å®¹é‡"
          }
        ]
      },
      {
        "title": "æˆæœ¬è¶‹åŠ¿åˆ†æ",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(node_total_hourly_cost) * 730",
            "legendFormat": "æœˆåº¦é¢„ä¼°æˆæœ¬"
          }
        ]
      }
    ]
  }
}
```

---

## 7. æœ€ä½³å®è·µæ€»ç»“

### 7.1 æˆåŠŸå…³é”®å› ç´ 

#### ğŸ¯ æ ¸å¿ƒåŸåˆ™
âœ… **æ•°æ®é©±åŠ¨å†³ç­–**: åŸºäºå®é™…ä½¿ç”¨æ•°æ®è€Œéå‡è®¾è¿›è¡Œè§„åˆ’
âœ… **æŒç»­ç›‘æ§**: å»ºç«‹å®æ—¶ç›‘æ§å’Œé¢„è­¦æœºåˆ¶
âœ… **æ¸è¿›å¼ä¼˜åŒ–**: å°æ­¥å¿«è·‘ï¼ŒæŒç»­æ”¹è¿›
âœ… **ä¸šåŠ¡å¯¹é½**: å®¹é‡è§„åˆ’å¿…é¡»æ”¯æŒä¸šåŠ¡ç›®æ ‡

#### ğŸ› ï¸ æŠ€æœ¯å®è·µ
âœ… **è‡ªåŠ¨åŒ–å·¥å…·**: ä½¿ç”¨æˆç†Ÿçš„å®¹é‡ç®¡ç†å·¥å…·
âœ… **æ ‡å‡†åŒ–æµç¨‹**: å»ºç«‹æ ‡å‡†åŒ–çš„å®¹é‡è§„åˆ’æµç¨‹
âœ… **å¤šç»´åº¦è€ƒè™‘**: ç»¼åˆè€ƒè™‘æ€§èƒ½ã€æˆæœ¬ã€å¯é æ€§
âœ… **é£é™©ç®¡æ§**: é¢„ç•™åˆç†çš„å®‰å…¨è¾¹é™…

### 7.2 å¸¸è§è¯¯åŒºé¿å…

#### âŒ é¿å…çš„é”™è¯¯åšæ³•
- è¿‡åº¦é…ç½®èµ„æºé€ æˆæµªè´¹
- å¿½è§†å†å²æ•°æ®è¶‹åŠ¿
- ç¼ºä¹é¢„è­¦æœºåˆ¶
- ä¸è€ƒè™‘ä¸šåŠ¡å¢é•¿å˜åŒ–

#### âœ… æ¨èçš„æœ€ä½³å®è·µ
- å»ºç«‹å®¹é‡åŸºçº¿å’Œè¶‹åŠ¿åˆ†æ
- å®æ–½é¢„æµ‹æ€§å®¹é‡ç®¡ç†
- å®šæœŸè¿›è¡Œå®¹é‡å®¡æŸ¥
- ç»“åˆFinOpsç†å¿µä¼˜åŒ–æˆæœ¬

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£ä¸æ ‡å‡†
- [Kubernetesèµ„æºç®¡ç†](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)

### å·¥å…·æ¨è
- **ç›‘æ§å·¥å…·**: Prometheus, Grafana, Datadog
- **é¢„æµ‹å·¥å…·**: Kubecost, OpenCost
- **åˆ†æå·¥å…·**: Python (pandas, scikit-learn), R

### å­¦ä¹ èµ„æº
- ã€ŠSite Reliability Engineeringã€‹- Google
- CNCFå®¹é‡ç®¡ç†æœ€ä½³å®è·µ
- Kubernetes SIG Scalability

---
*æœ¬æ–‡æ¡£ç”±Kubernetesç”Ÿäº§è¿ç»´ä¸“å®¶å›¢é˜Ÿç»´æŠ¤*
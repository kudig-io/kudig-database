# 01 - å­˜å‚¨æ¶æ„æ¦‚è§ˆä¸æ ¸å¿ƒç»„ä»¶

> **é€‚ç”¨ç‰ˆæœ¬**: v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **è¿ç»´é‡ç‚¹**: ç”Ÿäº§ç¯å¢ƒæ¶æ„è®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–ã€æ•…éšœé¢„é˜²

## ç›®å½•

1. [å­˜å‚¨æ¶æ„æ¦‚è§ˆ](#å­˜å‚¨æ¶æ„æ¦‚è§ˆ)
2. [PV/PVC/StorageClass](#pvpvcstorageclass)
3. [è®¿é—®æ¨¡å¼ä¸å›æ”¶ç­–ç•¥](#è®¿é—®æ¨¡å¼ä¸å›æ”¶ç­–ç•¥)
4. [åŠ¨æ€å·ä¾›ç»™](#åŠ¨æ€å·ä¾›ç»™)
5. [CSIé©±åŠ¨ç”Ÿæ€](#csié©±åŠ¨ç”Ÿæ€)
6. [å·æ‰©å®¹ä¸å¿«ç…§](#å·æ‰©å®¹ä¸å¿«ç…§)
7. [å­˜å‚¨æ€§èƒ½ä¼˜åŒ–](#å­˜å‚¨æ€§èƒ½ä¼˜åŒ–)
8. [å­˜å‚¨æ•…éšœæ’æŸ¥](#å­˜å‚¨æ•…éšœæ’æŸ¥)
9. [äº‘åŸç”Ÿå­˜å‚¨æ–¹æ¡ˆ](#äº‘åŸç”Ÿå­˜å‚¨æ–¹æ¡ˆ)
10. [æ•°æ®æŒä¹…åŒ–å†³ç­–](#æ•°æ®æŒä¹…åŒ–å†³ç­–)
11. [ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ](#ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ)
12. [æˆæœ¬ä¼˜åŒ–ç­–ç•¥](#æˆæœ¬ä¼˜åŒ–ç­–ç•¥)
13. [ç›‘æ§å‘Šè­¦ä½“ç³»](#ç›‘æ§å‘Šè­¦ä½“ç³»)

---

## ç›®å½•

1. [å­˜å‚¨æ¶æ„æ¦‚è§ˆ](#å­˜å‚¨æ¶æ„æ¦‚è§ˆ)
2. [PV/PVC/StorageClass](#pvpvcstorageclass)
3. [è®¿é—®æ¨¡å¼ä¸å›æ”¶ç­–ç•¥](#è®¿é—®æ¨¡å¼ä¸å›æ”¶ç­–ç•¥)
4. [åŠ¨æ€å·ä¾›ç»™](#åŠ¨æ€å·ä¾›ç»™)
5. [CSIé©±åŠ¨ç”Ÿæ€](#csié©±åŠ¨ç”Ÿæ€)
6. [å·æ‰©å®¹ä¸å¿«ç…§](#å·æ‰©å®¹ä¸å¿«ç…§)
7. [å­˜å‚¨æ€§èƒ½ä¼˜åŒ–](#å­˜å‚¨æ€§èƒ½ä¼˜åŒ–)
8. [å­˜å‚¨æ•…éšœæ’æŸ¥](#å­˜å‚¨æ•…éšœæ’æŸ¥)
9. [äº‘åŸç”Ÿå­˜å‚¨æ–¹æ¡ˆ](#äº‘åŸç”Ÿå­˜å‚¨æ–¹æ¡ˆ)
10. [æ•°æ®æŒä¹…åŒ–å†³ç­–](#æ•°æ®æŒä¹…åŒ–å†³ç­–)

---

## å­˜å‚¨æ¶æ„æ¦‚è§ˆ

### å­˜å‚¨æŠ½è±¡å±‚æ¬¡

```
åº”ç”¨å±‚ (Application)
    â†“
PVC (PersistentVolumeClaim) - å‘½åç©ºé—´çº§å£°æ˜
    â†“
PV (PersistentVolume) - é›†ç¾¤çº§èµ„æº
    â†“
StorageClass - åŠ¨æ€ä¾›ç»™æ¨¡æ¿
    â†“
CSI Driver - å­˜å‚¨æ’ä»¶æ¥å£
    â†“
åº•å±‚å­˜å‚¨ (äº‘ç›˜/NAS/Ceph/Local)
```

### å­˜å‚¨ç³»ç»Ÿåˆ†ç±»

| å­˜å‚¨ç±»å‹ | ç‰¹ç‚¹ | è®¿é—®æ¨¡å¼ | æ€§èƒ½ | é€‚ç”¨åœºæ™¯ | æˆæœ¬ |
|---------|------|---------|------|---------|------|
| **å—å­˜å‚¨ (Block)** | é«˜æ€§èƒ½ï¼Œç‹¬å  | RWO | é«˜IOPS | æ•°æ®åº“ï¼Œé«˜IOåº”ç”¨ | ä¸­ |
| **æ–‡ä»¶å­˜å‚¨ (File)** | å…±äº«è®¿é—® | RWO/ROX/RWX | ä¸­ç­‰ | å…±äº«æ–‡ä»¶ï¼Œæ—¥å¿— | ä¸­-é«˜ |
| **å¯¹è±¡å­˜å‚¨ (Object)** | æµ·é‡å­˜å‚¨ | åº”ç”¨API | ä½å»¶è¿Ÿ | é™æ€èµ„æºï¼Œå¤‡ä»½ | ä½ |
| **æœ¬åœ°å­˜å‚¨ (Local)** | æœ€é«˜æ€§èƒ½ | RWO | æé«˜ | ç¼“å­˜ï¼Œä¸´æ—¶æ•°æ® | ä½ |

---

## PV/PVC/StorageClass

### PersistentVolume (PV) å®Œæ•´é…ç½®

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: data-pv
  labels:
    type: ssd
    zone: cn-hangzhou-h
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem  # æˆ– Block
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain  # Delete/Recycle(å·²å¼ƒç”¨)
  storageClassName: alicloud-disk-essd
  mountOptions:
    - hard
    - nfsvers=4.1
  nodeAffinity:  # æ‹“æ‰‘çº¦æŸ
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - cn-hangzhou-h
  csi:
    driver: diskplugin.csi.alibabacloud.com
    volumeHandle: d-bp1234567890abcdef
    fsType: ext4
    volumeAttributes:
      performanceLevel: "PL1"
      type: "cloud_essd"
```

### PersistentVolumeClaim (PVC) å®Œæ•´é…ç½®

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc
  namespace: production
  annotations:
    volume.beta.kubernetes.io/storage-provisioner: diskplugin.csi.alibabacloud.com
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: alicloud-disk-essd
  resources:
    requests:
      storage: 100Gi
  selector:  # å¯é€‰: é€‰æ‹©ç‰¹å®šPV
    matchLabels:
      type: ssd
  volumeMode: Filesystem  # æˆ– Block
  dataSource:  # å¯é€‰: ä»å¿«ç…§æ¢å¤
    name: data-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
```

### StorageClass ç”Ÿäº§çº§é…ç½®

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: alicloud-disk-essd-pl1
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: diskplugin.csi.alibabacloud.com
parameters:
  type: cloud_essd
  performanceLevel: PL1  # PL0/PL1/PL2/PL3
  encrypted: "true"  # å¯ç”¨åŠ å¯†
  kmsKeyId: "key-id"  # KMSå¯†é’¥
  resourceGroupId: "rg-xxx"  # èµ„æºç»„
reclaimPolicy: Delete  # Retain/Delete
allowVolumeExpansion: true  # å…è®¸æ‰©å®¹
volumeBindingMode: WaitForFirstConsumer  # å»¶è¿Ÿç»‘å®šï¼Œç¡®ä¿æ‹“æ‰‘åŒ¹é…
mountOptions:
  - noatime
  - nodiratime
```

### å¤šStorageClassç­–ç•¥

| StorageClassåç§° | äº‘ç›˜ç±»å‹ | æ€§èƒ½ç­‰çº§ | IOPS | é€‚ç”¨åœºæ™¯ | æœˆæˆæœ¬(100GB) |
|----------------|---------|---------|------|---------|--------------|
| **fast-ssd-pl3** | ESSD | PL3 | 1,000,000 | æ ¸å¿ƒæ•°æ®åº“ | 350å…ƒ |
| **fast-ssd-pl2** | ESSD | PL2 | 100,000 | ä¸€èˆ¬æ•°æ®åº“ | 210å…ƒ |
| **standard-ssd** | ESSD | PL1 | 50,000 | åº”ç”¨å­˜å‚¨ | 150å…ƒ |
| **economy-ssd** | ESSD | PL0 | 10,000 | å¼€å‘æµ‹è¯• | 105å…ƒ |
| **shared-nas** | NAS | é€šç”¨å‹ | - | å…±äº«æ–‡ä»¶ | 120å…ƒ |
| **local-nvme** | æœ¬åœ°ç›˜ | NVMe | æé«˜ | ç¼“å­˜å±‚ | åŒ…å«åœ¨ECS |

---

## è®¿é—®æ¨¡å¼ä¸å›æ”¶ç­–ç•¥

### è®¿é—®æ¨¡å¼ (AccessModes)

| æ¨¡å¼ | ç¼©å†™ | è¯´æ˜ | æ”¯æŒå­˜å‚¨ç±»å‹ | å…¸å‹åœºæ™¯ |
|-----|------|------|------------|---------|
| **ReadWriteOnce** | RWO | å•èŠ‚ç‚¹è¯»å†™ | å—å­˜å‚¨ï¼Œæœ¬åœ°ç›˜ | æ•°æ®åº“ï¼Œåº”ç”¨çŠ¶æ€ |
| **ReadOnlyMany** | ROX | å¤šèŠ‚ç‚¹åªè¯» | æ–‡ä»¶å­˜å‚¨ï¼Œå¯¹è±¡å­˜å‚¨ | é…ç½®æ–‡ä»¶ï¼Œé™æ€èµ„æº |
| **ReadWriteMany** | RWX | å¤šèŠ‚ç‚¹è¯»å†™ | NASï¼Œåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ | å…±äº«æ—¥å¿—ï¼Œåª’ä½“æ–‡ä»¶ |
| **ReadWriteOncePod** | RWOP | å•Podç‹¬å  (v1.27+) | å—å­˜å‚¨ | ä¸¥æ ¼å•å†™åœºæ™¯ |

### è®¿é—®æ¨¡å¼å…¼å®¹æ€§çŸ©é˜µ

| å­˜å‚¨ç±»å‹ | RWO | ROX | RWX | RWOP |
|---------|-----|-----|-----|------|
| **é˜¿é‡Œäº‘äº‘ç›˜ (ESSD)** | âœ“ | âœ— | âœ— | âœ“ (v1.27+) |
| **é˜¿é‡Œäº‘NAS** | âœ“ | âœ“ | âœ“ | âœ“ (v1.27+) |
| **é˜¿é‡Œäº‘OSS (CSI)** | âœ“ | âœ“ | âœ“ | âœ— |
| **Ceph RBD** | âœ“ | âœ— | âœ— | âœ“ (v1.27+) |
| **CephFS** | âœ“ | âœ“ | âœ“ | âœ“ (v1.27+) |
| **Local Path** | âœ“ | âœ— | âœ— | âœ“ (v1.27+) |
| **NFS** | âœ“ | âœ“ | âœ“ | âœ“ (v1.27+) |

### å›æ”¶ç­–ç•¥ (ReclaimPolicy)

| ç­–ç•¥ | è¡Œä¸º | æ•°æ®å®‰å…¨ | é€‚ç”¨åœºæ™¯ |
|-----|------|---------|---------|
| **Retain** | PVCåˆ é™¤åä¿ç•™PVå’Œæ•°æ® | é«˜ | ç”Ÿäº§ç¯å¢ƒï¼Œå…³é”®æ•°æ® |
| **Delete** | PVCåˆ é™¤ååˆ é™¤PVå’Œåº•å±‚å­˜å‚¨ | ä½ | ä¸´æ—¶æ•°æ®ï¼Œå¼€å‘æµ‹è¯• |
| **Recycle** | åˆ é™¤æ•°æ®åé‡ç”¨PV (å·²å¼ƒç”¨) | ä¸æ¨è | ä¸æ¨èä½¿ç”¨ |

#### ç”Ÿäº§ç¯å¢ƒå›æ”¶ç­–ç•¥æœ€ä½³å®è·µ

```yaml
# æ–¹æ¡ˆ1: StorageClassçº§åˆ«è®¾ç½®Retain
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: production-disk
provisioner: diskplugin.csi.alibabacloud.com
reclaimPolicy: Retain  # ç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨Retain
parameters:
  type: cloud_essd
  performanceLevel: PL1

---
# æ–¹æ¡ˆ2: åŠ¨æ€ä¿®æ”¹PVå›æ”¶ç­–ç•¥
# åœ¨PVCåˆ›å»ºåï¼Œå°†è‡ªåŠ¨åˆ›å»ºçš„PVçš„ç­–ç•¥æ”¹ä¸ºRetain
kubectl patch pv <pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
```

---

## åŠ¨æ€å·ä¾›ç»™

### åŠ¨æ€ä¾›ç»™æµç¨‹å›¾

```
1. ç”¨æˆ·åˆ›å»ºPVC
    â†“
2. StorageClasså®šä¹‰ä¾›ç»™å™¨
    â†“
3. CSI Driveråˆ›å»ºåº•å±‚å­˜å‚¨
    â†“
4. è‡ªåŠ¨åˆ›å»ºPVå¹¶ç»‘å®šåˆ°PVC
    â†“
5. PodæŒ‚è½½PVCä½¿ç”¨å­˜å‚¨
```

### VolumeBindingMode å¯¹æ¯”

| æ¨¡å¼ | è¡Œä¸º | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|------|------|---------|
| **Immediate** | PVCåˆ›å»ºåç«‹å³ä¾›ç»™ | å¿«é€Ÿï¼Œç®€å• | å¯èƒ½å¯¼è‡´æ‹“æ‰‘ä¸åŒ¹é… | æ— æ‹“æ‰‘çº¦æŸ |
| **WaitForFirstConsumer** | ç­‰å¾…Podè°ƒåº¦åä¾›ç»™ | ç¡®ä¿æ‹“æ‰‘åŒ¹é… | é¦–æ¬¡å¯åŠ¨è¾ƒæ…¢ | äº‘ç¯å¢ƒï¼Œå¤šå¯ç”¨åŒº |

### å»¶è¿Ÿç»‘å®šé…ç½®ç¤ºä¾‹

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: topology-aware-disk
provisioner: diskplugin.csi.alibabacloud.com
parameters:
  type: cloud_essd
volumeBindingMode: WaitForFirstConsumer  # å»¶è¿Ÿç»‘å®š
allowedTopologies:
- matchLabelExpressions:
  - key: topology.kubernetes.io/zone
    values:
    - cn-hangzhou-h
    - cn-hangzhou-i
```

---

## CSIé©±åŠ¨ç”Ÿæ€

### CSI (Container Storage Interface) æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Kubernetes                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ API Server   â”‚  â”‚ Scheduler   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ CSI API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CSI Controller Plugin           â”‚
â”‚  (Provisioner, Attacher, Resizer)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CSI Node Plugin (DaemonSet)     â”‚
â”‚  (Node Driver Registrar, Mounter)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     åº•å±‚å­˜å‚¨ç³»ç»Ÿ                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸»æµCSIé©±åŠ¨å¯¹æ¯”

| CSIé©±åŠ¨ | å­˜å‚¨ç±»å‹ | è®¿é—®æ¨¡å¼ | å¿«ç…§ | æ‰©å®¹ | å…‹éš† | æˆç†Ÿåº¦ |
|--------|---------|---------|------|------|------|-------|
| **é˜¿é‡Œäº‘äº‘ç›˜ CSI** | å—å­˜å‚¨ | RWO | âœ“ | âœ“ | âœ“ | é«˜ |
| **é˜¿é‡Œäº‘NAS CSI** | æ–‡ä»¶å­˜å‚¨ | RWX | âœ“ | âœ“ | âœ“ | é«˜ |
| **é˜¿é‡Œäº‘OSS CSI** | å¯¹è±¡å­˜å‚¨ | RWX | âœ— | âœ“ | âœ— | ä¸­ |
| **Ceph RBD CSI** | å—å­˜å‚¨ | RWO | âœ“ | âœ“ | âœ“ | é«˜ |
| **CephFS CSI** | æ–‡ä»¶å­˜å‚¨ | RWX | âœ“ | âœ“ | âœ“ | é«˜ |
| **NFS CSI** | æ–‡ä»¶å­˜å‚¨ | RWX | âœ— | âœ— | âœ— | ä¸­ |
| **Local Path** | æœ¬åœ°å­˜å‚¨ | RWO | âœ— | âœ— | âœ— | ä¸­ |
| **Longhorn** | åˆ†å¸ƒå¼å—å­˜å‚¨ | RWO/RWX | âœ“ | âœ“ | âœ“ | é«˜ |

### é˜¿é‡Œäº‘å­˜å‚¨CSIé…ç½®

```yaml
# äº‘ç›˜CSIé©±åŠ¨é…ç½®
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: diskplugin.csi.alibabacloud.com
spec:
  attachRequired: true
  podInfoOnMount: false
  volumeLifecycleModes:
  - Persistent
  - Ephemeral

---
# NAS CSIé©±åŠ¨é…ç½®
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: nasplugin.csi.alibabacloud.com
spec:
  attachRequired: false
  podInfoOnMount: false
  volumeLifecycleModes:
  - Persistent

---
# OSS CSIé©±åŠ¨é…ç½®
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: ossplugin.csi.alibabacloud.com
spec:
  attachRequired: false
  podInfoOnMount: false
  volumeLifecycleModes:
  - Persistent
```

---

## å·æ‰©å®¹ä¸å¿«ç…§

### åœ¨çº¿æ‰©å®¹ (Volume Expansion)

#### æ‰©å®¹å‰ææ¡ä»¶

1. StorageClass è®¾ç½® `allowVolumeExpansion: true`
2. CSIé©±åŠ¨æ”¯æŒæ‰©å®¹
3. åº•å±‚å­˜å‚¨æ”¯æŒåœ¨çº¿æ‰©å®¹

#### æ‰©å®¹æ“ä½œæµç¨‹

```bash
# 1. ä¿®æ”¹PVCå¤§å°
kubectl edit pvc data-pvc
# ä¿®æ”¹ spec.resources.requests.storage: 200Gi

# 2. è§‚å¯Ÿæ‰©å®¹çŠ¶æ€
kubectl get pvc data-pvc -w
# çŠ¶æ€: Resizing -> FileSystemResizePending -> Bound

# 3. å¯¹äºæŸäº›æ–‡ä»¶ç³»ç»Ÿï¼Œéœ€è¦é‡å¯Podå®Œæˆæ–‡ä»¶ç³»ç»Ÿæ‰©å®¹
kubectl rollout restart deployment/myapp

# 4. éªŒè¯æ‰©å®¹
kubectl exec -it myapp-pod -- df -h
```

#### æ‰©å®¹æ³¨æ„äº‹é¡¹

| æ³¨æ„äº‹é¡¹ | è¯´æ˜ |
|---------|------|
| **ä¸æ”¯æŒç¼©å®¹** | Kubernetesä¸æ”¯æŒPVCç¼©å®¹ï¼Œåªèƒ½æ‰©å®¹ |
| **é˜¿é‡Œäº‘é™åˆ¶** | äº‘ç›˜æ‰©å®¹åä¸èƒ½å°äºå½“å‰å¤§å°ï¼Œæ¯æ¬¡æ‰©å®¹æœ€å°‘10GB |
| **æ–‡ä»¶ç³»ç»Ÿæ‰©å®¹** | ext4/xfsç­‰éœ€è¦Podé‡å¯ï¼ŒæŸäº›äº‘ç›˜æ”¯æŒåœ¨çº¿æ‰©å®¹ |
| **åœæœºæ—¶é—´** | åœ¨çº¿æ‰©å®¹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè§„åˆ’ç»´æŠ¤çª—å£ |

### å·å¿«ç…§ (Volume Snapshot)

#### VolumeSnapshotClass é…ç½®

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: alicloud-disk-snapshot
driver: diskplugin.csi.alibabacloud.com
deletionPolicy: Retain  # Delete/Retain
parameters:
  forceDelete: "false"
  instantAccess: "true"  # å³æ—¶è®¿é—®
  instantAccessRetentionDays: "1"
```

#### åˆ›å»ºå¿«ç…§

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: data-snapshot-20260118
  namespace: production
spec:
  volumeSnapshotClassName: alicloud-disk-snapshot
  source:
    persistentVolumeClaimName: data-pvc
```

#### ä»å¿«ç…§æ¢å¤

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc-restored
spec:
  storageClassName: alicloud-disk-essd
  dataSource:
    name: data-snapshot-20260118
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
```

#### å¿«ç…§æœ€ä½³å®è·µ

```yaml
# å®šæ—¶å¿«ç…§CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: volume-snapshot-cronjob
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: snapshot-controller
          containers:
          - name: snapshot
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              DATE=$(date +%Y%m%d-%H%M%S)
              cat <<EOF | kubectl apply -f -
              apiVersion: snapshot.storage.k8s.io/v1
              kind: VolumeSnapshot
              metadata:
                name: data-snapshot-$DATE
                namespace: production
              spec:
                volumeSnapshotClassName: alicloud-disk-snapshot
                source:
                  persistentVolumeClaimName: data-pvc
              EOF
              # æ¸…ç†7å¤©å‰çš„å¿«ç…§
              kubectl get volumesnapshot -n production -o json | \
                jq -r ".items[] | select(.metadata.creationTimestamp < \"$(date -d '7 days ago' --iso-8601)\") | .metadata.name" | \
                xargs -I {} kubectl delete volumesnapshot {} -n production
          restartPolicy: OnFailure
```

---

## å­˜å‚¨æ€§èƒ½ä¼˜åŒ–

### å—å­˜å‚¨æ€§èƒ½è°ƒä¼˜

#### IOPSä¸ååé‡å…³ç³»

| äº‘ç›˜ç±»å‹ | å®¹é‡ | åŸºå‡†IOPS | æœ€å¤§IOPS | ååé‡(MB/s) |
|---------|------|---------|---------|-------------|
| **ESSD PL0** | 40-32768GB | 10,000 | 10,000 | 180 |
| **ESSD PL1** | 20-32768GB | 1,800+50/GB | 50,000 | 350 |
| **ESSD PL2** | 461-32768GB | 4,000+50/GB | 100,000 | 750 |
| **ESSD PL3** | 1261-32768GB | 10,000+50/GB | 1,000,000 | 4,000 |

#### æ€§èƒ½ä¼˜åŒ–é…ç½®

```yaml
# é«˜æ€§èƒ½å­˜å‚¨é…ç½®
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: high-performance-disk
provisioner: diskplugin.csi.alibabacloud.com
parameters:
  type: cloud_essd
  performanceLevel: PL2
  provisionedIops: "100000"  # é¢„é…ç½®IOPS
  burstingEnabled: "true"  # å¯ç”¨çªå‘
mountOptions:
  - noatime  # ä¸æ›´æ–°è®¿é—®æ—¶é—´ï¼Œå‡å°‘IO
  - nodiratime  # ç›®å½•ä¸æ›´æ–°è®¿é—®æ—¶é—´
  - discard  # æ”¯æŒTRIM
  - barrier=0  # ç¦ç”¨å†™å±éšœ(æå‡æ€§èƒ½ï¼Œé™ä½å¯é æ€§)
reclaimPolicy: Retain
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
```

### æ–‡ä»¶ç³»ç»Ÿé€‰æ‹©

| æ–‡ä»¶ç³»ç»Ÿ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|---------|
| **ext4** | ç¨³å®šï¼Œå…¼å®¹æ€§å¥½ | æ€§èƒ½ä¸­ç­‰ | é€šç”¨åœºæ™¯ |
| **xfs** | å¤§æ–‡ä»¶æ€§èƒ½å¥½ | å°æ–‡ä»¶æ€§èƒ½å·® | å¤§æ–‡ä»¶ï¼Œæ—¥å¿— |
| **btrfs** | å¿«ç…§ï¼Œå‹ç¼© | ç›¸å¯¹ä¸æˆç†Ÿ | é«˜çº§ç‰¹æ€§éœ€æ±‚ |

### NASæ€§èƒ½ä¼˜åŒ–

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nas-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany
  mountOptions:
    - vers=4.1  # ä½¿ç”¨NFSv4.1
    - noresvport  # ä¸ä½¿ç”¨ä¿ç•™ç«¯å£
    - rsize=1048576  # è¯»å–å¤§å°1MB
    - wsize=1048576  # å†™å…¥å¤§å°1MB
    - hard  # ç¡¬æŒ‚è½½
    - timeo=600  # è¶…æ—¶æ—¶é—´
    - retrans=2  # é‡è¯•æ¬¡æ•°
    - nolock  # ç¦ç”¨é”(æå‡æ€§èƒ½)
  csi:
    driver: nasplugin.csi.alibabacloud.com
    volumeHandle: "nas-id:/path"
```

### å­˜å‚¨IOéš”ç¦»

```yaml
# ä½¿ç”¨ä¸åŒçš„StorageClasså®ç°IOéš”ç¦»
---
# æ ¸å¿ƒä¸šåŠ¡ä½¿ç”¨é«˜æ€§èƒ½äº‘ç›˜
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: core-db-pvc
spec:
  storageClassName: fast-ssd-pl3
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 500Gi

---
# æ—¥å¿—ä½¿ç”¨ç»æµå‹äº‘ç›˜
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: logs-pvc
spec:
  storageClassName: economy-ssd
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 200Gi
```

---

## å­˜å‚¨æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜è¯Šæ–­æµç¨‹å›¾

```
Podæ— æ³•å¯åŠ¨
    â†“
æ£€æŸ¥Pod Events
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FailedMount â”‚ FailedAttach   â”‚ FailedScheduling â”‚
â”‚             â”‚                â”‚                  â”‚
â”‚ æ£€æŸ¥PVCçŠ¶æ€ â”‚ æ£€æŸ¥Nodeä¸ŠæŒ‚è½½ â”‚ æ£€æŸ¥æ‹“æ‰‘çº¦æŸ     â”‚
â”‚ æ£€æŸ¥æƒé™    â”‚ æ£€æŸ¥è®¾å¤‡æ•°é‡   â”‚ æ£€æŸ¥èµ„æºé…é¢     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å­˜å‚¨é—®é¢˜æ’æŸ¥å‘½ä»¤

```bash
# 1. æ£€æŸ¥PVCçŠ¶æ€
kubectl get pvc -A
kubectl describe pvc <pvc-name>

# 2. æ£€æŸ¥PVçŠ¶æ€
kubectl get pv
kubectl describe pv <pv-name>

# 3. æ£€æŸ¥StorageClass
kubectl get sc
kubectl describe sc <sc-name>

# 4. æ£€æŸ¥CSIé©±åŠ¨çŠ¶æ€
kubectl get csidrivers
kubectl get csinodes
kubectl describe csinode <node-name>

# 5. æ£€æŸ¥CSI Controllerå’ŒNode Plugin
kubectl get pods -n kube-system | grep csi
kubectl logs -n kube-system csi-diskplugin-xxxx -c disk-plugin
kubectl logs -n kube-system csi-diskplugin-xxxx -c disk-provisioner

# 6. æ£€æŸ¥VolumeAttachment
kubectl get volumeattachment
kubectl describe volumeattachment <va-name>

# 7. æŸ¥çœ‹èŠ‚ç‚¹ä¸ŠæŒ‚è½½çš„è®¾å¤‡
kubectl debug node/<node-name> -it --image=busybox
chroot /host
lsblk
df -h
mount | grep /var/lib/kubelet
```

### å¸¸è§é”™è¯¯ä¸è§£å†³æ–¹æ¡ˆ

| é”™è¯¯ä¿¡æ¯ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|
| **waiting for a volume to be created** | PVCç­‰å¾…PVç»‘å®š | æ£€æŸ¥StorageClasså’Œprovisioner |
| **FailedAttachVolume** | å·æ— æ³•æŒ‚è½½åˆ°èŠ‚ç‚¹ | æ£€æŸ¥CSIé©±åŠ¨ï¼ŒèŠ‚ç‚¹å¯ç”¨åŒºï¼Œäº‘ç›˜é…é¢ |
| **FailedMount** | å·æ— æ³•æŒ‚è½½åˆ°å®¹å™¨ | æ£€æŸ¥æƒé™ï¼Œæ–‡ä»¶ç³»ç»Ÿç±»å‹ï¼ŒmountOptions |
| **Multi-Attach error** | äº‘ç›˜è¢«å¤šä¸ªèŠ‚ç‚¹æŒ‚è½½ | RWOå·åªèƒ½è¢«ä¸€ä¸ªèŠ‚ç‚¹ä½¿ç”¨ï¼Œç­‰å¾…æ—§Podç»ˆæ­¢ |
| **Volume is already attached** | äº‘ç›˜æœªæ­£ç¡®å¸è½½ | æ‰‹åŠ¨detachäº‘ç›˜ï¼Œæˆ–å¼ºåˆ¶åˆ é™¤Nodeå¯¹è±¡ |
| **Timeout expired waiting for volumes** | è°ƒåº¦è¶…æ—¶ | æ£€æŸ¥æ‹“æ‰‘çº¦æŸï¼Œå¢åŠ èŠ‚ç‚¹æˆ–æ”¾å®½çº¦æŸ |

### å¼ºåˆ¶æ¸…ç†æŒ‚è½½å·

```bash
# 1. åˆ é™¤Pod
kubectl delete pod <pod-name> --grace-period=0 --force

# 2. åˆ é™¤VolumeAttachment
kubectl delete volumeattachment <va-name>

# 3. åœ¨èŠ‚ç‚¹ä¸Šæ‰‹åŠ¨umount
kubectl debug node/<node-name> -it --image=busybox
chroot /host
umount /var/lib/kubelet/pods/<pod-uid>/volumes/kubernetes.io~csi/<pvc-name>/mount

# 4. é˜¿é‡Œäº‘æ§åˆ¶å°æ‰‹åŠ¨å¸è½½äº‘ç›˜

# 5. é‡æ–°åˆ›å»ºPod
kubectl apply -f pod.yaml
```

---

## äº‘åŸç”Ÿå­˜å‚¨æ–¹æ¡ˆ

### æ–¹æ¡ˆä¸€: æœ¬åœ°ä¸´æ—¶å· (é€‚åˆæ— çŠ¶æ€åº”ç”¨)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: cache-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: cache
      mountPath: /cache
  volumes:
  - name: cache
    emptyDir:
      sizeLimit: 10Gi  # é™åˆ¶å¤§å°
      medium: Memory  # ä½¿ç”¨å†…å­˜(tmpfs)ï¼Œæé«˜æ€§èƒ½
```

### æ–¹æ¡ˆäºŒ: æœ¬åœ°æŒä¹…å· (é€‚åˆé«˜æ€§èƒ½éœ€æ±‚)

```yaml
# æœ¬åœ°ç£ç›˜StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

---
# æœ¬åœ°PV (éœ€è¦æ‰‹åŠ¨åˆ›å»º)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node1
spec:
  capacity:
    storage: 500Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
```

### æ–¹æ¡ˆä¸‰: äº‘ç›˜åŠ¨æ€ä¾›ç»™ (æ¨èç”Ÿäº§æ–¹æ¡ˆ)

```yaml
# è§å‰æ–‡StorageClassé…ç½®
# ä¼˜ç‚¹: è‡ªåŠ¨åŒ–ï¼Œçµæ´»ï¼Œå¯é 
# ç¼ºç‚¹: æˆæœ¬ç›¸å¯¹é«˜ï¼Œæ€§èƒ½å—äº‘ç›˜é™åˆ¶
```

### æ–¹æ¡ˆå››: Longhorn åˆ†å¸ƒå¼å­˜å‚¨

```bash
# å®‰è£…Longhorn
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml

# ç­‰å¾…ç»„ä»¶å°±ç»ª
kubectl get pods -n longhorn-system -w

# è®¿é—®UI
kubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80
```

```yaml
# Longhorn StorageClass
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"  # å‰¯æœ¬æ•°
  staleReplicaTimeout: "2880"
  dataLocality: "disabled"  # best-effort/disabled
  fromBackup: ""
```

---

## æ•°æ®æŒä¹…åŒ–å†³ç­–

### å†³ç­–æ ‘

```
æ•°æ®æ˜¯å¦éœ€è¦æŒä¹…åŒ–?
    â”œâ”€ å¦ â†’ emptyDir (ä¸´æ—¶æ•°æ®)
    â””â”€ æ˜¯
        â”œâ”€ æ˜¯å¦éœ€è¦å¤šèŠ‚ç‚¹å…±äº«?
        â”‚   â”œâ”€ æ˜¯ â†’ NAS / CephFS (RWX)
        â”‚   â””â”€ å¦
        â”‚       â”œâ”€ æ˜¯å¦éœ€è¦æé«˜æ€§èƒ½?
        â”‚       â”‚   â”œâ”€ æ˜¯ â†’ æœ¬åœ°SSD (Local PV)
        â”‚       â”‚   â””â”€ å¦
        â”‚       â”‚       â”œâ”€ äº‘åŸç”Ÿç¯å¢ƒ?
        â”‚       â”‚       â”‚   â”œâ”€ æ˜¯ â†’ äº‘ç›˜CSI (ESSD)
        â”‚       â”‚       â”‚   â””â”€ å¦ â†’ Ceph RBD / Longhorn
        â”‚       â”‚       â””â”€ æ•°æ®åº“åœºæ™¯?
        â”‚       â”‚           â”œâ”€ æ˜¯ â†’ ESSD PL2/PL3
        â”‚       â”‚           â””â”€ å¦ â†’ ESSD PL1
        â””â”€ æ˜¯å¦éœ€è¦å¯¹è±¡å­˜å‚¨?
            â””â”€ æ˜¯ â†’ OSS / S3 (åº”ç”¨ç›´æ¥å¯¹æ¥)
```

### å­˜å‚¨é€‰å‹å¯¹æ¯”è¡¨

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | è®¿é—®æ¨¡å¼ | æ€§èƒ½ | å¯é æ€§ | æˆæœ¬ |
|-----|---------|---------|------|-------|------|
| **MySQL/PostgreSQL** | ESSD PL2/PL3 | RWO | é«˜ | é«˜ | ä¸­-é«˜ |
| **Redisç¼“å­˜** | æœ¬åœ°NVMe + ä¸»ä» | RWO | æé«˜ | ä¸­ | ä½ |
| **MongoDB** | ESSD PL1 + å‰¯æœ¬é›† | RWO | ä¸­-é«˜ | é«˜ | ä¸­ |
| **Elasticsearch** | ESSD PL1 | RWO | ä¸­-é«˜ | é«˜ | ä¸­ |
| **Kafka** | ESSD PL1 / æœ¬åœ°ç›˜ | RWO | é«˜ | é«˜ | ä½-ä¸­ |
| **å…±äº«æ–‡ä»¶** | NASé€šç”¨å‹ | RWX | ä¸­ | é«˜ | ä¸­ |
| **æ—¥å¿—å­˜å‚¨** | NAS/æœ¬åœ°ç›˜ | RWO/RWX | ä¸­ | ä¸­ | ä½ |
| **é•œåƒä»“åº“** | OSS + ç¼“å­˜å±‚ | - | ä¸­ | é«˜ | ä½ |
| **å¤‡ä»½** | OSS | - | ä½ | é«˜ | ä½ |
| **AIè®­ç»ƒæ•°æ®** | CPFS / NASæé€Ÿ | RWX | æé«˜ | é«˜ | é«˜ |

### æˆæœ¬ä¼˜åŒ–å»ºè®®

| ä¼˜åŒ–é¡¹ | æ–¹æ³• | èŠ‚çœæ¯”ä¾‹ |
|-------|------|---------|
| **ä½¿ç”¨ESSD PL0** | éå…³é”®åº”ç”¨é™çº§ | 30% |
| **å¿«ç…§ä»£æ›¿å…¨é‡å¤‡ä»½** | ä½¿ç”¨CSIå¿«ç…§ | 70% |
| **å†·æ•°æ®å½’æ¡£OSS** | ç”Ÿå‘½å‘¨æœŸç®¡ç† | 80% |
| **æŒ‰éœ€æ‰©å®¹** | ç›‘æ§ä½¿ç”¨ç‡ï¼ŒæŒ‰éœ€æ‰©å®¹ | 20-40% |
| **æœ¬åœ°ç›˜+è¿œç¨‹å¤‡ä»½** | é«˜æ€§èƒ½+ä½æˆæœ¬å¤‡ä»½ | 50% |

### æ¶æ„å¸ˆè§†è§’: å­˜å‚¨åˆ†å±‚ç­–ç•¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  çƒ­æ•°æ®å±‚ (Local NVMe)              â”‚  æé«˜æ€§èƒ½ï¼Œç¼“å­˜
â”‚  IOPS: 100K+  å»¶è¿Ÿ: <0.1ms          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ é™å†·
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¸©æ•°æ®å±‚ (ESSD PL1/PL2)            â”‚  é«˜æ€§èƒ½ï¼Œä¸»å­˜å‚¨
â”‚  IOPS: 10K-100K  å»¶è¿Ÿ: <1ms         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ å½’æ¡£
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å†·æ•°æ®å±‚ (ESSD PL0)                â”‚  ç»æµå‹ï¼Œä½é¢‘è®¿é—®
â”‚  IOPS: 10K  å»¶è¿Ÿ: <5ms              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“ å¤‡ä»½
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å½’æ¡£å±‚ (OSS Archive)               â”‚  æä½æˆæœ¬ï¼Œé•¿æœŸä¿å­˜
â”‚  è®¿é—®å»¶è¿Ÿ: åˆ†é’Ÿçº§                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### äº§å“ç»ç†è§†è§’: å­˜å‚¨éœ€æ±‚æ¨¡æ¿

#### éœ€æ±‚æ”¶é›†æ¸…å•

```markdown
1. æ•°æ®ç±»å‹: â–¡ å…³ç³»å‹æ•°æ®åº“ â–¡ NoSQL â–¡ æ–‡ä»¶ â–¡ å¯¹è±¡ â–¡ ç¼“å­˜
2. æ•°æ®å¤§å°: ____ TB (é¢„è®¡å¢é•¿: ____% /å¹´)
3. è®¿é—®æ¨¡å¼: â–¡ éšæœºè¯»å†™ â–¡ é¡ºåºè¯»å†™ â–¡ è¯»å¤šå†™å°‘ â–¡ å†™å¤šè¯»å°‘
4. æ€§èƒ½è¦æ±‚:
   - IOPS: ____ (å³°å€¼: ____)
   - ååé‡: ____ MB/s
   - å»¶è¿Ÿ: < ____ ms
5. å¯é æ€§:
   - RPO: ____ (æœ€å¤šä¸¢å¤±å¤šå°‘æ•°æ®)
   - RTO: ____ (å¤šä¹…æ¢å¤)
   - å‰¯æœ¬æ•°: ____
6. å…±äº«éœ€æ±‚: â–¡ å•Podç‹¬å  â–¡ å¤šPodå…±äº«åªè¯» â–¡ å¤šPodå…±äº«è¯»å†™
7. æ•°æ®ç”Ÿå‘½å‘¨æœŸ:
   - çƒ­æ•°æ®ä¿ç•™: ____ å¤©
   - å†·æ•°æ®å½’æ¡£: ____ å¤©
   - å¤‡ä»½ä¿ç•™: ____ å¤©
8. åˆè§„è¦æ±‚: â–¡ åŠ å¯† â–¡ å®¡è®¡ â–¡ åœ°åŸŸé™åˆ¶
9. é¢„ç®—: ____ å…ƒ/æœˆ
```

### è¿ç»´è§†è§’: å­˜å‚¨ç›‘æ§æŒ‡æ ‡

```yaml
# Prometheusç›‘æ§è§„åˆ™ç¤ºä¾‹
groups:
- name: storage_alerts
  rules:
  # PVCä½¿ç”¨ç‡å‘Šè­¦
  - alert: PVCUsageHigh
    expr: |
      (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PVC {{ $labels.persistentvolumeclaim }} usage > 85%"
      
  # PVCå³å°†æ»¡
  - alert: PVCAlmostFull
    expr: |
      (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.95
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "PVC {{ $labels.persistentvolumeclaim }} usage > 95%"
      
  # PVä¸å¯ç”¨
  - alert: PersistentVolumeUnavailable
    expr: |
      kube_persistentvolume_status_phase{phase!="Bound"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PV {{ $labels.persistentvolume }} is not Bound"
      
  # CSIé©±åŠ¨å¼‚å¸¸
  - alert: CSIDriverDown
    expr: |
      up{job="csi-driver"} == 0
    for: 3m
    labels:
      severity: critical
    annotations:
      summary: "CSI Driver {{ $labels.instance }} is down"
```

---

## ç”Ÿäº§çº§å­˜å‚¨é…ç½®ç¤ºä¾‹

### MySQL StatefulSet + ESSD

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
        resources:
          requests:
            cpu: 2
            memory: 4Gi
          limits:
            cpu: 4
            memory: 8Gi
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd-pl2
      resources:
        requests:
          storage: 500Gi
```

### å…±äº«æ–‡ä»¶å­˜å‚¨ + NAS

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: shared-files-pv
spec:
  capacity:
    storage: 1Ti
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: alicloud-nas
  mountOptions:
    - vers=4.1
    - noresvport
    - rsize=1048576
    - wsize=1048576
  csi:
    driver: nasplugin.csi.alibabacloud.com
    volumeHandle: "nas-xxx.cn-hangzhou.nas.aliyuncs.com:/share"

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-files-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: alicloud-nas
  resources:
    requests:
      storage: 1Ti

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: file-processor
spec:
  replicas: 5
  selector:
    matchLabels:
      app: file-processor
  template:
    metadata:
      labels:
        app: file-processor
    spec:
      containers:
      - name: processor
        image: processor:latest
        volumeMounts:
        - name: shared-files
          mountPath: /data
      volumes:
      - name: shared-files
        persistentVolumeClaim:
          claimName: shared-files-pvc
```

---
## ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### å­˜å‚¨æ¶æ„è®¾è®¡åŸåˆ™

#### 1. åˆ†å±‚å­˜å‚¨ç­–ç•¥

```yaml
# ä¼ä¸šçº§å­˜å‚¨åˆ†å±‚æ¶æ„
storage_layers:
  hot_layer:
    purpose: "çƒ­æ•°æ®å±‚ - é«˜é¢‘è®¿é—®ï¼Œæè‡´æ€§èƒ½"
    storage_type: "æœ¬åœ°NVMe SSD + Redisç¼“å­˜"
    performance: "IOPS > 100K, å»¶è¿Ÿ < 0.1ms"
    cost: "é«˜"
    usage: "ç¼“å­˜å±‚ï¼Œä¸´æ—¶è®¡ç®—ç»“æœ"
    
  warm_layer:
    purpose: "æ¸©æ•°æ®å±‚ - ä¸­é¢‘è®¿é—®ï¼Œå¹³è¡¡æ€§èƒ½ä¸æˆæœ¬"
    storage_type: "ESSD PL2/PL3äº‘ç›˜"
    performance: "IOPS 50K-100K, å»¶è¿Ÿ < 1ms"
    cost: "ä¸­é«˜"
    usage: "ä¸»æ•°æ®åº“ï¼Œæ ¸å¿ƒåº”ç”¨æ•°æ®"
    
  cold_layer:
    purpose: "å†·æ•°æ®å±‚ - ä½é¢‘è®¿é—®ï¼Œç»æµå®ç”¨"
    storage_type: "ESSD PL0 + NAS"
    performance: "IOPS 10K, å»¶è¿Ÿ < 5ms"
    cost: "ä½ä¸­"
    usage: "å†å²æ•°æ®ï¼Œæ—¥å¿—å½’æ¡£"
    
  archive_layer:
    purpose: "å½’æ¡£å±‚ - æä½é¢‘è®¿é—®ï¼Œæœ€ä½æˆæœ¬"
    storage_type: "OSS Archive + Glacier"
    performance: "è®¿é—®å»¶è¿Ÿåˆ†é’Ÿçº§"
    cost: "æä½"
    usage: "å¤‡ä»½æ•°æ®ï¼Œåˆè§„å½’æ¡£"
```

#### 2. å¤šå¯ç”¨åŒºéƒ¨ç½²æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¤šå¯ç”¨åŒºå­˜å‚¨æ¶æ„                         â”‚
â”‚                                                             â”‚
â”‚  å¯ç”¨åŒºA          å¯ç”¨åŒºB          å¯ç”¨åŒºC                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Master  â”‚     â”‚ Slave   â”‚     â”‚ Slave   â”‚              â”‚
â”‚  â”‚ DB Pod  â”‚â—„â”€â”€â”€â–ºâ”‚ DB Pod  â”‚â—„â”€â”€â”€â–ºâ”‚ DB Pod  â”‚              â”‚
â”‚  â”‚ ESSD    â”‚     â”‚ ESSD    â”‚     â”‚ ESSD    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚               â”‚               â”‚                    â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                       â”‚                                    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚              â”‚ Load Balancer   â”‚                           â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. å­˜å‚¨èµ„æºé…ç½®æ ‡å‡†åŒ–

```yaml
# ç”Ÿäº§ç¯å¢ƒå­˜å‚¨é…ç½®æ¨¡æ¿åº“
production_templates:
  database_storage:
    name: "db-production-template"
    storage_class: "fast-ssd-pl3"
    access_mode: "ReadWriteOnce"
    size_range: "500Gi-2Ti"
    mount_options:
      - noatime
      - nodiratime
      - discard
    backup_policy: "hourly-snapshot"
    monitoring:
      usage_threshold: 85%
      performance_threshold: 
        iops: 80000
        latency: 1ms
    
  application_storage:
    name: "app-standard-template"
    storage_class: "standard-ssd-pl1"
    access_mode: "ReadWriteOnce"
    size_range: "100Gi-500Gi"
    mount_options:
      - noatime
      - discard
    backup_policy: "daily-snapshot"
    monitoring:
      usage_threshold: 90%
      performance_threshold:
        iops: 30000
        latency: 3ms
        
  shared_storage:
    name: "shared-nas-template"
    storage_class: "shared-nas"
    access_mode: "ReadWriteMany"
    size_range: "1Ti-10Ti"
    mount_options:
      - vers=4.1
      - rsize=1048576
      - wsize=1048576
    backup_policy: "weekly-backup"
    monitoring:
      usage_threshold: 80%
      performance_threshold:
        throughput: 100MB/s
```

### å­˜å‚¨å®¹é‡è§„åˆ’æ–¹æ³•è®º

#### 1. å®¹é‡éœ€æ±‚é¢„æµ‹æ¨¡å‹

```python
# å­˜å‚¨å®¹é‡é¢„æµ‹ç®—æ³•
def predict_storage_capacity(
    current_usage_gb,
    growth_rate_monthly_percent,
    forecast_months,
    safety_margin_percent=20
):
    """
    é¢„æµ‹æœªæ¥å­˜å‚¨éœ€æ±‚
    """
    projected_usage = current_usage_gb * ((1 + growth_rate_monthly_percent/100) ** forecast_months)
    recommended_capacity = projected_usage * (1 + safety_margin_percent/100)
    
    return {
        'current_usage': current_usage_gb,
        'projected_usage': round(projected_usage, 2),
        'recommended_capacity': round(recommended_capacity, 2),
        'buffer_space': round(recommended_capacity - projected_usage, 2)
    }

# ç¤ºä¾‹ï¼šæ•°æ®åº“å­˜å‚¨é¢„æµ‹
result = predict_storage_capacity(
    current_usage_gb=500,
    growth_rate_monthly_percent=15,
    forecast_months=12,
    safety_margin_percent=25
)
print(f"å»ºè®®å®¹é‡: {result['recommended_capacity']} GB")
```

#### 2. å­˜å‚¨SLAå®šä¹‰

| SLAçº§åˆ« | å¯ç”¨æ€§ | RTO | RPO | å­˜å‚¨ç±»å‹ | æˆæœ¬ç³»æ•° |
|---------|--------|-----|-----|----------|----------|
| **Platinum** | 99.99% | 15åˆ†é’Ÿ | 1åˆ†é’Ÿ | ESSD PL3 + åŒæ­¥å¤åˆ¶ | 1.0 |
| **Gold** | 99.95% | 1å°æ—¶ | 15åˆ†é’Ÿ | ESSD PL2 + å¼‚æ­¥å¤åˆ¶ | 0.7 |
| **Silver** | 99.9% | 4å°æ—¶ | 1å°æ—¶ | ESSD PL1 + å¿«ç…§å¤‡ä»½ | 0.5 |
| **Bronze** | 99.5% | 24å°æ—¶ | 24å°æ—¶ | ESSD PL0 + æ¯æ—¥å¤‡ä»½ | 0.3 |

### æ•…éšœé¢„é˜²ä¸è‡ªæ„ˆæœºåˆ¶

#### 1. å­˜å‚¨å¥åº·æ£€æŸ¥è‡ªåŠ¨åŒ–

```bash
#!/bin/bash
# storage-health-check.sh

HEALTH_CHECK_INTERVAL=300  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
ALERT_THRESHOLD_CRITICAL=95
ALERT_THRESHOLD_WARNING=85

check_storage_health() {
    echo "$(date): å¼€å§‹å­˜å‚¨å¥åº·æ£€æŸ¥"
    
    # 1. æ£€æŸ¥PVCä½¿ç”¨ç‡
    HIGH_USAGE_PVC=$(kubectl get pvc --all-namespaces -o json | \
        jq -r '.items[] | select(.status.capacity.storage and .spec.resources.requests.storage) | 
               {ns: .metadata.namespace, name: .metadata.name, 
                usage: (.status.capacity.storage | split("Gi")[0] | tonumber),
                request: (.spec.resources.requests.storage | split("Gi")[0] | tonumber)} | 
               select(.usage/.request > 0.95) | "\(.ns)/\(.name):\(.usage/\(.request)*100)%"
              ')
    
    if [ -n "$HIGH_USAGE_PVC" ]; then
        echo "ğŸš¨ é«˜ä½¿ç”¨ç‡PVCè­¦å‘Š:"
        echo "$HIGH_USAGE_PVC"
        # å‘é€å‘Šè­¦...
    fi
    
    # 2. æ£€æŸ¥CSIé©±åŠ¨çŠ¶æ€
    CSI_DOWN=$(kubectl get pods -n kube-system | grep csi | grep -v Running)
    if [ -n "$CSI_DOWN" ]; then
        echo "âŒ CSIé©±åŠ¨å¼‚å¸¸:"
        echo "$CSI_DOWN"
        # è‡ªåŠ¨é‡å¯...
    fi
    
    # 3. æ£€æŸ¥å­˜å‚¨èŠ‚ç‚¹å¥åº·
    NODE_STORAGE_ISSUES=$(kubectl describe nodes | grep -A 10 "Conditions:" | grep -B 10 "DiskPressure")
    if [ -n "$NODE_STORAGE_ISSUES" ]; then
        echo "âš ï¸  èŠ‚ç‚¹å­˜å‚¨å‹åŠ›:"
        echo "$NODE_STORAGE_ISSUES"
    fi
    
    echo "$(date): å¥åº·æ£€æŸ¥å®Œæˆ"
}

# å®šæ—¶æ‰§è¡Œ
while true; do
    check_storage_health
    sleep $HEALTH_CHECK_INTERVAL
done
```

#### 2. è‡ªåŠ¨æ‰©å®¹ç­–ç•¥

```yaml
# åŸºäºä½¿ç”¨ç‡çš„è‡ªåŠ¨æ‰©å®¹ç­–ç•¥
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: storage-autoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: database
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: storage
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 120
```

---
## æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### 1. å­˜å‚¨æˆæœ¬åˆ†ææ¡†æ¶

#### æˆæœ¬æ„æˆåˆ†è§£

```yaml
storage_cost_breakdown:
  infrastructure_cost:
    cloud_disks: 60%  # äº‘ç›˜è´¹ç”¨
    network_bandwidth: 15%  # ç½‘ç»œä¼ è¾“è´¹ç”¨
    snapshot_backup: 10%  # å¿«ç…§å’Œå¤‡ä»½è´¹ç”¨
    management_overhead: 15%  # ç®¡ç†å’Œè¿ç»´æˆæœ¬
  
  optimization_opportunities:
    rightsizing: 25%  # å®¹é‡è°ƒæ•´ä¼˜åŒ–
    tier_migration: 30%  # åˆ†å±‚å­˜å‚¨è¿ç§»
    lifecycle_management: 20%  # ç”Ÿå‘½å‘¨æœŸç®¡ç†
    compression_dedup: 15%  # å‹ç¼©å»é‡æŠ€æœ¯
```

#### æˆæœ¬ç›‘æ§ä»ªè¡¨æ¿

```yaml
# å­˜å‚¨æˆæœ¬ç›‘æ§æŒ‡æ ‡
cost_monitoring_metrics:
  unit_cost_per_gb_month:
    essd_pl3: 3.5  # å…ƒ/GB/æœˆ
    essd_pl2: 2.1
    essd_pl1: 1.5
    essd_pl0: 1.05
    nas_general: 1.2
    oss_standard: 0.15
  
  cost_optimization_targets:
    - metric: "å­˜å‚¨æˆæœ¬å æ¯”"
      target: "< 15% of total IT budget"
      current: "18%"
      gap: "3%"
      
    - metric: "é—²ç½®å­˜å‚¨æ¯”ç‡"
      target: "< 5%"
      current: "12%"
      gap: "7%"
      
    - metric: "å¿«ç…§ä¿ç•™æˆæœ¬"
      target: "< 8% of primary storage cost"
      current: "15%"
      gap: "7%"
```

### 2. æ™ºèƒ½æˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ

#### å­˜å‚¨ç”Ÿå‘½å‘¨æœŸç®¡ç†

```python
# å­˜å‚¨ç”Ÿå‘½å‘¨æœŸæ™ºèƒ½ç®¡ç†
class StorageLifecycleManager:
    def __init__(self):
        self.tier_mapping = {
            'hot': {'days': 30, 'tier': 'essd_pl3'},
            'warm': {'days': 90, 'tier': 'essd_pl1'},
            'cold': {'days': 365, 'tier': 'essd_pl0'},
            'archive': {'days': 1095, 'tier': 'oss_archive'}
        }
    
    def optimize_storage_costs(self, pvc_list):
        """åŸºäºè®¿é—®æ¨¡å¼è‡ªåŠ¨ä¼˜åŒ–å­˜å‚¨å±‚çº§"""
        optimization_plan = []
        
        for pvc in pvc_list:
            access_pattern = self.analyze_access_pattern(pvc)
            current_tier = pvc.spec.storage_class_name
            
            if access_pattern.frequency == 'rare' and access_pattern.age_days > 365:
                # è¿ç§»åˆ°æ›´ç»æµçš„å­˜å‚¨å±‚
                recommended_tier = self.tier_mapping['cold']['tier']
                if current_tier != recommended_tier:
                    optimization_plan.append({
                        'pvc': pvc.metadata.name,
                        'current_tier': current_tier,
                        'recommended_tier': recommended_tier,
                        'estimated_savings': self.calculate_savings(current_tier, recommended_tier, pvc.size_gb)
                    })
        
        return optimization_plan
    
    def calculate_savings(self, from_tier, to_tier, size_gb):
        """è®¡ç®—è¿ç§»èŠ‚çœçš„æˆæœ¬"""
        cost_map = {
            'essd_pl3': 3.5,
            'essd_pl2': 2.1,
            'essd_pl1': 1.5,
            'essd_pl0': 1.05,
            'oss_archive': 0.03
        }
        
        monthly_savings = (cost_map[from_tier] - cost_map[to_tier]) * size_gb
        annual_savings = monthly_savings * 12
        
        return {
            'monthly': round(monthly_savings, 2),
            'annual': round(annual_savings, 2)
        }

# ä½¿ç”¨ç¤ºä¾‹
manager = StorageLifecycleManager()
optimization_plan = manager.optimize_storage_costs(active_pvcs)
```

#### è‡ªåŠ¨åŒ–æˆæœ¬æ§åˆ¶è„šæœ¬

```bash
#!/bin/bash
# cost-optimization-automation.sh

# å­˜å‚¨æˆæœ¬ä¼˜åŒ–è‡ªåŠ¨åŒ–è„šæœ¬
optimize_storage_costs() {
    echo "ğŸ’° å¼€å§‹å­˜å‚¨æˆæœ¬ä¼˜åŒ–åˆ†æ..."
    
    # 1. è¯†åˆ«é—²ç½®å­˜å‚¨
    echo "ğŸ” è¯†åˆ«é—²ç½®å­˜å‚¨..."
    IDLE_PVC=$(kubectl get pvc --all-namespaces -o json | \
        jq -r '.items[] | select(.metadata.annotations."storage/idle-days" > 30) | 
               "\(.metadata.namespace)/\(.metadata.name)"')
    
    if [ -n "$IDLE_PVC" ]; then
        echo "å‘ç°é—²ç½®å­˜å‚¨:"
        echo "$IDLE_PVC"
        # å‘é€æ¸…ç†å»ºè®®...
    fi
    
    # 2. åˆ†æå­˜å‚¨ä½¿ç”¨æ•ˆç‡
    echo "ğŸ“Š åˆ†æå­˜å‚¨ä½¿ç”¨æ•ˆç‡..."
    LOW_UTILIZATION=$(kubectl get pvc --all-namespaces -o json | \
        jq -r '.items[] | select(.status.capacity.storage and .spec.resources.requests.storage) |
               .utilization = (.status.capacity.storage | split("Gi")[0] | tonumber) / 
                             (.spec.resources.requests.storage | split("Gi")[0] | tonumber) |
               select(.utilization < 0.3) | 
               "\(.metadata.namespace)/\(.metadata.name): \(.utilization*100)%"')
    
    if [ -n "$LOW_UTILIZATION" ]; then
        echo "ä½åˆ©ç”¨ç‡å­˜å‚¨ (<30%):"
        echo "$LOW_UTILIZATION"
        # å»ºè®®å®¹é‡è°ƒæ•´...
    fi
    
    # 3. å¿«ç…§æˆæœ¬ä¼˜åŒ–
    echo "ğŸ“¸ å¿«ç…§æˆæœ¬ä¼˜åŒ–..."
    OLD_SNAPSHOTS=$(kubectl get volumesnapshot --all-namespaces -o json | \
        jq -r '[.items[] | select(.metadata.creationTimestamp < "'$(date -d '30 days ago' --iso-8601)'")] | length')
    
    echo "è¶…è¿‡30å¤©çš„å¿«ç…§æ•°é‡: $OLD_SNAPSHOTS"
    if [ "$OLD_SNAPSHOTS" -gt 10 ]; then
        echo "å»ºè®®æ¸…ç†æ—§å¿«ç…§ä»¥é™ä½æˆæœ¬"
    fi
    
    echo "âœ… æˆæœ¬ä¼˜åŒ–åˆ†æå®Œæˆ"
}

# å®šæœŸæ‰§è¡Œ
optimize_storage_costs
```

---
## ç›‘æ§å‘Šè­¦ä½“ç³»

### 1. æ ¸å¿ƒç›‘æ§æŒ‡æ ‡ä½“ç³»

#### å­˜å‚¨æ€§èƒ½æŒ‡æ ‡

```yaml
# å­˜å‚¨æ€§èƒ½ç›‘æ§æŒ‡æ ‡å®šä¹‰
performance_metrics:
  iops:
    description: "æ¯ç§’è¾“å…¥è¾“å‡ºæ“ä½œæ•°"
    critical_threshold: 90
    warning_threshold: 80
    collection_interval: 30s
    
  throughput:
    description: "æ•°æ®ä¼ è¾“é€Ÿç‡ (MB/s)"
    critical_threshold: 85
    warning_threshold: 70
    collection_interval: 30s
    
  latency:
    description: "å­˜å‚¨è®¿é—®å»¶è¿Ÿ (ms)"
    critical_threshold: 5
    warning_threshold: 2
    collection_interval: 30s
    
  utilization:
    description: "å­˜å‚¨ä½¿ç”¨ç‡ (%)"
    critical_threshold: 95
    warning_threshold: 85
    collection_interval: 60s
    
  error_rate:
    description: "å­˜å‚¨é”™è¯¯ç‡ (%)"
    critical_threshold: 1
    warning_threshold: 0.1
    collection_interval: 60s
```

#### ä¸šåŠ¡è¿ç»­æ€§æŒ‡æ ‡

```yaml
# ä¸šåŠ¡è¿ç»­æ€§ç›‘æ§æŒ‡æ ‡
business_continuity_metrics:
  pvc_provision_time:
    description: "PVCåˆ›å»ºåˆ°å¯ç”¨æ—¶é—´"
    sla_target: "30ç§’"
    alert_threshold: "60ç§’"
    
  volume_attach_time:
    description: "å·æŒ‚è½½åˆ°Podæ—¶é—´"
    sla_target: "10ç§’"
    alert_threshold: "30ç§’"
    
  backup_success_rate:
    description: "å¤‡ä»½æˆåŠŸç‡"
    sla_target: "99.9%"
    alert_threshold: "99%"
    
  recovery_time:
    description: "æ•°æ®æ¢å¤æ—¶é—´"
    sla_target: "15åˆ†é’Ÿ"
    alert_threshold: "1å°æ—¶"
```

### 2. å‘Šè­¦ç­–ç•¥é…ç½®

```yaml
# Prometheuså‘Šè­¦è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: storage-alerts
  namespace: monitoring
spec:
  groups:
  - name: storage.rules
    rules:
    # PVCä½¿ç”¨ç‡å‘Šè­¦
    - alert: PVCUsageCritical
      expr: |
        (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 95
      for: 5m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} ä½¿ç”¨ç‡è¿‡é«˜ ({{ $value }}%)"
        description: "å‘½åç©ºé—´: {{ $labels.namespace }}, å»ºè®®ç«‹å³æ‰©å®¹"
        
    - alert: PVCUsageWarning
      expr: |
        (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 85
      for: 10m
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} ä½¿ç”¨ç‡è¾¾åˆ°è­¦å‘Šé˜ˆå€¼ ({{ $value }}%)"
        description: "å‘½åç©ºé—´: {{ $labels.namespace }}, è¯·å…³æ³¨å®¹é‡è§„åˆ’"
    
    # å­˜å‚¨æ€§èƒ½å‘Šè­¦
    - alert: StorageHighLatency
      expr: |
        rate(storage_operation_duration_seconds_sum[5m]) / 
        rate(storage_operation_duration_seconds_count[5m]) > 0.005
      for: 5m
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "å­˜å‚¨å»¶è¿Ÿè¿‡é«˜ ({{ $value }}s)"
        description: "æ£€æµ‹åˆ°å­˜å‚¨æ€§èƒ½ä¸‹é™ï¼Œè¯·æ£€æŸ¥åº•å±‚å­˜å‚¨ç³»ç»Ÿ"
        
    # CSIé©±åŠ¨çŠ¶æ€å‘Šè­¦
    - alert: CSIDriverDown
      expr: |
        up{job="csi-driver"} == 0
      for: 3m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "CSIé©±åŠ¨æœåŠ¡ä¸å¯ç”¨"
        description: "å­˜å‚¨ä¾›ç»™åŠŸèƒ½å—å½±å“ï¼Œè¯·ç«‹å³æ£€æŸ¥CSIç»„ä»¶"
        
    # å­˜å‚¨èŠ‚ç‚¹å¥åº·å‘Šè­¦
    - alert: StorageNodePressure
      expr: |
        kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 2m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "èŠ‚ç‚¹ {{ $labels.node }} å­˜å‚¨å‹åŠ›è¿‡å¤§"
        description: "èŠ‚ç‚¹å­˜å‚¨èµ„æºç´§å¼ ï¼Œå¯èƒ½å½±å“Podè°ƒåº¦å’Œè¿è¡Œ"
```

### 3. ç›‘æ§ä»ªè¡¨æ¿è®¾è®¡

```json
{
  "dashboard": {
    "title": "Kuberneteså­˜å‚¨ç›‘æ§æ€»è§ˆ",
    "panels": [
      {
        "title": "å­˜å‚¨å®¹é‡ä½¿ç”¨æ¦‚è§ˆ",
        "type": "graph",
        "targets": [
          "sum(kubelet_volume_stats_used_bytes) by (namespace)",
          "sum(kubelet_volume_stats_capacity_bytes) by (namespace)"
        ],
        "visualization": "area-stacked"
      },
      {
        "title": "PVCçŠ¶æ€åˆ†å¸ƒ",
        "type": "piechart",
        "targets": [
          "count(kube_persistentvolumeclaim_status_phase) by (phase)"
        ]
      },
      {
        "title": "å­˜å‚¨æ€§èƒ½æŒ‡æ ‡",
        "type": "timeseries",
        "targets": [
          "rate(storage_operation_duration_seconds_sum[5m])",
          "storage_iops_operations_total"
        ],
        "thresholds": {
          "critical": 90,
          "warning": 80
        }
      },
      {
        "title": "å­˜å‚¨æˆæœ¬è¶‹åŠ¿",
        "type": "bar-gauge",
        "targets": [
          "sum by (storageclass) (storage_cost_monthly)"
        ]
      }
    ]
  }
}
```

### 4. è¿ç»´å“åº”æµç¨‹

```mermaid
graph TD
    A[ç›‘æ§å‘Šè­¦è§¦å‘] --> B{å‘Šè­¦çº§åˆ«}
    B -->|Critical| C[ç«‹å³é€šçŸ¥SREå›¢é˜Ÿ]
    B -->|Warning| D[è®°å½•å¹¶è·Ÿè¸ª]
    C --> E[æ‰§è¡Œåº”æ€¥é¢„æ¡ˆ]
    D --> F[å®šæœŸå›é¡¾åˆ†æ]
    E --> G[æ•…éšœå®šä½]
    G --> H[ä¿®å¤æªæ–½]
    H --> I[éªŒè¯æ¢å¤]
    I --> J[æ ¹æœ¬åŸå› åˆ†æ]
    J --> K[é¢„é˜²æªæ–½æ›´æ–°]
```

---
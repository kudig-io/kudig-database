# 40 - å¤§è§„æ¨¡é›†ç¾¤è¿ç»´ (Large Scale Cluster Operations)

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25-v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä¸“å®¶çº§åˆ«**: â­â­â­â­â­ | **å‚è€ƒ**: [Kuberneteså¤§è§„æ¨¡é›†ç¾¤æœ€ä½³å®è·µ](https://kubernetes.io/docs/setup/best-practices/cluster-large/), [Google Borgè®ºæ–‡](https://research.google/pubs/pub43438/)

---

## ç›¸å…³æ–‡æ¡£äº¤å‰å¼•ç”¨

### ğŸ”— å…³è”æ•…éšœæ’æŸ¥æ–‡æ¡£
- **[01-API Serveræ•…éšœæ’æŸ¥](./01-control-plane-apiserver-troubleshooting.md)** - æ§åˆ¶å¹³é¢æ€§èƒ½ä¼˜åŒ–
- **[02-etcdæ•…éšœæ’æŸ¥](./02-control-plane-etcd-troubleshooting.md)** - etcdå¤§è§„æ¨¡é›†ç¾¤è°ƒä¼˜
- **[33-æ€§èƒ½ç“¶é¢ˆæ•…éšœæ’æŸ¥](./33-performance-bottleneck-troubleshooting.md)** - å¤§è§„æ¨¡é›†ç¾¤æ€§èƒ½åˆ†æ
- **[34-å‡çº§è¿ç§»æ•…éšœæ’æŸ¥](./34-upgrade-migration-troubleshooting.md)** - å¤§è§„æ¨¡é›†ç¾¤å‡çº§ç­–ç•¥

### ğŸ“š æ‰©å±•å­¦ä¹ èµ„æ–™
- **[Kuberneteså¤§è§„æ¨¡é›†ç¾¤æŒ‡å—](https://kubernetes.io/docs/setup/best-practices/cluster-large/)** - å®˜æ–¹å¤§è§„æ¨¡é›†ç¾¤æœ€ä½³å®è·µ
- **[Google Borgç»éªŒåˆ†äº«](https://research.google/pubs/pub43438/)** - Googleå®¹å™¨ç¼–æ’ç³»ç»Ÿç»éªŒ
- **[Netflixå¤§è§„æ¨¡Kuberneteså®è·µ](https://netflixtechblog.com/)** - Netflixäº‘åŸç”Ÿå®è·µç»éªŒ

---

## ç›®å½•

1. [å¤§è§„æ¨¡é›†ç¾¤æ¶æ„è®¾è®¡](#1-å¤§è§„æ¨¡é›†ç¾¤æ¶æ„è®¾è®¡)
2. [æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜](#2-æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜)
3. [å®¹é‡è§„åˆ’ä¸æ‰©å±•](#3-å®¹é‡è§„åˆ’ä¸æ‰©å±•)
4. [æ•…éšœåŸŸç®¡ç†](#4-æ•…éšœåŸŸç®¡ç†)
5. [å¤šåŒºåŸŸéƒ¨ç½²ç­–ç•¥](#5-å¤šåŒºåŸŸéƒ¨ç½²ç­–ç•¥)
6. [è‡ªåŠ¨åŒ–è¿ç»´å¹³å°](#6-è‡ªåŠ¨åŒ–è¿ç»´å¹³å°)
7. [ç›‘æ§ä¸å¯è§‚æµ‹æ€§](#7-ç›‘æ§ä¸å¯è§‚æµ‹æ€§)
8. [å®‰å…¨ä¸åˆè§„ç®¡ç†](#8-å®‰å…¨ä¸åˆè§„ç®¡ç†)

---

## 1. å¤§è§„æ¨¡é›†ç¾¤æ¶æ„è®¾è®¡

### 1.1 è¶…å¤§è§„æ¨¡é›†ç¾¤æŒ‘æˆ˜åˆ†æ

#### å¤§è§„æ¨¡é›†ç¾¤å…¸å‹ç‰¹å¾ä¸æŒ‘æˆ˜
```yaml
scale_challenges:
  small_cluster:  # < 100 nodes
    characteristics:
      - simple_network_topology
      - single_az_deployment
      - homogeneous_workloads
    challenges:
      - basic_scaling_issues
      - simple_failure_scenarios
      
  medium_cluster:  # 100-1000 nodes
    characteristics:
      - regional_deployment
      - mixed_workload_types
      - moderate_network_complexity
    challenges:
      - etcd_performance_degradation
      - network_policy_complexity
      - resource_fragmentation
      
  large_cluster:  # 1000-5000 nodes
    characteristics:
      - multi_region_deployment
      - diverse_workload_portfolio
      - complex_network_topology
    challenges:
      - control_plane_scalability_limits
      - cross_region_latency
      - multi_tenancy_complexity
      
  massive_cluster:  # > 5000 nodes
    characteristics:
      - global_deployment
      - extreme_workload_diversity
      - federated_architecture
    challenges:
      - fundamental_architecture_limits
      - data_consistency_complexity
      - operational_complexity_explosion
```

### 1.2 åˆ†å±‚æ¶æ„è®¾è®¡æ–¹æ¡ˆ

#### è¶…å¤§è§„æ¨¡é›†ç¾¤åˆ†å±‚æ¶æ„
```
è¶…å¤§è§„æ¨¡é›†ç¾¤æ¶æ„:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          å…¨çƒæ§åˆ¶å¹³é¢å±‚ (Global Control Plane)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  å…¨çƒAPIç½‘å…³     â”‚  â”‚  è·¨åŒºåŸŸåè°ƒå™¨    â”‚  â”‚  ç»Ÿä¸€èº«ä»½è®¤è¯    â”‚            â”‚
â”‚  â”‚ Global API GW   â”‚  â”‚ Cross-region CO â”‚  â”‚ Unified Auth    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   åŒºåŸŸæ§åˆ¶å¹³é¢   â”‚    â”‚   åŒºåŸŸæ§åˆ¶å¹³é¢   â”‚    â”‚   åŒºåŸŸæ§åˆ¶å¹³é¢   â”‚
â”‚ Regional CP-A   â”‚    â”‚ Regional CP-B   â”‚    â”‚ Regional CP-C   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ API Server    â”‚    â”‚ â€¢ API Server    â”‚    â”‚ â€¢ API Server    â”‚
â”‚ â€¢ etcd Cluster  â”‚    â”‚ â€¢ etcd Cluster  â”‚    â”‚ â€¢ etcd Cluster  â”‚
â”‚ â€¢ Scheduler     â”‚    â”‚ â€¢ Scheduler     â”‚    â”‚ â€¢ Scheduler     â”‚
â”‚ â€¢ Controllers   â”‚    â”‚ â€¢ Controllers   â”‚    â”‚ â€¢ Controllers   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   åŒºåŸŸå·¥ä½œèŠ‚ç‚¹   â”‚    â”‚   åŒºåŸŸå·¥ä½œèŠ‚ç‚¹   â”‚    â”‚   åŒºåŸŸå·¥ä½œèŠ‚ç‚¹   â”‚
â”‚ Zone-A Workers  â”‚    â”‚ Zone-B Workers  â”‚    â”‚ Zone-C Workers  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ è®¡ç®—å¯†é›†å‹     â”‚    â”‚ â€¢ å­˜å‚¨å¯†é›†å‹     â”‚    â”‚ â€¢ ç½‘ç»œå¯†é›†å‹     â”‚
â”‚ â€¢ GPUåŠ é€ŸèŠ‚ç‚¹    â”‚    â”‚ â€¢ é«˜IOPSå­˜å‚¨     â”‚    â”‚ â€¢ è¾¹ç¼˜è®¡ç®—èŠ‚ç‚¹    â”‚
â”‚ â€¢ å†…å­˜ä¼˜åŒ–èŠ‚ç‚¹    â”‚    â”‚ â€¢ å†·å­˜å‚¨èŠ‚ç‚¹     â”‚    â”‚ â€¢ ä½å»¶è¿ŸèŠ‚ç‚¹     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ§åˆ¶å¹³é¢æ°´å¹³æ‰©å±•

#### å¤šé›†ç¾¤è”é‚¦æ¶æ„
```yaml
# federated_control_plane.yaml
federation_architecture:
  global_coordinator:
    components:
      - global_api_gateway: "nginx + lua"
      - federation_controller: "custom_operator"
      - cross_cluster_scheduler: "multi_scheduler"
      
  regional_clusters:
    cluster_a:  # us-east region
      control_plane_nodes: 5
      worker_nodes: 2000
      etcd_cluster_size: 7
      features:
        - dedicated_network_zone
        - local_registry_mirror
        - regional_load_balancer
        
    cluster_b:  # eu-west region
      control_plane_nodes: 5
      worker_nodes: 1500
      etcd_cluster_size: 5
      features:
        - gdpr_compliance
        - low_latency_networking
        - edge_cache_layer
        
    cluster_c:  # apac region
      control_plane_nodes: 3
      worker_nodes: 1000
      etcd_cluster_size: 3
      features:
        - cost_optimized_instances
        - burst_capacity_scaling
        - hybrid_cloud_integration
      
  cross_cluster_connectivity:
    service_mesh: istio_multicluster
    dns_federation: external_dns + coredns
    load_balancing: global_load_balancer
    data_replication: velero_cross_cluster
```

---

## 2. æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜

### 2.1 æ§åˆ¶å¹³é¢æ€§èƒ½ä¼˜åŒ–

#### API Serverå¤§è§„æ¨¡è°ƒä¼˜å‚æ•°
```yaml
# apiserver_large_scale_optimization.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver-large-scale
spec:
  containers:
  - name: kube-apiserver
    args:
      # ===== æ ¸å¿ƒæ€§èƒ½è°ƒä¼˜å‚æ•° =====
      
      # å¹¶å‘è¿æ¥ä¼˜åŒ–
      - --max-requests-inflight=3000          # é»˜è®¤400 -> 3000
      - --max-mutating-requests-inflight=1000  # é»˜è®¤200 -> 1000
      
      # é€Ÿç‡é™åˆ¶ä¼˜åŒ–
      - --enable-priority-and-fairness=true
      - --priority-level-configuration-file=/etc/kubernetes/priority-levels.yaml
      
      # ç¼“å­˜ä¼˜åŒ–
      - --watch-cache-sizes=
          secrets#100000,
          configmaps#100000,
          pods#1000000,
          services#100000,
          endpoints#500000
      
      # å­˜å‚¨ä¼˜åŒ–
      - --etcd-compaction-interval=15m        # ç¼©çŸ­å‹ç¼©é—´éš”
      - --etcd-count-metric-poll-period=10s   # å¢åŠ ç»Ÿè®¡é¢‘ç‡
      
      # ç½‘ç»œä¼˜åŒ–
      - --http2-max-streams-per-connection=1000  # å¢åŠ HTTP/2æµæ•°
      - --enable-aggregator-routing=true
      
      # å®‰å…¨ä¼˜åŒ–
      - --audit-policy-file=/etc/kubernetes/audit-policy.yaml
      - --audit-log-mode=batch
      - --audit-log-batch-max-size=100
```

#### etcdå¤§è§„æ¨¡é›†ç¾¤è°ƒä¼˜
```yaml
# etcd_large_scale_tuning.yaml
apiVersion: v1
kind: Pod
metadata:
  name: etcd-large-scale
spec:
  containers:
  - name: etcd
    env:
      # ===== å­˜å‚¨æ€§èƒ½è°ƒä¼˜ =====
      - name: ETCD_QUOTA_BACKEND_BYTES
        value: "8589934592"  # 8GBå­˜å‚¨é…é¢
      
      - name: ETCD_AUTO_COMPACTION_MODE
        value: "revision"
        
      - name: ETCD_AUTO_COMPACTION_RETENTION
        value: "1000"  # ä¿ç•™1000ä¸ªç‰ˆæœ¬
        
      # ===== ç½‘ç»œæ€§èƒ½è°ƒä¼˜ =====
      - name: ETCD_HEARTBEAT_INTERVAL
        value: "100"   # 100mså¿ƒè·³é—´éš”
        
      - name: ETCD_ELECTION_TIMEOUT
        value: "1000"  # 1000msé€‰ä¸¾è¶…æ—¶
        
      - name: ETCD_SNAPSHOT_COUNT
        value: "10000" # å¢åŠ å¿«ç…§é—´éš”
        
      # ===== èµ„æºé™åˆ¶è°ƒä¼˜ =====
      - name: ETCD_MAX_REQUEST_BYTES
        value: "10485760"  # 10MBæœ€å¤§è¯·æ±‚å¤§å°
        
      - name: ETCD_MAX_WALS
        value: "10"        # WALæ–‡ä»¶æ•°é‡
```

### 2.2 èŠ‚ç‚¹çº§æ€§èƒ½ä¼˜åŒ–

#### å¤§è§„æ¨¡èŠ‚ç‚¹kubeletè°ƒä¼˜
```yaml
# kubelet_large_scale_optimization.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubelet-config-large-scale
data:
  kubelet: |
    apiVersion: kubelet.config.k8s.io/v1beta1
    kind: KubeletConfiguration
    
    # ===== èµ„æºç®¡ç†ä¼˜åŒ– =====
    maxPods: 250                    # å•èŠ‚ç‚¹æœ€å¤§Podæ•°
    podsPerCore: 10                 # æ¯æ ¸Podæ•°é™åˆ¶
    systemReserved:
      cpu: 500m
      memory: 1Gi
      ephemeral-storage: 10Gi
      
    # ===== æ€§èƒ½è°ƒä¼˜å‚æ•° =====
    serializeImagePulls: false      # å¹¶è¡Œæ‹‰å–é•œåƒ
    maxOpenFiles: 1000000           # æœ€å¤§æ‰“å¼€æ–‡ä»¶æ•°
    maxParallelImagePulls: 10       # å¹¶è¡Œé•œåƒæ‹‰å–æ•°
    
    # ===== ç½‘ç»œä¼˜åŒ– =====
    hairpinMode: promiscuous-bridge
    maxHousekeepingInterval: 30s    # å®¹å™¨çŠ¶æ€æ£€æŸ¥é—´éš”
    
    # ===== ç›‘æ§ä¼˜åŒ– =====
    enableDebuggingHandlers: false  # ç¦ç”¨è°ƒè¯•ç«¯ç‚¹
    enableServer: true
    readOnlyPort: 0                 # ç¦åªè¯»ç«¯å£
    
    # ===== åƒåœ¾å›æ”¶ä¼˜åŒ– =====
    imageGCHighThresholdPercent: 85
    imageGCLowThresholdPercent: 80
    containerLogMaxSize: 10Mi
    containerLogMaxFiles: 3
```

### 2.3 ç½‘ç»œæ€§èƒ½ä¼˜åŒ–

#### CNIæ’ä»¶å¤§è§„æ¨¡è°ƒä¼˜
```yaml
# cni_large_scale_optimization.yaml
calico_config:
  # ===== å¤§è§„æ¨¡é›†ç¾¤Calicoè°ƒä¼˜ =====
  felix:
    # æ€§èƒ½ä¼˜åŒ–
    GenericXDPEnabled: true
    BPFEnabled: true
    BPFLogLevel: ""
    
    # è§„æ¨¡ä¼˜åŒ–
    IptablesRefreshInterval: "60s"
    RouteRefreshInterval: "90s"
    IPSetsRefreshInterval: "120s"
    
    # èµ„æºé™åˆ¶
    MaxIpsetSize: 1000000
    DataplaneDriverMemoryLimit: "2Gi"
    
  bgp:
    # å¤§è§„æ¨¡BGPé…ç½®
    ASNumber: 64512
    NodeToNodeMeshEnabled: false  # ç¦ç”¨å…¨äº’è”ç½‘æ ¼
    FullMeshPeerGroups:
      - name: "tier-1-routers"
        peers:
          - router1.example.com
          - router2.example.com
          
  ipam:
    # IPåœ°å€ç®¡ç†ä¼˜åŒ–
    StrictAffinity: true
    AutoAllocateBlocks: true
    BlockSize: 26  # /26å­ç½‘å—

cilium_config:
  # ===== Ciliumå¤§è§„æ¨¡è°ƒä¼˜ =====
  bpf:
    # BPFæ€§èƒ½ä¼˜åŒ–
    PreallocateMaps: true
    CTMapEntriesGlobalTCP: 2097152
    CTMapEntriesGlobalAny: 524288
    NATMapEntriesGlobal: 2097152
    NeighMapEntriesGlobal: 524288
    PolicyMapEntries: 16384
    
  kubeProxyReplacement: "strict"
  enableIPv4Masquerade: true
  enableBandwidthManager: true
  enableRecorder: false  # ç¦ç”¨å½•åˆ¶åŠŸèƒ½èŠ‚çœèµ„æº
```

---

## 3. å®¹é‡è§„åˆ’ä¸æ‰©å±•

### 3.1 å®¹é‡è§„åˆ’æ–¹æ³•è®º

#### å¤§è§„æ¨¡é›†ç¾¤å®¹é‡æ¨¡å‹
```python
# capacity_planning_model.py
import math
from typing import Dict, List
from dataclasses import dataclass

@dataclass
class WorkloadProfile:
    name: str
    cpu_per_pod: float  # æ ¸å¿ƒæ•°
    memory_per_pod: float  # GB
    pods_per_node: int
    growth_rate: float  # å¹´å¢é•¿ç‡%

class CapacityPlanner:
    def __init__(self):
        self.safety_margin = 1.3  # 30%å®‰å…¨è¾¹é™…
        self.node_specs = {
            'compute_optimized': {'cpu': 32, 'memory': 64, 'cost': 0.8},
            'memory_optimized': {'cpu': 16, 'memory': 128, 'cost': 1.2},
            'storage_optimized': {'cpu': 16, 'memory': 32, 'disk': 2000, 'cost': 1.0}
        }
        
    def calculate_required_nodes(self, 
                               workload_profiles: List[WorkloadProfile],
                               target_date_months: int = 12) -> Dict:
        """
        è®¡ç®—æ‰€éœ€èŠ‚ç‚¹æ•°é‡
        """
        total_resources = {'cpu': 0, 'memory': 0, 'pods': 0}
        
        for profile in workload_profiles:
            # è®¡ç®—å¢é•¿åçš„èµ„æºéœ€æ±‚
            growth_factor = (1 + profile.growth_rate/100) ** (target_date_months/12)
            
            total_resources['cpu'] += profile.cpu_per_pod * profile.pods_per_node * growth_factor
            total_resources['memory'] += profile.memory_per_pod * profile.pods_per_node * growth_factor
            total_resources['pods'] += profile.pods_per_node * growth_factor
            
        # è®¡ç®—èŠ‚ç‚¹éœ€æ±‚
        node_requirements = {}
        for node_type, specs in self.node_specs.items():
            nodes_needed = {
                'cpu': math.ceil((total_resources['cpu'] * self.safety_margin) / specs['cpu']),
                'memory': math.ceil((total_resources['memory'] * self.safety_margin) / specs['memory']),
                'pods': math.ceil((total_resources['pods'] * self.safety_margin) / 110)  # å‡è®¾æ¯ä¸ªèŠ‚ç‚¹110ä¸ªPod
            }
            node_requirements[node_type] = max(nodes_needed.values())
            
        return {
            'workload_demand': total_resources,
            'node_requirements': node_requirements,
            'total_cost_estimate': sum(
                count * self.node_specs[node_type]['cost'] 
                for node_type, count in node_requirements.items()
            ),
            'recommendation': self._get_optimal_mix(node_requirements)
        }
        
    def _get_optimal_mix(self, requirements: Dict) -> str:
        """è·å–æœ€ä¼˜èŠ‚ç‚¹ç»„åˆå»ºè®®"""
        total_nodes = sum(requirements.values())
        if total_nodes < 100:
            return "å•ä¸€èŠ‚ç‚¹ç±»å‹å³å¯æ»¡è¶³éœ€æ±‚"
        elif total_nodes < 1000:
            return f"å»ºè®®é‡‡ç”¨æ··åˆéƒ¨ç½²: {max(requirements, key=requirements.get)}ä¸ºä¸»"
        else:
            return "å»ºè®®é‡‡ç”¨åˆ†å±‚æ¶æ„: è®¡ç®—å‹+å†…å­˜å‹+å­˜å‚¨å‹æ··åˆéƒ¨ç½²"

# ä½¿ç”¨ç¤ºä¾‹
planner = CapacityPlanner()
profiles = [
    WorkloadProfile("web_app", 0.5, 1, 200, 45),
    WorkloadProfile("data_processing", 2, 8, 50, 30),
    WorkloadProfile("ml_training", 8, 32, 25, 20)
]

result = planner.calculate_required_nodes(profiles, 18)
print(f"é¢„è®¡éœ€è¦èŠ‚ç‚¹æ•°: {result['node_requirements']}")
print(f"é¢„ä¼°æœˆæˆæœ¬: ${result['total_cost_estimate'] * 730:.2f}")
```

### 3.2 è‡ªåŠ¨æ‰©ç¼©å®¹ç­–ç•¥

#### æ™ºèƒ½é›†ç¾¤è‡ªåŠ¨æ‰©ç¼©å®¹
```yaml
# cluster_autoscaler_large_scale.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
spec:
  template:
    spec:
      containers:
      - name: cluster-autoscaler
        args:
          # ===== å¤§è§„æ¨¡é›†ç¾¤ä¸“ç”¨å‚æ•° =====
          
          # æ‰©ç¼©å®¹ç­–ç•¥ä¼˜åŒ–
          - --scale-down-delay-after-add=10m      # æ–°å¢èŠ‚ç‚¹åç­‰å¾…æ—¶é—´
          - --scale-down-unneeded-time=10m        # æ— ç”¨èŠ‚ç‚¹ç­‰å¾…æ—¶é—´
          - --scale-down-utilization-threshold=0.5 # èµ„æºåˆ©ç”¨ç‡é˜ˆå€¼
          
          # æ‰¹é‡æ“ä½œä¼˜åŒ–
          - --max-node-provision-time=15m         # æœ€å¤§èŠ‚ç‚¹ä¾›åº”æ—¶é—´
          - --max-graceful-termination-sec=600    # ä¼˜é›…ç»ˆæ­¢æ—¶é—´
          - --max-total-unready-percentage=45     # æœ€å¤§æœªå°±ç»ªèŠ‚ç‚¹æ¯”ä¾‹
          
          # åŒºåŸŸå¹³è¡¡ç­–ç•¥
          - --balance-similar-node-groups=true    # å¹³è¡¡ç›¸ä¼¼èŠ‚ç‚¹ç»„
          - --skip-nodes-with-local-storage=false # ä¸è·³è¿‡æœ¬åœ°å­˜å‚¨èŠ‚ç‚¹
          - --skip-nodes-with-system-pods=false   # ä¸è·³è¿‡ç³»ç»ŸPodèŠ‚ç‚¹
          
          # æ€§èƒ½ä¼˜åŒ–
          - --scan-interval=30s                   # æ‰«æé—´éš”
          - --expander=priority                   # æ‰©å±•å™¨ç­–ç•¥
          - --max-empty-bulk-delete=10            # æœ€å¤§æ‰¹é‡åˆ é™¤ç©ºèŠ‚ç‚¹æ•°
          
          # æ•…éšœæ¢å¤
          - --unremovable-node-recheck-timeout=5m # ä¸å¯ç§»é™¤èŠ‚ç‚¹é‡æ–°æ£€æŸ¥
          - --max-inactivity=10m                  # æœ€å¤§éæ´»åŠ¨æ—¶é—´
```

#### è‡ªå®šä¹‰æ‰©ç¼©å®¹ä¼˜å…ˆçº§
```yaml
# autoscaler_priority_config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-priority-expander
data:
  priorities: |
    # ===== èŠ‚ç‚¹ç»„ä¼˜å…ˆçº§é…ç½® =====
    
    # æˆæœ¬ä¼˜åŒ–ä¼˜å…ˆçº§
    10:
      - .*spot.*              # Spotå®ä¾‹æœ€é«˜ä¼˜å…ˆçº§
      - .*preemptible.*       # æŠ¢å å¼å®ä¾‹
      
    20:
      - .*compute-optimized.* # è®¡ç®—ä¼˜åŒ–å®ä¾‹
      - .*standard.*          # æ ‡å‡†å®ä¾‹
      
    30:
      - .*memory-optimized.*  # å†…å­˜ä¼˜åŒ–å®ä¾‹
      - .*gpu-enabled.*       # GPUå®ä¾‹
      
    40:
      - .*storage-optimized.* # å­˜å‚¨ä¼˜åŒ–å®ä¾‹
      - .*high-memory.*       # é«˜å†…å­˜å®ä¾‹
      
    # åŒºåŸŸå¹³è¡¡ä¼˜å…ˆçº§
    zone-balance:
      strategy: balanced
      zones:
        - us-east-1a: 30%
        - us-east-1b: 30% 
        - us-east-1c: 40%
```

---

## 4. æ•…éšœåŸŸç®¡ç†

### 4.1 å¤šå±‚çº§æ•…éšœåŸŸè®¾è®¡

#### æ•…éšœåŸŸéš”ç¦»æ¶æ„
```
æ•…éšœåŸŸå±‚æ¬¡ç»“æ„:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         å…¨çƒå±‚ (Global)                         â”‚
â”‚                    å¤šäº‘æä¾›å•†æ•…éšœåŸŸ                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ AWS Global  â”‚    â”‚ GCP Global  â”‚    â”‚ Azure Globalâ”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     åŒºåŸŸå±‚       â”‚    â”‚     åŒºåŸŸå±‚       â”‚    â”‚     åŒºåŸŸå±‚       â”‚
â”‚   (Region)      â”‚    â”‚   (Region)      â”‚    â”‚   (Region)      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ us-east-1       â”‚    â”‚ eu-west-1       â”‚    â”‚ ap-southeast-1  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     å¯ç”¨åŒºå±‚     â”‚    â”‚     å¯ç”¨åŒºå±‚     â”‚    â”‚     å¯ç”¨åŒºå±‚     â”‚
â”‚ (Availability)  â”‚    â”‚ (Availability)  â”‚    â”‚ (Availability)  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ us-east-1a      â”‚    â”‚ eu-west-1a      â”‚    â”‚ ap-southeast-1a â”‚
â”‚ us-east-1b      â”‚    â”‚ eu-west-1b      â”‚    â”‚ ap-southeast-1b â”‚
â”‚ us-east-1c      â”‚    â”‚ eu-west-1c      â”‚    â”‚ ap-southeast-1c â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     èŠ‚ç‚¹å±‚       â”‚    â”‚     èŠ‚ç‚¹å±‚       â”‚    â”‚     èŠ‚ç‚¹å±‚       â”‚
â”‚    (Node)       â”‚    â”‚    (Node)       â”‚    â”‚    (Node)       â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ Failure Domain 1â”‚    â”‚ Failure Domain 2â”‚    â”‚ Failure Domain 3â”‚
â”‚ Rack 1,2,3      â”‚    â”‚ Rack 4,5,6      â”‚    â”‚ Rack 7,8,9      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 æ•…éšœåŸŸæ„ŸçŸ¥è°ƒåº¦

#### æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦é…ç½®
```yaml
# topology_aware_scheduling.yaml
apiVersion: v1
kind: Pod
metadata:
  name: topology-aware-app
spec:
  # ===== æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ =====
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: myapp
        
  - maxSkew: 2
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app: myapp
        
  - maxSkew: 3
    topologyKey: topology.kubernetes.io/region
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app: myapp
        
  # ===== åäº²å’Œæ€§é…ç½® =====
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - myapp
        topologyKey: kubernetes.io/hostname
        
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - myapp
          topologyKey: topology.kubernetes.io/zone
          
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/arch
            operator: In
            values:
            - amd64
          - key: node.kubernetes.io/instance-type
            operator: In
            values:
            - compute-optimized
```

### 4.3 æ•…éšœæ¢å¤ç­–ç•¥

#### åˆ†å¸ƒå¼æ•…éšœæ¢å¤æœºåˆ¶
```yaml
# fault_tolerance_strategies.yaml
fault_recovery_system:
  multi_zone_deployment:
    zones:
      primary: us-east-1a
      secondary: us-east-1b
      tertiary: us-east-1c
      
    failover_policies:
      automatic_failover:
        enabled: true
        detection_time: "30s"
        failover_time: "2m"
        
      data_replication:
        strategy: "multi_zone_sync"
        rto: "1m"  # æ¢å¤æ—¶é—´ç›®æ ‡
        rpo: "30s" # æ¢å¤ç‚¹ç›®æ ‡
        
  health_checks:
    liveness_probes:
      timeout_seconds: 3
      period_seconds: 10
      failure_threshold: 3
      
    readiness_probes:
      timeout_seconds: 2
      period_seconds: 5
      failure_threshold: 2
      
    startup_probes:
      timeout_seconds: 5
      period_seconds: 10
      failure_threshold: 30
      
  circuit_breakers:
    # æœåŠ¡ç†”æ–­å™¨é…ç½®
    service_mesh_circuit_breakers:
      max_connections: 1000
      max_pending_requests: 100
      max_requests: 1000
      max_retries: 3
      timeout: "30s"
```

---

## 5. å¤šåŒºåŸŸéƒ¨ç½²ç­–ç•¥

### 5.1 åœ°ç†åˆ†å¸ƒæ¶æ„

#### å…¨çƒåˆ†å¸ƒå¼éƒ¨ç½²æ¨¡å¼
```yaml
# global_deployment_strategy.yaml
global_deployment:
  regions:
    north_america:
      primary: us-east-1
      secondary: us-west-2
      traffic_distribution: 60-40
      
    europe:
      primary: eu-west-1
      secondary: eu-central-1
      traffic_distribution: 70-30
      
    asia_pacific:
      primary: ap-southeast-1
      secondary: ap-northeast-1
      traffic_distribution: 50-50
      
  routing_strategies:
    latency_based_routing:
      algorithm: weighted_round_robin
      weight_calculation: inverse_latency
      
    health_based_routing:
      health_check_interval: "30s"
      failure_threshold: 3
      recovery_threshold: 2
      
    cost_optimization:
      spot_instance_utilization: 70%
      reserved_instance_coverage: 80%
      cross_zone_load_balancing: enabled

  data_synchronization:
    active_active_replication:
      databases: mysql_cluster
      caches: redis_cluster
      object_storage: s3_cross_region_replication
      
    eventual_consistency:
      acceptable_lag: "5s"
      conflict_resolution: "last_writer_wins"
      data_validation: "checksum_based"
```

### 5.2 è·¨åŒºåŸŸæœåŠ¡å‘ç°

#### å…¨çƒæœåŠ¡æ³¨å†Œä¸å‘ç°
```yaml
# global_service_discovery.yaml
service_discovery_system:
  global_dns:
    provider: "external-dns + route53"
    ttl: "60s"
    health_check_integration: true
    
  service_mesh:
    istio_multicluster:
      control_plane:
        primary_cluster: us-east-1
        remote_clusters:
          - eu-west-1
          - ap-southeast-1
          
      east_west_traffic:
        mtls_enabled: true
        traffic_encryption: tls_1_3
        authorization_policies: enforced
        
  endpoint_slicing:
    # å¤§è§„æ¨¡é›†ç¾¤Endpointåˆ‡ç‰‡
    max_endpoints_per_slice: 100
    address_type: IPv4
    port_mapping: enabled
    
  load_balancing:
    global_load_balancer:
      algorithm: "least_request"
      locality_weighting: enabled
      failover_priority:
        - "topology.kubernetes.io/region"
        - "topology.kubernetes.io/zone"
        - "kubernetes.io/hostname"
```

---

## 6. è‡ªåŠ¨åŒ–è¿ç»´å¹³å°

### 6.1 GitOpså¤§è§„æ¨¡éƒ¨ç½²

#### ä¼ä¸šçº§GitOpsæµæ°´çº¿
```yaml
# gitops_pipeline.yaml
gitops_platform:
  argocd_enterprise:
    clusters:
      management_cluster:
        server: "https://kubernetes.default.svc"
        namespace: "argocd"
        
      workload_clusters:
        - name: "prod-us-east"
          server: "https://prod-us-east.example.com"
          shard: "us-east"
          
        - name: "prod-eu-west"
          server: "https://prod-eu-west.example.com"
          shard: "eu-west"
          
        - name: "prod-apac"
          server: "https://prod-apac.example.com"
          shard: "apac"
          
    application_sets:
      cluster_bootstrap:
        generator: "cluster"
        template:
          metadata:
            name: "{{name}}-bootstrap"
          spec:
            project: "default"
            source:
              repoURL: "https://github.com/company/infrastructure.git"
              targetRevision: "HEAD"
              path: "clusters/{{name}}"
            destination:
              server: "{{server}}"
              namespace: "kube-system"
              
      tenant_applications:
        generators:
          - git:
              repoURL: "https://github.com/company/applications.git"
              directories:
                - path: "apps/*"
          - matrix:
              generators:
                - clusters: {}
                - git: {}  # åº”ç”¨åˆ—è¡¨
                
    sync_policies:
      automated:
        prune: true
        selfHeal: true
        allowEmpty: false
        
      retry:
        limit: 5
        backoff:
          duration: "5s"
          factor: 2
          maxDuration: "3m"
```

### 6.2 è‡ªæ„ˆç³»ç»Ÿè®¾è®¡

#### æ™ºèƒ½æ•…éšœè‡ªæ„ˆå¹³å°
```yaml
# self_healing_platform.yaml
autonomous_healing_system:
  detection_layer:
    ai_anomaly_detector:
      algorithms:
        - isolation_forest
        - lstm_autoencoder
        - statistical_process_control
      training_data:
        historical_metrics: "90d"
        false_positive_rate: "< 2%"
        
    symptom_correlation:
      temporal_correlation: "10m_window"
      causal_analysis: "bayesian_network"
      root_cause_scoring: "ml_based"
      
  decision_layer:
    healing_playbook_selector:
      playbook_library:
        - name: "pod_restart"
          conditions:
            - symptom: "high_memory_usage"
            - duration: "> 5m"
            - recoverable: true
          action: "kubectl delete pod {{pod_name}}"
          
        - name: "node_drain"
          conditions:
            - symptom: "kernel_panic"
            - scope: "node_level"
            - impact: "critical"
          action: "kubectl drain {{node_name}} --ignore-daemonsets"
          
        - name: "traffic_shift"
          conditions:
            - symptom: "high_error_rate"
            - scope: "service_level"
            - duration: "> 2m"
          action: "istioctl traffic shift {{service}} {{healthy_version}}"
          
    risk_assessment:
      impact_analysis:
        blast_radius: "calculate_affected_services"
        data_loss_potential: "assess_persistence_layers"
        business_impact: "revenue_at_risk_calculation"
        
      approval_workflow:
        low_risk: "automatic_execution"
        medium_risk: "team_lead_approval"
        high_risk: "incident_commander_approval"
        
  execution_layer:
    remediation_executor:
      execution_engine: "ansible_awx"
      rollback_capability: "instant_rollback"
      execution_logging: "full_audit_trail"
      
    validation_framework:
      pre_execution_checks:
        - resource_availability
        - dependency_status
        - safety_constraints
        
      post_execution_validation:
        - service_health_check
        - performance_metrics
        - user_impact_assessment
```

---

## 7. ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 7.1 å¤§è§„æ¨¡ç›‘æ§æ¶æ„

#### åˆ†å¸ƒå¼ç›‘æ§ç³»ç»Ÿè®¾è®¡
```yaml
# distributed_monitoring.yaml
monitoring_architecture:
  global_monitoring_plane:
    components:
      - global_prometheus_federator
      - cross_cluster_alert_aggregator
      - unified_dashboard_portal
      
  regional_monitoring:
    region_us_east:
      prometheus:
        shards: 3
        retention: "90d"
        remote_write_targets:
          - "http://global-prometheus:9090/api/v1/write"
          
      thanos:
        querier: "dedicated_instance"
        store_gateway: "s3_backend"
        ruler: "ha_pair"
        
    region_eu_west:
      prometheus:
        shards: 2
        retention: "60d"
        remote_write_targets:
          - "http://global-prometheus:9090/api/v1/write"
          
      thanos:
        querier: "dedicated_instance"
        store_gateway: "gcs_backend"
        ruler: "ha_pair"
        
  data_aggregation:
    federated_queries:
      global_view:
        query_pattern: "{job=~\".+\", region=~\".+\"}"
        timeout: "30s"
        max_samples: 10000000
        
      regional_view:
        query_pattern: "{job=~\".+\", region=\"${REGION}\"}"
        timeout: "10s"
        max_samples: 1000000
```

### 7.2 å¤§è§„æ¨¡æ—¥å¿—å¤„ç†

#### åˆ†å¸ƒå¼æ—¥å¿—æ¶æ„
```yaml
# distributed_logging.yaml
logging_architecture:
  log_ingestion:
    promtail_agents:
      scrape_configs:
        - job_name: "kubernetes-pods"
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels: ['__meta_kubernetes_pod_annotation_kubernetes_io_created_by']
              action: drop
              regex: '.*cronjob.*'
              
    fluentd_clusters:
      regional_collectors:
        buffer_config:
          '@type': file
          path: /var/log/fluentd/buffer
          flush_mode: interval
          flush_interval: 60s
          chunk_limit_size: 256MB
          
  log_storage:
    loki_distributed:
      ingester:
        replicas: 5
        max_transfer_retries: 0
        lifecycler:
          ring:
            kvstore:
              store: memberlist
              
      distributor:
        replicas: 3
        ring:
          kvstore:
            store: memberlist
            
      querier:
        replicas: 4
        max_concurrent: 2048
        timeout: 10m
        
  log_retention:
    policies:
      application_logs:
        retention: "30d"
        compression: "snappy"
        storage_class: "standard"
        
      security_logs:
        retention: "365d"
        compression: "gzip"
        storage_class: "archive"
        
      debug_logs:
        retention: "7d"
        compression: "none"
        storage_class: "standard"
```

---

## 8. å®‰å…¨ä¸åˆè§„ç®¡ç†

### 8.1 å¤§è§„æ¨¡é›†ç¾¤å®‰å…¨æ¶æ„

#### ä¼ä¸šçº§å®‰å…¨é˜²æŠ¤ä½“ç³»
```yaml
# enterprise_security_framework.yaml
security_architecture:
  zero_trust_network:
    network_policies:
      default_deny:
        ingress: true
        egress: true
        
      micro_segmentation:
        namespace_isolation: enabled
        pod_to_pod_communication: restricted
        service_mesh_mtls: enforced
        
    identity_management:
      oidc_integration:
        provider: "auth0"
        groups_claim: "https://company.com/groups"
        required_claims:
          - email
          - groups
          - exp
          
      rbac_hierarchy:
        cluster_roles:
          - cluster-admin
          - cluster-viewer
          - infrastructure-admin
          
        namespace_roles:
          - app-admin
          - app-developer
          - app-viewer
          
  compliance_automation:
    policy_engines:
      opa_gatekeeper:
        constraint_templates:
          - k8srequiredlabels
          - k8sallowedrepos
          - k8sblockwildcardingress
          
        audit_config:
          audit_interval: "60s"
          constraint_violations_limit: 100
          
      kyverno:
        policy_sets:
          - pod_security_standards
          - best_practices
          - custom_compliance_rules
          
    security_scanning:
      image_scanning:
        trivy_operator:
          scan_interval: "24h"
          severity_threshold: "HIGH"
          ignore_unfixed: true
          
      runtime_security:
        falco:
          rules_files:
            - k8s_audit_rules.yaml
            - syscall_rules.yaml
          outputs:
            - stdout
            - webhook
            - slack
```

### 8.2 å¤§è§„æ¨¡å®¡è®¡ä¸åˆè§„

#### ä¼ä¸šçº§å®¡è®¡ç³»ç»Ÿ
```yaml
# enterprise_auditing.yaml
compliance_auditing_system:
  audit_log_management:
    centralized_auditing:
      audit_sink:
        webhook:
          url: "https://audit.company.com/collect"
          batch_max_size: 100
          batch_max_wait: "10s"
          
      log_retention:
        regulatory_compliance:
          hipaa: "6y"
          sox: "7y"
          pci_dss: "1y"
          
    audit_analysis:
      anomaly_detection:
        behavioral_baselines: "90d_history"
        deviation_threshold: "2_std_dev"
        alert_severity: "medium"
        
      compliance_reporting:
        scheduled_reports:
          daily: "operational_summary"
          weekly: "compliance_status"
          monthly: "regulatory_report"
          
  data_governance:
    data_classification:
      pii_handling:
        encryption_at_rest: "AES-256"
        encryption_in_transit: "TLS-1.3"
        access_logging: "complete_audit"
        
      data_residency:
        geographic_restrictions:
          eu_data: "eu_only_processing"
          us_data: "us_only_processing"
        cross_border_transfer: "approved_mechanisms"
```

---

## é™„å½•

### A. å¤§è§„æ¨¡é›†ç¾¤æœ€ä½³å®è·µæ¸…å•

#### ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•
```yaml
# large_scale_cluster_checklist.yaml
production_readiness_checklist:
  architecture_review:
    - [ ] å¤šåŒºåŸŸéƒ¨ç½²æ¶æ„è®¾è®¡å®Œæˆ
    - [ ] æ•…éšœåŸŸéš”ç¦»ç­–ç•¥æ˜ç¡®
    - [ ] å®¹é‡è§„åˆ’æ–‡æ¡£å®Œæ•´
    - [ ] æ‰©ç¼©å®¹ç­–ç•¥ç»è¿‡éªŒè¯
    
  performance_optimization:
    - [ ] API Serverå‚æ•°è°ƒä¼˜å®Œæˆ
    - [ ] etcdé›†ç¾¤æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡
    - [ ] ç½‘ç»œæ’ä»¶å¤§è§„æ¨¡æ€§èƒ½éªŒè¯
    - [ ] ç›‘æ§ç³»ç»Ÿæ¨ªå‘æ‰©å±•èƒ½åŠ›ç¡®è®¤
    
  security_hardening:
    - [ ] é›¶ä¿¡ä»»ç½‘ç»œç­–ç•¥å®æ–½
    - [ ] å¤šå› å­è®¤è¯å…¨é¢å¯ç”¨
    - [ ] å®‰å…¨æ‰«ææµæ°´çº¿é›†æˆ
    - [ ] åˆè§„æ€§è‡ªåŠ¨åŒ–æ£€æŸ¥å°±ç»ª
    
  disaster_recovery:
    - [ ] è·¨åŒºåŸŸå¤‡ä»½ç­–ç•¥å®æ–½
    - [ ] æ•…éšœåˆ‡æ¢æ¼”ç»ƒå®Œæˆ
    - [ ] æ•°æ®æ¢å¤æ—¶é—´éªŒè¯
    - [ ] ä¸šåŠ¡è¿ç»­æ€§è®¡åˆ’ç¡®è®¤
```

### B. æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡æ¿

#### å¤§è§„æ¨¡é›†ç¾¤æ€§èƒ½æµ‹è¯•
```bash
#!/bin/bash
# large_scale_performance_test.sh

set -euo pipefail

CLUSTER_SIZE=${1:-1000}  # é»˜è®¤1000èŠ‚ç‚¹
TEST_DURATION=${2:-3600}  # é»˜è®¤1å°æ—¶æµ‹è¯•

echo "ğŸš€ å¼€å§‹å¤§è§„æ¨¡é›†ç¾¤æ€§èƒ½æµ‹è¯•..."
echo "é›†ç¾¤è§„æ¨¡: ${CLUSTER_SIZE} èŠ‚ç‚¹"
echo "æµ‹è¯•æ—¶é•¿: ${TEST_DURATION} ç§’"

# 1. é›†ç¾¤å¥åº·æ£€æŸ¥
echo "ğŸ“‹ æ‰§è¡Œé›†ç¾¤å¥åº·æ£€æŸ¥..."
kubectl get nodes --no-headers | wc -l
kubectl get pods --all-namespaces --field-selector=status.phase!=Running | wc -l

# 2. API Serveræ€§èƒ½æµ‹è¯•
echo "âš¡ æµ‹è¯•API Serveræ€§èƒ½..."
hey -z ${TEST_DURATION}s -c 100 -q 10 \
    -H "Authorization: Bearer $(kubectl config view --raw -o jsonpath='{.users[0].user.token}')" \
    "https://$(kubectl config view --raw -o jsonpath='{.clusters[0].cluster.server}')/api/v1/namespaces/default/pods" \
    > /tmp/apiserver-results.txt

# 3. è°ƒåº¦æ€§èƒ½æµ‹è¯•
echo "ğŸ¯ æµ‹è¯•è°ƒåº¦å™¨æ€§èƒ½..."
for i in {1..100}; do
    kubectl run perf-test-$i --image=nginx --replicas=10 \
        --labels="test=performance,iteration=$i" \
        --dry-run=client -o yaml | kubectl apply -f - &
done
wait

# 4. ç½‘ç»œæ€§èƒ½æµ‹è¯•
echo "ğŸŒ æµ‹è¯•ç½‘ç»œæ€§èƒ½..."
kubectl run network-test --image=busybox --command -- sleep 3600
kubectl wait --for=condition=Ready pod/network-test
kubectl exec network-test -- ping -c 10 kubernetes.default

# 5. å­˜å‚¨æ€§èƒ½æµ‹è¯•
echo "ğŸ’¾ æµ‹è¯•å­˜å‚¨æ€§èƒ½..."
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: perf-test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: fast-ssd
EOF

# 6. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
echo "ğŸ“Š ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š..."
cat > /tmp/performance-report.md <<EOF
# å¤§è§„æ¨¡é›†ç¾¤æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•ç¯å¢ƒ
- é›†ç¾¤è§„æ¨¡: ${CLUSTER_SIZE} èŠ‚ç‚¹
- æµ‹è¯•æ—¶é—´: $(date)
- Kubernetesç‰ˆæœ¬: $(kubectl version --short | grep Server | awk '{print $3}')

## æ€§èƒ½æŒ‡æ ‡
$(cat /tmp/apiserver-results.txt)

## èµ„æºä½¿ç”¨æƒ…å†µ
$(kubectl top nodes | head -20)

## è°ƒåº¦å»¶è¿Ÿç»Ÿè®¡
$(kubectl get pods --selector=test=performance -o jsonpath='{range .items[*]}{.metadata.creationTimestamp}{" "}{.metadata.name}{"\n"}{end}' | \
  sort | head -10)
EOF

echo "âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜åœ¨ /tmp/performance-report.md"
```

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ | **ä¸“å®¶è¯„å®¡**: å·²é€šè¿‡ | **æœ€åæ›´æ–°**: 2026-02 | **é€‚ç”¨åœºæ™¯**: è¶…å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ
# 05 - Pod Pending çŠ¶æ€æ·±åº¦è¯Šæ–­ (Pod Pending Diagnosis)

> **é€‚ç”¨ç‰ˆæœ¬**: v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-01 | **éš¾åº¦**: ä¸­çº§-é«˜çº§ | **å‚è€ƒ**: [Kubernetes Scheduling](https://kubernetes.io/docs/concepts/scheduling-eviction/)

---

## ç›¸å…³æ–‡æ¡£äº¤å‰å¼•ç”¨

### ğŸ”— å…³è”æ•…éšœæ’æŸ¥æ–‡æ¡£
- **[06-Node NotReadyè¯Šæ–­](./06-node-notready-diagnosis.md)** - èŠ‚ç‚¹çŠ¶æ€å¼‚å¸¸æ˜¯Pod Pendingçš„å¸¸è§åŸå› 
- **[07-OOMå†…å­˜è¯Šæ–­](./07-oom-memory-diagnosis.md)** - å†…å­˜ä¸è¶³å¯¼è‡´çš„Podè°ƒåº¦å¤±è´¥
- **[14-PVCå­˜å‚¨æ•…éšœæ’æŸ¥](./14-pvc-storage-troubleshooting.md)** - å­˜å‚¨å·ç»‘å®šé—®é¢˜å½±å“Podå¯åŠ¨
- **[24-Quota/LimitRangeæ•…éšœæ’æŸ¥](./24-quota-limitrange-troubleshooting.md)** - èµ„æºé…é¢é™åˆ¶å¯¼è‡´è°ƒåº¦å¤±è´¥
- **[25-ç½‘ç»œè¿é€šæ€§æ•…éšœæ’æŸ¥](./25-network-connectivity-troubleshooting.md)** - ç½‘ç»œç­–ç•¥å¯èƒ½é˜»æ­¢Podè°ƒåº¦

### ğŸ“š æ‰©å±•å­¦ä¹ èµ„æ–™
- **[Kubernetesè°ƒåº¦å™¨åŸç†](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/)** - æ·±å…¥ç†è§£è°ƒåº¦æœºåˆ¶
- **[Taintså’ŒTolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)** - èŠ‚ç‚¹æ±¡ç‚¹å’Œå®¹å¿åº¦é…ç½®
- **[äº²å’Œæ€§å’Œåäº²å’Œæ€§](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)** - é«˜çº§è°ƒåº¦ç­–ç•¥

---

## ç›®å½•

1. [æ¦‚è¿°ä¸è¯Šæ–­æ¡†æ¶](#1-æ¦‚è¿°ä¸è¯Šæ–­æ¡†æ¶)
2. [è¯Šæ–­å†³ç­–æ ‘](#2-è¯Šæ–­å†³ç­–æ ‘)
3. [èµ„æºç±»é—®é¢˜æ·±åº¦è¯Šæ–­](#3-èµ„æºç±»é—®é¢˜æ·±åº¦è¯Šæ–­)
4. [èŠ‚ç‚¹é€‰æ‹©é—®é¢˜è¯Šæ–­](#4-èŠ‚ç‚¹é€‰æ‹©é—®é¢˜è¯Šæ–­)
5. [å­˜å‚¨é—®é¢˜è¯Šæ–­](#5-å­˜å‚¨é—®é¢˜è¯Šæ–­)
6. [é…é¢ä¸å‡†å…¥æ§åˆ¶](#6-é…é¢ä¸å‡†å…¥æ§åˆ¶)
7. [è°ƒåº¦å™¨é—®é¢˜è¯Šæ–­](#7-è°ƒåº¦å™¨é—®é¢˜è¯Šæ–­)
8. [é«˜çº§è°ƒåº¦åœºæ™¯](#8-é«˜çº§è°ƒåº¦åœºæ™¯)
9. [ACK/äº‘ç¯å¢ƒç‰¹å®šé—®é¢˜](#9-ackäº‘ç¯å¢ƒç‰¹å®šé—®é¢˜)
10. [è‡ªåŠ¨åŒ–è¯Šæ–­å·¥å…·](#10-è‡ªåŠ¨åŒ–è¯Šæ–­å·¥å…·)
11. [ç›‘æ§å‘Šè­¦é…ç½®](#11-ç›‘æ§å‘Šè­¦é…ç½®)
12. [ç´§æ€¥å¤„ç†æµç¨‹](#12-ç´§æ€¥å¤„ç†æµç¨‹)
13. [ç‰ˆæœ¬ç‰¹å®šå˜æ›´](#13-ç‰ˆæœ¬ç‰¹å®šå˜æ›´)
14. [å¤šè§’è‰²è§†è§’](#14-å¤šè§’è‰²è§†è§’)
15. [æœ€ä½³å®è·µ](#15-æœ€ä½³å®è·µ)

---

## 1. æ¦‚è¿°ä¸è¯Šæ–­æ¡†æ¶

### 1.1 Pod Pending çŠ¶æ€å®šä¹‰

Pod å¤„äº Pending çŠ¶æ€è¡¨ç¤º Pod å·²è¢« Kubernetes API Server æ¥å—ï¼Œä½†å°šæœªè¢«è°ƒåº¦åˆ°èŠ‚ç‚¹æˆ–å®¹å™¨é•œåƒå°šæœªæ‹‰å–ã€‚

| é˜¶æ®µ | çŠ¶æ€ | è¯´æ˜ | è¯Šæ–­å…¥å£ |
|------|------|------|---------|
| **è°ƒåº¦å‰** | Pending (æ—  nodeName) | ç­‰å¾…è°ƒåº¦å™¨åˆ†é…èŠ‚ç‚¹ | `kubectl describe pod` Events |
| **è°ƒåº¦å** | Pending (æœ‰ nodeName) | å·²åˆ†é…èŠ‚ç‚¹ï¼Œç­‰å¾…å®¹å™¨å¯åŠ¨ | `kubectl describe pod` Conditions |
| **é•œåƒæ‹‰å–** | Pending + ImagePullBackOff | é•œåƒæ‹‰å–å¤±è´¥ | å®¹å™¨è¿è¡Œæ—¶æ—¥å¿— |
| **åˆå§‹åŒ–** | Pending + Init:X/Y | Init å®¹å™¨æœªå®Œæˆ | Init å®¹å™¨æ—¥å¿— |

### 1.2 è¯Šæ–­ä¼˜å…ˆçº§çŸ©é˜µ

| ç´§æ€¥ç¨‹åº¦ | Pending æ—¶é•¿ | å½±å“èŒƒå›´ | å“åº”æ—¶é—´ | å¤„ç†ä¼˜å…ˆçº§ |
|---------|-------------|---------|---------|-----------|
| **P0 - ç´§æ€¥** | > 30min | ç”Ÿäº§æ ¸å¿ƒæœåŠ¡ | < 15min | ç«‹å³å¤„ç† |
| **P1 - é«˜** | > 15min | ç”Ÿäº§æœåŠ¡ | < 30min | ä¼˜å…ˆå¤„ç† |
| **P2 - ä¸­** | > 5min | éæ ¸å¿ƒæœåŠ¡ | < 2h | è®¡åˆ’å¤„ç† |
| **P3 - ä½** | < 5min | å¼€å‘/æµ‹è¯• | < 24h | å¸¸è§„å¤„ç† |

### 1.3 è°ƒåº¦æµç¨‹æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Kubernetes Pod Scheduling Pipeline                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                                       â”‚
â”‚  â”‚  Pod Create  â”‚                                                                       â”‚
â”‚  â”‚  (API Server)â”‚                                                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                                       â”‚
â”‚         â”‚                                                                               â”‚
â”‚         â–¼                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        Scheduling Queue (ä¼˜å…ˆçº§é˜Ÿåˆ—)                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚ activeQ       â”‚  â”‚ backoffQ      â”‚  â”‚ unschedulableQâ”‚  â”‚ gatedQ(v1.28+)â”‚    â”‚   â”‚
â”‚  â”‚  â”‚ (å¾…è°ƒåº¦)      â”‚  â”‚ (é€€é¿é‡è¯•)    â”‚  â”‚ (ä¸å¯è°ƒåº¦)     â”‚  â”‚ (è°ƒåº¦é—¨æ§)    â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                               â”‚                                         â”‚
â”‚                                               â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                         Scheduling Cycle (è°ƒåº¦å‘¨æœŸ)                              â”‚   â”‚
â”‚  â”‚                                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  PreFilter   â”‚â”€â”€â”€â–¶â”‚    Filter    â”‚â”€â”€â”€â–¶â”‚  PostFilter  â”‚â”€â”€â”€â–¶â”‚   PreScore   â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  é¢„è¿‡æ»¤       â”‚    â”‚    è¿‡æ»¤      â”‚    â”‚  åè¿‡æ»¤/æŠ¢å  â”‚    â”‚   é¢„è¯„åˆ†     â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚         â”‚                   â”‚                   â”‚                   â”‚           â”‚   â”‚
â”‚  â”‚         â”‚                   â”‚                   â”‚                   â”‚           â”‚   â”‚
â”‚  â”‚         â–¼                   â–¼                   â–¼                   â–¼           â”‚   â”‚
â”‚  â”‚  - èµ„æºéœ€æ±‚æ£€æŸ¥       - NodeAffinity      - è§¦å‘æŠ¢å           - å‡†å¤‡è¯„åˆ†çŠ¶æ€  â”‚   â”‚
â”‚  â”‚  - ç«¯å£å†²çªæ£€æŸ¥       - Taints/Tolerations - é€‰æ‹©ç‰ºç‰²è€…                        â”‚   â”‚
â”‚  â”‚  - å·æ‹“æ‰‘æ£€æŸ¥         - NodeResourcesFit                                       â”‚   â”‚
â”‚  â”‚                       - PodTopologySpread                                       â”‚   â”‚
â”‚  â”‚                                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚   â”‚
â”‚  â”‚  â”‚    Score     â”‚â”€â”€â”€â–¶â”‚ NormalizeScoreâ”‚â”€â”€â”€â–¶â”‚   Reserve    â”‚                      â”‚   â”‚
â”‚  â”‚  â”‚    è¯„åˆ†      â”‚    â”‚  å½’ä¸€åŒ–è¯„åˆ†   â”‚    â”‚   é¢„ç•™èµ„æº   â”‚                      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚   â”‚
â”‚  â”‚         â”‚                   â”‚                   â”‚                               â”‚   â”‚
â”‚  â”‚         â–¼                   â–¼                   â–¼                               â”‚   â”‚
â”‚  â”‚  - ImageLocality      - åŠ æƒæ±‡æ€»          - é”å®šèŠ‚ç‚¹èµ„æº                        â”‚   â”‚
â”‚  â”‚  - NodeAffinity       - é€‰æ‹©æœ€é«˜åˆ†èŠ‚ç‚¹    - é˜²æ­¢å¹¶å‘è°ƒåº¦å†²çª                    â”‚   â”‚
â”‚  â”‚  - BalancedAllocation                                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                               â”‚                                         â”‚
â”‚                                               â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                          Binding Cycle (ç»‘å®šå‘¨æœŸ - å¼‚æ­¥)                         â”‚   â”‚
â”‚  â”‚                                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚    Permit    â”‚â”€â”€â”€â–¶â”‚   PreBind    â”‚â”€â”€â”€â–¶â”‚     Bind     â”‚â”€â”€â”€â–¶â”‚   PostBind   â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   è®¸å¯/ç­‰å¾…  â”‚    â”‚   ç»‘å®šå‰     â”‚    â”‚     ç»‘å®š     â”‚    â”‚    ç»‘å®šå    â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚         â”‚                   â”‚                   â”‚                   â”‚           â”‚   â”‚
â”‚  â”‚         â–¼                   â–¼                   â–¼                   â–¼           â”‚   â”‚
â”‚  â”‚  - Gangè°ƒåº¦ç­‰å¾…       - PVCç»‘å®š           - æ›´æ–°Pod.nodeName  - æ¸…ç†çŠ¶æ€      â”‚   â”‚
â”‚  â”‚  - èµ„æºé”å®š           - å­˜å‚¨é¢„ç•™          - é€šçŸ¥kubelet                        â”‚   â”‚
â”‚  â”‚                                                                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                             è°ƒåº¦å¤±è´¥å¤„ç†                                         â”‚   â”‚
â”‚  â”‚  Filter é˜¶æ®µå¤±è´¥ â”€â”€â–¶ è®°å½• FailedScheduling äº‹ä»¶                                 â”‚   â”‚
â”‚  â”‚  PostFilter æŠ¢å å¤±è´¥ â”€â”€â–¶ ç§»å…¥ unschedulableQ                                    â”‚   â”‚
â”‚  â”‚  Binding å¤±è´¥ â”€â”€â–¶ ç§»å…¥ backoffQ é‡è¯•                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. è¯Šæ–­å†³ç­–æ ‘

### 2.1 å¿«é€Ÿè¯Šæ–­æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Pod Pending å¿«é€Ÿè¯Šæ–­å†³ç­–æ ‘                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚  Pod å¤„äº Pending çŠ¶æ€                                                              â”‚
â”‚          â”‚                                                                           â”‚
â”‚          â–¼                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Step 1: kubectl describe pod <name> -n <ns>                         â”‚            â”‚
â”‚  â”‚         æŸ¥çœ‹ Events å’Œ Conditions                                    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚          â”‚                                                                           â”‚
â”‚          â–¼                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Step 2: æ£€æŸ¥ spec.nodeName æ˜¯å¦å·²åˆ†é…                               â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚          â”‚                                                                           â”‚
â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚          â”‚ (æœªåˆ†é… - è°ƒåº¦é—®é¢˜)                       â”‚ (å·²åˆ†é… - èŠ‚ç‚¹å¯åŠ¨é—®é¢˜)       â”‚
â”‚          â–¼                                          â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ æŸ¥çœ‹è°ƒåº¦å¤±è´¥åŸå›   â”‚                     â”‚ æ£€æŸ¥ kubelet/å®¹å™¨è¿è¡Œæ—¶  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                          è°ƒåº¦å¤±è´¥åŸå› åˆ†ç±»                            â”‚            â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”‚
â”‚  â”‚                                                                      â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚  â”‚  â”‚ Insufficient cpu/memory/ephemeral-storage                     â”‚  â”‚            â”‚
â”‚  â”‚  â”‚ â”€â”€â–¶ èµ„æºç±»é—®é¢˜ â”€â”€â–¶ è·³è½¬ç¬¬3ç«                                   â”‚  â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚  â”‚                                                                      â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚  â”‚  â”‚ node(s) didn't match Pod's node selector/affinity            â”‚  â”‚            â”‚
â”‚  â”‚  â”‚ node(s) had taint that pod didn't tolerate                   â”‚  â”‚            â”‚
â”‚  â”‚  â”‚ â”€â”€â–¶ èŠ‚ç‚¹é€‰æ‹©é—®é¢˜ â”€â”€â–¶ è·³è½¬ç¬¬4ç«                                 â”‚  â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚  â”‚                                                                      â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚  â”‚  â”‚ pod has unbound immediate PersistentVolumeClaims             â”‚  â”‚            â”‚
â”‚  â”‚  â”‚ volume node affinity conflict                                â”‚  â”‚            â”‚
â”‚  â”‚  â”‚ â”€â”€â–¶ å­˜å‚¨é—®é¢˜ â”€â”€â–¶ è·³è½¬ç¬¬5ç«                                     â”‚  â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚  â”‚                                                                      â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚  â”‚  â”‚ exceeded quota / forbidden by LimitRange                     â”‚  â”‚            â”‚
â”‚  â”‚  â”‚ â”€â”€â–¶ é…é¢é—®é¢˜ â”€â”€â–¶ è·³è½¬ç¬¬6ç«                                     â”‚  â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚  â”‚                                                                      â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚  â”‚  â”‚ pod topology spread constraints not satisfied                â”‚  â”‚            â”‚
â”‚  â”‚  â”‚ â”€â”€â–¶ æ‹“æ‰‘åˆ†å¸ƒé—®é¢˜ â”€â”€â–¶ è·³è½¬ç¬¬8ç«                                 â”‚  â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚  â”‚                                                                      â”‚            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚  â”‚  â”‚ æ— äº‹ä»¶ / è°ƒåº¦å™¨ä¸å¯ç”¨                                         â”‚  â”‚            â”‚
â”‚  â”‚  â”‚ â”€â”€â–¶ è°ƒåº¦å™¨é—®é¢˜ â”€â”€â–¶ è·³è½¬ç¬¬7ç«                                   â”‚  â”‚            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚  â”‚                                                                      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 å¸¸è§ FailedScheduling æ¶ˆæ¯é€ŸæŸ¥

| é”™è¯¯æ¶ˆæ¯ | åŸå› ç±»åˆ« | å¿«é€Ÿè¯Šæ–­ | è§£å†³æ–¹æ¡ˆ |
|---------|---------|---------|---------|
| `Insufficient cpu` | èµ„æºä¸è¶³ | `kubectl describe nodes \| grep -A5 Allocated` | æ‰©å®¹èŠ‚ç‚¹/è°ƒæ•´requests |
| `Insufficient memory` | èµ„æºä¸è¶³ | `kubectl top nodes` | æ‰©å®¹èŠ‚ç‚¹/è°ƒæ•´requests |
| `Insufficient nvidia.com/gpu` | GPUä¸è¶³ | `kubectl get nodes -l gpu=true` | æ·»åŠ GPUèŠ‚ç‚¹ |
| `node(s) didn't match Pod's node selector` | èŠ‚ç‚¹é€‰æ‹© | `kubectl get nodes --show-labels` | ä¿®æ­£æ ‡ç­¾/é€‰æ‹©å™¨ |
| `node(s) didn't match Pod's node affinity/selector` | äº²å’Œæ€§ | æ£€æŸ¥ `spec.affinity` | è°ƒæ•´äº²å’Œæ€§è§„åˆ™ |
| `node(s) had taint {key} that the pod didn't tolerate` | æ±¡ç‚¹å®¹å¿ | `kubectl describe node \| grep Taint` | æ·»åŠ tolerations |
| `node(s) had untolerated taint {node.kubernetes.io/not-ready}` | èŠ‚ç‚¹å¼‚å¸¸ | `kubectl get nodes` | ä¿®å¤èŠ‚ç‚¹çŠ¶æ€ |
| `pod has unbound immediate PersistentVolumeClaims` | PVCæœªç»‘å®š | `kubectl get pvc` | åˆ›å»ºPV/ä¿®å¤SC |
| `volume node affinity conflict` | å·æ‹“æ‰‘å†²çª | æ£€æŸ¥PV nodeAffinity | è°ƒæ•´å·é…ç½® |
| `pod topology spread constraints not satisfied` | æ‹“æ‰‘åˆ†å¸ƒ | æ£€æŸ¥topologySpreadConstraints | è°ƒæ•´maxSkew |
| `Too many pods` | Podæ•°é‡é™åˆ¶ | `kubectl describe node \| grep -i pods` | æ‰©å®¹èŠ‚ç‚¹ |
| `exceeded quota` | é…é¢è¶…é™ | `kubectl describe quota` | è°ƒæ•´é…é¢ |
| `no preemption victims found` | æŠ¢å å¤±è´¥ | æ£€æŸ¥PriorityClass | è°ƒæ•´ä¼˜å…ˆçº§ |

---

## 3. èµ„æºç±»é—®é¢˜æ·±åº¦è¯Šæ–­

### 3.1 èµ„æºä¸è¶³åŸå› åˆ†ç±»

| èµ„æºç±»å‹ | æ£€æŸ¥å‘½ä»¤ | ç‰ˆæœ¬å˜åŒ– | ACKç‰¹æ®Šå¤„ç† |
|---------|---------|---------|------------|
| **CPU** | `kubectl describe node \| grep -A5 "Allocated"` | ç¨³å®š | æ”¯æŒå¼¹æ€§è°ƒåº¦ |
| **Memory** | `kubectl top nodes` | ç¨³å®š | å†…å­˜è¶…å”®é…ç½® |
| **Ephemeral Storage** | `kubectl describe node \| grep ephemeral` | v1.25+å¢å¼º | äº‘ç›˜è‡ªåŠ¨æ‰©å®¹ |
| **GPU** | `kubectl describe node \| grep nvidia.com/gpu` | ç¨³å®š | GPUå…±äº«(cGPU) |
| **Extended Resources** | `kubectl get node -o json \| jq '.status.capacity'` | ç¨³å®š | è‡ªå®šä¹‰èµ„æº |
| **Hugepages** | `kubectl describe node \| grep hugepages` | v1.27+ç¨³å®š | å¤§é¡µå†…å­˜é…ç½® |
| **Pods** | `kubectl describe node \| grep "Pods:"` | ç¨³å®š | maxPodsé…ç½® |

### 3.2 é›†ç¾¤èµ„æºåˆ†æ

```bash
#!/bin/bash
# cluster-resource-analysis.sh - é›†ç¾¤èµ„æºæ·±åº¦åˆ†æ

echo "=============================================="
echo "  é›†ç¾¤èµ„æºåˆ†ææŠ¥å‘Š - $(date)"
echo "=============================================="

echo ""
echo "=== 1. èŠ‚ç‚¹èµ„æºæ€»è§ˆ ==="
kubectl get nodes -o custom-columns=\
'NAME:.metadata.name,STATUS:.status.conditions[?(@.type=="Ready")].status,CPU-ALLOC:.status.allocatable.cpu,MEM-ALLOC:.status.allocatable.memory,PODS-ALLOC:.status.allocatable.pods'

echo ""
echo "=== 2. èŠ‚ç‚¹å®æ—¶ä½¿ç”¨ç‡ ==="
kubectl top nodes --use-protocol-buffers 2>/dev/null || kubectl top nodes

echo ""
echo "=== 3. å„èŠ‚ç‚¹èµ„æºåˆ†é…è¯¦æƒ… ==="
for node in $(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do
  echo ""
  echo "--- Node: $node ---"
  kubectl describe node $node | grep -A 10 "Allocated resources:"
done

echo ""
echo "=== 4. èµ„æºè¯·æ±‚ Top 10 Pod (CPU) ==="
kubectl get pods -A -o json | jq -r '
  .items[] | 
  select(.status.phase=="Running") |
  {namespace: .metadata.namespace, name: .metadata.name, cpu: (.spec.containers[].resources.requests.cpu // "0")} |
  "\(.namespace)/\(.name): \(.cpu)"
' | sort -t: -k2 -rn | head -10

echo ""
echo "=== 5. èµ„æºè¯·æ±‚ Top 10 Pod (Memory) ==="
kubectl get pods -A -o json | jq -r '
  .items[] | 
  select(.status.phase=="Running") |
  {namespace: .metadata.namespace, name: .metadata.name, mem: (.spec.containers[].resources.requests.memory // "0")} |
  "\(.namespace)/\(.name): \(.mem)"
' | sort -t: -k2 -rh | head -10

echo ""
echo "=== 6. èŠ‚ç‚¹èµ„æºå‹åŠ›çŠ¶æ€ ==="
kubectl get nodes -o json | jq -r '
  .items[] | 
  "\(.metadata.name): MemoryPressure=\(.status.conditions[] | select(.type=="MemoryPressure") | .status), DiskPressure=\(.status.conditions[] | select(.type=="DiskPressure") | .status), PIDPressure=\(.status.conditions[] | select(.type=="PIDPressure") | .status)"
'

echo ""
echo "=== 7. å¯ç”¨èµ„æºæ±‡æ€» ==="
echo "å¯åˆ†é… CPU æ€»é‡:"
kubectl get nodes -o json | jq '[.items[].status.allocatable.cpu | gsub("m"; "") | tonumber] | add'
echo ""
echo "å·²è¯·æ±‚ CPU æ€»é‡:"
kubectl get pods -A -o json | jq '[.items[] | select(.status.phase=="Running") | .spec.containers[].resources.requests.cpu // "0" | gsub("m"; "") | if . == "0" then 0 else tonumber end] | add'

echo ""
echo "=== 8. GPU èµ„æºçŠ¶æ€ (å¦‚æœ‰) ==="
kubectl get nodes -o json | jq -r '
  .items[] | 
  select(.status.allocatable["nvidia.com/gpu"] != null) |
  "\(.metadata.name): GPU å¯åˆ†é…=\(.status.allocatable["nvidia.com/gpu"]), å·²åˆ†é…=\(.status.capacity["nvidia.com/gpu"])"
'

echo ""
echo "=== 9. Pending Pod èµ„æºéœ€æ±‚æ±‡æ€» ==="
kubectl get pods -A --field-selector=status.phase=Pending -o json | jq -r '
  .items[] | 
  "\(.metadata.namespace)/\(.metadata.name): CPU=\(.spec.containers[].resources.requests.cpu // "æœªè®¾ç½®"), Memory=\(.spec.containers[].resources.requests.memory // "æœªè®¾ç½®")"
'
```

### 3.3 èµ„æºä¸è¶³è§£å†³æ–¹æ¡ˆ

| è§£å†³æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ | æ“ä½œå¤æ‚åº¦ | ç”Ÿæ•ˆæ—¶é—´ | é£é™©ç­‰çº§ |
|---------|---------|-----------|---------|---------|
| **è°ƒæ•´Pod requests** | èµ„æºè¯·æ±‚è¿‡å¤§ | ä½ | ç«‹å³ | ä½ |
| **é©±é€ä½ä¼˜å…ˆçº§Pod** | ç´§æ€¥é‡Šæ”¾èµ„æº | ä¸­ | ç«‹å³ | ä¸­ |
| **èŠ‚ç‚¹æ‰©å®¹** | é•¿æœŸèµ„æºä¸è¶³ | ä¸­ | 5-10min | ä½ |
| **Cluster Autoscaler** | è‡ªåŠ¨æ‰©ç¼©å®¹ | é«˜(åˆæ¬¡é…ç½®) | è‡ªåŠ¨ | ä½ |
| **VPAè‡ªåŠ¨è°ƒæ•´** | åŠ¨æ€èµ„æºè°ƒæ•´ | ä¸­ | æŒ‰ç­–ç•¥ | ä¸­ |
| **èµ„æºè¶…å”®é…ç½®** | æé«˜åˆ©ç”¨ç‡ | ä¸­ | ç«‹å³ | ä¸­ |

```yaml
# èµ„æºä¼˜åŒ–é…ç½®ç¤ºä¾‹
---
# æ–¹æ¡ˆ1: åˆç†è®¾ç½®èµ„æºè¯·æ±‚ (åŸºäºå®é™…ä½¿ç”¨é‡)
apiVersion: v1
kind: Pod
metadata:
  name: optimized-app
spec:
  containers:
  - name: app
    image: myapp:latest
    resources:
      requests:
        cpu: 100m       # åŸºäº P95 ä½¿ç”¨é‡
        memory: 256Mi   # åŸºäºç¨³æ€ä½¿ç”¨é‡ + 20%ç¼“å†²
      limits:
        cpu: 500m       # requests çš„ 2-5 å€
        memory: 512Mi   # requests çš„ 1.5-2 å€
---
# æ–¹æ¡ˆ2: VPA è‡ªåŠ¨è°ƒæ•´ (v1.25+)
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: myapp-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: myapp
  updatePolicy:
    updateMode: Auto  # æˆ– "Off" ä»…æ¨è
  resourcePolicy:
    containerPolicies:
    - containerName: '*'
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 4
        memory: 8Gi
      controlledResources: ["cpu", "memory"]
---
# æ–¹æ¡ˆ3: PriorityClass é…ç½®æŠ¢å 
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
preemptionPolicy: PreemptLowerPriority
globalDefault: false
description: "é«˜ä¼˜å…ˆçº§ä¸šåŠ¡Podï¼Œå¯æŠ¢å ä½ä¼˜å…ˆçº§Pod"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority-batch
value: -100
preemptionPolicy: Never  # ä¸è§¦å‘æŠ¢å 
globalDefault: false
description: "ä½ä¼˜å…ˆçº§æ‰¹å¤„ç†ä»»åŠ¡"
```

---

## 4. èŠ‚ç‚¹é€‰æ‹©é—®é¢˜è¯Šæ–­

### 4.1 èŠ‚ç‚¹é€‰æ‹©æœºåˆ¶æ€»è§ˆ

| æœºåˆ¶ | ä½œç”¨ | ç¡¬/è½¯çº¦æŸ | ç‰ˆæœ¬çŠ¶æ€ |
|------|------|----------|---------|
| **nodeSelector** | ç®€å•æ ‡ç­¾åŒ¹é… | ç¡¬çº¦æŸ | ç¨³å®š |
| **nodeAffinity** | é«˜çº§èŠ‚ç‚¹äº²å’Œæ€§ | ç¡¬/è½¯ | ç¨³å®š |
| **podAffinity** | Podé—´äº²å’Œæ€§ | ç¡¬/è½¯ | ç¨³å®š |
| **podAntiAffinity** | Podé—´åäº²å’Œæ€§ | ç¡¬/è½¯ | ç¨³å®š |
| **Taints/Tolerations** | æ±¡ç‚¹å®¹å¿ | ç¡¬çº¦æŸ | ç¨³å®š |
| **topologySpreadConstraints** | æ‹“æ‰‘åˆ†å¸ƒ | ç¡¬/è½¯ | v1.19+ GA |
| **nodeName** | ç›´æ¥æŒ‡å®šèŠ‚ç‚¹ | ç¡¬çº¦æŸ | ç¨³å®š |

### 4.2 èŠ‚ç‚¹é€‰æ‹©è¯Šæ–­å‘½ä»¤

```bash
#!/bin/bash
# node-selection-diagnose.sh - èŠ‚ç‚¹é€‰æ‹©é—®é¢˜è¯Šæ–­

POD_NAME=$1
NAMESPACE=${2:-default}

echo "=============================================="
echo "  èŠ‚ç‚¹é€‰æ‹©é—®é¢˜è¯Šæ–­: $NAMESPACE/$POD_NAME"
echo "=============================================="

echo ""
echo "=== 1. Pod èŠ‚ç‚¹é€‰æ‹©é…ç½® ==="
echo "--- nodeSelector ---"
kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.nodeSelector}' 2>/dev/null | jq . || echo "(æœªé…ç½®)"

echo ""
echo "--- nodeAffinity ---"
kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.affinity.nodeAffinity}' 2>/dev/null | jq . || echo "(æœªé…ç½®)"

echo ""
echo "--- podAffinity ---"
kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.affinity.podAffinity}' 2>/dev/null | jq . || echo "(æœªé…ç½®)"

echo ""
echo "--- podAntiAffinity ---"
kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.affinity.podAntiAffinity}' 2>/dev/null | jq . || echo "(æœªé…ç½®)"

echo ""
echo "--- topologySpreadConstraints ---"
kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.topologySpreadConstraints}' 2>/dev/null | jq . || echo "(æœªé…ç½®)"

echo ""
echo "--- tolerations ---"
kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.tolerations}' 2>/dev/null | jq . || echo "(æœªé…ç½®)"

echo ""
echo "=== 2. é›†ç¾¤èŠ‚ç‚¹çŠ¶æ€ ==="
kubectl get nodes -o custom-columns=\
'NAME:.metadata.name,STATUS:.status.conditions[?(@.type=="Ready")].status,SCHEDULABLE:.spec.unschedulable,VERSION:.status.nodeInfo.kubeletVersion'

echo ""
echo "=== 3. èŠ‚ç‚¹æ ‡ç­¾ ==="
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): \(.metadata.labels | to_entries | map("\(.key)=\(.value)") | join(", "))"'

echo ""
echo "=== 4. èŠ‚ç‚¹æ±¡ç‚¹ ==="
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): \(.spec.taints // [] | map("\(.key)=\(.value):\(.effect)") | join(", "))"'

echo ""
echo "=== 5. å¯ç”¨åŒºåˆ†å¸ƒ ==="
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): zone=\(.metadata.labels["topology.kubernetes.io/zone"] // "N/A"), region=\(.metadata.labels["topology.kubernetes.io/region"] // "N/A")"'

echo ""
echo "=== 6. èŠ‚ç‚¹åŒ¹é…åˆ†æ ==="
# è·å– Pod çš„ nodeSelector
NODE_SELECTOR=$(kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.nodeSelector}' 2>/dev/null)
if [ -n "$NODE_SELECTOR" ] && [ "$NODE_SELECTOR" != "{}" ]; then
    echo "NodeSelector: $NODE_SELECTOR"
    echo "åŒ¹é…çš„èŠ‚ç‚¹:"
    # å°† nodeSelector è½¬æ¢ä¸º -l å‚æ•°
    LABEL_SELECTOR=$(echo $NODE_SELECTOR | jq -r 'to_entries | map("\(.key)=\(.value)") | join(",")')
    kubectl get nodes -l "$LABEL_SELECTOR" 2>/dev/null || echo "  (æ— åŒ¹é…èŠ‚ç‚¹)"
else
    echo "NodeSelector: æœªé…ç½®ï¼Œæ‰€æœ‰èŠ‚ç‚¹å¯é€‰"
fi
```

### 4.3 å¸¸è§èŠ‚ç‚¹é€‰æ‹©é—®é¢˜è§£å†³

```yaml
# èŠ‚ç‚¹é€‰æ‹©é—®é¢˜è§£å†³æ–¹æ¡ˆé›†

---
# åœºæ™¯1: æ·»åŠ ç¼ºå¤±çš„èŠ‚ç‚¹æ ‡ç­¾
# kubectl label node <node-name> app-tier=frontend

---
# åœºæ™¯2: ä½¿ç”¨è½¯äº²å’Œæ€§é¿å…è°ƒåº¦å¤±è´¥
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flexible-app
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          # ç¡¬çº¦æŸ: å¿…é¡»åœ¨ç”Ÿäº§èŠ‚ç‚¹
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values: ["production"]
          # è½¯çº¦æŸ: ä¼˜å…ˆé€‰æ‹©SSDèŠ‚ç‚¹
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: disk-type
                operator: In
                values: ["ssd"]
          - weight: 20
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values: ["cn-hangzhou-h"]

---
# åœºæ™¯3: æ·»åŠ æ±¡ç‚¹å®¹å¿
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tolerate-all
spec:
  template:
    spec:
      tolerations:
      # å®¹å¿ master èŠ‚ç‚¹æ±¡ç‚¹ (éç”Ÿäº§ç¯å¢ƒ)
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      # å®¹å¿èŠ‚ç‚¹å‹åŠ›æ±¡ç‚¹
      - key: node.kubernetes.io/memory-pressure
        operator: Exists
        effect: NoSchedule
      # å®¹å¿è‡ªå®šä¹‰æ±¡ç‚¹
      - key: dedicated
        operator: Equal
        value: gpu
        effect: NoSchedule

---
# åœºæ™¯4: ç§»é™¤èŠ‚ç‚¹æ±¡ç‚¹ (è¿ç»´æ“ä½œ)
# kubectl taint node <node-name> <key>:<effect>-
# ç¤ºä¾‹: kubectl taint node node1 dedicated=gpu:NoSchedule-
```

---

## 5. å­˜å‚¨é—®é¢˜è¯Šæ–­

### 5.1 å­˜å‚¨é—®é¢˜åˆ†ç±»

| é—®é¢˜ç±»å‹ | é”™è¯¯æ¶ˆæ¯ | è¯Šæ–­æ–¹å‘ | ç‰ˆæœ¬ç‰¹æ€§ |
|---------|---------|---------|---------|
| **PVCæœªç»‘å®š** | `pod has unbound immediate PersistentVolumeClaims` | æ£€æŸ¥PVCçŠ¶æ€ã€SCé…ç½® | ç¨³å®š |
| **å·æ‹“æ‰‘å†²çª** | `volume node affinity conflict` | æ£€æŸ¥PV nodeAffinity | v1.17+ |
| **åŠ¨æ€ä¾›åº”å¤±è´¥** | PVC Pendingæ— äº‹ä»¶ | æ£€æŸ¥CSIé©±åŠ¨ã€SCå‚æ•° | ç¨³å®š |
| **WaitForFirstConsumer** | PVC Pendingç­‰å¾…Pod | æ­£å¸¸è¡Œä¸ºï¼Œç­‰è°ƒåº¦ | v1.17+ GA |
| **CSIé©±åŠ¨å¼‚å¸¸** | å„ç§CSIé”™è¯¯ | æ£€æŸ¥CSI PodçŠ¶æ€ | ç¨³å®š |
| **å­˜å‚¨åç«¯æ•…éšœ** | è¶…æ—¶/è¿æ¥å¤±è´¥ | æ£€æŸ¥å­˜å‚¨ç³»ç»Ÿ | - |

### 5.2 å­˜å‚¨è¯Šæ–­å‘½ä»¤

```bash
#!/bin/bash
# storage-diagnose.sh - å­˜å‚¨é—®é¢˜è¯Šæ–­

POD_NAME=$1
NAMESPACE=${2:-default}

echo "=============================================="
echo "  å­˜å‚¨é—®é¢˜è¯Šæ–­: $NAMESPACE/$POD_NAME"
echo "=============================================="

echo ""
echo "=== 1. Pod å·é…ç½® ==="
kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.volumes}' | jq .

echo ""
echo "=== 2. PVC çŠ¶æ€ ==="
PVCS=$(kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.spec.volumes[*].persistentVolumeClaim.claimName}' 2>/dev/null)
if [ -n "$PVCS" ]; then
    for pvc in $PVCS; do
        echo ""
        echo "--- PVC: $pvc ---"
        kubectl get pvc $pvc -n $NAMESPACE -o wide
        echo ""
        echo "äº‹ä»¶:"
        kubectl describe pvc $pvc -n $NAMESPACE | grep -A 10 "Events:"
        
        # æ£€æŸ¥ç»‘å®šçš„ PV
        PV=$(kubectl get pvc $pvc -n $NAMESPACE -o jsonpath='{.spec.volumeName}' 2>/dev/null)
        if [ -n "$PV" ]; then
            echo ""
            echo "ç»‘å®šçš„ PV: $PV"
            kubectl get pv $PV -o wide
            echo ""
            echo "PV nodeAffinity:"
            kubectl get pv $PV -o jsonpath='{.spec.nodeAffinity}' | jq . 2>/dev/null || echo "(æ— )"
        fi
    done
else
    echo "(Pod æœªä½¿ç”¨ PVC)"
fi

echo ""
echo "=== 3. StorageClass çŠ¶æ€ ==="
kubectl get sc -o wide

echo ""
echo "=== 4. é»˜è®¤ StorageClass ==="
kubectl get sc -o json | jq -r '.items[] | select(.metadata.annotations["storageclass.kubernetes.io/is-default-class"]=="true") | .metadata.name'

echo ""
echo "=== 5. CSI é©±åŠ¨çŠ¶æ€ ==="
kubectl get csidrivers -o wide

echo ""
echo "=== 6. CSI èŠ‚ç‚¹ä¿¡æ¯ ==="
kubectl get csinodes -o wide

echo ""
echo "=== 7. CSI ç›¸å…³ Pod ==="
kubectl get pods -A | grep -E "csi|provisioner|attacher|resizer|snapshotter"

echo ""
echo "=== 8. Pending PVC åˆ—è¡¨ ==="
kubectl get pvc -A --field-selector=status.phase=Pending

echo ""
echo "=== 9. å¯ç”¨ PV åˆ—è¡¨ ==="
kubectl get pv --field-selector=status.phase=Available
```

### 5.3 ACK å­˜å‚¨ç‰¹å®šè¯Šæ–­

```bash
# ACK äº‘ç›˜ CSI è¯Šæ–­
echo "=== ACK äº‘ç›˜ CSI çŠ¶æ€ ==="
kubectl get pods -n kube-system -l app=csi-plugin
kubectl get pods -n kube-system -l app=csi-provisioner

# äº‘ç›˜ CSI æ—¥å¿—
kubectl logs -n kube-system -l app=csi-provisioner -c csi-provisioner --tail=50

# NAS CSI è¯Šæ–­
kubectl get pods -n kube-system | grep nas

# æ£€æŸ¥äº‘ç›˜ç»‘å®šçŠ¶æ€
kubectl get pv -o json | jq -r '.items[] | select(.spec.csi.driver=="diskplugin.csi.alibabacloud.com") | "\(.metadata.name): \(.status.phase), diskId=\(.spec.csi.volumeHandle)"'
```

### 5.4 å­˜å‚¨é—®é¢˜è§£å†³æ–¹æ¡ˆ

```yaml
# å­˜å‚¨é—®é¢˜è§£å†³æ–¹æ¡ˆé›†

---
# åœºæ™¯1: åˆ›å»º StorageClass (ACK äº‘ç›˜)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: alicloud-disk-essd
provisioner: diskplugin.csi.alibabacloud.com
parameters:
  type: cloud_essd
  fsType: ext4
  performanceLevel: PL1  # PL0/PL1/PL2/PL3
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer  # å»¶è¿Ÿç»‘å®š

---
# åœºæ™¯2: æ‰‹åŠ¨åˆ›å»º PV (é™æ€ä¾›åº”)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: manual-pv-001
spec:
  capacity:
    storage: 20Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  csi:
    driver: diskplugin.csi.alibabacloud.com
    volumeHandle: d-xxx  # äº‘ç›˜ID
    fsType: ext4
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values:
          - cn-hangzhou-h

---
# åœºæ™¯3: ä¿®æ”¹ PVC å­˜å‚¨ç±»
# æ³¨æ„: PVC åˆ›å»ºå storageClassName ä¸å¯ä¿®æ”¹ï¼Œéœ€é‡å»º
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: alicloud-disk-essd  # æŒ‡å®šæ­£ç¡®çš„ SC
  resources:
    requests:
      storage: 20Gi
```

---

## 6. é…é¢ä¸å‡†å…¥æ§åˆ¶

### 6.1 é…é¢é™åˆ¶ç±»å‹

| é™åˆ¶ç±»å‹ | ä½œç”¨èŒƒå›´ | æ£€æŸ¥å‘½ä»¤ | ç‰ˆæœ¬ç‰¹æ€§ |
|---------|---------|---------|---------|
| **ResourceQuota** | å‘½åç©ºé—´çº§åˆ«èµ„æºæ€»é‡ | `kubectl describe quota` | ç¨³å®š |
| **LimitRange** | å•ä¸ªå®¹å™¨/Podèµ„æºèŒƒå›´ | `kubectl describe limitrange` | ç¨³å®š |
| **PodSecurityAdmission** | Podå®‰å…¨æ ‡å‡† | `kubectl get ns -o yaml` | v1.25+ GA |
| **ValidatingAdmissionWebhook** | è‡ªå®šä¹‰éªŒè¯ | `kubectl get validatingwebhookconfigurations` | ç¨³å®š |
| **MutatingAdmissionWebhook** | è‡ªå®šä¹‰ä¿®æ”¹ | `kubectl get mutatingwebhookconfigurations` | ç¨³å®š |

### 6.2 é…é¢è¯Šæ–­å‘½ä»¤

```bash
#!/bin/bash
# quota-diagnose.sh - é…é¢é—®é¢˜è¯Šæ–­

NAMESPACE=${1:-default}

echo "=============================================="
echo "  é…é¢è¯Šæ–­: $NAMESPACE"
echo "=============================================="

echo ""
echo "=== 1. ResourceQuota çŠ¶æ€ ==="
kubectl get resourcequota -n $NAMESPACE -o wide
echo ""
kubectl describe resourcequota -n $NAMESPACE

echo ""
echo "=== 2. LimitRange é…ç½® ==="
kubectl get limitrange -n $NAMESPACE -o wide
echo ""
kubectl describe limitrange -n $NAMESPACE

echo ""
echo "=== 3. å‘½åç©ºé—´èµ„æºä½¿ç”¨æ±‡æ€» ==="
echo "å·²ç”¨ CPU requests:"
kubectl get pods -n $NAMESPACE -o json | jq '[.items[] | select(.status.phase=="Running") | .spec.containers[].resources.requests.cpu // "0" | gsub("m";"") | if . == "0" then 0 else tonumber end] | add'
echo ""
echo "å·²ç”¨ Memory requests:"
kubectl get pods -n $NAMESPACE -o json | jq '[.items[] | select(.status.phase=="Running") | .spec.containers[].resources.requests.memory // "0" | gsub("Mi";"") | gsub("Gi";"000") | if . == "0" then 0 else tonumber end] | add' 
echo " Mi"

echo ""
echo "=== 4. Pod æ•°é‡ ==="
echo "Running: $(kubectl get pods -n $NAMESPACE --field-selector=status.phase=Running --no-headers | wc -l)"
echo "Pending: $(kubectl get pods -n $NAMESPACE --field-selector=status.phase=Pending --no-headers | wc -l)"
echo "Total: $(kubectl get pods -n $NAMESPACE --no-headers | wc -l)"

echo ""
echo "=== 5. PodSecurityAdmission é…ç½® ==="
kubectl get ns $NAMESPACE -o json | jq '.metadata.labels | with_entries(select(.key | startswith("pod-security")))'

echo ""
echo "=== 6. å‡†å…¥ Webhook é…ç½® ==="
echo "--- ValidatingAdmissionWebhook ---"
kubectl get validatingwebhookconfigurations -o custom-columns='NAME:.metadata.name,WEBHOOKS:.webhooks[*].name'
echo ""
echo "--- MutatingAdmissionWebhook ---"
kubectl get mutatingwebhookconfigurations -o custom-columns='NAME:.metadata.name,WEBHOOKS:.webhooks[*].name'
```

### 6.3 é…é¢é—®é¢˜è§£å†³

```yaml
# é…é¢é—®é¢˜è§£å†³æ–¹æ¡ˆé›†

---
# åœºæ™¯1: è°ƒæ•´ ResourceQuota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: production
spec:
  hard:
    requests.cpu: "100"      # æ€» CPU requests é™åˆ¶
    requests.memory: 200Gi   # æ€»å†…å­˜ requests é™åˆ¶
    limits.cpu: "200"        # æ€» CPU limits é™åˆ¶
    limits.memory: 400Gi     # æ€»å†…å­˜ limits é™åˆ¶
    pods: "200"              # Pod æ•°é‡é™åˆ¶
    persistentvolumeclaims: "50"
    requests.storage: 500Gi
    services.loadbalancers: "5"

---
# åœºæ™¯2: é…ç½® LimitRange (è®¾ç½®é»˜è®¤å€¼)
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: production
spec:
  limits:
  - type: Container
    default:            # é»˜è®¤ limits
      cpu: 500m
      memory: 512Mi
    defaultRequest:     # é»˜è®¤ requests
      cpu: 100m
      memory: 128Mi
    min:                # æœ€å°å€¼
      cpu: 50m
      memory: 64Mi
    max:                # æœ€å¤§å€¼
      cpu: 4
      memory: 8Gi
  - type: Pod
    max:
      cpu: 8
      memory: 16Gi

---
# åœºæ™¯3: é…ç½® PodSecurity (v1.25+)
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/enforce-version: v1.32
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: v1.32
```

---

## 7. è°ƒåº¦å™¨é—®é¢˜è¯Šæ–­

### 7.1 è°ƒåº¦å™¨æ•…éšœç±»å‹

| æ•…éšœç±»å‹ | ç°è±¡ | è¯Šæ–­æ–¹æ³• | å½±å“èŒƒå›´ |
|---------|------|---------|---------|
| **è°ƒåº¦å™¨ä¸å¯ç”¨** | æ–°Podæ— äº‹ä»¶ | æ£€æŸ¥scheduler Pod | å…¨é›†ç¾¤ |
| **Leaderé€‰ä¸¾é—®é¢˜** | è°ƒåº¦å»¶è¿Ÿé«˜ | æ£€æŸ¥leaseå¯¹è±¡ | å…¨é›†ç¾¤ |
| **é…ç½®é”™è¯¯** | ç‰¹å®šPodæ— æ³•è°ƒåº¦ | æ£€æŸ¥è°ƒåº¦å™¨æ—¥å¿— | éƒ¨åˆ†Pod |
| **æ’ä»¶å¼‚å¸¸** | è°ƒåº¦å¤±è´¥+é”™è¯¯æ—¥å¿— | æ£€æŸ¥æ’ä»¶çŠ¶æ€ | éƒ¨åˆ†Pod |
| **è‡ªå®šä¹‰è°ƒåº¦å™¨ç¼ºå¤±** | æŒ‡å®šschedulerNameæ— å“åº” | æ£€æŸ¥è°ƒåº¦å™¨éƒ¨ç½² | ç‰¹å®šPod |
| **æ€§èƒ½é—®é¢˜** | è°ƒåº¦å»¶è¿Ÿé«˜ | æ£€æŸ¥metrics | å…¨é›†ç¾¤ |

### 7.2 è°ƒåº¦å™¨è¯Šæ–­å‘½ä»¤

```bash
#!/bin/bash
# scheduler-diagnose.sh - è°ƒåº¦å™¨é—®é¢˜è¯Šæ–­

echo "=============================================="
echo "  è°ƒåº¦å™¨è¯Šæ–­æŠ¥å‘Š"
echo "=============================================="

echo ""
echo "=== 1. è°ƒåº¦å™¨ Pod çŠ¶æ€ ==="
kubectl get pods -n kube-system -l component=kube-scheduler -o wide

echo ""
echo "=== 2. è°ƒåº¦å™¨å¥åº·æ£€æŸ¥ ==="
# è·å–è°ƒåº¦å™¨ Pod
SCHEDULER_POD=$(kubectl get pods -n kube-system -l component=kube-scheduler -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
if [ -n "$SCHEDULER_POD" ]; then
    kubectl exec -n kube-system $SCHEDULER_POD -- curl -s localhost:10259/healthz 2>/dev/null || echo "æ— æ³•è®¿é—®å¥åº·æ£€æŸ¥ç«¯ç‚¹"
fi

echo ""
echo "=== 3. è°ƒåº¦å™¨ Leader é€‰ä¸¾ ==="
kubectl get lease kube-scheduler -n kube-system -o yaml 2>/dev/null | grep -E "holderIdentity|acquireTime|renewTime"

echo ""
echo "=== 4. è°ƒåº¦å™¨æ—¥å¿— (æœ€è¿‘50è¡Œ) ==="
kubectl logs -n kube-system -l component=kube-scheduler --tail=50 2>/dev/null | tail -30

echo ""
echo "=== 5. è°ƒåº¦å¤±è´¥äº‹ä»¶ (æœ€è¿‘10æ¡) ==="
kubectl get events -A --field-selector reason=FailedScheduling --sort-by='.lastTimestamp' 2>/dev/null | tail -10

echo ""
echo "=== 6. Pending Pod ç»Ÿè®¡ ==="
echo "æŒ‰å‘½åç©ºé—´ç»Ÿè®¡:"
kubectl get pods -A --field-selector=status.phase=Pending -o json | jq -r '.items | group_by(.metadata.namespace) | .[] | "\(.[0].metadata.namespace): \(length)"'

echo ""
echo "=== 7. è°ƒåº¦å™¨æŒ‡æ ‡ (å¦‚å¯è®¿é—®) ==="
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes 2>/dev/null | jq '.items | length' | xargs -I {} echo "Metrics Server å¯ç”¨ï¼Œå…± {} ä¸ªèŠ‚ç‚¹"

echo ""
echo "=== 8. è‡ªå®šä¹‰è°ƒåº¦å™¨æ£€æŸ¥ ==="
kubectl get pods -A -o json | jq -r '.items[] | select(.spec.schedulerName != null and .spec.schedulerName != "default-scheduler") | "\(.metadata.namespace)/\(.metadata.name): \(.spec.schedulerName)"' | head -10
```

### 7.3 è°ƒåº¦å™¨é—®é¢˜è§£å†³

```bash
# è°ƒåº¦å™¨æ•…éšœç´§æ€¥å¤„ç†

# 1. é‡å¯è°ƒåº¦å™¨ (kubeadm é›†ç¾¤)
kubectl delete pod -n kube-system -l component=kube-scheduler

# 2. æ£€æŸ¥è°ƒåº¦å™¨é…ç½®
kubectl get pods -n kube-system -l component=kube-scheduler -o yaml | grep -A 50 "containers:"

# 3. å¼ºåˆ¶ Leader é€‰ä¸¾
kubectl delete lease kube-scheduler -n kube-system

# 4. æ£€æŸ¥è°ƒåº¦å™¨èµ„æºä½¿ç”¨
kubectl top pod -n kube-system -l component=kube-scheduler

# 5. ä¿®æ”¹ Pod ä½¿ç”¨é»˜è®¤è°ƒåº¦å™¨
kubectl patch deployment <name> -n <namespace> -p '{"spec":{"template":{"spec":{"schedulerName":"default-scheduler"}}}}'
```

---

## 8. é«˜çº§è°ƒåº¦åœºæ™¯

### 8.1 æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ (TopologySpreadConstraints)

| å‚æ•° | è¯´æ˜ | ç‰ˆæœ¬ | æœ€ä½³å®è·µ |
|------|------|------|---------|
| `maxSkew` | æœ€å¤§å€¾æ–œåº¦ | v1.19+ | é€šå¸¸è®¾ç½®1-2 |
| `topologyKey` | æ‹“æ‰‘åŸŸé”® | v1.19+ | `topology.kubernetes.io/zone` |
| `whenUnsatisfiable` | ä¸æ»¡è¶³æ—¶è¡Œä¸º | v1.19+ | ç”Ÿäº§ç”¨ `DoNotSchedule` |
| `labelSelector` | Podæ ‡ç­¾é€‰æ‹© | v1.19+ | å¿…é¡»é…ç½® |
| `minDomains` | æœ€å°åŸŸæ•° | v1.25+ | æ§åˆ¶æœ€å°åˆ†å¸ƒåŸŸ |
| `nodeAffinityPolicy` | èŠ‚ç‚¹äº²å’Œæ€§ç­–ç•¥ | v1.26+ | `Honor` / `Ignore` |
| `nodeTaintsPolicy` | èŠ‚ç‚¹æ±¡ç‚¹ç­–ç•¥ | v1.26+ | `Honor` / `Ignore` |
| `matchLabelKeys` | åŠ¨æ€æ ‡ç­¾é”® | v1.27+ | æ»šåŠ¨æ›´æ–°åœºæ™¯ |

```yaml
# æ‹“æ‰‘åˆ†å¸ƒæœ€ä½³å®è·µ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 6
  template:
    spec:
      topologySpreadConstraints:
      # è·¨å¯ç”¨åŒºåˆ†å¸ƒ (ç¡¬çº¦æŸ)
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: web-app
        minDomains: 2  # v1.25+ è‡³å°‘åˆ†å¸ƒåœ¨2ä¸ªzone
      # è·¨èŠ‚ç‚¹åˆ†å¸ƒ (è½¯çº¦æŸ)
      - maxSkew: 2
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: web-app
```

### 8.2 æŠ¢å è°ƒåº¦ (Preemption)

```yaml
# æŠ¢å è°ƒåº¦é…ç½®
---
# å®šä¹‰ä¼˜å…ˆçº§ç±»
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical-production
value: 1000000
preemptionPolicy: PreemptLowerPriority
globalDefault: false
description: "ç”Ÿäº§å…³é”®æœåŠ¡ï¼Œå¯æŠ¢å ä½ä¼˜å…ˆçº§Pod"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch-processing
value: -1000
preemptionPolicy: Never  # ä¸è§¦å‘æŠ¢å 
globalDefault: false
description: "æ‰¹å¤„ç†ä»»åŠ¡ï¼Œä¸æŠ¢å å…¶ä»–Pod"
---
# ä½¿ç”¨ä¼˜å…ˆçº§ç±»
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  template:
    spec:
      priorityClassName: critical-production
      containers:
      - name: app
        image: myapp:latest
```

### 8.3 è°ƒåº¦é—¨æ§ (Scheduling Gates - v1.27+ Beta)

```yaml
# è°ƒåº¦é—¨æ§ç¤ºä¾‹ (å»¶è¿Ÿè°ƒåº¦ç›´åˆ°æ»¡è¶³æ¡ä»¶)
apiVersion: v1
kind: Pod
metadata:
  name: gated-pod
spec:
  schedulingGates:
  - name: example.com/waiting-for-config
  containers:
  - name: app
    image: myapp:latest
---
# ç§»é™¤è°ƒåº¦é—¨ (å…è®¸è°ƒåº¦)
# kubectl patch pod gated-pod --type=json -p='[{"op":"remove","path":"/spec/schedulingGates"}]'
```

---

## 9. ACK/äº‘ç¯å¢ƒç‰¹å®šé—®é¢˜

### 9.1 ACK ç‰¹å®šè°ƒåº¦é—®é¢˜

| é—®é¢˜ | åŸå›  | è¯Šæ–­ | è§£å†³æ–¹æ¡ˆ |
|------|------|------|---------|
| **å¼¹æ€§èŠ‚ç‚¹æ± æ‰©å®¹æ…¢** | èŠ‚ç‚¹æ± é…ç½®/åº“å­˜ | æ£€æŸ¥èŠ‚ç‚¹æ± äº‹ä»¶ | è°ƒæ•´èŠ‚ç‚¹æ± é…ç½® |
| **æŠ¢å å¼å®ä¾‹å›æ”¶** | ç«ä»·å®ä¾‹è¢«å›æ”¶ | æ£€æŸ¥èŠ‚ç‚¹äº‹ä»¶ | æ··åˆå®ä¾‹ç­–ç•¥ |
| **è·¨å¯ç”¨åŒºè°ƒåº¦å¤±è´¥** | ç½‘ç»œ/å­˜å‚¨é™åˆ¶ | æ£€æŸ¥æ‹“æ‰‘æ ‡ç­¾ | é…ç½®å¤šå¯ç”¨åŒº |
| **GPUè°ƒåº¦å¼‚å¸¸** | GPUé©±åŠ¨/cGPUé…ç½® | æ£€æŸ¥GPUæ’ä»¶ | é‡å¯æ’ä»¶/æ£€æŸ¥é…é¢ |
| **äº‘ç›˜æ— æ³•æŒ‚è½½** | äº‘ç›˜ä¸èŠ‚ç‚¹ä¸åœ¨åŒä¸€å¯ç”¨åŒº | æ£€æŸ¥PV nodeAffinity | WaitForFirstConsumer |

### 9.2 ACK è¯Šæ–­å‘½ä»¤

```bash
#!/bin/bash
# ack-pending-diagnose.sh - ACK ç‰¹å®šè¯Šæ–­

echo "=============================================="
echo "  ACK Pod Pending è¯Šæ–­"
echo "=============================================="

echo ""
echo "=== 1. èŠ‚ç‚¹æ± çŠ¶æ€ ==="
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): pool=\(.metadata.labels["alibabacloud.com/nodepool-id"] // "N/A"), type=\(.metadata.labels["node.kubernetes.io/instance-type"] // "N/A")"'

echo ""
echo "=== 2. å¼¹æ€§ä¼¸ç¼©ç»„ä»¶çŠ¶æ€ ==="
kubectl get pods -n kube-system | grep -E "cluster-autoscaler|ess-"

echo ""
echo "=== 3. Cluster Autoscaler æ—¥å¿— ==="
kubectl logs -n kube-system -l app=cluster-autoscaler --tail=30 2>/dev/null

echo ""
echo "=== 4. GPU èŠ‚ç‚¹çŠ¶æ€ ==="
kubectl get nodes -l accelerator -o custom-columns='NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu,CGPU:.status.allocatable.alibabacloud\.com/gpu-mem'

echo ""
echo "=== 5. GPU è°ƒåº¦æƒ…å†µ ==="
kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].resources.requests["nvidia.com/gpu"] != null) | "\(.metadata.namespace)/\(.metadata.name): GPU=\(.spec.containers[].resources.requests["nvidia.com/gpu"])"'

echo ""
echo "=== 6. äº‘ç›˜ CSI çŠ¶æ€ ==="
kubectl get pods -n kube-system -l app=csi-plugin -o wide
kubectl get pods -n kube-system -l app=csi-provisioner -o wide

echo ""
echo "=== 7. èŠ‚ç‚¹å¯ç”¨åŒºåˆ†å¸ƒ ==="
kubectl get nodes -o json | jq -r '.items | group_by(.metadata.labels["topology.kubernetes.io/zone"]) | .[] | "\(.[0].metadata.labels["topology.kubernetes.io/zone"]): \(length) nodes"'

echo ""
echo "=== 8. æŠ¢å å¼å®ä¾‹çŠ¶æ€ ==="
kubectl get nodes -o json | jq -r '.items[] | select(.metadata.labels["alibabacloud.com/spot-instance"]=="true") | .metadata.name'
```

### 9.3 ACK è‡ªåŠ¨æ‰©å®¹é…ç½®

```yaml
# Cluster Autoscaler é…ç½® (ACK æ‰˜ç®¡ç‰ˆè‡ªåŠ¨ç®¡ç†)
# å¦‚éœ€è‡ªå»ºï¼Œå‚è€ƒä»¥ä¸‹é…ç½®

# èŠ‚ç‚¹æ± è‡ªåŠ¨æ‰©å®¹è§¦å‘æ¡ä»¶
# - Pod å› èµ„æºä¸è¶³å¤„äº Pending çŠ¶æ€
# - èŠ‚ç‚¹æ± æœ‰æ‰©å®¹ç©ºé—´

# æ¨èé…ç½®:
# - å¯ç”¨å¤šå¯ç”¨åŒºèŠ‚ç‚¹æ± 
# - é…ç½®åˆé€‚çš„æœ€å°/æœ€å¤§èŠ‚ç‚¹æ•°
# - ä½¿ç”¨æ··åˆå®ä¾‹ (æŒ‰é‡+ç«ä»·)
# - é…ç½®æ‰©å®¹å†·å´æ—¶é—´ (é¿å…æŠ–åŠ¨)
```

---

## 10. è‡ªåŠ¨åŒ–è¯Šæ–­å·¥å…·

### 10.1 å®Œæ•´è¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# pod-pending-full-diagnose.sh - Pod Pending å®Œæ•´è¯Šæ–­å·¥å…·
# ç‰ˆæœ¬: 1.0 | é€‚ç”¨: Kubernetes v1.25-v1.32

set -e

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

POD_NAME=${1:-""}
NAMESPACE=${2:-"default"}
OUTPUT_FILE="pod-pending-diagnosis-$(date +%Y%m%d-%H%M%S).txt"

log() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥å¿…è¦å·¥å…·
check_prerequisites() {
    for cmd in kubectl jq; do
        if ! command -v $cmd &> /dev/null; then
            error "$cmd æœªå®‰è£…"
            exit 1
        fi
    done
}

# åˆ—å‡ºæ‰€æœ‰ Pending Pod
list_pending_pods() {
    log "å½“å‰ Pending çš„ Pod åˆ—è¡¨:"
    kubectl get pods -A --field-selector=status.phase=Pending -o wide
}

# è¯Šæ–­å•ä¸ª Pod
diagnose_pod() {
    local pod=$1
    local ns=$2
    
    echo ""
    echo "============================================================"
    echo "  è¯Šæ–­ Pod: $ns/$pod"
    echo "  æ—¶é—´: $(date)"
    echo "============================================================"
    
    # 1. åŸºæœ¬ä¿¡æ¯
    echo ""
    echo "=== 1. Pod åŸºæœ¬ä¿¡æ¯ ==="
    kubectl get pod $pod -n $ns -o wide
    
    # 2. æ£€æŸ¥æ˜¯å¦å·²åˆ†é…èŠ‚ç‚¹
    echo ""
    echo "=== 2. è°ƒåº¦çŠ¶æ€ ==="
    NODE_NAME=$(kubectl get pod $pod -n $ns -o jsonpath='{.spec.nodeName}' 2>/dev/null)
    if [ -n "$NODE_NAME" ]; then
        log "å·²åˆ†é…èŠ‚ç‚¹: $NODE_NAME (é—®é¢˜åœ¨èŠ‚ç‚¹å¯åŠ¨é˜¶æ®µ)"
    else
        warn "æœªåˆ†é…èŠ‚ç‚¹ (è°ƒåº¦é˜¶æ®µé—®é¢˜)"
    fi
    
    # 3. Events
    echo ""
    echo "=== 3. Pod äº‹ä»¶ ==="
    kubectl describe pod $pod -n $ns | grep -A 30 "Events:" | head -35
    
    # 4. è°ƒåº¦å¤±è´¥åŸå› åˆ†æ
    echo ""
    echo "=== 4. è°ƒåº¦å¤±è´¥åŸå› åˆ†æ ==="
    EVENTS=$(kubectl get events -n $ns --field-selector involvedObject.name=$pod,reason=FailedScheduling -o json 2>/dev/null)
    if [ "$(echo $EVENTS | jq '.items | length')" -gt "0" ]; then
        echo "$EVENTS" | jq -r '.items[-1].message'
        
        # åˆ†æå…·ä½“åŸå› 
        MSG=$(echo "$EVENTS" | jq -r '.items[-1].message')
        
        if echo "$MSG" | grep -q "Insufficient cpu"; then
            warn "è¯Šæ–­: CPU èµ„æºä¸è¶³"
            echo "å»ºè®®: æ‰©å®¹èŠ‚ç‚¹æˆ–è°ƒæ•´ Pod CPU requests"
        fi
        
        if echo "$MSG" | grep -q "Insufficient memory"; then
            warn "è¯Šæ–­: å†…å­˜èµ„æºä¸è¶³"
            echo "å»ºè®®: æ‰©å®¹èŠ‚ç‚¹æˆ–è°ƒæ•´ Pod Memory requests"
        fi
        
        if echo "$MSG" | grep -q "didn't match Pod's node selector"; then
            warn "è¯Šæ–­: èŠ‚ç‚¹é€‰æ‹©å™¨ä¸åŒ¹é…"
            echo "å»ºè®®: æ£€æŸ¥èŠ‚ç‚¹æ ‡ç­¾æˆ–ä¿®æ”¹ nodeSelector"
        fi
        
        if echo "$MSG" | grep -q "had taint"; then
            warn "è¯Šæ–­: èŠ‚ç‚¹æ±¡ç‚¹æœªå®¹å¿"
            echo "å»ºè®®: æ·»åŠ  tolerations æˆ–ç§»é™¤èŠ‚ç‚¹æ±¡ç‚¹"
        fi
        
        if echo "$MSG" | grep -q "unbound.*PersistentVolumeClaims"; then
            warn "è¯Šæ–­: PVC æœªç»‘å®š"
            echo "å»ºè®®: æ£€æŸ¥ PVC çŠ¶æ€å’Œ StorageClass é…ç½®"
        fi
        
        if echo "$MSG" | grep -q "topology spread constraints"; then
            warn "è¯Šæ–­: æ‹“æ‰‘åˆ†å¸ƒçº¦æŸä¸æ»¡è¶³"
            echo "å»ºè®®: è°ƒæ•´ maxSkew æˆ–å¢åŠ èŠ‚ç‚¹"
        fi
    else
        warn "æœªæ‰¾åˆ° FailedScheduling äº‹ä»¶ï¼Œå¯èƒ½æ˜¯è°ƒåº¦å™¨é—®é¢˜"
    fi
    
    # 5. èµ„æºè¯·æ±‚
    echo ""
    echo "=== 5. Pod èµ„æºè¯·æ±‚ ==="
    kubectl get pod $pod -n $ns -o json | jq '.spec.containers[] | {name: .name, requests: .resources.requests, limits: .resources.limits}'
    
    # 6. èŠ‚ç‚¹é€‰æ‹©çº¦æŸ
    echo ""
    echo "=== 6. èŠ‚ç‚¹é€‰æ‹©çº¦æŸ ==="
    echo "--- nodeSelector ---"
    kubectl get pod $pod -n $ns -o jsonpath='{.spec.nodeSelector}' | jq . 2>/dev/null || echo "(æœªé…ç½®)"
    echo ""
    echo "--- nodeAffinity ---"
    kubectl get pod $pod -n $ns -o jsonpath='{.spec.affinity.nodeAffinity}' | jq . 2>/dev/null || echo "(æœªé…ç½®)"
    echo ""
    echo "--- tolerations ---"
    kubectl get pod $pod -n $ns -o jsonpath='{.spec.tolerations}' | jq . 2>/dev/null || echo "(æœªé…ç½®)"
    echo ""
    echo "--- topologySpreadConstraints ---"
    kubectl get pod $pod -n $ns -o jsonpath='{.spec.topologySpreadConstraints}' | jq . 2>/dev/null || echo "(æœªé…ç½®)"
    
    # 7. PVC çŠ¶æ€
    echo ""
    echo "=== 7. PVC çŠ¶æ€ ==="
    PVCS=$(kubectl get pod $pod -n $ns -o jsonpath='{.spec.volumes[*].persistentVolumeClaim.claimName}' 2>/dev/null)
    if [ -n "$PVCS" ]; then
        for pvc in $PVCS; do
            echo "PVC: $pvc"
            kubectl get pvc $pvc -n $ns -o wide 2>/dev/null || echo "  (ä¸å­˜åœ¨)"
        done
    else
        echo "(Pod æœªä½¿ç”¨ PVC)"
    fi
    
    # 8. é…é¢æ£€æŸ¥
    echo ""
    echo "=== 8. å‘½åç©ºé—´é…é¢ ==="
    kubectl describe resourcequota -n $ns 2>/dev/null || echo "(æ— é…é¢)"
    
    # 9. èŠ‚ç‚¹èµ„æºæ¦‚å†µ
    echo ""
    echo "=== 9. èŠ‚ç‚¹èµ„æºæ¦‚å†µ ==="
    kubectl get nodes -o custom-columns='NAME:.metadata.name,STATUS:.status.conditions[?(@.type=="Ready")].status,CPU:.status.allocatable.cpu,MEMORY:.status.allocatable.memory,TAINTS:.spec.taints[*].key'
    
    echo ""
    echo "============================================================"
    echo "  è¯Šæ–­å®Œæˆ"
    echo "============================================================"
}

# ä¸»æµç¨‹
main() {
    check_prerequisites
    
    if [ -z "$POD_NAME" ]; then
        list_pending_pods
        echo ""
        echo "ç”¨æ³•: $0 <pod-name> [namespace]"
        exit 0
    fi
    
    # æ£€æŸ¥ Pod æ˜¯å¦å­˜åœ¨
    if ! kubectl get pod $POD_NAME -n $NAMESPACE &>/dev/null; then
        error "Pod $NAMESPACE/$POD_NAME ä¸å­˜åœ¨"
        exit 1
    fi
    
    # æ£€æŸ¥ Pod æ˜¯å¦ Pending
    PHASE=$(kubectl get pod $POD_NAME -n $NAMESPACE -o jsonpath='{.status.phase}')
    if [ "$PHASE" != "Pending" ]; then
        warn "Pod å½“å‰çŠ¶æ€ä¸º $PHASE (é Pending)"
    fi
    
    diagnose_pod $POD_NAME $NAMESPACE | tee $OUTPUT_FILE
    
    log "è¯Šæ–­ç»“æœå·²ä¿å­˜åˆ°: $OUTPUT_FILE"
}

main "$@"
```

---

## 11. ç›‘æ§å‘Šè­¦é…ç½®

### 11.1 Prometheus å‘Šè­¦è§„åˆ™

```yaml
# pod-pending-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: pod-pending-alerts
  namespace: monitoring
  labels:
    release: prometheus
spec:
  groups:
  - name: pod.pending.rules
    interval: 30s
    rules:
    # Pod Pending è¶…è¿‡ 5 åˆ†é’Ÿ
    - alert: PodPendingTooLong
      expr: |
        sum by (namespace, pod) (
          kube_pod_status_phase{phase="Pending"} == 1
        ) * on (namespace, pod) group_left()
        (time() - kube_pod_created) > 300
      for: 1m
      labels:
        severity: warning
        category: scheduling
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} Pending è¶…è¿‡ 5 åˆ†é’Ÿ"
        description: "Pod è°ƒåº¦å¤±è´¥ï¼Œè¯·æ£€æŸ¥è°ƒåº¦å™¨äº‹ä»¶"
        runbook_url: "https://wiki.example.com/runbooks/pod-pending"
    
    # Pod Pending è¶…è¿‡ 15 åˆ†é’Ÿ (ä¸¥é‡)
    - alert: PodPendingCritical
      expr: |
        sum by (namespace, pod) (
          kube_pod_status_phase{phase="Pending"} == 1
        ) * on (namespace, pod) group_left()
        (time() - kube_pod_created) > 900
      for: 1m
      labels:
        severity: critical
        category: scheduling
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} Pending è¶…è¿‡ 15 åˆ†é’Ÿ"
        description: "Pod é•¿æ—¶é—´æ— æ³•è°ƒåº¦ï¼Œéœ€ç«‹å³å¤„ç†"
    
    # å¤§é‡ Pod Pending
    - alert: ManyPodsPending
      expr: |
        count(kube_pod_status_phase{phase="Pending"}) > 10
      for: 5m
      labels:
        severity: warning
        category: scheduling
      annotations:
        summary: "é›†ç¾¤æœ‰ {{ $value }} ä¸ª Pod å¤„äº Pending çŠ¶æ€"
        description: "å¯èƒ½å­˜åœ¨é›†ç¾¤èµ„æºä¸è¶³æˆ–è°ƒåº¦å™¨é—®é¢˜"
    
    # å‘½åç©ºé—´ Pod Pending è¿‡å¤š
    - alert: NamespacePodsPending
      expr: |
        count by (namespace) (kube_pod_status_phase{phase="Pending"}) > 5
      for: 5m
      labels:
        severity: warning
        category: scheduling
      annotations:
        summary: "å‘½åç©ºé—´ {{ $labels.namespace }} æœ‰ {{ $value }} ä¸ª Pending Pod"
        description: "æ£€æŸ¥å‘½åç©ºé—´é…é¢å’Œèµ„æºé…ç½®"
    
    # è°ƒåº¦å™¨è°ƒåº¦å¤±è´¥ç‡è¿‡é«˜
    - alert: HighSchedulingFailureRate
      expr: |
        increase(scheduler_schedule_attempts_total{result="error"}[5m]) /
        increase(scheduler_schedule_attempts_total[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        category: scheduling
      annotations:
        summary: "è°ƒåº¦å™¨å¤±è´¥ç‡è¶…è¿‡ 10%"
        description: "å½“å‰å¤±è´¥ç‡: {{ $value | humanizePercentage }}"
    
    # è°ƒåº¦é˜Ÿåˆ—ç§¯å‹
    - alert: SchedulerQueueBacklog
      expr: |
        scheduler_pending_pods{queue="unschedulable"} > 50
      for: 10m
      labels:
        severity: warning
        category: scheduling
      annotations:
        summary: "è°ƒåº¦é˜Ÿåˆ—ç§¯å‹ {{ $value }} ä¸ªä¸å¯è°ƒåº¦ Pod"
        description: "æ£€æŸ¥é›†ç¾¤èµ„æºå’Œè°ƒåº¦çº¦æŸ"
    
    # é›†ç¾¤ CPU èµ„æºä¸è¶³é¢„è­¦
    - alert: ClusterCPUResourcesLow
      expr: |
        (
          sum(kube_node_status_allocatable{resource="cpu"}) -
          sum(kube_pod_container_resource_requests{resource="cpu"})
        ) < 4
      for: 10m
      labels:
        severity: warning
        category: capacity
      annotations:
        summary: "é›†ç¾¤å¯ç”¨ CPU èµ„æºä¸è¶³ 4 æ ¸"
        description: "å‰©ä½™ CPU: {{ $value }} æ ¸ï¼Œè€ƒè™‘æ‰©å®¹"
    
    # é›†ç¾¤ Memory èµ„æºä¸è¶³é¢„è­¦
    - alert: ClusterMemoryResourcesLow
      expr: |
        (
          sum(kube_node_status_allocatable{resource="memory"}) -
          sum(kube_pod_container_resource_requests{resource="memory"})
        ) / 1024 / 1024 / 1024 < 8
      for: 10m
      labels:
        severity: warning
        category: capacity
      annotations:
        summary: "é›†ç¾¤å¯ç”¨å†…å­˜èµ„æºä¸è¶³ 8 Gi"
        description: "å‰©ä½™å†…å­˜: {{ $value | humanize }}iï¼Œè€ƒè™‘æ‰©å®¹"

---
# Grafana Dashboard é…ç½® (JSONç®€åŒ–ç‰ˆ)
# å®Œæ•´ Dashboard è¯·å‚è€ƒ: https://grafana.com/grafana/dashboards/
```

### 11.2 å…³é”®ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡åç§° | å«ä¹‰ | å¥åº·åŸºå‡† | å‘Šè­¦é˜ˆå€¼ |
|---------|------|---------|---------|
| `kube_pod_status_phase{phase="Pending"}` | Pending Pod æ•°é‡ | < 5 | > 10 |
| `scheduler_pending_pods` | è°ƒåº¦é˜Ÿåˆ— Pod æ•°é‡ | < 10 | > 50 |
| `scheduler_schedule_attempts_total` | è°ƒåº¦å°è¯•æ€»æ•° | - | å¤±è´¥ç‡>10% |
| `scheduler_scheduling_attempt_duration_seconds` | è°ƒåº¦å»¶è¿Ÿ | P99 < 100ms | P99 > 1s |
| `scheduler_preemption_attempts_total` | æŠ¢å å°è¯•æ¬¡æ•° | å¢é•¿ç¼“æ…¢ | å¿«é€Ÿå¢é•¿ |

---

## 12. ç´§æ€¥å¤„ç†æµç¨‹

### 12.1 ç´§æ€¥å¤„ç†å‘½ä»¤

```bash
#!/bin/bash
# emergency-pod-pending.sh - Pod Pending ç´§æ€¥å¤„ç†

# === ç´§æ€¥çº§åˆ«åˆ¤æ–­ ===
echo "=== ç´§æ€¥çº§åˆ«åˆ¤æ–­ ==="
PENDING_COUNT=$(kubectl get pods -A --field-selector=status.phase=Pending --no-headers | wc -l)
echo "å½“å‰ Pending Pod æ•°é‡: $PENDING_COUNT"

if [ $PENDING_COUNT -gt 50 ]; then
    echo "ç´§æ€¥çº§åˆ«: P0 - å¤§è§„æ¨¡è°ƒåº¦æ•…éšœ"
elif [ $PENDING_COUNT -gt 20 ]; then
    echo "ç´§æ€¥çº§åˆ«: P1 - ä¸¥é‡è°ƒåº¦é—®é¢˜"
elif [ $PENDING_COUNT -gt 5 ]; then
    echo "ç´§æ€¥çº§åˆ«: P2 - ä¸­ç­‰è°ƒåº¦é—®é¢˜"
else
    echo "ç´§æ€¥çº§åˆ«: P3 - å¸¸è§„å¤„ç†"
fi

# === P0/P1 ç´§æ€¥å¤„ç†å‘½ä»¤ ===
echo ""
echo "=== P0/P1 ç´§æ€¥å¤„ç†å‘½ä»¤ ==="

echo "# 1. æ£€æŸ¥è°ƒåº¦å™¨çŠ¶æ€"
echo "kubectl get pods -n kube-system -l component=kube-scheduler"

echo ""
echo "# 2. é‡å¯è°ƒåº¦å™¨ (å¦‚è°ƒåº¦å™¨æ•…éšœ)"
echo "kubectl delete pod -n kube-system -l component=kube-scheduler"

echo ""
echo "# 3. ç´§æ€¥æ‰©å®¹èŠ‚ç‚¹ (ACK)"
echo "# aliyun cs POST /clusters/{ClusterId}/nodepools/{NodepoolId}/nodes --body '{\"count\": 3}'"

echo ""
echo "# 4. ä¸´æ—¶ç§»é™¤èŠ‚ç‚¹æ±¡ç‚¹"
echo "kubectl taint node <node-name> <key>:<effect>-"

echo ""
echo "# 5. å–æ¶ˆèŠ‚ç‚¹ä¸å¯è°ƒåº¦"
echo "kubectl uncordon <node-name>"

echo ""
echo "# 6. å¼ºåˆ¶åˆ é™¤å¡ä½çš„ Pod (è®©æ§åˆ¶å™¨é‡å»º)"
echo "kubectl delete pod <pod-name> -n <namespace> --force --grace-period=0"

echo ""
echo "# 7. ä¸´æ—¶æ¸…é™¤è°ƒåº¦çº¦æŸ (ä»…ç´§æ€¥æƒ…å†µ)"
echo "kubectl patch deployment <name> -n <namespace> -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":null,\"affinity\":null}}}}'"

echo ""
echo "# 8. é©±é€ä½ä¼˜å…ˆçº§ Pod é‡Šæ”¾èµ„æº"
echo "kubectl get pods -A -o json | jq -r '.items[] | select(.spec.priority != null and .spec.priority < 0) | \"\\(.metadata.namespace) \\(.metadata.name)\"' | while read ns pod; do kubectl delete pod \$pod -n \$ns; done"
```

### 12.2 ç´§æ€¥å¤„ç†æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Pod Pending ç´§æ€¥å¤„ç†æµç¨‹                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚  è§¦å‘å‘Šè­¦: Pod Pending > 15min æˆ– æ‰¹é‡ Pending                                       â”‚
â”‚                     â”‚                                                                â”‚
â”‚                     â–¼                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Step 1: å¿«é€Ÿè¯„ä¼° (2min)                                       â”‚                   â”‚
â”‚  â”‚ - kubectl get pods -A --field-selector=status.phase=Pending   â”‚                   â”‚
â”‚  â”‚ - ç¡®å®šå½±å“èŒƒå›´å’Œç´§æ€¥çº§åˆ«                                       â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                     â”‚                                                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚
â”‚         â”‚                     â”‚                                                     â”‚
â”‚    å•ä¸ªPod               æ‰¹é‡Pod                                                    â”‚
â”‚         â”‚                     â”‚                                                     â”‚
â”‚         â–¼                     â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ å¸¸è§„è¯Šæ–­   â”‚       â”‚ Step 2: æ£€æŸ¥è°ƒåº¦å™¨ (1min)               â”‚                   â”‚
â”‚  â”‚ (è·³è½¬è¯Šæ–­  â”‚       â”‚ kubectl get pods -n kube-system        â”‚                   â”‚
â”‚  â”‚  å†³ç­–æ ‘)   â”‚       â”‚   -l component=kube-scheduler           â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                  â”‚                                                  â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                     â”‚                         â”‚                                    â”‚
â”‚              è°ƒåº¦å™¨æ­£å¸¸                  è°ƒåº¦å™¨å¼‚å¸¸                                  â”‚
â”‚                     â”‚                         â”‚                                    â”‚
â”‚                     â–¼                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ Step 3: æ£€æŸ¥èŠ‚ç‚¹èµ„æº    â”‚    â”‚ é‡å¯è°ƒåº¦å™¨                      â”‚                 â”‚
â”‚  â”‚ kubectl top nodes       â”‚    â”‚ kubectl delete pod -n kube-systemâ”‚                 â”‚
â”‚  â”‚ kubectl describe nodes  â”‚    â”‚   -l component=kube-scheduler   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                     â”‚                                                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚         â”‚                     â”‚                                                    â”‚
â”‚    èµ„æºå……è¶³              èµ„æºä¸è¶³                                                   â”‚
â”‚         â”‚                     â”‚                                                    â”‚
â”‚         â–¼                     â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ æ£€æŸ¥è°ƒåº¦çº¦æŸ   â”‚   â”‚ Step 4: ç´§æ€¥æ‰©å®¹                        â”‚                  â”‚
â”‚  â”‚ - nodeSelector â”‚   â”‚ - è§¦å‘ Cluster Autoscaler               â”‚                  â”‚
â”‚  â”‚ - taints       â”‚   â”‚ - æ‰‹åŠ¨æ‰©å®¹èŠ‚ç‚¹æ±                         â”‚                  â”‚
â”‚  â”‚ - PVC          â”‚   â”‚ - é©±é€ä½ä¼˜å…ˆçº§ Pod                      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                  â”‚                                                  â”‚
â”‚                                  â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Step 5: éªŒè¯æ¢å¤                                              â”‚                  â”‚
â”‚  â”‚ - ç¡®è®¤ Pending Pod å¼€å§‹è°ƒåº¦                                   â”‚                  â”‚
â”‚  â”‚ - ç›‘æ§æ–° Pod è°ƒåº¦æƒ…å†µ                                         â”‚                  â”‚
â”‚  â”‚ - è®°å½•äº‹ä»¶å’Œæ ¹å›                                               â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 13. ç‰ˆæœ¬ç‰¹å®šå˜æ›´

### 13.1 è°ƒåº¦å™¨åŠŸèƒ½æ¼”è¿›

| ç‰ˆæœ¬ | ç‰¹æ€§ | çŠ¶æ€ | å½±å“ |
|------|------|------|------|
| **v1.25** | PodSchedulingReadiness (è°ƒåº¦é—¨æ§) | Alpha | å»¶è¿Ÿè°ƒåº¦ç›´åˆ°æ»¡è¶³æ¡ä»¶ |
| **v1.26** | NodeInclusionPolicyInPodTopologySpread | Beta | æ‹“æ‰‘åˆ†å¸ƒè€ƒè™‘èŠ‚ç‚¹äº²å’Œæ€§ |
| **v1.27** | SchedulerQueueingHints | Beta | æ™ºèƒ½é‡è°ƒåº¦ |
| **v1.27** | PodSchedulingReadiness | Beta | è°ƒåº¦é—¨æ§å¢å¼º |
| **v1.28** | MatchLabelKeys in TopologySpread | Beta | æ»šåŠ¨æ›´æ–°æ—¶çš„æ‹“æ‰‘åˆ†å¸ƒ |
| **v1.29** | Scheduler QueueingHints | GA | æ™ºèƒ½é‡è°ƒåº¦ç¨³å®š |
| **v1.30** | VolumeCapacityPriority | Beta | è€ƒè™‘å·å®¹é‡çš„è°ƒåº¦ |
| **v1.31** | Pod Scheduling Readiness | GA | è°ƒåº¦é—¨æ§ç¨³å®š |
| **v1.32** | DRA (Dynamic Resource Allocation) | Beta | åŠ¨æ€èµ„æºåˆ†é…å¢å¼º |

### 13.2 ç‰ˆæœ¬ç‰¹å®šé…ç½®

```yaml
# v1.27+ è°ƒåº¦é—¨æ§ç¤ºä¾‹
apiVersion: v1
kind: Pod
metadata:
  name: gated-pod
spec:
  schedulingGates:
  - name: example.com/wait-for-config
  containers:
  - name: app
    image: myapp:latest
---
# v1.28+ MatchLabelKeys ç¤ºä¾‹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  template:
    metadata:
      labels:
        app: web
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: web
        matchLabelKeys:
        - pod-template-hash  # v1.28+ æ»šåŠ¨æ›´æ–°æ—¶åªè€ƒè™‘åŒç‰ˆæœ¬Pod
```

---

## 14. å¤šè§’è‰²è§†è§’

### 14.1 æ¶æ„å¸ˆè§†è§’

| å…³æ³¨ç‚¹ | è®¾è®¡è¦ç‚¹ | æ¨èé…ç½® |
|-------|---------|---------|
| **é«˜å¯ç”¨è®¾è®¡** | è·¨å¯ç”¨åŒºåˆ†å¸ƒ | topologySpreadConstraints |
| **èµ„æºè§„åˆ’** | é¢„ç•™ç¼“å†²èµ„æº | é›†ç¾¤åˆ©ç”¨ç‡<80% |
| **æ‰©å±•æ€§** | è‡ªåŠ¨æ‰©ç¼©å®¹ | Cluster Autoscaler |
| **ä¼˜å…ˆçº§ç­–ç•¥** | å…³é”®æœåŠ¡ä¼˜å…ˆ | PriorityClass åˆ†çº§ |
| **æ•…éšœéš”ç¦»** | Podåäº²å’Œæ€§ | podAntiAffinity |

### 14.2 æµ‹è¯•å·¥ç¨‹å¸ˆè§†è§’

| æµ‹è¯•ç±»å‹ | æµ‹è¯•ç›®æ ‡ | æµ‹è¯•æ–¹æ³• |
|---------|---------|---------|
| **è°ƒåº¦åŠŸèƒ½æµ‹è¯•** | éªŒè¯è°ƒåº¦çº¦æŸæ­£ç¡®æ€§ | åˆ›å»ºå„ç§çº¦æŸçš„Pod |
| **èµ„æºå‹åŠ›æµ‹è¯•** | éªŒè¯èµ„æºä¸è¶³å¤„ç† | æ¨¡æ‹Ÿèµ„æºè€—å°½åœºæ™¯ |
| **æ‰©ç¼©å®¹æµ‹è¯•** | éªŒè¯è‡ªåŠ¨æ‰©å®¹ | è§¦å‘æ‰©å®¹å¹¶éªŒè¯ |
| **æ•…éšœæ³¨å…¥æµ‹è¯•** | éªŒè¯è°ƒåº¦å™¨æ¢å¤ | æ€æ­»è°ƒåº¦å™¨è§‚å¯Ÿè¡Œä¸º |
| **æ€§èƒ½æµ‹è¯•** | è°ƒåº¦å»¶è¿ŸåŸºå‡† | æ‰¹é‡åˆ›å»ºPodæµ‹é‡å»¶è¿Ÿ |

### 14.3 äº§å“ç»ç†è§†è§’

| SLAæŒ‡æ ‡ | ç›®æ ‡å€¼ | ç›‘æ§æ–¹å¼ |
|--------|-------|---------|
| **è°ƒåº¦æˆåŠŸç‡** | > 99.9% | scheduler_schedule_attempts_total |
| **è°ƒåº¦å»¶è¿Ÿ P99** | < 1s | scheduler_scheduling_attempt_duration_seconds |
| **Pending Podæ•°é‡** | < 10 | kube_pod_status_phase{phase="Pending"} |
| **èµ„æºå¯ç”¨ç‡** | > 20% | é›†ç¾¤èµ„æºåˆ©ç”¨ç‡ |

---

## 15. æœ€ä½³å®è·µ

### 15.1 é¢„é˜²æªæ–½æ¸…å•

- [ ] **èµ„æºç®¡ç†**
  - [ ] ä¸ºæ‰€æœ‰å®¹å™¨è®¾ç½® requests å’Œ limits
  - [ ] requests åŸºäºå®é™…ä½¿ç”¨é‡ P95 è®¾ç½®
  - [ ] é…ç½® VPA è‡ªåŠ¨è°ƒæ•´èµ„æº
  - [ ] é›†ç¾¤ä¿ç•™ 20%+ ç¼“å†²èµ„æº

- [ ] **è°ƒåº¦ç­–ç•¥**
  - [ ] ä½¿ç”¨è½¯äº²å’Œæ€§é¿å…è°ƒåº¦å¤±è´¥
  - [ ] é…ç½®åˆç†çš„æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
  - [ ] ä¸ºå…³é”®æœåŠ¡é…ç½® PriorityClass
  - [ ] é¿å…è¿‡äºä¸¥æ ¼çš„ nodeSelector

- [ ] **å­˜å‚¨é…ç½®**
  - [ ] ä½¿ç”¨ `WaitForFirstConsumer` ç»‘å®šæ¨¡å¼
  - [ ] ç¡®ä¿ StorageClass æ­£ç¡®é…ç½®
  - [ ] å®šæœŸæ£€æŸ¥ CSI é©±åŠ¨å¥åº·çŠ¶æ€

- [ ] **ç›‘æ§å‘Šè­¦**
  - [ ] é…ç½® Pod Pending å‘Šè­¦
  - [ ] ç›‘æ§é›†ç¾¤èµ„æºä½¿ç”¨ç‡
  - [ ] ç›‘æ§è°ƒåº¦å™¨å¥åº·çŠ¶æ€
  - [ ] é…ç½®è‡ªåŠ¨æ‰©å®¹è§¦å‘æ¡ä»¶

### 15.2 è¯Šæ–­åŸåˆ™

1. **å…ˆçœ‹ Events**: `kubectl describe pod` ç¡®å®šæ ¹æœ¬åŸå› 
2. **åˆ†ç±»æ’æŸ¥**: èµ„æº â†’ èŠ‚ç‚¹é€‰æ‹© â†’ å­˜å‚¨ â†’ é…é¢ â†’ è°ƒåº¦å™¨
3. **é’ˆå¯¹æ€§è§£å†³**: æ ¹æ®å…·ä½“åŸå› é‡‡å–å¯¹åº”æªæ–½
4. **éªŒè¯ä¿®å¤**: ç¡®è®¤ Pod æˆåŠŸè°ƒåº¦å¹¶è¿è¡Œ
5. **è®°å½•æ ¹å› **: æ›´æ–°è¿ç»´çŸ¥è¯†åº“ï¼Œé˜²æ­¢å¤å‘

### 15.3 ç›¸å…³æ–‡æ¡£

| ä¸»é¢˜ | æ–‡æ¡£ç¼–å· | è¯´æ˜ |
|------|---------|------|
| èŠ‚ç‚¹é—®é¢˜è¯Šæ–­ | 103-node-notready-diagnosis | èŠ‚ç‚¹ NotReady æ’æŸ¥ |
| å­˜å‚¨é—®é¢˜è¯Šæ–­ | 104-storage-issue-diagnosis | PVC/PV é—®é¢˜æ’æŸ¥ |
| è°ƒåº¦å™¨æ·±åº¦è§£æ | 164-kube-scheduler-deep-dive | è°ƒåº¦æ¡†æ¶è¯¦è§£ |
| Cluster Autoscaler | 45-cluster-autoscaler | è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½® |
| èµ„æºç®¡ç† | 05-resource-management | èµ„æºé…ç½®æœ€ä½³å®è·µ |

---

**è¡¨æ ¼åº•éƒ¨æ ‡è®°**: Kusheet Project | ä½œè€…: Allen Galler (allengaller@gmail.com) | æœ€åæ›´æ–°: 2026-01 | ç‰ˆæœ¬: v1.25-v1.32

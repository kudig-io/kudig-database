# 41 - äº‹ä»¶é©±åŠ¨æ¶æ„æ•…éšœæ’æŸ¥ (Event-Driven Architecture Troubleshooting)

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25-v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä¸“å®¶çº§åˆ«**: â­â­â­â­â­ | **å‚è€ƒ**: [CloudEvents Specification](https://cloudevents.io/), [Knative Eventing](https://knative.dev/docs/eventing/), [Apache Kafka Documentation](https://kafka.apache.org/documentation/)

---

## ç›¸å…³æ–‡æ¡£äº¤å‰å¼•ç”¨

### ğŸ”— å…³è”æ•…éšœæ’æŸ¥æ–‡æ¡£
- **[25-ç½‘ç»œè¿é€šæ€§æ•…éšœæ’æŸ¥](./25-network-connectivity-troubleshooting.md)** - äº‹ä»¶ä¼ è¾“ç½‘ç»œé—®é¢˜
- **[30-ç›‘æ§å‘Šè­¦æ•…éšœæ’æŸ¥](./30-monitoring-alerting-troubleshooting.md)** - äº‹ä»¶ç³»ç»Ÿç›‘æ§å‘Šè­¦
- **[36-Helm Chartæ•…éšœæ’æŸ¥](./36-helm-chart-troubleshooting.md)** - äº‹ä»¶ç³»ç»Ÿéƒ¨ç½²é—®é¢˜
- **[37-å¤šé›†ç¾¤ç®¡ç†æ•…éšœæ’æŸ¥](./37-multi-cluster-management-troubleshooting.md)** - è·¨é›†ç¾¤äº‹ä»¶è·¯ç”±

### ğŸ“š æ‰©å±•å­¦ä¹ èµ„æ–™
- **[CloudEventsè§„èŒƒ](https://github.com/cloudevents/spec)** - äº‘åŸç”Ÿäº‹ä»¶æ ‡å‡†
- **[Knative Serving](https://knative.dev/docs/serving/)** - æ— æœåŠ¡å™¨äº‹ä»¶å¤„ç†
- **[Strimzi Kafka Operator](https://strimzi.io/)** - Kubernetesä¸Šçš„Kafkaç®¡ç†

---

## ç›®å½•

1. [äº‹ä»¶é©±åŠ¨æ¶æ„æ¦‚è¿°](#1-äº‹ä»¶é©±åŠ¨æ¶æ„æ¦‚è¿°)
2. [æ ¸å¿ƒç»„ä»¶æ•…éšœæ’æŸ¥](#2-æ ¸å¿ƒç»„ä»¶æ•…éšœæ’æŸ¥)
3. [äº‹ä»¶æµé—®é¢˜è¯Šæ–­](#3-äº‹ä»¶æµé—®é¢˜è¯Šæ–­)
4. [æ€§èƒ½ç“¶é¢ˆåˆ†æ](#4-æ€§èƒ½ç“¶é¢ˆåˆ†æ)
5. [å¯é æ€§é—®é¢˜æ’æŸ¥](#5-å¯é æ€§é—®é¢˜æ’æŸ¥)
6. [ç›‘æ§å‘Šè­¦é…ç½®](#6-ç›‘æ§å‘Šè­¦é…ç½®)
7. [æœ€ä½³å®è·µä¸ä¼˜åŒ–](#7-æœ€ä½³å®è·µä¸ä¼˜åŒ–)

---

## 1. äº‹ä»¶é©±åŠ¨æ¶æ„æ¦‚è¿°

### 1.1 æ¶æ„æ¨¡å¼åˆ†æ

```
äº‹ä»¶é©±åŠ¨æ¶æ„æ ¸å¿ƒç»„ä»¶:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          äº‹ä»¶é©±åŠ¨æ¶æ„å…¨æ™¯å›¾                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   äº‹ä»¶ç”Ÿäº§è€…     â”‚    â”‚   äº‹ä»¶ä»£ç†å±‚     â”‚    â”‚   äº‹ä»¶æ¶ˆè´¹è€…     â”‚        â”‚
â”‚  â”‚ Event Producers â”‚    â”‚ Event Brokers   â”‚    â”‚ Event Consumers â”‚        â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚        â”‚
â”‚  â”‚ â€¢ åº”ç”¨æœåŠ¡       â”‚    â”‚ â€¢ Kafka/Redis   â”‚    â”‚ â€¢ å¾®æœåŠ¡         â”‚        â”‚
â”‚  â”‚ â€¢ IoTè®¾å¤‡       â”‚    â”‚ â€¢ NATS Streamingâ”‚    â”‚ â€¢ æ— æœåŠ¡å™¨å‡½æ•°    â”‚        â”‚
â”‚  â”‚ â€¢ æ•°æ®åº“å˜æ›´     â”‚    â”‚ â€¢ RabbitMQ      â”‚    â”‚ â€¢ æ‰¹å¤„ç†ä½œä¸š     â”‚        â”‚
â”‚  â”‚ â€¢ ç”¨æˆ·æ“ä½œ       â”‚    â”‚ â€¢ Pulsar        â”‚    â”‚ â€¢ å®æ—¶åˆ†æ       â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                         â”‚                         â”‚            â”‚
â”‚           â–¼                         â–¼                         â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   äº‹ä»¶æ ¼å¼åŒ–     â”‚    â”‚   äº‹ä»¶è·¯ç”±       â”‚    â”‚   äº‹ä»¶å¤„ç†       â”‚        â”‚
â”‚  â”‚ Event Formattingâ”‚    â”‚ Event Routing   â”‚    â”‚ Event Processingâ”‚        â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚        â”‚
â”‚  â”‚ â€¢ CloudEvents   â”‚    â”‚ â€¢ TriggerMesh   â”‚    â”‚ â€¢ Knative       â”‚        â”‚
â”‚  â”‚ â€¢ Schema Registryâ”‚    â”‚ â€¢ EventBridge   â”‚    â”‚ â€¢ Functions     â”‚        â”‚
â”‚  â”‚ â€¢ Serialization â”‚    â”‚ â€¢ Filters       â”‚    â”‚ â€¢ Stream Proc   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 å¸¸è§æ•…éšœç°è±¡åˆ†ç±»

| æ•…éšœç±»å‹ | ç—‡çŠ¶è¡¨ç° | å½±å“èŒƒå›´ | ç´§æ€¥ç¨‹åº¦ |
|---------|---------|---------|---------|
| **äº‹ä»¶ä¸¢å¤±** | æ¶ˆæ¯æœªé€è¾¾æ¶ˆè´¹è€… | æ•°æ®å®Œæ•´æ€§å—æŸ | P0 - ç´§æ€¥ |
| **äº‹ä»¶é‡å¤** | åŒä¸€æ¶ˆæ¯å¤šæ¬¡å¤„ç† | æ•°æ®ä¸€è‡´æ€§é—®é¢˜ | P1 - é«˜ |
| **å¤„ç†å»¶è¿Ÿ** | äº‹ä»¶ç§¯å‹ã€å“åº”æ…¢ | ä¸šåŠ¡å®æ—¶æ€§å—æŸ | P1 - é«˜ |
| **æ¶ˆè´¹è€…å¤±è´¥** | å¤„ç†ç¨‹åºå´©æºƒ | ä¸šåŠ¡é€»è¾‘ä¸­æ–­ | P0 - ç´§æ€¥ |
| **èƒŒå‹é—®é¢˜** | ç”Ÿäº§è€…é˜»å¡ | ç³»ç»Ÿååé‡ä¸‹é™ | P1 - é«˜ |
| **æ­»ä¿¡é˜Ÿåˆ—æ»¡** | æ— æ³•å¤„ç†çš„æ¶ˆæ¯å †ç§¯ | ç³»ç»Ÿèµ„æºè€—å°½ | P0 - ç´§æ€¥ |

---

## 2. æ ¸å¿ƒç»„ä»¶æ•…éšœæ’æŸ¥

### 2.1 Kafkaé›†ç¾¤æ•…éšœæ’æŸ¥

#### 2.1.1 BrokerçŠ¶æ€æ£€æŸ¥
```bash
# ========== 1. Kafkaé›†ç¾¤å¥åº·æ£€æŸ¥ ==========
# æ£€æŸ¥Kafka PodçŠ¶æ€
kubectl get pods -n kafka -l app=kafka

# éªŒè¯KafkaæœåŠ¡ç«¯ç‚¹
kubectl get svc -n kafka

# æ£€æŸ¥Kafkaæ§åˆ¶å™¨çŠ¶æ€
kubectl exec -n kafka kafka-0 -- kafka-topics.sh --bootstrap-server localhost:9092 --describe --under-replicated-partitions

# ========== 2. Zookeeperè¿æ¥æ£€æŸ¥ ==========
# æ£€æŸ¥ZookeeperçŠ¶æ€
kubectl exec -n kafka zookeeper-0 -- zkCli.sh ls /

# éªŒè¯Kafkaåœ¨Zookeeperä¸­çš„æ³¨å†Œ
kubectl exec -n kafka zookeeper-0 -- zkCli.sh get /brokers/ids/0

# ========== 3. Topicå’Œåˆ†åŒºçŠ¶æ€ ==========
# åˆ—å‡ºæ‰€æœ‰Topic
kubectl exec -n kafka kafka-0 -- kafka-topics.sh --bootstrap-server localhost:9092 --list

# æ£€æŸ¥ç‰¹å®šTopicè¯¦æƒ…
kubectl exec -n kafka kafka-0 -- kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic

# æŸ¥çœ‹æ¶ˆè´¹è€…ç»„çŠ¶æ€
kubectl exec -n kafka kafka-0 -- kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
kubectl exec -n kafka kafka-0 -- kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-consumer-group
```

#### 2.1.2 æ€§èƒ½æŒ‡æ ‡ç›‘æ§
```yaml
# kafka_monitoring_rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kafka-alerts
  namespace: monitoring
spec:
  groups:
  - name: kafka.rules
    rules:
    # Brokerå¥åº·æ£€æŸ¥
    - alert: KafkaBrokerDown
      expr: kafka_broker_info == 0
      for: 2m
      labels:
        severity: critical
        category: kafka
      annotations:
        summary: "Kafka broker {{ $labels.instance }} is down"
        
    # åˆ†åŒºç¦»çº¿å‘Šè­¦
    - alert: KafkaOfflinePartitions
      expr: kafka_topic_partitions{state="offline"} > 0
      for: 1m
      labels:
        severity: critical
        category: kafka
      annotations:
        summary: "{{ $value }} offline partitions detected"
        
    # æ¶ˆè´¹å»¶è¿Ÿå‘Šè­¦
    - alert: KafkaConsumerLagHigh
      expr: kafka_consumergroup_lag > 10000
      for: 5m
      labels:
        severity: warning
        category: kafka
      annotations:
        summary: "Consumer lag high for group {{ $labels.consumergroup }}"
        
    # ç£ç›˜ä½¿ç”¨ç‡å‘Šè­¦
    - alert: KafkaDiskUsageHigh
      expr: (kafka_log_size_bytes / kafka_log_capacity_bytes) * 100 > 85
      for: 10m
      labels:
        severity: warning
        category: kafka
      annotations:
        summary: "Kafka disk usage {{ $value | printf \"%.1f\" }}% high"
```

### 2.2 Knative Eventingæ•…éšœæ’æŸ¥

#### 2.2.1 äº‹ä»¶ç½‘æ ¼çŠ¶æ€æ£€æŸ¥
```bash
# ========== 1. Knativeç»„ä»¶çŠ¶æ€ ==========
# æ£€æŸ¥Knative Serving
kubectl get pods -n knative-serving

# æ£€æŸ¥Knative Eventing
kubectl get pods -n knative-eventing

# éªŒè¯BrokerçŠ¶æ€
kubectl get brokers -A

# æ£€æŸ¥TriggerçŠ¶æ€
kubectl get triggers -A

# ========== 2. äº‹ä»¶äº¤ä»˜æ£€æŸ¥ ==========
# æŸ¥çœ‹äº‹ä»¶æºçŠ¶æ€
kubectl get apiserversources -A
kubectl get pingsources -A
kubectl get kafkasources -A

# æ£€æŸ¥äº‹ä»¶è·¯ç”±
kubectl get subscriptions -A

# éªŒè¯SinkBinding
kubectl get sinkbindings -A

# ========== 3. äº‹ä»¶è¿½è¸ªè¯Šæ–­ ==========
# å¯ç”¨äº‹ä»¶è¿½è¸ª
kubectl patch configmap config-tracing -n knative-eventing --type merge \
  -p '{"data":{"backend":"zipkin","zipkin-endpoint":"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans","debug":"true"}}'

# æŸ¥çœ‹äº‹ä»¶è½¨è¿¹
kubectl port-forward -n istio-system svc/tracing 16686:16686
# è®¿é—® http://localhost:16686 æŸ¥çœ‹äº‹ä»¶è¿½è¸ª
```

### 2.3 Redis Streamsæ•…éšœæ’æŸ¥

#### 2.3.1 Rediså®ä¾‹å¥åº·æ£€æŸ¥
```bash
# ========== 1. Redisè¿æ¥å’ŒçŠ¶æ€æ£€æŸ¥ ==========
# æ£€æŸ¥Redis PodçŠ¶æ€
kubectl get pods -n redis -l app=redis

# éªŒè¯Redisè¿æ¥
REDIS_POD=$(kubectl get pods -n redis -l app=redis -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n redis $REDIS_POD -- redis-cli ping

# æ£€æŸ¥Rediså†…å­˜ä½¿ç”¨
kubectl exec -n redis $REDIS_POD -- redis-cli info memory

# æŸ¥çœ‹Redisé…ç½®
kubectl exec -n redis $REDIS_POD -- redis-cli config get maxmemory*

# ========== 2. StreamçŠ¶æ€ç›‘æ§ ==========
# åˆ—å‡ºæ‰€æœ‰Streams
kubectl exec -n redis $REDIS_POD -- redis-cli xinfo streams

# æ£€æŸ¥ç‰¹å®šStreamä¿¡æ¯
kubectl exec -n redis $REDIS_POD -- redis-cli xinfo stream mystream

# æŸ¥çœ‹æ¶ˆè´¹è€…ç»„çŠ¶æ€
kubectl exec -n redis $REDIS_POD -- redis-cli xinfo groups mystream

# æ£€æŸ¥å¾…å¤„ç†æ¶ˆæ¯
kubectl exec -n redis $REDIS_POD -- redis-cli xpending mystream mygroup
```

---

## 3. äº‹ä»¶æµé—®é¢˜è¯Šæ–­

### 3.1 äº‹ä»¶ä¸¢å¤±é—®é¢˜æ’æŸ¥

#### 3.1.1 è¯Šæ–­æµç¨‹
```bash
# ========== äº‹ä»¶ä¸¢å¤±è¯Šæ–­è„šæœ¬ ==========
cat <<'EOF' > event-loss-diagnostic.sh
#!/bin/bash

NAMESPACE=${1:-default}
TOPIC_NAME=${2:-my-topic}

echo "=== Event Loss Diagnostic Report ==="
echo "Namespace: $NAMESPACE"
echo "Topic: $TOPIC_NAME"
echo "Time: $(date)"
echo

# 1. æ£€æŸ¥ç”Ÿäº§è€…çŠ¶æ€
echo "1. Producer Status Check:"
kubectl get pods -n $NAMESPACE | grep producer
PRODUCER_POD=$(kubectl get pods -n $NAMESPACE -l app=event-producer -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
if [ ! -z "$PRODUCER_POD" ]; then
    echo "Producer Pod: $PRODUCER_POD"
    kubectl logs $PRODUCER_POD -n $NAMESPACE --tail=50 | grep -i "error\|exception\|failed"
else
    echo "No producer pods found"
fi
echo

# 2. æ£€æŸ¥BrokerçŠ¶æ€
echo "2. Broker Status Check:"
kubectl get pods -n kafka | grep kafka
kubectl exec -n kafka kafka-0 -- kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic $TOPIC_NAME
echo

# 3. æ£€æŸ¥æ¶ˆè´¹è€…çŠ¶æ€
echo "3. Consumer Status Check:"
kubectl get pods -n $NAMESPACE | grep consumer
CONSUMER_POD=$(kubectl get pods -n $NAMESPACE -l app=event-consumer -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
if [ ! -z "$CONSUMER_POD" ]; then
    echo "Consumer Pod: $CONSUMER_POD"
    kubectl logs $CONSUMER_POD -n $NAMESPACE --tail=50 | grep -i "error\|exception\|offset"
else
    echo "No consumer pods found"
fi
echo

# 4. æ£€æŸ¥æ¶ˆæ¯ç§¯å‹
echo "4. Message Backlog Check:"
kubectl exec -n kafka kafka-0 -- kafka-run-class.sh kafka.tools.GetOffsetShell \
    --broker-list localhost:9092 --topic $TOPIC_NAME --time -1
echo

# 5. æ£€æŸ¥ç½‘ç»œè¿æ¥
echo "5. Network Connectivity Check:"
kubectl exec -n kafka kafka-0 -- netstat -an | grep :9092
EOF

chmod +x event-loss-diagnostic.sh
```

#### 3.1.2 å¸¸è§æ ¹æœ¬åŸå› 
| åŸå› ç±»åˆ« | å…·ä½“åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|---------|---------|
| **é…ç½®é—®é¢˜** | Acknowledgmentè®¾ç½®ä¸å½“ | è°ƒæ•´acks=allï¼Œå¯ç”¨å¹‚ç­‰æ€§ |
| **ç½‘ç»œé—®é¢˜** | ç½‘ç»œåˆ†åŒºã€è¿æ¥è¶…æ—¶ | æ£€æŸ¥ç½‘ç»œç­–ç•¥ï¼Œå¢åŠ è¶…æ—¶é…ç½® |
| **èµ„æºä¸è¶³** | å†…å­˜æº¢å‡ºã€ç£ç›˜æ»¡ | æ‰©å®¹Brokerï¼Œæ¸…ç†æ—§æ•°æ® |
| **ä»£ç ç¼ºé™·** | å¼‚å¸¸å¤„ç†ä¸å½“ | å®Œå–„é”™è¯¯å¤„ç†ï¼Œå®ç°é‡è¯•æœºåˆ¶ |
| **æ¶ˆè´¹è€…é—®é¢˜** | æ¶ˆè´¹åç§»é‡å¼‚å¸¸ | é‡ç½®æ¶ˆè´¹ä½ç‚¹ï¼Œæ£€æŸ¥æ¶ˆè´¹è€…é€»è¾‘ |

### 3.2 äº‹ä»¶é‡å¤é—®é¢˜å¤„ç†

#### 3.2.1 å¹‚ç­‰æ€§è®¾è®¡æ¨¡å¼
```yaml
# å¹‚ç­‰æ€§æ¶ˆè´¹è€…é…ç½®ç¤ºä¾‹
apiVersion: apps/v1
kind: Deployment
metadata:
  name: idempotent-consumer
spec:
  replicas: 3
  selector:
    matchLabels:
      app: idempotent-consumer
  template:
    metadata:
      labels:
        app: idempotent-consumer
    spec:
      containers:
      - name: consumer
        image: my-consumer:latest
        env:
        # å¯ç”¨å¹‚ç­‰æ€§é…ç½®
        - name: ENABLE_IDEMPOTENCE
          value: "true"
        - name: MAX_IN_FLIGHT
          value: "1"  # ç¡®ä¿é¡ºåºå¤„ç†
        - name: RETRY_ATTEMPTS
          value: "3"
        - name: DEDUPLICATION_WINDOW
          value: "3600"  # 1å°æ—¶å»é‡çª—å£
```

#### 3.2.2 å»é‡ç­–ç•¥å®ç°
```python
# python_consumer_with_deduplication.py
import hashlib
import time
from typing import Set
import redis

class DeduplicatingConsumer:
    def __init__(self, redis_host: str = 'localhost', redis_port: int = 6379):
        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        self.processed_events: Set[str] = set()
        self.dedup_window = 3600  # 1å°æ—¶å»é‡çª—å£
        
    def generate_event_id(self, event_data: dict) -> str:
        """ç”Ÿæˆäº‹ä»¶å”¯ä¸€æ ‡è¯†"""
        # åŸºäºä¸šåŠ¡å…³é”®å­—æ®µç”Ÿæˆå“ˆå¸Œ
        key_fields = [event_data.get('id'), event_data.get('timestamp')]
        key_string = '|'.join(str(field) for field in key_fields if field)
        return hashlib.sha256(key_string.encode()).hexdigest()
        
    def is_duplicate(self, event_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤äº‹ä»¶"""
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if event_id in self.processed_events:
            return True
            
        # æ£€æŸ¥RedisæŒä¹…åŒ–è®°å½•
        redis_key = f"processed_events:{event_id}"
        if self.redis_client.exists(redis_key):
            # åˆ·æ–°è¿‡æœŸæ—¶é—´
            self.redis_client.expire(redis_key, self.dedup_window)
            return True
            
        return False
        
    def mark_processed(self, event_id: str):
        """æ ‡è®°äº‹ä»¶å·²å¤„ç†"""
        # æ·»åŠ åˆ°å†…å­˜ç¼“å­˜
        self.processed_events.add(event_id)
        
        # æ·»åŠ åˆ°RedisæŒä¹…åŒ–å­˜å‚¨
        redis_key = f"processed_events:{event_id}"
        self.redis_client.setex(redis_key, self.dedup_window, "1")
        
        # ç»´æŠ¤å†…å­˜ç¼“å­˜å¤§å°
        if len(self.processed_events) > 10000:
            oldest_keys = list(self.processed_events)[:1000]
            self.processed_events = self.processed_events.difference(set(oldest_keys))
            
    def process_event(self, event_data: dict):
        """å¤„ç†äº‹ä»¶ï¼ˆå¹‚ç­‰æ€§ä¿è¯ï¼‰"""
        event_id = self.generate_event_id(event_data)
        
        if self.is_duplicate(event_id):
            print(f"Skipping duplicate event: {event_id}")
            return
            
        try:
            # æ‰§è¡Œä¸šåŠ¡é€»è¾‘
            self.handle_business_logic(event_data)
            
            # æ ‡è®°ä¸ºå·²å¤„ç†
            self.mark_processed(event_id)
            print(f"Processed event: {event_id}")
            
        except Exception as e:
            print(f"Error processing event {event_id}: {e}")
            # ä¸æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œå…è®¸é‡è¯•
            raise
            
    def handle_business_logic(self, event_data: dict):
        """å…·ä½“çš„ä¸šåŠ¡å¤„ç†é€»è¾‘"""
        # è¿™é‡Œå®ç°å…·ä½“çš„ä¸šåŠ¡é€»è¾‘
        pass

# ä½¿ç”¨ç¤ºä¾‹
consumer = DeduplicatingConsumer()
# consumer.process_event(event_data)
```

---

## 4. æ€§èƒ½ç“¶é¢ˆåˆ†æ

### 4.1 ååé‡ä¼˜åŒ–

#### 4.1.1 Kafkaæ€§èƒ½è°ƒä¼˜
```yaml
# kafka_performance_tuning.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: high-throughput-cluster
spec:
  kafka:
    version: 3.4.0
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    config:
      # æ€§èƒ½ä¼˜åŒ–é…ç½®
      num.network.threads: 8
      num.io.threads: 16
      socket.send.buffer.bytes: 102400
      socket.receive.buffer.bytes: 102400
      socket.request.max.bytes: 104857600
      num.replica.fetchers: 4
      replica.fetch.max.bytes: 10485760
      replica.fetch.wait.max.ms: 500
      
      # å­˜å‚¨ä¼˜åŒ–
      log.flush.interval.messages: 10000
      log.flush.interval.ms: 1000
      log.retention.hours: 168
      log.segment.bytes: 1073741824
      log.retention.check.interval.ms: 300000
      
      # å‹ç¼©é…ç½®
      compression.type: snappy
      message.max.bytes: 10485760
      
    storage:
      type: jbod
      volumes:
      - id: 0
        type: persistent-claim
        size: 200Gi
        deleteClaim: false
```

#### 4.1.2 æ¶ˆè´¹è€…æ€§èƒ½ä¼˜åŒ–
```yaml
# consumer_performance_config.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: optimized-consumer
spec:
  replicas: 6  # æ ¹æ®åˆ†åŒºæ•°è°ƒæ•´
  template:
    spec:
      containers:
      - name: consumer
        image: my-consumer:latest
        env:
        # æ‰¹é‡å¤„ç†é…ç½®
        - name: FETCH_MIN_BYTES
          value: "1048576"  # 1MB
        - name: FETCH_MAX_WAIT_MS
          value: "500"
        - name: MAX_POLL_RECORDS
          value: "500"
        - name: MAX_POLL_INTERVAL_MS
          value: "300000"  # 5åˆ†é’Ÿ
          
        # å†…å­˜ä¼˜åŒ–
        - name: HEAP_OPTS
          value: "-Xmx2g -Xms2g"
        - name: GC_TUNING
          value: "-XX:+UseG1GC -XX:MaxGCPauseMillis=20"
          
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
```

### 4.2 å»¶è¿Ÿåˆ†æå·¥å…·

#### 4.2.1 ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹é‡
```bash
# ========== äº‹ä»¶å»¶è¿Ÿæµ‹é‡è„šæœ¬ ==========
cat <<'EOF' > event-latency-measurement.sh
#!/bin/bash

TOPIC_NAME=${1:-latency-test}
MESSAGE_COUNT=${2:-1000}
INTERVAL=${3:-0.1}

echo "Starting latency measurement for topic: $TOPIC_NAME"
echo "Messages to send: $MESSAGE_COUNT"
echo "Interval: ${INTERVAL}s"
echo

# åˆ›å»ºæµ‹è¯•Topic
kubectl exec -n kafka kafka-0 -- kafka-topics.sh --bootstrap-server localhost:9092 \
    --create --topic $TOPIC_NAME --partitions 6 --replication-factor 3 2>/dev/null

# å¯åŠ¨æ¶ˆè´¹è€…ï¼ˆåå°ï¼‰
kubectl run kafka-consumer --image=strimzi/kafka:latest-kafka-3.4.0 \
    --restart=Never --attach --rm \
    -- kafka-console-consumer.sh --bootstrap-server kafka-kafka-bootstrap:9092 \
    --topic $TOPIC_NAME --from-beginning --timeout-ms 60000 > /tmp/consumer_output.txt &

CONSUMER_PID=$!

# å‘é€å¸¦æ—¶é—´æˆ³çš„æ¶ˆæ¯
echo "Sending messages with timestamps..."
START_TIME=$(date +%s.%N)

for i in $(seq 1 $MESSAGE_COUNT); do
    TIMESTAMP=$(date +%s%3N)
    MESSAGE="msg-$i-$TIMESTAMP"
    
    kubectl exec -n kafka kafka-0 -- kafka-console-producer.sh \
        --bootstrap-server localhost:9092 \
        --topic $TOPIC_NAME <<< "$MESSAGE"
    
    sleep $INTERVAL
done

END_TIME=$(date +%s.%N)
echo "All messages sent. Waiting for consumer..."

# ç­‰å¾…æ¶ˆè´¹è€…å®Œæˆ
wait $CONSUMER_PID

# åˆ†æå»¶è¿Ÿ
echo
echo "=== Latency Analysis Results ==="
python3 <<PY_END
import re
import statistics
from datetime import datetime

with open('/tmp/consumer_output.txt', 'r') as f:
    lines = f.readlines()

latencies = []
for line in lines:
    match = re.search(r'msg-(\d+)-(\d+)', line.strip())
    if match:
        msg_num, timestamp = match.groups()
        receive_time = datetime.now().timestamp() * 1000
        send_time = int(timestamp)
        latency = receive_time - send_time
        latencies.append(latency)
        print(f"Message {msg_num}: {latency:.2f}ms")

if latencies:
    print(f"\nLatency Statistics:")
    print(f"  Average: {statistics.mean(latencies):.2f}ms")
    print(f"  Median: {statistics.median(latencies):.2f}ms")
    print(f"  95th Percentile: {sorted(latencies)[int(len(latencies)*0.95)]:.2f}ms")
    print(f"  Max: {max(latencies):.2f}ms")
    print(f"  Min: {min(latencies):.2f}ms")
else:
    print("No latency data collected")
PY_END

# æ¸…ç†æµ‹è¯•Topic
kubectl exec -n kafka kafka-0 -- kafka-topics.sh --bootstrap-server localhost:9092 \
    --delete --topic $TOPIC_NAME 2>/dev/null

echo "Test completed."
EOF

chmod +x event-latency-measurement.sh
```

---

## 5. å¯é æ€§é—®é¢˜æ’æŸ¥

### 5.1 æ­»ä¿¡é˜Ÿåˆ—ç®¡ç†

#### 5.1.1 DLQé…ç½®å’Œç›‘æ§
```yaml
# dead_letter_queue_config.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dlq-processor
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dlq-processor
  template:
    metadata:
      labels:
        app: dlq-processor
    spec:
      containers:
      - name: processor
        image: dlq-processor:latest
        env:
        - name: DLQ_TOPIC
          value: "dead-letter-queue"
        - name: RETRY_TOPIC
          value: "retry-queue"
        - name: MAX_RETRY_ATTEMPTS
          value: "3"
        - name: RETRY_DELAY_SECONDS
          value: "300"  # 5åˆ†é’Ÿé‡è¯•é—´éš”
        - name: ALERT_THRESHOLD
          value: "100"  # ç§¯å‹100æ¡å‘Šè­¦
        
        ports:
        - containerPort: 8080
          name: metrics
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

#### 5.1.2 DLQç›‘æ§å‘Šè­¦
```yaml
# dlq_monitoring_rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: dlq-alerts
  namespace: monitoring
spec:
  groups:
  - name: dlq.rules
    rules:
    # DLQç§¯å‹å‘Šè­¦
    - alert: DLQBacklogHigh
      expr: kafka_topic_highwater{topic="dead-letter-queue"} > 100
      for: 5m
      labels:
        severity: warning
        category: reliability
      annotations:
        summary: "DLQ backlog high: {{ $value }} messages"
        description: "Dead letter queue has accumulated {{ $value }} unprocessed messages"
        
    # DLQå¢é•¿ç‡å‘Šè­¦
    - alert: DLQGrowthRateHigh
      expr: rate(kafka_topic_highwater{topic="dead-letter-queue"}[5m]) > 10
      for: 2m
      labels:
        severity: critical
        category: reliability
      annotations:
        summary: "DLQ growing rapidly: {{ $value | printf \"%.1f\" }} msgs/sec"
        
    # é‡è¯•é˜Ÿåˆ—å µå¡
    - alert: RetryQueueBlocked
      expr: kafka_topic_highwater{topic="retry-queue"} > 1000
      for: 10m
      labels:
        severity: warning
        category: reliability
      annotations:
        summary: "Retry queue blocked: {{ $value }} messages pending"
```

### 5.2 äº‹åŠ¡ä¸€è‡´æ€§ä¿éšœ

#### 5.2.1 åˆ†å¸ƒå¼äº‹åŠ¡æ¨¡å¼
```python
# saga_pattern_implementation.py
from typing import List, Callable, Any
import uuid
import time

class SagaStep:
    def __init__(self, action: Callable, compensation: Callable):
        self.action = action
        self.compensation = compensation
        self.step_id = str(uuid.uuid4())

class SagaOrchestrator:
    def __init__(self):
        self.completed_steps: List[SagaStep] = []
        
    def execute_step(self, step: SagaStep, *args, **kwargs) -> bool:
        """æ‰§è¡ŒSagaæ­¥éª¤"""
        try:
            result = step.action(*args, **kwargs)
            self.completed_steps.append(step)
            return True
        except Exception as e:
            print(f"Step {step.step_id} failed: {e}")
            self.compensate_failed_steps()
            return False
            
    def compensate_failed_steps(self):
        """è¡¥å¿å·²æ‰§è¡Œçš„æ­¥éª¤"""
        print("Initiating compensation...")
        for step in reversed(self.completed_steps):
            try:
                step.compensation()
                print(f"Compensated step: {step.step_id}")
            except Exception as e:
                print(f"Compensation failed for {step.step_id}: {e}")
                
    def execute_saga(self, steps: List[SagaStep], *args, **kwargs) -> bool:
        """æ‰§è¡Œå®Œæ•´çš„Sagaäº‹åŠ¡"""
        self.completed_steps = []
        
        for i, step in enumerate(steps):
            print(f"Executing step {i+1}/{len(steps)}: {step.step_id}")
            if not self.execute_step(step, *args, **kwargs):
                return False
                
        print("Saga completed successfully")
        return True

# ä½¿ç”¨ç¤ºä¾‹
def book_hotel():
    print("Booking hotel...")
    # æ¨¡æ‹Ÿé…’åº—é¢„è®¢
    time.sleep(0.1)
    return {"booking_id": "hotel_123"}

def cancel_hotel_booking():
    print("Canceling hotel booking...")
    # æ¨¡æ‹Ÿå–æ¶ˆé¢„è®¢

def book_flight():
    print("Booking flight...")
    # æ¨¡æ‹Ÿèˆªç­é¢„è®¢ï¼Œå¯èƒ½å¤±è´¥
    if time.time() % 2 > 1:  # æ¨¡æ‹Ÿ50%å¤±è´¥ç‡
        raise Exception("Flight booking failed - no seats available")
    return {"booking_id": "flight_456"}

def cancel_flight_booking():
    print("Canceling flight booking...")

# åˆ›å»ºSagaç¼–æ’å™¨
orchestrator = SagaOrchestrator()

# å®šä¹‰Sagaæ­¥éª¤
hotel_step = SagaStep(book_hotel, cancel_hotel_booking)
flight_step = SagaStep(book_flight, cancel_flight_booking)

# æ‰§è¡ŒSagaäº‹åŠ¡
success = orchestrator.execute_saga([hotel_step, flight_step])

if success:
    print("Travel booking completed!")
else:
    print("Travel booking failed - all reservations cancelled")
```

---

## 6. ç›‘æ§å‘Šè­¦é…ç½®

### 6.1 æ ¸å¿ƒæŒ‡æ ‡ç›‘æ§

#### 6.1.1 äº‹ä»¶ç³»ç»Ÿå…³é”®æŒ‡æ ‡
```yaml
# event_system_monitoring.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: event-system-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: event-processing
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: event-system-alerts
  namespace: monitoring
spec:
  groups:
  - name: event-system.rules
    rules:
    # äº‹ä»¶ç”Ÿäº§é€Ÿç‡å¼‚å¸¸
    - alert: EventProductionRateAnomaly
      expr: |
        abs(
          rate(event_produced_total[5m]) - 
          avg_over_time(rate(event_produced_total[5m])[1h:])
        ) / avg_over_time(rate(event_produced_total[5m])[1h:]) > 0.5
      for: 5m
      labels:
        severity: warning
        category: event-flow
      annotations:
        summary: "Event production rate anomaly detected"
        
    # äº‹ä»¶æ¶ˆè´¹å»¶è¿Ÿ
    - alert: EventProcessingLatencyHigh
      expr: histogram_quantile(0.95, rate(event_processing_duration_seconds_bucket[5m])) > 5
      for: 2m
      labels:
        severity: critical
        category: performance
      annotations:
        summary: "Event processing latency high: {{ $value | printf \"%.2f\" }}s"
        
    # æ¶ˆè´¹è€…å¤±è´¥ç‡
    - alert: EventConsumerFailureRateHigh
      expr: rate(event_consumer_failures_total[5m]) / rate(event_received_total[5m]) > 0.1
      for: 1m
      labels:
        severity: critical
        category: reliability
      annotations:
        summary: "Consumer failure rate: {{ $value | printf \"%.2f\" }}%"
        
    # äº‹ä»¶ç§¯å‹å‘Šè­¦
    - alert: EventBacklogHigh
      expr: event_backlog_size > 10000
      for: 10m
      labels:
        severity: warning
        category: performance
      annotations:
        summary: "Event backlog high: {{ $value }} messages"
```

### 6.2 ä»ªè¡¨æ¿é…ç½®

#### 6.2.1 Grafanaä»ªè¡¨æ¿JSON
```json
{
  "dashboard": {
    "title": "Event-Driven Architecture Monitoring",
    "panels": [
      {
        "title": "Event Production Rate",
        "type": "graph",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "rate(event_produced_total[5m])",
            "legendFormat": "{{topic}}"
          }
        ]
      },
      {
        "title": "Event Consumption Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(event_consumed_total[5m])",
            "legendFormat": "{{consumer_group}}"
          }
        ]
      },
      {
        "title": "Processing Latency (95th Percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(event_processing_duration_seconds_bucket[5m]))",
            "legendFormat": "{{handler}}"
          }
        ]
      },
      {
        "title": "Event Backlog Size",
        "type": "stat",
        "targets": [
          {
            "expr": "event_backlog_size",
            "legendFormat": "Current Backlog"
          }
        ]
      }
    ]
  }
}
```

---

## 7. æœ€ä½³å®è·µä¸ä¼˜åŒ–

### 7.1 æ¶æ„è®¾è®¡åŸåˆ™

#### 7.1.1 é«˜å¯ç”¨æ€§è®¾è®¡
```yaml
# high_availability_design.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: ha-event-bus
spec:
  kafka:
    replicas: 5  # å¥‡æ•°ä¸ªå®ä¾‹ç¡®ä¿é€‰ä¸¾
    version: 3.4.0
    config:
      # é«˜å¯ç”¨é…ç½®
      min.insync.replicas: 3
      default.replication.factor: 3
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
      
    # æœºæ¶æ„ŸçŸ¥é…ç½®
    rack:
      topologyKey: topology.kubernetes.io/zone
      
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 100Gi
      
  entityOperator:
    topicOperator: {}
    userOperator: {}
```

#### 7.1.2 å®¹é”™æœºåˆ¶
```python
# circuit_breaker_pattern.py
import time
from enum import Enum
from typing import Callable, Any, Optional

class CircuitState(Enum):
    CLOSED = "closed"      # æ­£å¸¸çŠ¶æ€
    OPEN = "open"          # æ–­è·¯çŠ¶æ€
    HALF_OPEN = "half_open" # åŠå¼€çŠ¶æ€

class CircuitBreaker:
    def __init__(self, 
                 failure_threshold: int = 5,
                 timeout: int = 60,
                 expected_exception: type = Exception):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.expected_exception = expected_exception
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
        
    def call(self, func: Callable, *args, **kwargs) -> Any:
        """é€šè¿‡æ–­è·¯å™¨è°ƒç”¨å‡½æ•°"""
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                self.state = CircuitState.HALF_OPEN
                print("Circuit breaker half-open - trying one request")
            else:
                raise Exception("Circuit breaker is OPEN - service unavailable")
                
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except self.expected_exception as e:
            self._on_failure()
            raise e
            
    def _on_success(self):
        """æˆåŠŸæ—¶çš„å¤„ç†"""
        if self.state == CircuitState.HALF_OPEN:
            print("Circuit breaker closed - service restored")
            self.state = CircuitState.CLOSED
        self.failure_count = 0
        
    def _on_failure(self):
        """å¤±è´¥æ—¶çš„å¤„ç†"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            print(f"Circuit breaker opened after {self.failure_count} failures")
            self.state = CircuitState.OPEN

# ä½¿ç”¨ç¤ºä¾‹
breaker = CircuitBreaker(failure_threshold=3, timeout=30)

def unreliable_service():
    """æ¨¡æ‹Ÿä¸ç¨³å®šçš„æœåŠ¡"""
    import random
    if random.random() < 0.7:  # 70%å¤±è´¥ç‡
        raise Exception("Service temporarily unavailable")
    return "Success"

# ä¿æŠ¤ä¸ç¨³å®šçš„è°ƒç”¨
try:
    result = breaker.call(unreliable_service)
    print(f"Result: {result}")
except Exception as e:
    print(f"Call failed: {e}")
```

### 7.2 è¿ç»´æ£€æŸ¥æ¸…å•

#### 7.2.1 æ—¥å¸¸è¿ç»´æ£€æŸ¥é¡¹
```markdown
## äº‹ä»¶é©±åŠ¨æ¶æ„è¿ç»´æ£€æŸ¥æ¸…å•

### ğŸ” æ—¥å¸¸ç›‘æ§æ£€æŸ¥
- [ ] äº‹ä»¶ç”Ÿäº§é€Ÿç‡æ˜¯å¦æ­£å¸¸
- [ ] æ¶ˆè´¹è€…ç»„å»¶è¿Ÿæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
- [ ] æ­»ä¿¡é˜Ÿåˆ—æ˜¯å¦æœ‰ç§¯å‹
- [ ] ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡ï¼ˆCPUã€å†…å­˜ã€ç£ç›˜ï¼‰
- [ ] ç½‘ç»œè¿æ¥çŠ¶æ€å’Œå»¶è¿Ÿ

### ğŸ› ï¸ å®šæœŸç»´æŠ¤ä»»åŠ¡
- [ ] æ¸…ç†è¿‡æœŸçš„Topicæ•°æ®
- [ ] ä¼˜åŒ–æ¶ˆè´¹è€…ç»„åˆ†é…
- [ ] æ›´æ–°å®‰å…¨è¯ä¹¦å’Œå¯†é’¥
- [ ] æ‰§è¡Œç¾éš¾æ¢å¤æ¼”ç»ƒ
- [ ] å®¡æŸ¥å’Œä¼˜åŒ–é…ç½®å‚æ•°

### ğŸš¨ ç´§æ€¥å“åº”æµç¨‹
- [ ] äº‹ä»¶ä¸¢å¤±åº”æ€¥å¤„ç†
- [ ] ç³»ç»Ÿæ€§èƒ½æ€¥å‰§ä¸‹é™å¤„ç†
- [ ] å¤§è§„æ¨¡äº‹ä»¶ç§¯å‹å¤„ç†
- [ ] å…³é”®ç»„ä»¶æ•…éšœåˆ‡æ¢
- [ ] æ•°æ®ä¸€è‡´æ€§é—®é¢˜ä¿®å¤
```

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ | **ä¸“å®¶è¯„å®¡**: å·²é€šè¿‡ | **æœ€åæ›´æ–°**: 2026-02 | **é€‚ç”¨åœºæ™¯**: äº‘åŸç”Ÿäº‹ä»¶é©±åŠ¨æ¶æ„ç”Ÿäº§ç¯å¢ƒ
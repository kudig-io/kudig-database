# 10 - å¤šäº‘æ··åˆäº‘è¿ç»´æ‰‹å†Œ

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25-v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä½œè€…**: Allen Galler | **è´¨é‡ç­‰çº§**: â­â­â­â­â­ ä¸“å®¶çº§

---

## çŸ¥è¯†åœ°å›¾

| å±æ€§ | è¯´æ˜ |
|------|------|
| **æ–‡ä»¶è§’è‰²** | å¤šäº‘æ··åˆäº‘è¿ç»´æ‰‹å†Œ â€” è·¨äº‘ç¯å¢ƒçš„K8sé›†ç¾¤ç®¡ç†æŒ‡å— |
| **é€‚åˆè¯»è€…** | å•é›†ç¾¤è¿ç»´â†’å¤šé›†ç¾¤ç®¡ç†â†’è·¨äº‘æ¶æ„è®¾è®¡ |
| **å‰ç½®çŸ¥è¯†** | 01(è¿ç»´å®è·µ)ã€05(æ¦‚å¿µå‚è€ƒ) |
| **å…³è”æ–‡ä»¶** | 01(è¿ç»´æœ€ä½³å®è·µ)ã€04(SREæˆç†Ÿåº¦)ã€15(SLI/SLO) |

### å¤šäº‘ vs æ··åˆäº‘ vs å¤šé›†ç¾¤

| æ¦‚å¿µ | å®šä¹‰ | å…¸å‹åœºæ™¯ |
|------|------|----------|
| **å¤šé›†ç¾¤** | åŒä¸€äº‘ä¸Šè¿è¡Œå¤šä¸ªK8sé›†ç¾¤ | ç¯å¢ƒéš”ç¦»(dev/staging/prod)ã€åœ°åŸŸéƒ¨ç½² |
| **æ··åˆäº‘** | ç§æœ‰äº‘+å…¬æœ‰äº‘æ··åˆ | æ ¸å¿ƒä¸šåŠ¡ç§æœ‰äº‘ã€çªå‘æµé‡å…¬æœ‰äº‘ |
| **å¤šäº‘** | ä½¿ç”¨å¤šä¸ªå…¬æœ‰äº‘å‚å•† | é¿å…å‚å•†é”å®šã€åˆ©ç”¨å„äº‘ä¼˜åŠ¿ |

---

## ç›®å½•

- [1. å¤šäº‘æ¶æ„è®¾è®¡](#1-å¤šäº‘æ¶æ„è®¾è®¡)
- [2. æˆæœ¬ä¼˜åŒ–ç­–ç•¥](#2-æˆæœ¬ä¼˜åŒ–ç­–ç•¥)
- [3. ç»Ÿä¸€ç›‘æ§ä½“ç³»](#3-ç»Ÿä¸€ç›‘æ§ä½“ç³»)
- [4. è¿ç»´è‡ªåŠ¨åŒ–](#4-è¿ç»´è‡ªåŠ¨åŒ–)
- [5. ç¾å¤‡ä¸å®¹ç¾](#5-ç¾å¤‡ä¸å®¹ç¾)
- [6. å¤šäº‘æ²»ç†æ¡†æ¶](#6-å¤šäº‘æ²»ç†æ¡†æ¶)

---

## 1. å¤šäº‘æ¶æ„è®¾è®¡

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: å¤šäº‘æ¶æ„çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯"ç»Ÿä¸€ç®¡ç†å¼‚æ„ç¯å¢ƒ"ã€‚å°±åƒç®¡ç†ä¸åŒå“ç‰Œçš„æ±½è½¦è½¦é˜Ÿ,éœ€è¦ç»Ÿä¸€çš„è°ƒåº¦ç³»ç»Ÿã€ç»´æŠ¤æ ‡å‡†å’Œç›‘æ§æ‰‹æ®µã€‚å…³é”®å†³ç­–:é›†ç¾¤è”é‚¦ vs ç‹¬ç«‹é›†ç¾¤+ç»Ÿä¸€ç®¡ç†å¹³é¢ã€‚

### 1.1 å¤šäº‘éƒ¨ç½²æ¨¡å¼

| éƒ¨ç½²æ¨¡å¼ | æ¶æ„ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ | ä¼˜åŠ¿ | åŠ£åŠ¿ | å¤æ‚åº¦ |
|----------|----------|----------|------|------|--------|
| **ä¸»åŠ¨-ä¸»åŠ¨** | å¤šä¸ªäº‘ç¯å¢ƒåŒæ—¶æä¾›æœåŠ¡ | å…¨çƒç”¨æˆ·ã€é«˜å¯ç”¨è¦æ±‚ | æœ€é«˜å¯ç”¨æ€§ã€è´Ÿè½½åˆ†æ‹… | æˆæœ¬æœ€é«˜ã€ç®¡ç†å¤æ‚ | â­â­â­â­â­ |
| **ä¸»åŠ¨-è¢«åŠ¨** | ä¸»äº‘æä¾›æœåŠ¡ï¼Œå¤‡äº‘å¾…å‘½ | ç¾å¤‡åœºæ™¯ã€æˆæœ¬æ•æ„Ÿ | æˆæœ¬é€‚ä¸­ã€ç¾å¤‡èƒ½åŠ›å¼º | èµ„æºåˆ©ç”¨ç‡ä½ | â­â­â­ |
| **æ··åˆéƒ¨ç½²** | å…¬æœ‰äº‘+ç§æœ‰äº‘ç»“åˆ | æ•°æ®åˆè§„ã€æ··åˆå·¥ä½œè´Ÿè½½ | åˆè§„å‹å¥½ã€çµæ´»æ€§å¼º | ç½‘ç»œå¤æ‚ã€è¿ç»´éš¾åº¦å¤§ | â­â­â­â­ |
| **åˆ†åŒºåŸŸéƒ¨ç½²** | æŒ‰åœ°ç†åŒºåŸŸé€‰æ‹©æœ€ä¼˜äº‘ | æœ¬åœ°åŒ–æœåŠ¡ã€å»¶è¿Ÿä¼˜åŒ– | ç”¨æˆ·ä½“éªŒä½³ã€åˆè§„æ»¡è¶³ | å¤šä¾›åº”å•†ç®¡ç† | â­â­â­ |
| **åŠŸèƒ½åˆ†ç¦»** | ä¸åŒäº‘æ‰¿æ‹…ä¸åŒåŠŸèƒ½ | ä¸“ä¸šåŒ–åˆ†å·¥ã€æˆæœ¬ä¼˜åŒ– | èµ„æºä¸“ä¸šåŒ–ã€æˆæœ¬æ§åˆ¶ | ä¾èµ–æ€§å¼ºã€é›†æˆå¤æ‚ | â­â­â­â­ |

### 1.2 å¤šäº‘ç½‘ç»œæ¶æ„

| ç½‘ç»œæ¨¡å¼ | æŠ€æœ¯å®ç° | æ€§èƒ½ç‰¹å¾ | å®‰å…¨æ€§ | æˆæœ¬ | å®æ–½å¤æ‚åº¦ |
|----------|----------|----------|--------|------|------------|
| **VPNäº’è”** | IPSec/OpenVPNéš§é“ | ä¸­ç­‰å»¶è¿Ÿã€å¸¦å®½å—é™ | é«˜å®‰å…¨ | ä½æˆæœ¬ | â­â­ |
| **ä¸“çº¿è¿æ¥** | AWS Direct Connectã€Azure ExpressRoute | ä½å»¶è¿Ÿã€é«˜å¸¦å®½ | æœ€é«˜å®‰å…¨ | é«˜æˆæœ¬ | â­â­â­â­ |
| **SD-WAN** | è½¯ä»¶å®šä¹‰å¹¿åŸŸç½‘ | æ™ºèƒ½è·¯ç”±ã€ä¼˜åŒ–ä¼ è¾“ | ä¸­ç­‰å®‰å…¨ | ä¸­ç­‰æˆæœ¬ | â­â­â­ |
| **äº‘é—´å¯¹ç­‰** | VPC Peeringã€Private Link | æœ€ä½å»¶è¿Ÿã€åŸç”Ÿæ€§èƒ½ | äº‘å‚å•†é™åˆ¶ | ä½æˆæœ¬ | â­â­ |
| **æœåŠ¡ç½‘æ ¼** | Istioå¤šé›†ç¾¤ã€Linkerd | åº”ç”¨å±‚é€æ˜ | é«˜çº§å®‰å…¨ | ä¸­ç­‰æˆæœ¬ | â­â­â­â­â­ |

### 1.3 å¤šäº‘æ¶æ„è®¾è®¡åŸåˆ™

```mermaid
graph TB
    A[ä¸šåŠ¡éœ€æ±‚åˆ†æ] --> B{éƒ¨ç½²ç­–ç•¥é€‰æ‹©}
    B --> C[ä¸»åŠ¨-ä¸»åŠ¨æ¨¡å¼]
    B --> D[ä¸»åŠ¨-è¢«åŠ¨æ¨¡å¼]
    B --> E[æ··åˆéƒ¨ç½²æ¨¡å¼]
    
    C --> F[å…¨çƒè´Ÿè½½å‡è¡¡]
    D --> G[ç¾å¤‡åˆ‡æ¢æœºåˆ¶]
    E --> H[æ··åˆç½‘ç»œæ¶æ„]
    
    F --> I[å¤šäº‘DNSæœåŠ¡]
    G --> J[è‡ªåŠ¨åŒ–æ•…éšœè½¬ç§»]
    H --> K[å®‰å…¨è¾¹ç•Œè®¾è®¡]
    
    I --> L[ç”¨æˆ·ä½“éªŒä¼˜åŒ–]
    J --> M[ä¸šåŠ¡è¿ç»­æ€§ä¿éšœ]
    K --> N[åˆè§„æ€§æ»¡è¶³]
    
    L --> O[æˆæœ¬æ•ˆç›Šåˆ†æ]
    M --> O
    N --> O
    
    style A fill:#e3f2fd
    style F fill:#f3e5f5
    style I fill:#e8f5e8
    style L fill:#fff3e0
```

### 1.4 å¤šäº‘åŸºç¡€è®¾æ–½ä»£ç åŒ–

```yaml
# ========== Terraformå¤šäº‘åŸºç¡€è®¾æ–½å®šä¹‰ ==========
# main.tf - å¤šäº‘åŸºç¡€è®¾æ–½ä¸»é…ç½®
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }
}

# AWSåŸºç¡€è®¾æ–½
provider "aws" {
  region = var.aws_region
  alias  = "aws_primary"
}

provider "aws" {
  region = var.aws_backup_region
  alias  = "aws_backup"
}

# AzureåŸºç¡€è®¾æ–½
provider "azurerm" {
  features {}
  alias = "azure_primary"
}

# GCPåŸºç¡€è®¾æ–½
provider "google" {
  project = var.gcp_project
  region  = var.gcp_region
  alias   = "gcp_primary"
}

# ========== å¤šäº‘Kubernetesé›†ç¾¤å®šä¹‰ ==========
# aws_cluster.tf
resource "aws_eks_cluster" "primary" {
  provider = aws.aws_primary
  name     = "primary-cluster"
  role_arn = aws_iam_role.eks_cluster.arn
  
  vpc_config {
    subnet_ids = aws_subnet.private[*].id
  }
  
  # å¯ç”¨å¤šäº‘é›†æˆåŠŸèƒ½
  enabled_cluster_log_types = ["api", "audit", "authenticator", "controllerManager", "scheduler"]
  
  # é›†ç¾¤ç‰ˆæœ¬ç®¡ç†
  version = var.k8s_version
}

resource "aws_eks_node_group" "primary_workers" {
  provider = aws.aws_primary
  cluster_name    = aws_eks_cluster.primary.name
  node_group_name = "primary-workers"
  node_role_arn   = aws_iam_role.node.arn
  
  scaling_config {
    desired_size = 3
    max_size     = 10
    min_size     = 2
  }
  
  instance_types = ["m5.large", "m5.xlarge"]
  capacity_type  = "ON_DEMAND"
  
  # å¤šäº‘æ ‡ç­¾æ ‡å‡†åŒ–
  tags = {
    Environment    = "production"
    ClusterRole    = "primary"
    MultiCloudId   = var.multicloud_id
    CostCenter     = var.cost_center
  }
}

# azure_cluster.tf
resource "azurerm_kubernetes_cluster" "backup" {
  provider            = azurerm.azure_primary
  name                = "backup-cluster"
  location            = var.azure_location
  resource_group_name = azurerm_resource_group.backup.name
  dns_prefix          = "backup-cluster"
  
  default_node_pool {
    name       = "default"
    node_count = 3
    vm_size    = "Standard_D2_v2"
  }
  
  identity {
    type = "SystemAssigned"
  }
  
  # ç½‘ç»œé…ç½®
  network_profile {
    network_plugin = "azure"
    network_policy = "calico"
  }
  
  tags = {
    Environment  = "production"
    ClusterRole  = "backup"
    MultiCloudId = var.multicloud_id
    CostCenter   = var.cost_center
  }
}

# gcp_cluster.tf
resource "google_container_cluster" "disaster_recovery" {
  provider           = google.gcp_primary
  name               = "dr-cluster"
  location           = var.gcp_zone
  initial_node_count = 1
  
  # ç§æœ‰é›†ç¾¤é…ç½®
  private_cluster_config {
    enable_private_endpoint = true
    enable_private_nodes    = true
    master_ipv4_cidr_block  = "172.16.0.0/28"
  }
  
  # å®‰å…¨é…ç½®
  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }
  
  # ç½‘ç»œç­–ç•¥
  network_policy {
    enabled  = true
    provider = "CALICO"
  }
  
  tags = {
    Environment  = "disaster-recovery"
    ClusterRole  = "dr"
    MultiCloudId = var.multicloud_id
    CostCenter   = var.cost_center
  }
}

# ========== å¤šäº‘ç½‘ç»œäº’è”é…ç½® ==========
# network_connectivity.tf
resource "aws_dx_connection" "primary_dx" {
  provider      = aws.aws_primary
  name          = "primary-direct-connect"
  bandwidth     = "1Gbps"
  location      = var.dx_location
  tags = {
    Purpose = "multi-cloud-interconnect"
  }
}

resource "azurerm_express_route_circuit" "azure_er" {
  provider            = azurerm.azure_primary
  name                = "azure-express-route"
  resource_group_name = azurerm_resource_group.network.name
  location            = var.azure_location
  
  service_provider_name = "Equinix"
  peering_location      = var.er_peering_location
  bandwidth_in_mbps     = 1000
  
  sku {
    tier   = "Standard"
    family = "MeteredData"
  }
  
  tags = {
    Purpose = "multi-cloud-interconnect"
  }
}

# ========== å¤šäº‘DNSé…ç½® ==========
# dns_multicloud.tf
resource "aws_route53_zone" "multicloud_dns" {
  provider = aws.aws_primary
  name     = var.domain_name
}

resource "aws_route53_record" "primary_lb" {
  provider = aws.aws_primary
  zone_id  = aws_route53_zone.multicloud_dns.zone_id
  name     = "app.${var.domain_name}"
  type     = "A"
  
  alias {
    name                   = aws_lb.primary.dns_name
    zone_id                = aws_lb.primary.zone_id
    evaluate_target_health = true
  }
}

resource "azurerm_dns_a_record" "azure_lb" {
  provider            = azurerm.azure_primary
  name                = "app"
  zone_name           = var.domain_name
  resource_group_name = azurerm_resource_group.dns.name
  ttl                 = 300
  records             = [azurerm_public_ip.lb.ip_address]
}

# å…¨çƒè´Ÿè½½å‡è¡¡é…ç½®
resource "google_compute_global_forwarding_rule" "global_lb" {
  provider   = google.gcp_primary
  name       = "global-lb"
  target     = google_compute_target_http_proxy.default.id
  port_range = "80"
  
  # åŸºäºå»¶è¿Ÿçš„æ™ºèƒ½è·¯ç”±
  load_balancing_scheme = "EXTERNAL"
}
```

---

## 2. æˆæœ¬ä¼˜åŒ–ç­–ç•¥

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: å¤šäº‘æˆæœ¬ç®¡ç†æ¯”å•äº‘æ›´å¤æ‚,å› ä¸ºæ¯ä¸ªäº‘çš„è®¡è´¹æ¨¡å‹ä¸åŒã€‚æ ¸å¿ƒç­–ç•¥:ç»Ÿä¸€æˆæœ¬å¯è§†åŒ–ã€åˆ©ç”¨å„äº‘çš„Reserved/Spotå®ä¾‹ã€é¿å…è·¨äº‘æ•°æ®ä¼ è¾“è´¹ã€‚FinOpså›¢é˜Ÿæ˜¯å…³é”®ã€‚

### 2.1 å¤šäº‘æˆæœ¬åˆ†æç»´åº¦

| åˆ†æç»´åº¦ | å…³é”®æŒ‡æ ‡ | åˆ†æå·¥å…· | ä¼˜åŒ–ç­–ç•¥ | å®æ–½éš¾åº¦ |
|----------|----------|----------|----------|----------|
| **èµ„æºåˆ©ç”¨ç‡** | CPU/å†…å­˜ä½¿ç”¨ç‡ã€GPUåˆ©ç”¨ç‡ | CloudWatchã€Stackdriver | è‡ªåŠ¨æ‰©ç¼©å®¹ã€èµ„æºå…±äº« | â­â­ |
| **å®ä¾‹é€‰æ‹©** | å®ä¾‹ç±»å‹æ€§ä»·æ¯”ã€é¢„ç•™å®ä¾‹ | å„äº‘å‚å•†å®šä»·è®¡ç®—å™¨ | Spotå®ä¾‹ã€é¢„ç•™å®ä¾‹ | â­â­â­ |
| **å­˜å‚¨æˆæœ¬** | å­˜å‚¨ç±»å‹é€‰æ‹©ã€ç”Ÿå‘½å‘¨æœŸç®¡ç† | å­˜å‚¨åˆ†æå·¥å…· | åˆ†å±‚å­˜å‚¨ã€è‡ªåŠ¨æ¸…ç† | â­â­ |
| **ç½‘ç»œè´¹ç”¨** | æ•°æ®ä¼ è¾“æˆæœ¬ã€è·¨åŒºåŸŸè´¹ç”¨ | ç½‘ç»œæµé‡ç›‘æ§ | æœ¬åœ°å¤„ç†ã€CDNä¼˜åŒ– | â­â­â­â­ |
| **æœåŠ¡è´¹ç”¨** | æ‰˜ç®¡æœåŠ¡æˆæœ¬ã€APIè°ƒç”¨è´¹ç”¨ | æœåŠ¡ä½¿ç”¨ç›‘æ§ | å¼€æºè‡ªå»ºã€æ‰¹é‡å¤„ç† | â­â­â­ |

### 2.2 æˆæœ¬ä¼˜åŒ–æœ€ä½³å®è·µ

```yaml
# ========== å¤šäº‘æˆæœ¬ä¼˜åŒ–ç­–ç•¥ ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-optimization-strategies
  namespace: platform-ops
data:
  # AWSæˆæœ¬ä¼˜åŒ–é…ç½®
  aws-cost-optimization.yaml: |
    # Spotå®ä¾‹æ··åˆéƒ¨ç½²ç­–ç•¥
    spot_instance_strategy:
      enabled: true
      spot_percentage: 70
      fallback_to_on_demand: true
      spot_price_buffer: 0.2
      
    # é¢„ç•™å®ä¾‹è§„åˆ’
    reserved_instances:
      recommendation_enabled: true
      utilization_threshold: 0.7
      term_length: "1_YEAR"
      payment_option: "PARTIAL_UPFRONT"
      
    # å­˜å‚¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
    storage_lifecycle:
      ebs_volumes:
        - transition_to_gp3_after_days: 30
        - delete_after_days: 90
      s3_buckets:
        - transition_to_standard_ia_after_days: 30
        - transition_to_glacier_after_days: 90
        - delete_after_days: 365
  
  # Azureæˆæœ¬ä¼˜åŒ–é…ç½®
  azure-cost-optimization.yaml: |
    # è™šæ‹Ÿæœºä¼˜åŒ–
    virtual_machine_optimization:
      low_priority_vms_enabled: true
      low_priority_percentage: 60
      eviction_policy: "Deallocate"
      
    # é¢„ç•™å®¹é‡è§„åˆ’
    reserved_capacity:
      recommendation_enabled: true
      utilization_threshold: 0.65
      term: "1_YEAR"
      
    # å­˜å‚¨ä¼˜åŒ–
    storage_optimization:
      blob_storage:
        - cool_tier_after_days: 30
        - archive_tier_after_days: 180
        - delete_after_days: 730
  
  # GCPæˆæœ¬ä¼˜åŒ–é…ç½®
  gcp-cost-optimization.yaml: |
    # é¢„ç•™å®ä¾‹é…ç½®
    committed_use_discounts:
      enabled: true
      commitment_type: "MONTHLY"
      discount_tiers:
        - cpu: 1
          memory: 4
          discount: 0.30
        - cpu: 2
          memory: 8
          discount: 0.40
          
    # Spotå®ä¾‹ç­–ç•¥
    preemptible_instances:
      enabled: true
      percentage: 50
      termination_handler: "graceful_shutdown"
      
    # å­˜å‚¨ç”Ÿå‘½å‘¨æœŸ
    storage_lifecycle:
      standard_storage:
        - nearline_after_days: 30
        - coldline_after_days: 90
        - archive_after_days: 365

---
# ========== æˆæœ¬ç›‘æ§å’Œå‘Šè­¦ ==========
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: multicloud-cost-monitoring
  namespace: monitoring
spec:
  groups:
  - name: cost.optimization.rules
    rules:
    # èµ„æºåˆ©ç”¨ç‡å‘Šè­¦
    - alert: LowResourceUtilization
      expr: |
        avg(rate(container_cpu_usage_seconds_total[1h])) by (cluster, namespace) < 0.2
      for: 24h
      labels:
        severity: warning
      annotations:
        summary: "èµ„æºåˆ©ç”¨ç‡è¿‡ä½ ({{ $value }}%)"
        description: "æ£€æµ‹åˆ°æŒç»­ä½èµ„æºåˆ©ç”¨ç‡ï¼Œå»ºè®®ä¼˜åŒ–èµ„æºé…ç½®"
        
    # æˆæœ¬è¶…æ ‡å‘Šè­¦
    - alert: CostBudgetExceeded
      expr: |
        sum by(cluster) (rate(aws_billing_cost_estimate[1h])) > 1000
      for: 1h
      labels:
        severity: critical
      annotations:
        summary: "äº‘æœåŠ¡æˆæœ¬è¶…å‡ºé¢„ç®—"
        description: "å½“å‰å°æ—¶æˆæœ¬ä¼°ç®—è¶…è¿‡1000ç¾å…ƒé˜ˆå€¼"
        
    # Spotå®ä¾‹ä¸­æ–­é£é™©
    - alert: SpotInstanceInterruptionRisk
      expr: |
        avg(aws_ec2_spot_instance_interruption_rate) > 0.3
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: "Spotå®ä¾‹ä¸­æ–­é£é™©è¾ƒé«˜"
        description: "å½“å‰Spotå®ä¾‹ä¸­æ–­æ¦‚ç‡è¶…è¿‡30%ï¼Œå»ºè®®è°ƒæ•´ç­–ç•¥"
        
    # å­˜å‚¨æˆæœ¬å¼‚å¸¸
    - alert: StorageCostAnomaly
      expr: |
        rate(aws_s3_storage_cost[1h]) > 100
      for: 6h
      labels:
        severity: info
      annotations:
        summary: "å­˜å‚¨æˆæœ¬å¼‚å¸¸å¢é•¿"
        description: "S3å­˜å‚¨æˆæœ¬å‡ºç°å¼‚å¸¸å¢é•¿è¶‹åŠ¿"
```

### 2.3 å¤šäº‘æˆæœ¬æ²»ç†æ¡†æ¶

```yaml
# ========== å¤šäº‘æˆæœ¬æ²»ç†ç­–ç•¥ ==========
apiVersion: costmanagement.example.com/v1
kind: CostGovernancePolicy
metadata:
  name: enterprise-cost-governance
  namespace: platform-ops
spec:
  # é¢„ç®—ç®¡ç†
  budget_management:
    monthly_budget:
      aws: 50000    # ç¾å…ƒ
      azure: 30000
      gcp: 20000
    
    alerting:
      warning_threshold: 0.8   # 80%é¢„ç®—æ—¶å‘Šè­¦
      critical_threshold: 0.95  # 95%é¢„ç®—æ—¶ç´§æ€¥å‘Šè­¦
      overrun_action: "notify_and_restrict"
      
  # èµ„æºé…é¢ç®¡ç†
  resource_quotas:
    per_team:
      development:
        cpu_cores: 100
        memory_gb: 500
        storage_tb: 10
      production:
        cpu_cores: 500
        memory_gb: 2000
        storage_tb: 50
      testing:
        cpu_cores: 50
        memory_gb: 200
        storage_tb: 5
        
  # è‡ªåŠ¨åŒ–ä¼˜åŒ–è§„åˆ™
  optimization_rules:
    - name: "dev-environment-hours"
      condition: "namespace startsWith 'dev-' and hour not in (9-18)"
      action: "scale_down_to_minimum"
      
    - name: "non-prod-weekend-shutdown"
      condition: "namespace not in ('prod', 'staging') and day_of_week in (6,7)"
      action: "shutdown_non_critical_resources"
      
    - name: "spot-instance-fallback"
      condition: "spot_instance_unavailable and budget_available"
      action: "switch_to_on_demand_with_notification"
      
    - name: "unused-resource-cleanup"
      condition: "resource_idle_for > 7_days"
      action: "send_warning_then_delete_after_14_days"

---
# ========== æˆæœ¬åˆ†æä»ªè¡¨æ¿ ==========
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: multicloud-cost-analytics
  namespace: monitoring
spec:
  json: |
    {
      "dashboard": {
        "title": "å¤šäº‘æˆæœ¬åˆ†æä»ªè¡¨æ¿",
        "panels": [
          {
            "title": "å„äº‘å¹³å°æˆæœ¬åˆ†å¸ƒ",
            "type": "piechart",
            "targets": [
              {"expr": "sum by(provider) (cloud_cost_daily_total)", "legendFormat": "{{provider}}"}
            ]
          },
          {
            "title": "æˆæœ¬è¶‹åŠ¿åˆ†æ",
            "type": "graph",
            "targets": [
              {"expr": "sum(cloud_cost_daily_total)", "legendFormat": "æ€»æˆæœ¬"},
              {"expr": "cloud_cost_daily_total{provider=\"aws\"}", "legendFormat": "AWS"},
              {"expr": "cloud_cost_daily_total{provider=\"azure\"}", "legendFormat": "Azure"},
              {"expr": "cloud_cost_daily_total{provider=\"gcp\"}", "legendFormat": "GCP"}
            ]
          },
          {
            "title": "èµ„æºåˆ©ç”¨ç‡vsæˆæœ¬",
            "type": "barchart",
            "targets": [
              {"expr": "avg by(cluster) (container_cpu_usage_seconds_total)", "legendFormat": "{{cluster}} åˆ©ç”¨ç‡"},
              {"expr": "sum by(cluster) (cloud_cost_daily_total)", "legendFormat": "{{cluster}} æˆæœ¬"}
            ]
          },
          {
            "title": "é¢„ç®—æ‰§è¡Œæƒ…å†µ",
            "type": "gauge",
            "targets": [
              {"expr": "sum(cloud_cost_monthly_total) / sum(budget_monthly_limit) * 100", "legendFormat": "é¢„ç®—ä½¿ç”¨ç‡"}
            ]
          }
        ]
      }
    }
```

---

## 3. ç»Ÿä¸€ç›‘æ§ä½“ç³»

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: å¤šäº‘ç¯å¢ƒå¿…é¡»æœ‰ç»Ÿä¸€çš„ç›‘æ§è§†å›¾,å¦åˆ™å‡ºæ•…éšœæ—¶æ— æ³•å¿«é€Ÿå®šä½ã€‚æ¨èæ–¹æ¡ˆ:æ¯ä¸ªé›†ç¾¤éƒ¨ç½²Prometheusé‡‡é›†â†’é€šè¿‡Thanos/Mimirèšåˆâ†’Grafanaç»Ÿä¸€å±•ç¤ºã€‚

### 3.1 å¤šäº‘ç›‘æ§æ¶æ„

| ç›‘æ§å±‚çº§ | ç›‘æ§å†…å®¹ | æŠ€æœ¯æ–¹æ¡ˆ | æ•°æ®æµå‘ | å®æ–½è¦ç‚¹ |
|----------|----------|----------|----------|----------|
| **åŸºç¡€è®¾æ–½å±‚** | VMã€å®¹å™¨ã€ç½‘ç»œã€å­˜å‚¨ | Prometheus + Exporter | å„äº‘å‚å•† â†’ ä¸­å¤®Prometheus | æ ‡å‡†åŒ–æŒ‡æ ‡æ ¼å¼ |
| **å¹³å°æœåŠ¡å±‚** | K8sç»„ä»¶ã€ä¸­é—´ä»¶ã€æ•°æ®åº“ | kube-state-metricsã€æœåŠ¡Exporter | é›†ç¾¤å†…æ”¶é›† â†’ è¿œç¨‹å†™å…¥ | ç»Ÿä¸€æœåŠ¡å‘ç° |
| **åº”ç”¨å±‚** | ä¸šåŠ¡æŒ‡æ ‡ã€APMã€æ—¥å¿— | OpenTelemetryã€EFK Stack | Sidecaræ³¨å…¥ â†’ ä¸­å¤®å­˜å‚¨ | åº”ç”¨åŸ‹ç‚¹æ ‡å‡†åŒ– |
| **ç”¨æˆ·ä½“éªŒå±‚** | å‰ç«¯æ€§èƒ½ã€ç”¨æˆ·è¡Œä¸º | RUMã€Synthetic Monitoring | å®¢æˆ·ç«¯ â†’ ç›‘æ§å¹³å° | çœŸå®ç”¨æˆ·ç›‘æ§ |
| **å®‰å…¨å±‚** | å¨èƒæ£€æµ‹ã€åˆè§„å®¡è®¡ | SIEMã€å®‰å…¨äº‹ä»¶æ—¥å¿— | å„ç»„ä»¶ â†’ å®‰å…¨å¹³å° | ç»Ÿä¸€æ—¥å¿—æ ¼å¼ |

### 3.2 ç»Ÿä¸€ç›‘æ§å®æ–½æ–¹æ¡ˆ

```yaml
# ========== å¤šäº‘ç»Ÿä¸€ç›‘æ§æ¶æ„ ==========
apiVersion: v1
kind: Namespace
metadata:
  name: unified-monitoring
  labels:
    monitoring-tier: "central"

---
# Prometheusè”é‚¦é›†ç¾¤é…ç½®
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: central-prometheus
  namespace: unified-monitoring
spec:
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      team: sre
  ruleSelector:
    matchLabels:
      role: alert-rules
  remoteWrite:
    # AWSç›‘æ§æ•°æ®æ¥æ”¶
    - url: "https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-12345678/api/v1/remote_write"
      writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: "(aws|eks)_.*"
          action: keep
      queueConfig:
        capacity: 10000
        maxShards: 10
        
    # Azureç›‘æ§æ•°æ®æ¥æ”¶
    - url: "https://azure-monitor.azure.com/v1/api/prom/write"
      bearerTokenSecret:
        name: azure-monitor-secret
        key: token
      writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: "(azure)_.*"
          action: keep
          
    # GCPç›‘æ§æ•°æ®æ¥æ”¶
    - url: "https://monitoring.googleapis.com/v3/projects/my-project/timeSeries:createService"
      bearerTokenSecret:
        name: gcp-monitoring-secret
        key: token
      writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: "(gcp)_.*"
          action: keep

---
# å¤šäº‘æœåŠ¡å‘ç°é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: multicloud-servicediscovery
  namespace: unified-monitoring
data:
  aws-sd.yaml: |
    scrape_configs:
    - job_name: 'aws-ec2-instances'
      ec2_sd_configs:
      - region: us-east-1
        access_key: YOUR_ACCESS_KEY
        secret_key: YOUR_SECRET_KEY
        port: 9100
      relabel_configs:
      - source_labels: [__meta_ec2_tag_Name]
        target_label: instance
      - source_labels: [__meta_ec2_availability_zone]
        target_label: zone
        
    - job_name: 'aws-eks-clusters'
      kubernetes_sd_configs:
      - role: node
        api_server: https://your-eks-cluster.gr7.us-east-1.eks.amazonaws.com
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        
  azure-sd.yaml: |
    scrape_configs:
    - job_name: 'azure-vms'
      azure_sd_configs:
      - environment: AzurePublicCloud
        authentication_method: OAuth
        subscription_id: YOUR_SUBSCRIPTION_ID
        tenant_id: YOUR_TENANT_ID
        client_id: YOUR_CLIENT_ID
        client_secret: YOUR_CLIENT_SECRET
        port: 9100
        
  gcp-sd.yaml: |
    scrape_configs:
    - job_name: 'gcp-instances'
      gce_sd_configs:
      - project: your-gcp-project
        zone: us-central1-a
        port: 9100
        credentials_file: /etc/gcp/credentials.json

---
# ç»Ÿä¸€å‘Šè­¦è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: unified-alerting-rules
  namespace: unified-monitoring
spec:
  groups:
  # è·¨äº‘å¹³å°åŸºç¡€ç›‘æ§
  - name: multicloud.infrastructure.rules
    rules:
    - alert: HighCPUUsageAcrossClouds
      expr: |
        avg by(instance, provider) (
          rate(node_cpu_seconds_total{mode!="idle"}[5m])
        ) > 0.8
      for: 10m
      labels:
        severity: warning
        provider: "{{ $labels.provider }}"
      annotations:
        summary: "è·¨äº‘å¹³å°CPUä½¿ç”¨ç‡è¿‡é«˜ ({{ $value }}%)"
        description: "å®ä¾‹ {{ $labels.instance }} åœ¨ {{ $labels.provider }} ä¸ŠCPUä½¿ç”¨ç‡æŒç»­é«˜äº80%"
        
    - alert: MemoryPressureMulticloud
      expr: |
        (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100) < 15
      for: 15m
      labels:
        severity: critical
      annotations:
        summary: "å†…å­˜å‹åŠ›è­¦å‘Š"
        description: "èŠ‚ç‚¹å†…å­˜å¯ç”¨ç‡ä½äº15%ï¼Œå¯èƒ½å½±å“æœåŠ¡ç¨³å®šæ€§"
        
  # Kubernetesè·¨é›†ç¾¤ç›‘æ§
  - name: multicloud.kubernetes.rules
    rules:
    - alert: ClusterDownMulticloud
      expr: |
        absent(up{job="kubernetes-apiservers"}) == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Kubernetesé›†ç¾¤ä¸å¯è¾¾"
        description: "æ£€æµ‹åˆ°Kubernetes API Serveræ— æ³•è®¿é—®"
        
    - alert: PodCrashLoopingMulticloud
      expr: |
        rate(kube_pod_container_status_restarts_total[15m]) > 0.1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Podé¢‘ç¹é‡å¯"
        description: "Podé‡å¯é¢‘ç‡è¶…è¿‡æ¯åˆ†é’Ÿ6æ¬¡"
        
  # åº”ç”¨å±‚ç»Ÿä¸€ç›‘æ§
  - name: multicloud.application.rules
    rules:
    - alert: HighErrorRateUnified
      expr: |
        sum(rate(http_requests_total{code=~"5.."}[5m])) by (app, provider) /
        sum(rate(http_requests_total[5m])) by (app, provider) > 0.05
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "åº”ç”¨é”™è¯¯ç‡è¿‡é«˜"
        description: "åº”ç”¨ {{ $labels.app }} åœ¨ {{ $labels.provider }} ä¸Š5xxé”™è¯¯ç‡è¶…è¿‡5%"
        
    - alert: HighLatencyUnified
      expr: |
        histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "åº”ç”¨å“åº”å»¶è¿Ÿé«˜"
        description: "95thç™¾åˆ†ä½å“åº”æ—¶é—´è¶…è¿‡2ç§’"
```

### 3.3 ç»Ÿä¸€æ—¥å¿—æ¶æ„

```yaml
# ========== å¤šäº‘ç»Ÿä¸€æ—¥å¿—æ”¶é›† ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: unified-logging-config
  namespace: unified-monitoring
data:
  fluentd-config.yaml: |
    # AWS CloudWatch Logsè¾“å…¥
    <source>
      @type cloudwatch_logs
      log_group_name /aws/containerinsights/cluster/application
      region us-east-1
      aws_access_key_id "#{ENV['AWS_ACCESS_KEY_ID']}"
      aws_secret_access_key "#{ENV['AWS_SECRET_ACCESS_KEY']}"
      <parse>
        @type json
        time_key time
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
      tag aws.*
    </source>
    
    # Azure Monitor Logsè¾“å…¥
    <source>
      @type azure_monitor_logs
      workspace_id "#{ENV['AZURE_WORKSPACE_ID']}"
      shared_key "#{ENV['AZURE_SHARED_KEY']}"
      <parse>
        @type json
      </parse>
      tag azure.*
    </source>
    
    # GCP Loggingè¾“å…¥
    <source>
      @type gcp_logging
      project_id "#{ENV['GCP_PROJECT_ID']}"
      credentials_file /etc/gcp/credentials.json
      <parse>
        @type json
      </parse>
      tag gcp.*
    </source>
    
    # ç»Ÿä¸€è¿‡æ»¤å’Œå¤„ç†
    <filter **>
      @type record_transformer
      <record>
        timestamp ${time.strftime('%Y-%m-%dT%H:%M:%S.%6N%:z')}
        cloud_provider ${tag.split('.')[0]}
        unified_timestamp ${Time.now.to_i}
      </record>
    </filter>
    
    # è¾“å‡ºåˆ°ä¸­å¤®Elasticsearch
    <match **>
      @type elasticsearch
      host elasticsearch.unified-monitoring.svc.cluster.local
      port 9200
      logstash_format true
      logstash_prefix "multicloud-logs"
      <buffer>
        @type file
        path /var/log/fluentd-buffers/unified.*.buffer
        flush_mode interval
        flush_interval 10s
        retry_type exponential_backoff
      </buffer>
    </match>

---
# ========== ç»Ÿä¸€æ—¥å¿—å­˜å‚¨é…ç½® ==========
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: unified-elasticsearch
  namespace: unified-monitoring
spec:
  version: 8.11.0
  nodeSets:
  - name: unified-logs
    count: 3
    config:
      node.roles: ["master", "data", "ingest"]
      xpack.security.enabled: true
      xpack.security.transport.ssl.enabled: true
      indices.lifecycle.poll_interval: "10m"
      
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 4Gi
              cpu: 2
            limits:
              memory: 8Gi
              cpu: 4
              
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Ti
        storageClassName: fast-ssd

---
# ========== Kibanaç»Ÿä¸€å¯è§†åŒ– ==========
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: unified-kibana
  namespace: unified-monitoring
spec:
  version: 8.11.0
  count: 1
  elasticsearchRef:
    name: unified-elasticsearch
  config:
    server.publicBaseUrl: "https://kibana.example.com"
    telemetry.optIn: false
    securitySolution:
      enabled: true
      
  http:
    service:
      spec:
        type: LoadBalancer
        annotations:
          service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
          cloud.google.com/load-balancer-type: "External"
          service.beta.kubernetes.io/azure-load-balancer-internal: "false"
```

### 3.4 å¤šé›†ç¾¤ç›‘æ§æ¶æ„

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**è¿é”åº—æ€»éƒ¨ç›‘æ§** â€” æ¯ä¸ªé—¨åº—(é›†ç¾¤)æœ‰è‡ªå·±çš„ç›‘æ§æ‘„åƒå¤´,ä½†æ€»éƒ¨éœ€è¦ä¸€ä¸ªå¤§å±å¹•åŒæ—¶æŸ¥çœ‹æ‰€æœ‰é—¨åº—æƒ…å†µã€‚å¤šé›†ç¾¤ç›‘æ§å°±æ˜¯æŠŠåˆ†æ•£çš„æ•°æ®èšåˆåˆ°ç»Ÿä¸€è§†å›¾,åŒæ—¶ä¿æŒå„é›†ç¾¤ç‹¬ç«‹è¿è¡Œã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[ç»Ÿä¸€ç›‘æ§è§†å›¾<br/>Grafana] --> B[é•¿æœŸå­˜å‚¨<br/>Thanos/Mimir]
    
    B --> C[AWSé›†ç¾¤<br/>Prometheus]
    B --> D[Azureé›†ç¾¤<br/>Prometheus]
    B --> E[GCPé›†ç¾¤<br/>Prometheus]
    
    C --> C1[Node Exporter]
    C --> C2[kube-state-metrics]
    C --> C3[åº”ç”¨æŒ‡æ ‡]
    
    D --> D1[Node Exporter]
    D --> D2[kube-state-metrics]
    D --> D3[åº”ç”¨æŒ‡æ ‡]
    
    E --> E1[Node Exporter]
    E --> E2[kube-state-metrics]
    E --> E3[åº”ç”¨æŒ‡æ ‡]
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
```

**æ¶æ„é€‰å‹å¯¹æ¯”:**

| æ–¹æ¡ˆ | æ¶æ„æ¨¡å¼ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|----------|------|------|----------|
| **Prometheusè”é‚¦** | åˆ†å±‚æ‹‰å– | ç®€å•,åŸç”Ÿæ”¯æŒ | æŸ¥è¯¢æ€§èƒ½å·®,æ•°æ®å†—ä½™ | <10ä¸ªé›†ç¾¤ |
| **Thanos** | å¯¹è±¡å­˜å‚¨+è¿œç¨‹æŸ¥è¯¢ | é«˜å¯ç”¨,æ— é™å­˜å‚¨,æˆæœ¬ä½ | ç»„ä»¶å¤š,è¿ç»´å¤æ‚ | 10-100ä¸ªé›†ç¾¤ |
| **Mimir** | åˆ†å¸ƒå¼å­˜å‚¨ | é«˜æ€§èƒ½,å¤šç§Ÿæˆ· | ç›¸å¯¹æ–°,ç¤¾åŒºå° | å¤§è§„æ¨¡å¤šç§Ÿæˆ· |
| **Grafana Mimir Cloud** | SaaSæ‰˜ç®¡ | é›¶è¿ç»´ | æŒ‰é‡ä»˜è´¹ | å¿«é€Ÿä¸Šçº¿ |

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== Thanoså¤šé›†ç¾¤ç›‘æ§æ¶æ„ ==========
# 1. å„é›†ç¾¤Prometheusé…ç½®(å¯ç”¨Thanos Sidecar)
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 2
  retention: 7d  # æœ¬åœ°åªä¿ç•™7å¤©,é•¿æœŸæ•°æ®æ¨é€åˆ°Thanos
  
  # Thanos sidecaré…ç½®
  thanos:
    image: quay.io/thanos/thanos:v0.34.0
    version: v0.34.0
    objectStorageConfig:
      key: thanos.yaml
      name: thanos-objstore-config
    
  # æ·»åŠ å¤–éƒ¨æ ‡ç­¾æ ‡è¯†é›†ç¾¤æ¥æº
  externalLabels:
    cluster: "aws-us-east-1"
    region: "us-east-1"
    provider: "aws"
    environment: "production"
    
  serviceMonitorSelector:
    matchLabels:
      team: platform
      
  ruleSelector:
    matchLabels:
      role: alert-rules

---
# å¯¹è±¡å­˜å‚¨é…ç½®(Thanosä½¿ç”¨)
apiVersion: v1
kind: Secret
metadata:
  name: thanos-objstore-config
  namespace: monitoring
type: Opaque
stringData:
  thanos.yaml: |
    type: S3
    config:
      bucket: "thanos-metrics-storage"
      endpoint: "s3.us-east-1.amazonaws.com"
      region: "us-east-1"
      access_key: "${AWS_ACCESS_KEY_ID}"
      secret_key: "${AWS_SECRET_ACCESS_KEY}"
      # ä¹Ÿå¯ä»¥ä½¿ç”¨Azure Blobæˆ–GCS
      # type: AZURE
      # config:
      #   storage_account: "thanosmetrics"
      #   storage_account_key: "xxx"
      #   container: "metrics"

---
# 2. Thanos Query(ç»Ÿä¸€æŸ¥è¯¢å…¥å£)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thanos-query
  namespace: monitoring
spec:
  replicas: 3
  selector:
    matchLabels:
      app: thanos-query
  template:
    metadata:
      labels:
        app: thanos-query
    spec:
      containers:
      - name: thanos-query
        image: quay.io/thanos/thanos:v0.34.0
        args:
        - query
        - --log.level=info
        - --query.replica-label=replica
        - --query.replica-label=prometheus_replica
        
        # è¿æ¥æ‰€æœ‰é›†ç¾¤çš„Thanos Sidecar
        - --endpoint=dnssrv+_grpc._tcp.thanos-sidecar-aws.monitoring.svc.cluster.local
        - --endpoint=dnssrv+_grpc._tcp.thanos-sidecar-azure.monitoring.svc.cluster.local
        - --endpoint=dnssrv+_grpc._tcp.thanos-sidecar-gcp.monitoring.svc.cluster.local
        
        # è¿æ¥Thanos Store Gateway(å†å²æ•°æ®)
        - --endpoint=dnssrv+_grpc._tcp.thanos-store.monitoring.svc.cluster.local
        
        ports:
        - name: http
          containerPort: 10902
        - name: grpc
          containerPort: 10901
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"

---
apiVersion: v1
kind: Service
metadata:
  name: thanos-query
  namespace: monitoring
spec:
  selector:
    app: thanos-query
  ports:
  - name: http
    port: 9090
    targetPort: 10902
  type: ClusterIP

---
# 3. Thanos Store Gateway(å†å²æ•°æ®æŸ¥è¯¢)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: thanos-store
  namespace: monitoring
spec:
  serviceName: thanos-store
  replicas: 2
  selector:
    matchLabels:
      app: thanos-store
  template:
    metadata:
      labels:
        app: thanos-store
    spec:
      containers:
      - name: thanos-store
        image: quay.io/thanos/thanos:v0.34.0
        args:
        - store
        - --log.level=info
        - --data-dir=/var/thanos/store
        - --objstore.config-file=/etc/thanos/objstore.yaml
        - --index-cache-size=2GB
        - --chunk-pool-size=2GB
        
        ports:
        - name: http
          containerPort: 10902
        - name: grpc
          containerPort: 10901
          
        volumeMounts:
        - name: data
          mountPath: /var/thanos/store
        - name: objstore-config
          mountPath: /etc/thanos
          
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "8Gi"
            cpu: "2000m"
            
      volumes:
      - name: objstore-config
        secret:
          secretName: thanos-objstore-config
          
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi

---
# 4. Grafanaé…ç½®(è¿æ¥Thanos Query)
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: monitoring
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
    # Thanosä½œä¸ºä¸»æ•°æ®æº
    - name: Thanos-MultiCluster
      type: prometheus
      access: proxy
      url: http://thanos-query.monitoring.svc.cluster.local:9090
      isDefault: true
      editable: false
      jsonData:
        timeInterval: "30s"
        queryTimeout: "2m"
        
---
# 5. å¤šé›†ç¾¤ç›‘æ§æŸ¥è¯¢ç¤ºä¾‹
apiVersion: v1
kind: ConfigMap
metadata:
  name: multicluster-queries
  namespace: monitoring
data:
  # æŒ‰é›†ç¾¤èšåˆCPUä½¿ç”¨ç‡
  cpu-by-cluster.promql: |
    sum by(cluster) (
      rate(container_cpu_usage_seconds_total{container!=""}[5m])
    )
    
  # è·¨é›†ç¾¤åº”ç”¨å¥åº·åº¦
  app-health-multicluster.promql: |
    sum by(cluster, namespace, app) (
      up{job="kubernetes-pods"}
    )
    
  # å„é›†ç¾¤æˆæœ¬å¯¹æ¯”
  cost-by-cluster.promql: |
    sum by(cluster) (
      label_replace(
        kube_pod_container_resource_requests{resource="cpu"},
        "cost", "$1", "cluster", "(.*)"
      ) * 0.03  # å‡è®¾$0.03/core/hour
    ) * 730
    
  # è·¨é›†ç¾¤P95å»¶è¿Ÿå¯¹æ¯”
  p95-latency-multicluster.promql: |
    histogram_quantile(0.95,
      sum by(cluster, le) (
        rate(http_request_duration_seconds_bucket[5m])
      )
    )
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: æŠŠæ‰€æœ‰æ•°æ®é›†ä¸­å­˜å‚¨** | åº”è¯¥æœ¬åœ°çŸ­æœŸ+å¯¹è±¡å­˜å‚¨é•¿æœŸ,é™ä½æˆæœ¬ | Prometheusä¿ç•™7-14å¤©,Thanoså¯¹è±¡å­˜å‚¨ä¿ç•™é•¿æœŸ |
| **è¯¯åŒº2: æ‰€æœ‰é›†ç¾¤ç”¨åŒä¸€å¥—å‘Šè­¦** | ä¸åŒé›†ç¾¤(dev/prod)åº”è¯¥æœ‰ä¸åŒå‘Šè­¦é˜ˆå€¼ | ä½¿ç”¨`cluster`æ ‡ç­¾åŒºåˆ†,è®¾ç½®ä¸åŒseverity |
| **è¯¯åŒº3: Thanos Queryæ˜¯å•ç‚¹æ•…éšœ** | Thanos Queryæ˜¯æ— çŠ¶æ€çš„,å¯ä»¥æ°´å¹³æ‰©å±• | éƒ¨ç½²3+å‰¯æœ¬,å‰é¢åŠ è´Ÿè½½å‡è¡¡ |
| **è¯¯åŒº4: å¿½ç•¥Cardinalityçˆ†ç‚¸** | å¤šé›†ç¾¤metricsæ•°é‡æŒ‡æ•°å¢é•¿,å¯èƒ½å¯¼è‡´æŸ¥è¯¢æ…¢ | ä½¿ç”¨relabelåˆ é™¤é«˜åŸºæ•°æ ‡ç­¾,é™åˆ¶é‡‡é›†èŒƒå›´ |

### 3.5 ç»Ÿä¸€å‘Šè­¦

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**119ç»Ÿä¸€æŠ¥è­¦** â€” ä¸ç®¡å“ªä¸ªåŸå¸‚å‘ç”Ÿç«ç¾,éƒ½æ‹¨æ‰“119,è°ƒåº¦ä¸­å¿ƒä¼šæ´¾æœ€è¿‘çš„æ¶ˆé˜²é˜Ÿã€‚ç»Ÿä¸€å‘Šè­¦å°±æ˜¯æŠŠæ‰€æœ‰é›†ç¾¤çš„å‘Šè­¦æ±‡æ€»åˆ°ä¸€ä¸ªå¹³å°,æ ¹æ®è§„åˆ™è·¯ç”±åˆ°å¯¹åº”å›¢é˜Ÿã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[å‘Šè­¦æº] --> B[Thanos Ruler<br/>ç»Ÿä¸€å‘Šè­¦è§„åˆ™]
    A --> C[Prometheus<br/>å„é›†ç¾¤æœ¬åœ°å‘Šè­¦]
    
    B --> D[Alertmanager<br/>å‘Šè­¦è·¯ç”±ä¸­å¿ƒ]
    C --> D
    
    D --> E{è·¯ç”±è§„åˆ™}
    
    E -->|severity=critical| F[PagerDuty<br/>24/7å€¼ç­]
    E -->|team=sre| G[Slack #sre-alerts]
    E -->|cluster=prod| H[Email + SMS]
    E -->|namespace=dev| I[Slack #dev-alerts]
    
    style D fill:#e3f2fd
    style E fill:#fff3e0
```

**å‘Šè­¦è·¯ç”±ç­–ç•¥:**
1. **æŒ‰ä¸¥é‡ç¨‹åº¦**: criticalâ†’ç«‹å³å‘¼å«,warningâ†’Slacké€šçŸ¥
2. **æŒ‰é›†ç¾¤/ç¯å¢ƒ**: ç”Ÿäº§å‘Šè­¦é«˜ä¼˜å…ˆçº§,å¼€å‘å‘Šè­¦å¯å»¶è¿Ÿ
3. **æŒ‰å›¢é˜Ÿ**: æ ¹æ®namespace ownerè·¯ç”±åˆ°å¯¹åº”å›¢é˜Ÿ
4. **æŒ‰æ—¶é—´**: å·¥ä½œæ—¶é—´å’Œéå·¥ä½œæ—¶é—´ä¸åŒç­–ç•¥

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== ç»Ÿä¸€å‘Šè­¦é…ç½® ==========
# 1. Alertmanageré…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/xxx'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
      
    # å‘Šè­¦æ¨¡æ¿
    templates:
    - '/etc/alertmanager/templates/*.tmpl'
    
    # è·¯ç”±è§„åˆ™
    route:
      receiver: 'default'
      group_by: ['cluster', 'alertname', 'namespace']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 12h
      
      routes:
      # 1. ç”Ÿäº§ç¯å¢ƒCriticalå‘Šè­¦ -> PagerDuty + Slack
      - match:
          environment: production
          severity: critical
        receiver: 'pagerduty-critical'
        continue: true  # ç»§ç»­åŒ¹é…å…¶ä»–è·¯ç”±
        
      - match:
          environment: production
          severity: critical
        receiver: 'slack-critical'
        
      # 2. ç”Ÿäº§ç¯å¢ƒWarningå‘Šè­¦ -> Slack
      - match:
          environment: production
          severity: warning
        receiver: 'slack-warning'
        group_wait: 5m  # ç­‰å¾…5åˆ†é’Ÿèšåˆ
        
      # 3. æŒ‰å›¢é˜Ÿè·¯ç”±
      - match_re:
          namespace: ^team-frontend-.*
        receiver: 'slack-frontend-team'
        
      - match_re:
          namespace: ^team-backend-.*
        receiver: 'slack-backend-team'
        
      # 4. éç”Ÿäº§ç¯å¢ƒ -> ä½ä¼˜å…ˆçº§Slack
      - match_re:
          environment: development|staging|testing
        receiver: 'slack-non-prod'
        group_wait: 30m  # ç­‰å¾…30åˆ†é’Ÿèšåˆ
        repeat_interval: 24h  # 24å°æ—¶é‡å¤ä¸€æ¬¡
        
      # 5. ç‰¹å®šäº‘å‚å•†å‘Šè­¦
      - match:
          provider: aws
          alertname: HighAWSCost
        receiver: 'email-finops'
        
    # å‘Šè­¦æ¥æ”¶å™¨å®šä¹‰
    receivers:
    # é»˜è®¤æ¥æ”¶å™¨
    - name: 'default'
      email_configs:
      - to: 'ops-team@example.com'
        
    # PagerDuty (ç”Ÿäº§Critical)
    - name: 'pagerduty-critical'
      pagerduty_configs:
      - service_key: 'xxxxxxxxxxxxx'
        description: '[{{ .GroupLabels.cluster }}] {{ .GroupLabels.alertname }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          cluster: '{{ .GroupLabels.cluster }}'
          
    # Slack Critical
    - name: 'slack-critical'
      slack_configs:
      - channel: '#alerts-critical'
        title: ':fire: [{{ .GroupLabels.cluster }}] {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Cluster:* {{ .Labels.cluster }}
          *Severity:* {{ .Labels.severity }}
          *Runbook:* {{ .Annotations.runbook_url }}
          {{ end }}
        color: 'danger'
        send_resolved: true
        
    # Slack Warning
    - name: 'slack-warning'
      slack_configs:
      - channel: '#alerts-warning'
        title: ':warning: [{{ .GroupLabels.cluster }}] {{ .GroupLabels.alertname }}'
        color: 'warning'
        
    # å‰ç«¯å›¢é˜Ÿ
    - name: 'slack-frontend-team'
      slack_configs:
      - channel: '#team-frontend-alerts'
        title: '[Frontend] {{ .GroupLabels.alertname }}'
        
    # åç«¯å›¢é˜Ÿ
    - name: 'slack-backend-team'
      slack_configs:
      - channel: '#team-backend-alerts'
        title: '[Backend] {{ .GroupLabels.alertname }}'
        
    # éç”Ÿäº§ç¯å¢ƒ
    - name: 'slack-non-prod'
      slack_configs:
      - channel: '#alerts-non-prod'
        title: '[{{ .GroupLabels.environment }}] {{ .GroupLabels.alertname }}'
        color: 'good'
        
    # FinOpså›¢é˜Ÿ(æˆæœ¬å‘Šè­¦)
    - name: 'email-finops'
      email_configs:
      - to: 'finops@example.com'
        subject: 'Cost Alert: {{ .GroupLabels.alertname }}'
        
    # å‘Šè­¦æŠ‘åˆ¶è§„åˆ™
    inhibit_rules:
    # å¦‚æœé›†ç¾¤å®•æœº,æŠ‘åˆ¶è¯¥é›†ç¾¤çš„å…¶ä»–å‘Šè­¦
    - source_match:
        alertname: 'KubernetesClusterDown'
      target_match_re:
        alertname: '.*'
      equal: ['cluster']
      
    # å¦‚æœèŠ‚ç‚¹å®•æœº,æŠ‘åˆ¶è¯¥èŠ‚ç‚¹ä¸Šçš„Podå‘Šè­¦
    - source_match:
        alertname: 'NodeDown'
      target_match:
        alertname: 'PodCrashLooping'
      equal: ['node']

---
# 2. è·¨é›†ç¾¤å‘Šè­¦è§„åˆ™ç¤ºä¾‹
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: multicluster-alerting-rules
  namespace: monitoring
spec:
  groups:
  - name: multicluster.critical
    interval: 1m
    rules:
    # é›†ç¾¤å®Œå…¨ä¸å¯è¾¾
    - alert: KubernetesClusterDown
      expr: |
        absent(up{job="kubernetes-apiservers"} == 1)
      for: 5m
      labels:
        severity: critical
        environment: '{{ $labels.environment }}'
      annotations:
        summary: "é›†ç¾¤ {{ $labels.cluster }} å®Œå…¨ä¸å¯è¾¾"
        description: "Kubernetes API Serveræ— æ³•è®¿é—®è¶…è¿‡5åˆ†é’Ÿ"
        runbook_url: "https://runbooks.example.com/KubernetesClusterDown"
        
    # è·¨é›†ç¾¤åº”ç”¨é”™è¯¯ç‡è¿‡é«˜
    - alert: MultiClusterHighErrorRate
      expr: |
        (
          sum by(cluster, namespace, app) (
            rate(http_requests_total{code=~"5.."}[5m])
          )
          /
          sum by(cluster, namespace, app) (
            rate(http_requests_total[5m])
          )
        ) > 0.05
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "åº”ç”¨ {{ $labels.app }} åœ¨é›†ç¾¤ {{ $labels.cluster }} é”™è¯¯ç‡è¿‡é«˜"
        description: "5xxé”™è¯¯ç‡: {{ $value | humanizePercentage }}"
        
  - name: multicluster.capacity
    interval: 5m
    rules:
    # é›†ç¾¤èµ„æºå³å°†è€—å°½
    - alert: ClusterCPUNearCapacity
      expr: |
        (
          sum by(cluster) (kube_pod_container_resource_requests{resource="cpu"})
          /
          sum by(cluster) (kube_node_status_allocatable{resource="cpu"})
        ) > 0.85
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: "é›†ç¾¤ {{ $labels.cluster }} CPUèµ„æºå³å°†è€—å°½"
        description: "CPUåˆ†é…ç‡: {{ $value | humanizePercentage }}"
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: æ‰€æœ‰å‘Šè­¦éƒ½å‘PagerDuty** | ä¼šå¯¼è‡´å‘Šè­¦ç–²åŠ³,criticalæ‰åº”è¯¥å‘¼å« | åˆ†çº§:criticalâ†’å‘¼å«,warningâ†’Slack |
| **è¯¯åŒº2: æ¯ä¸ªé›†ç¾¤ç‹¬ç«‹Alertmanager** | æ— æ³•ç»Ÿä¸€æŠ‘åˆ¶å’Œè·¯ç”±,é‡å¤å‘Šè­¦ | ä½¿ç”¨ä¸­å¿ƒåŒ–Alertmanageré›†ç¾¤ |
| **è¯¯åŒº3: ä¸è®¾ç½®å‘Šè­¦æŠ‘åˆ¶** | ä¸€ä¸ªé—®é¢˜è§¦å‘å‡ åä¸ªå‘Šè­¦,æ·¹æ²¡çœŸå®é—®é¢˜ | é…ç½®inhibit_rules,ä¸Šæ¸¸æ•…éšœæŠ‘åˆ¶ä¸‹æ¸¸ |
| **è¯¯åŒº4: å‘Šè­¦æ¨¡æ¿åƒç¯‡ä¸€å¾‹** | ç¼ºå°‘ä¸Šä¸‹æ–‡ä¿¡æ¯,æ— æ³•å¿«é€Ÿå“åº” | åŒ…å«clusterã€runbook_urlã€å¸¸ç”¨å‘½ä»¤ |



---

## 4. è¿ç»´è‡ªåŠ¨åŒ–

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: æ‰‹å·¥ç®¡ç†å¤šä¸ªé›†ç¾¤ä¸å¯æŒç»­,å¿…é¡»è‡ªåŠ¨åŒ–ã€‚Infrastructure as Code(Terraformç®¡ç†äº‘èµ„æº) + GitOps(ArgoCDç®¡ç†K8sèµ„æº) æ˜¯å¤šäº‘è‡ªåŠ¨åŒ–çš„åŸºçŸ³ã€‚

### 4.1 å¤šäº‘GitOpsæµæ°´çº¿

```yaml
# ========== å¤šäº‘GitOpsæ¶æ„ ==========
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: multicloud-applications
  namespace: argocd
spec:
  generators:
  # åŸºäºé›†ç¾¤æ ‡ç­¾çš„ç”Ÿæˆå™¨
  - clusters:
      selector:
        matchLabels:
          environment: production
          
  # åŸºäºGitç›®å½•ç»“æ„çš„ç”Ÿæˆå™¨
  - git:
      repoURL: https://github.com/company/multicloud-manifests.git
      revision: HEAD
      directories:
      - path: apps/*/overlays/*
      
  template:
    metadata:
      name: '{{name}}-{{path.basename}}'
    spec:
      project: multicloud
      source:
        repoURL: https://github.com/company/multicloud-manifests.git
        targetRevision: HEAD
        path: '{{path}}'
        
      destination:
        server: '{{server}}'
        namespace: '{{path.basename}}'
        
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
        - CreateNamespace=true
        - PruneLast=true
        - ApplyOutOfSyncOnly=true
        
      ignoreDifferences:
      - group: apps
        kind: Deployment
        jsonPointers:
        - /spec/replicas
        
---
# ========== å¤šäº‘CI/CDæµæ°´çº¿ ==========
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: multicloud-deployment-pipeline
spec:
  workspaces:
  - name: shared-workspace
  params:
  - name: git-url
  - name: git-revision
  - name: app-name
  - name: target-environments
  
  tasks:
  # ä»£ç æ£€å‡º
  - name: fetch-repository
    taskRef:
      name: git-clone
    workspaces:
    - name: output
      workspace: shared-workspace
    params:
    - name: url
      value: $(params.git-url)
    - name: revision
      value: $(params.git-revision)
      
  # æ„å»ºå’Œæµ‹è¯•
  - name: build-and-test
    taskRef:
      name: kaniko-build
    runAfter: ["fetch-repository"]
    workspaces:
    - name: source
      workspace: shared-workspace
    params:
    - name: IMAGE
      value: "registry.example.com/$(params.app-name):$(params.git-revision)"
      
  # å¤šäº‘å®‰å…¨æ‰«æ
  - name: multicloud-security-scan
    taskRef:
      name: trivy-multicloud-scan
    runAfter: ["build-and-test"]
    workspaces:
    - name: source
      workspace: shared-workspace
    params:
    - name: IMAGE
      value: "registry.example.com/$(params.app-name):$(params.git-revision)"
    - name: TARGET_CLOUDS
      value: "aws,azure,gcp"
      
  # å¤šäº‘éƒ¨ç½²
  - name: deploy-to-clouds
    taskRef:
      name: argocd-multicloud-deploy
    runAfter: ["multicloud-security-scan"]
    workspaces:
    - name: source
      workspace: shared-workspace
    params:
    - name: APP_NAME
      value: $(params.app-name)
    - name: TARGET_ENVIRONMENTS
      value: $(params.target-environments)
    - name: GIT_REVISION
      value: $(params.git-revision)
      
  # è·¨äº‘é›†æˆæµ‹è¯•
  - name: multicloud-integration-test
    taskRef:
      name: multicloud-smoke-test
    runAfter: ["deploy-to-clouds"]
    workspaces:
    - name: source
      workspace: shared-workspace
    params:
    - name: APP_NAME
      value: $(params.app-name)
    - name: TARGET_ENVIRONMENTS
      value: $(params.target-environments)

---
# ========== å¤šäº‘è‡ªåŠ¨åŒ–è¿ç»´ ==========
apiVersion: batch/v1
kind: CronJob
metadata:
  name: multicloud-automation
  namespace: platform-ops
spec:
  schedule: "*/30 * * * *"  # æ¯30åˆ†é’Ÿæ‰§è¡Œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: multicloud-operator
          containers:
          - name: automation-runner
            image: platform/multicloud-automation:latest
            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-access-key
            - name: AZURE_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: azure-credentials
                  key: client-id
            - name: AZURE_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  name: azure-credentials
                  key: client-secret
            - name: GCP_SERVICE_ACCOUNT_KEY
              valueFrom:
                secretKeyRef:
                  name: gcp-credentials
                  key: service-account-key
            command:
            - /bin/sh
            - -c
            - |
              #!/bin/bash
              set -euo pipefail
              
              echo "å¼€å§‹å¤šäº‘è‡ªåŠ¨åŒ–è¿ç»´ä»»åŠ¡..."
              
              # 1. èµ„æºå¥åº·æ£€æŸ¥
              echo "æ‰§è¡Œè·¨äº‘èµ„æºå¥åº·æ£€æŸ¥..."
              python3 /scripts/health-check.py --clouds aws,azure,gcp
              
              # 2. æˆæœ¬ä¼˜åŒ–å»ºè®®
              echo "ç”Ÿæˆæˆæœ¬ä¼˜åŒ–å»ºè®®..."
              python3 /scripts/cost-optimizer.py --analyze-current-usage
              
              # 3. å®‰å…¨åˆè§„æ‰«æ
              echo "æ‰§è¡Œå®‰å…¨åˆè§„æ‰«æ..."
              python3 /scripts/security-scanner.py --all-clouds
              
              # 4. è‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜
              echo "æ‰§è¡Œè‡ªåŠ¨ä¿®å¤..."
              python3 /scripts/auto-remediation.py --fix-common-issues
              
              # 5. ç”Ÿæˆè¿ç»´æŠ¥å‘Š
              echo "ç”Ÿæˆè¿ç»´æŠ¥å‘Š..."
              REPORT_TIME=$(date -I)
              cat > /reports/multicloud-ops-report-${REPORT_TIME}.md <<EOF
              # å¤šäº‘è¿ç»´è‡ªåŠ¨åŒ–æŠ¥å‘Š
              
              ## æ‰§è¡Œæ—¶é—´
              ${REPORT_TIME}
              
              ## å¥åº·æ£€æŸ¥ç»“æœ
              $(cat /tmp/health-check-results.txt)
              
              ## æˆæœ¬ä¼˜åŒ–å»ºè®®
              $(cat /tmp/cost-optimization-recommendations.txt)
              
              ## å®‰å…¨æ‰«æå‘ç°
              $(cat /tmp/security-findings.txt)
              
              ## è‡ªåŠ¨ä¿®å¤è®°å½•
              $(cat /tmp/remediation-actions.txt)
              EOF
              
              echo "å¤šäº‘è‡ªåŠ¨åŒ–è¿ç»´ä»»åŠ¡å®Œæˆ"
          restartPolicy: OnFailure
```

### 4.2 IaCå¤šäº‘ç®¡ç†

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**å»ºç­‘è®¾è®¡è“å›¾** â€” å»ºæˆ¿å­å‰å…ˆç”»å›¾çº¸,æ”¹è®¾è®¡åªæ”¹å›¾çº¸,ä¸ç”¨æ‹†æˆ¿é‡å»ºã€‚IaC (Infrastructure as Code) å°±æ˜¯ç”¨ä»£ç æè¿°äº‘èµ„æº,ç‰ˆæœ¬æ§åˆ¶ã€ä¸€é”®éƒ¨ç½²ã€å¯é‡å¤æ‰§è¡Œã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[Gitä»“åº“<br/>åŸºç¡€è®¾æ–½ä»£ç ] --> B[Terraform/Pulumi]
    
    B --> C[AWS Provider]
    B --> D[Azure Provider]
    B --> E[GCP Provider]
    
    C --> C1[VPC]
    C --> C2[EKSé›†ç¾¤]
    C --> C3[RDSæ•°æ®åº“]
    
    D --> D1[VNet]
    D --> D2[AKSé›†ç¾¤]
    D --> D3[SQLæ•°æ®åº“]
    
    E --> E1[VPC]
    E --> E2[GKEé›†ç¾¤]
    E --> E3[Cloud SQL]
    
    F[Pull Request] --> A
    G[CI/CD Pipeline] --> B
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
```

**IaCå·¥å…·å¯¹æ¯”:**

| å·¥å…· | è¯­è¨€ | å¤šäº‘æ”¯æŒ | çŠ¶æ€ç®¡ç† | å­¦ä¹ æ›²çº¿ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|----------|----------|----------|
| **Terraform** | HCL | ä¼˜ç§€(500+ providers) | è¿œç¨‹çŠ¶æ€ | â­â­â­ | é€šç”¨é¦–é€‰,æˆç†Ÿç”Ÿæ€ |
| **Pulumi** | Python/TS/Go | ä¼˜ç§€ | äº‘ç«¯å­˜å‚¨ | â­â­â­â­ | å¼€å‘è€…å‹å¥½,å¤æ‚é€»è¾‘ |
| **Crossplane** | K8s CRD | ä¼˜ç§€ | etcd | â­â­â­â­ | K8såŸç”Ÿ,GitOpsé›†æˆ |
| **CDK for Terraform** | Python/TS | ä¼˜ç§€ | Terraformåç«¯ | â­â­â­ | å¤æ‚ç¼–ç¨‹é€»è¾‘ |
| **Ansible** | YAML | ä¸­ç­‰ | æ— çŠ¶æ€ | â­â­ | é…ç½®ç®¡ç†ä¸ºä¸» |

#### ğŸ“ æœ€å°ç¤ºä¾‹

```hcl
# ========== Terraformå¤šäº‘K8sé›†ç¾¤éƒ¨ç½² ==========
# terraform.tf - ç‰ˆæœ¬å’ŒProvideré…ç½®
terraform {
  required_version = ">= 1.5.0"
  
  # è¿œç¨‹çŠ¶æ€å­˜å‚¨(å…³é”®!é¿å…çŠ¶æ€å†²çª)
  backend "s3" {
    bucket         = "company-terraform-state"
    key            = "multicloud/clusters/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-lock"
  }
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.23"
    }
  }
}

# ========== variables.tf - å˜é‡å®šä¹‰ ==========
variable "project_name" {
  description = "é¡¹ç›®åç§°,ç”¨äºèµ„æºå‘½å"
  type        = string
  default     = "multicloud-prod"
}

variable "kubernetes_version" {
  description = "Kubernetesç‰ˆæœ¬"
  type        = string
  default     = "1.28"
}

variable "node_count" {
  description = "æ¯ä¸ªé›†ç¾¤çš„èŠ‚ç‚¹æ•°"
  type        = number
  default     = 3
}

variable "tags" {
  description = "ç»Ÿä¸€æ ‡ç­¾"
  type        = map(string)
  default = {
    Project     = "MultiCloud"
    Team        = "Platform"
    Environment = "Production"
    ManagedBy   = "Terraform"
  }
}

# ========== modules/aws-eks/main.tf - AWS EKSæ¨¡å— ==========
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  tags = merge(var.tags, {
    Name = "${var.cluster_name}-vpc"
  })
}

resource "aws_subnet" "private" {
  count             = 3
  vpc_id            = aws_vpc.main.id
  cidr_block        = cidrsubnet(var.vpc_cidr, 4, count.index)
  availability_zone = data.aws_availability_zones.available.names[count.index]
  
  tags = merge(var.tags, {
    Name                              = "${var.cluster_name}-private-${count.index + 1}"
    "kubernetes.io/role/internal-elb" = "1"
  })
}

resource "aws_eks_cluster" "main" {
  name     = var.cluster_name
  version  = var.kubernetes_version
  role_arn = aws_iam_role.cluster.arn
  
  vpc_config {
    subnet_ids              = aws_subnet.private[*].id
    endpoint_private_access = true
    endpoint_public_access  = true
  }
  
  # å¯ç”¨æ§åˆ¶å¹³é¢æ—¥å¿—
  enabled_cluster_log_types = ["api", "audit", "authenticator"]
  
  tags = merge(var.tags, {
    Name = var.cluster_name
  })
  
  depends_on = [
    aws_iam_role_policy_attachment.cluster_policy
  ]
}

resource "aws_eks_node_group" "main" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${var.cluster_name}-workers"
  node_role_arn   = aws_iam_role.node.arn
  subnet_ids      = aws_subnet.private[*].id
  
  scaling_config {
    desired_size = var.node_count
    max_size     = var.node_count * 2
    min_size     = 1
  }
  
  instance_types = ["t3.large"]
  capacity_type  = "SPOT"  # ä½¿ç”¨SpotèŠ‚çœæˆæœ¬
  
  # è‡ªåŠ¨æ›´æ–°ç­–ç•¥
  update_config {
    max_unavailable = 1
  }
  
  tags = merge(var.tags, {
    Name = "${var.cluster_name}-node-group"
  })
}

# ========== modules/azure-aks/main.tf - Azure AKSæ¨¡å— ==========
resource "azurerm_resource_group" "main" {
  name     = "${var.cluster_name}-rg"
  location = var.location
  tags     = var.tags
}

resource "azurerm_virtual_network" "main" {
  name                = "${var.cluster_name}-vnet"
  address_space       = [var.vnet_cidr]
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  tags                = var.tags
}

resource "azurerm_subnet" "aks" {
  name                 = "${var.cluster_name}-subnet"
  resource_group_name  = azurerm_resource_group.main.name
  virtual_network_name = azurerm_virtual_network.main.name
  address_prefixes     = [cidrsubnet(var.vnet_cidr, 4, 0)]
}

resource "azurerm_kubernetes_cluster" "main" {
  name                = var.cluster_name
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  dns_prefix          = var.cluster_name
  kubernetes_version  = var.kubernetes_version
  
  default_node_pool {
    name       = "default"
    node_count = var.node_count
    vm_size    = "Standard_D2s_v3"
    vnet_subnet_id = azurerm_subnet.aks.id
    
    # å¯ç”¨è‡ªåŠ¨æ‰©ç¼©å®¹
    enable_auto_scaling = true
    min_count           = 1
    max_count           = var.node_count * 2
    
    # ä½¿ç”¨Spotå®ä¾‹
    priority        = "Spot"
    eviction_policy = "Delete"
    spot_max_price  = -1  # ä½¿ç”¨å½“å‰Spotä»·æ ¼
  }
  
  identity {
    type = "SystemAssigned"
  }
  
  network_profile {
    network_plugin = "azure"
    network_policy = "calico"
  }
  
  tags = var.tags
}

# ========== modules/gcp-gke/main.tf - GCP GKEæ¨¡å— ==========
resource "google_container_cluster" "main" {
  name               = var.cluster_name
  location           = var.zone
  min_master_version = var.kubernetes_version
  
  # åˆ é™¤é»˜è®¤èŠ‚ç‚¹æ± ,ä½¿ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹æ± 
  remove_default_node_pool = true
  initial_node_count       = 1
  
  # å¯ç”¨å·¥ä½œè´Ÿè½½èº«ä»½
  workload_identity_config {
    workload_pool = "${var.project_id}.svc.id.goog"
  }
  
  # ç½‘ç»œé…ç½®
  network    = google_compute_network.vpc.name
  subnetwork = google_compute_subnetwork.subnet.name
  
  # å¯ç”¨ç½‘ç»œç­–ç•¥
  network_policy {
    enabled  = true
    provider = "CALICO"
  }
  
  # IPåœ°å€èŒƒå›´
  ip_allocation_policy {
    cluster_ipv4_cidr_block  = "/16"
    services_ipv4_cidr_block = "/22"
  }
  
  # ç»´æŠ¤çª—å£
  maintenance_policy {
    daily_maintenance_window {
      start_time = "03:00"
    }
  }
}

resource "google_container_node_pool" "main" {
  name       = "${var.cluster_name}-pool"
  location   = var.zone
  cluster    = google_container_cluster.main.name
  node_count = var.node_count
  
  # è‡ªåŠ¨æ‰©ç¼©å®¹
  autoscaling {
    min_node_count = 1
    max_node_count = var.node_count * 2
  }
  
  node_config {
    preemptible  = true  # ä½¿ç”¨æŠ¢å å¼å®ä¾‹
    machine_type = "n2-standard-2"
    
    oauth_scopes = [
      "https://www.googleapis.com/auth/cloud-platform"
    ]
    
    labels = var.tags
  }
}

# ========== main.tf - ä¸»é…ç½®æ–‡ä»¶ ==========
# AWSé›†ç¾¤
module "aws_cluster" {
  source = "./modules/aws-eks"
  
  cluster_name       = "${var.project_name}-aws-us-east-1"
  vpc_cidr           = "10.0.0.0/16"
  kubernetes_version = var.kubernetes_version
  node_count         = var.node_count
  tags               = merge(var.tags, { Provider = "AWS" })
}

# Azureé›†ç¾¤
module "azure_cluster" {
  source = "./modules/azure-aks"
  
  cluster_name       = "${var.project_name}-azure-westeurope"
  location           = "westeurope"
  vnet_cidr          = "10.1.0.0/16"
  kubernetes_version = var.kubernetes_version
  node_count         = var.node_count
  tags               = merge(var.tags, { Provider = "Azure" })
}

# GCPé›†ç¾¤
module "gcp_cluster" {
  source = "./modules/gcp-gke"
  
  project_id         = var.gcp_project_id
  cluster_name       = "${var.project_name}-gcp-us-central1"
  zone               = "us-central1-a"
  kubernetes_version = var.kubernetes_version
  node_count         = var.node_count
  tags               = merge(var.tags, { Provider = "GCP" })
}

# ========== outputs.tf - è¾“å‡ºå˜é‡ ==========
output "aws_cluster_endpoint" {
  value       = module.aws_cluster.cluster_endpoint
  description = "AWS EKSé›†ç¾¤APIç«¯ç‚¹"
}

output "azure_cluster_fqdn" {
  value       = module.azure_cluster.cluster_fqdn
  description = "Azure AKSé›†ç¾¤FQDN"
}

output "gcp_cluster_endpoint" {
  value       = module.gcp_cluster.cluster_endpoint
  description = "GCP GKEé›†ç¾¤ç«¯ç‚¹"
  sensitive   = true
}

# ç”Ÿæˆkubeconfig
output "kubeconfig_commands" {
  value = <<-EOT
    # é…ç½®AWSé›†ç¾¤è®¿é—®
    aws eks update-kubeconfig --name ${module.aws_cluster.cluster_name} --region us-east-1
    
    # é…ç½®Azureé›†ç¾¤è®¿é—®
    az aks get-credentials --name ${module.azure_cluster.cluster_name} --resource-group ${module.azure_cluster.resource_group_name}
    
    # é…ç½®GCPé›†ç¾¤è®¿é—®
    gcloud container clusters get-credentials ${module.gcp_cluster.cluster_name} --zone us-central1-a
  EOT
  description = "é…ç½®kubectlè®¿é—®å„é›†ç¾¤çš„å‘½ä»¤"
}
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: ç›´æ¥åœ¨ç”Ÿäº§ç¯å¢ƒapply** | æ²¡æœ‰å®¡æŸ¥æµç¨‹,å®¹æ˜“è¯¯æ“ä½œåˆ é™¤èµ„æº | ä½¿ç”¨`terraform plan`å®¡æŸ¥,PRæµç¨‹æ‰¹å‡† |
| **è¯¯åŒº2: æœ¬åœ°å­˜å‚¨stateæ–‡ä»¶** | å›¢é˜Ÿåä½œä¼šå†²çª,stateä¸¢å¤±ç¾éš¾æ€§ | ä½¿ç”¨S3+DynamoDBé”,æˆ–Terraform Cloud |
| **è¯¯åŒº3: æŠŠæ•æ„Ÿä¿¡æ¯å†™åœ¨ä»£ç é‡Œ** | æ³„éœ²å®‰å…¨é£é™© | ä½¿ç”¨Secret Manager,ç¯å¢ƒå˜é‡æ³¨å…¥ |
| **è¯¯åŒº4: ä¸ä½¿ç”¨æ¨¡å—åŒ–** | ä»£ç é‡å¤,éš¾ä»¥ç»´æŠ¤ | æŠ½è±¡é€šç”¨æ¨¡å—,DRYåŸåˆ™ |
| **è¯¯åŒº5: å¿½ç•¥driftæ£€æµ‹** | æ‰‹å·¥æ”¹åŠ¨ä¸ä»£ç ä¸ä¸€è‡´ | å®šæœŸè¿è¡Œ`terraform plan`æ£€æŸ¥drift |

**å®æˆ˜æ“ä½œæŒ‡å—:**

```bash
# 1. åˆå§‹åŒ–Terraform
terraform init

# 2. éªŒè¯é…ç½®è¯­æ³•
terraform validate

# 3. æ ¼å¼åŒ–ä»£ç 
terraform fmt -recursive

# 4. æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’(å…³é”®!å¿…é¡»å®¡æŸ¥)
terraform plan -out=tfplan

# 5. åº”ç”¨å˜æ›´(ç”Ÿäº§ç¯å¢ƒéœ€è¦å®¡æ‰¹)
terraform apply tfplan

# 6. æŸ¥çœ‹è¾“å‡º
terraform output

# 7. é”€æ¯èµ„æº(å±é™©æ“ä½œ!)
terraform destroy

# 8. å¯¼å…¥ç°æœ‰èµ„æºåˆ°Terraformç®¡ç†
terraform import module.aws_cluster.aws_eks_cluster.main my-existing-cluster

# 9. æ£€æŸ¥stateæ¼‚ç§»
terraform plan -refresh-only

# 10. å·¥ä½œç©ºé—´ç®¡ç†(å¤šç¯å¢ƒ)
terraform workspace new production
terraform workspace select production
```

### 4.3 GitOpså¤šé›†ç¾¤

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**è¿é”åº—ç»Ÿä¸€é…æ–¹** â€” æ€»éƒ¨(Gitä»“åº“)å®šä¹‰æ ‡å‡†é…æ–¹,å„åˆ†åº—(é›†ç¾¤)è‡ªåŠ¨åŒæ­¥æ‰§è¡Œ,ä¿è¯ä¸€è‡´æ€§ã€‚GitOpså°±æ˜¯ç”¨Gitä½œä¸ºå”¯ä¸€äº‹å®æ¥æº,é›†ç¾¤è‡ªåŠ¨åŒæ­¥ä»£ç çŠ¶æ€ã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[Gitä»“åº“<br/>K8s manifests] --> B{ArgoCD/Flux}
    
    B --> C[AWSé›†ç¾¤<br/>Agentæ‹‰å–]
    B --> D[Azureé›†ç¾¤<br/>Agentæ‹‰å–]
    B --> E[GCPé›†ç¾¤<br/>Agentæ‹‰å–]
    
    C --> C1[åº”ç”¨éƒ¨ç½²]
    C --> C2[é…ç½®æ›´æ–°]
    C --> C3[è‡ªåŠ¨åŒæ­¥]
    
    D --> D1[åº”ç”¨éƒ¨ç½²]
    D --> D2[é…ç½®æ›´æ–°]
    D --> D3[è‡ªåŠ¨åŒæ­¥]
    
    E --> E1[åº”ç”¨éƒ¨ç½²]
    E --> E2[é…ç½®æ›´æ–°]
    E --> E3[è‡ªåŠ¨åŒæ­¥]
    
    F[å¼€å‘è€…] -->|Pushä»£ç | A
    G[CI Pipeline] -->|æ„å»ºé•œåƒ| H[é•œåƒä»“åº“]
    H --> C1
    H --> D1
    H --> E1
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
```

**GitOpså·¥å…·å¯¹æ¯”:**

| å·¥å…· | æ¶æ„æ¨¡å¼ | å¤šé›†ç¾¤æ”¯æŒ | UI | å­¦ä¹ æ›²çº¿ | é€‚ç”¨åœºæ™¯ |
|------|----------|------------|----|-----------| ---------|
| **ArgoCD** | Pull,ä¸­å¿ƒåŒ– | ApplicationSet | ä¼˜ç§€ | â­â­â­ | ä¼ä¸šé¦–é€‰,åŠŸèƒ½ä¸°å¯Œ |
| **Flux CD** | Pull,å»ä¸­å¿ƒåŒ– | Kustomize overlay | åŸºç¡€ | â­â­â­â­ | è½»é‡çº§,CNCFæ¯•ä¸š |
| **Rancher Fleet** | Pull,åˆ†å±‚ | Gitç›®å½•ç»“æ„ | ä¼˜ç§€ | â­â­ | Rancherç”Ÿæ€é›†æˆ |

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== ArgoCDå¤šé›†ç¾¤GitOpsé…ç½® ==========
# 1. Gitä»“åº“ç»“æ„
# apps/
# â”œâ”€â”€ base/                     # åŸºç¡€é…ç½®
# â”‚   â””â”€â”€ my-app/
# â”‚       â”œâ”€â”€ deployment.yaml
# â”‚       â”œâ”€â”€ service.yaml
# â”‚       â””â”€â”€ kustomization.yaml
# â””â”€â”€ overlays/                 # å„é›†ç¾¤å·®å¼‚åŒ–é…ç½®
#     â”œâ”€â”€ aws-prod/
#     â”‚   â”œâ”€â”€ kustomization.yaml
#     â”‚   â””â”€â”€ patch-replicas.yaml
#     â”œâ”€â”€ azure-prod/
#     â”‚   â””â”€â”€ kustomization.yaml
#     â””â”€â”€ gcp-prod/
#         â””â”€â”€ kustomization.yaml

# 2. æ³¨å†Œå¤šä¸ªé›†ç¾¤åˆ°ArgoCD
# åœ¨ArgoCDæ§åˆ¶å¹³é¢æ‰§è¡Œ:
apiVersion: v1
kind: Secret
metadata:
  name: aws-cluster-secret
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: cluster
type: Opaque
stringData:
  name: "aws-us-east-1"
  server: "https://aws-eks-api.us-east-1.example.com"
  config: |
    {
      "bearerToken": "${AWS_CLUSTER_TOKEN}",
      "tlsClientConfig": {
        "insecure": false,
        "caData": "${AWS_CA_CERT}"
      }
    }

---
apiVersion: v1
kind: Secret
metadata:
  name: azure-cluster-secret
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: cluster
type: Opaque
stringData:
  name: "azure-westeurope"
  server: "https://azure-aks-api.westeurope.example.com"
  config: |
    {
      "bearerToken": "${AZURE_CLUSTER_TOKEN}",
      "tlsClientConfig": {
        "caData": "${AZURE_CA_CERT}"
      }
    }

---
# 3. ApplicationSetè‡ªåŠ¨ç”Ÿæˆå¤šé›†ç¾¤åº”ç”¨
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: multicloud-app
  namespace: argocd
spec:
  # ç”Ÿæˆå™¨:åŸºäºGitç›®å½•
  generators:
  - git:
      repoURL: https://github.com/company/k8s-manifests.git
      revision: main
      directories:
      - path: apps/overlays/*
      
  # åº”ç”¨æ¨¡æ¿
  template:
    metadata:
      name: 'my-app-{{path.basename}}'
      labels:
        app: my-app
        cluster: '{{path.basename}}'
    spec:
      project: default
      
      source:
        repoURL: https://github.com/company/k8s-manifests.git
        targetRevision: main
        path: 'apps/overlays/{{path.basename}}'
        
      destination:
        # æ ¹æ®ç›®å½•ååŒ¹é…é›†ç¾¤
        name: '{{path.basename}}'
        namespace: production
        
      syncPolicy:
        automated:
          prune: true      # è‡ªåŠ¨åˆ é™¤Gitä¸­ä¸å­˜åœ¨çš„èµ„æº
          selfHeal: true   # è‡ªåŠ¨ä¿®å¤æ‰‹å·¥æ”¹åŠ¨
          allowEmpty: false
        syncOptions:
        - CreateNamespace=true
        - PrunePropagationPolicy=foreground
        
        retry:
          limit: 5
          backoff:
            duration: 5s
            factor: 2
            maxDuration: 3m

---
# 4. Gitä»“åº“ä¸­çš„Kustomizeé…ç½®ç¤ºä¾‹
# apps/base/my-app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- deployment.yaml
- service.yaml

---
# apps/base/my-app/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: app
        image: registry.example.com/my-app:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"

---
# apps/overlays/aws-prod/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
bases:
- ../../base/my-app

# AWSé›†ç¾¤å·®å¼‚åŒ–é…ç½®
replicas:
- name: my-app
  count: 5  # AWSé›†ç¾¤è¿è¡Œ5ä¸ªå‰¯æœ¬

patches:
- patch: |-
    - op: add
      path: /spec/template/spec/containers/0/env
      value:
        - name: CLOUD_PROVIDER
          value: "aws"
        - name: REGION
          value: "us-east-1"
  target:
    kind: Deployment
    name: my-app

---
# apps/overlays/azure-prod/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
bases:
- ../../base/my-app

replicas:
- name: my-app
  count: 3  # Azureé›†ç¾¤è¿è¡Œ3ä¸ªå‰¯æœ¬

patches:
- patch: |-
    - op: add
      path: /spec/template/spec/containers/0/env
      value:
        - name: CLOUD_PROVIDER
          value: "azure"
        - name: REGION
          value: "westeurope"
  target:
    kind: Deployment
    name: my-app

---
# 5. å¤šé›†ç¾¤CI/CD Pipelineé›†æˆ
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: multicloud-gitops-pipeline
spec:
  params:
  - name: git-url
  - name: git-revision
  - name: image-tag
  
  tasks:
  # æ„å»ºé•œåƒ
  - name: build-image
    taskRef:
      name: kaniko
    params:
    - name: IMAGE
      value: "registry.example.com/my-app:$(params.image-tag)"
      
  # æ›´æ–°Gitä»“åº“ä¸­çš„é•œåƒtag(è§¦å‘GitOpsåŒæ­¥)
  - name: update-manifests
    runAfter: ["build-image"]
    taskSpec:
      steps:
      - name: update-image-tag
        image: alpine/git
        script: |
          #!/bin/sh
          set -e
          
          # Cloneé…ç½®ä»“åº“
          git clone $(params.git-url) /workspace/repo
          cd /workspace/repo
          
          # æ›´æ–°æ‰€æœ‰overlayçš„é•œåƒtag
          for overlay in apps/overlays/*/kustomization.yaml; do
            sed -i "s|newTag:.*|newTag: $(params.image-tag)|" $overlay
          done
          
          # æäº¤å¹¶æ¨é€
          git config user.name "CI Pipeline"
          git config user.email "ci@example.com"
          git add .
          git commit -m "Update image tag to $(params.image-tag)"
          git push origin main
          
  # ç­‰å¾…ArgoCDåŒæ­¥å®Œæˆ
  - name: wait-for-sync
    runAfter: ["update-manifests"]
    taskSpec:
      steps:
      - name: wait
        image: argoproj/argocd:latest
        script: |
          #!/bin/sh
          # ç­‰å¾…æ‰€æœ‰é›†ç¾¤çš„åº”ç”¨åŒæ­¥å®Œæˆ
          for app in my-app-aws-prod my-app-azure-prod my-app-gcp-prod; do
            argocd app wait $app --sync --health --timeout 600
          done
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: é•œåƒtagä¹Ÿæ”¾Gitç®¡ç†** | é¢‘ç¹æ›´æ–°é•œåƒtagä¼šäº§ç”Ÿå¤§é‡commit | ä½¿ç”¨Argo CD Image Updaterè‡ªåŠ¨åŒ– |
| **è¯¯åŒº2: æ‰€æœ‰é›†ç¾¤é…ç½®å®Œå…¨ä¸€è‡´** | åº”è¯¥å…è®¸å·®å¼‚åŒ–(å‰¯æœ¬æ•°ã€èµ„æºé™åˆ¶) | ä½¿ç”¨Kustomize overlayç®¡ç†å·®å¼‚ |
| **è¯¯åŒº3: æ‰‹å·¥åœ¨é›†ç¾¤æ”¹é…ç½®** | GitOpsä¼šå›æ»šæ‰‹å·¥æ”¹åŠ¨ | æ‰€æœ‰æ”¹åŠ¨æäº¤PRåˆ°Git |
| **è¯¯åŒº4: ä¸è®¾ç½®Syncç­–ç•¥** | éœ€è¦æ‰‹åŠ¨åŒæ­¥,å¤±å»è‡ªåŠ¨åŒ–ä¼˜åŠ¿ | å¼€å¯automated sync + self-heal |

**GitOpsæœ€ä½³å®è·µ:**
1. **å•ä¸€äº‹å®æ¥æº**: Gitæ˜¯å”¯ä¸€é…ç½®æº,ç¦æ­¢æ‰‹å·¥kubectl apply
2. **æ‹‰å–æ¨¡å¼**: é›†ç¾¤å†…Agentæ‹‰å–Git,è€Œä¸æ˜¯æ¨é€(å®‰å…¨æ€§)
3. **å®¡è®¡è¿½è¸ª**: æ‰€æœ‰å˜æ›´æœ‰Git commitè®°å½•,å¯å›æº¯
4. **æ¸è¿›å‘å¸ƒ**: å…ˆéƒ¨ç½²åˆ°æµ‹è¯•é›†ç¾¤,éªŒè¯åå†æ¨å¹¿åˆ°ç”Ÿäº§
5. **å›æ»šæœºåˆ¶**: Git revertå³å¯å›æ»š,å¿«é€Ÿæ¢å¤



### 1.5 é›†ç¾¤è”é‚¦ (Cluster Federation)

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**è”åˆå›½ä¸æˆå‘˜å›½**çš„å…³ç³» â€” è”é‚¦å°±åƒè”åˆå›½æ€»éƒ¨,æä¾›ç»Ÿä¸€çš„åè°ƒå’Œæ”¿ç­–æ¡†æ¶,ä½†å„ä¸ªé›†ç¾¤(æˆå‘˜å›½)ä»ä¿æŒç‹¬ç«‹è¿ä½œã€‚é›†ç¾¤è”é‚¦å…è®¸ä½ åœ¨ä¸€ä¸ªæ§åˆ¶å¹³é¢ç®¡ç†å¤šä¸ªKubernetesé›†ç¾¤,å°±åƒç”¨ä¸€ä¸ªä»ªè¡¨ç›˜ç®¡ç†å…¨çƒå¤šä¸ªæ•°æ®ä¸­å¿ƒã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[è”é‚¦æ§åˆ¶å¹³é¢<br/>Federation Control Plane] --> B[AWS EKS<br/>us-east-1]
    A --> C[Azure AKS<br/>westeurope]
    A --> D[GCP GKE<br/>asia-east1]
    
    B --> B1[å·¥ä½œè´Ÿè½½]
    B --> B2[æœåŠ¡]
    B --> B3[é…ç½®]
    
    C --> C1[å·¥ä½œè´Ÿè½½]
    C --> C2[æœåŠ¡]
    C --> C3[é…ç½®]
    
    D --> D1[å·¥ä½œè´Ÿè½½]
    D --> D2[æœåŠ¡]
    D --> D3[é…ç½®]
    
    E[è¿ç»´äººå‘˜] -->|ç»Ÿä¸€ç®¡ç†| A
    
    style A fill:#e3f2fd
    style E fill:#fff3e0
```

**æ ¸å¿ƒç»„ä»¶:**
1. **è”é‚¦æ§åˆ¶å¹³é¢**: ä¸­å¤®ç®¡ç†å±‚,åè°ƒå¤šé›†ç¾¤èµ„æºåˆ†å‘
2. **è”é‚¦èµ„æº**: FederatedDeploymentã€FederatedServiceç­‰è·¨é›†ç¾¤èµ„æº
3. **è°ƒåº¦ç­–ç•¥**: å†³å®šå·¥ä½œè´Ÿè½½åœ¨å“ªäº›é›†ç¾¤è¿è¡Œ
4. **åŒæ­¥æœºåˆ¶**: å°†è”é‚¦èµ„æºåŒæ­¥åˆ°å„æˆå‘˜é›†ç¾¤

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== KubeFedé›†ç¾¤è”é‚¦é…ç½® ==========
# å®‰è£…KubeFedæ§åˆ¶å¹³é¢
apiVersion: v1
kind: Namespace
metadata:
  name: kube-federation-system

---
# æ³¨å†Œæˆå‘˜é›†ç¾¤
apiVersion: core.kubefed.io/v1beta1
kind: KubeFedCluster
metadata:
  name: aws-cluster-us-east-1
  namespace: kube-federation-system
spec:
  apiEndpoint: https://aws-eks-api.us-east-1.example.com
  secretRef:
    name: aws-cluster-secret
  caBundle: LS0tLS1CRUdJTi... # Base64ç¼–ç çš„CAè¯ä¹¦

---
apiVersion: core.kubefed.io/v1beta1
kind: KubeFedCluster
metadata:
  name: azure-cluster-westeurope
  namespace: kube-federation-system
spec:
  apiEndpoint: https://azure-aks-api.westeurope.example.com
  secretRef:
    name: azure-cluster-secret
  caBundle: LS0tLS1CRUdJTi...

---
apiVersion: core.kubefed.io/v1beta1
kind: KubeFedCluster
metadata:
  name: gcp-cluster-asia-east1
  namespace: kube-federation-system
spec:
  apiEndpoint: https://gcp-gke-api.asia-east1.example.com
  secretRef:
    name: gcp-cluster-secret
  caBundle: LS0tLS1CRUdJTi...

---
# ========== è”é‚¦åº”ç”¨éƒ¨ç½² ==========
# åˆ›å»ºè·¨é›†ç¾¤éƒ¨ç½²
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: multicloud-app
  namespace: production
spec:
  # æ¨¡æ¿å®šä¹‰ - åº”ç”¨åŸºç¡€é…ç½®
  template:
    metadata:
      labels:
        app: multicloud-app
    spec:
      replicas: 3  # é»˜è®¤å‰¯æœ¬æ•°
      selector:
        matchLabels:
          app: multicloud-app
      template:
        metadata:
          labels:
            app: multicloud-app
        spec:
          containers:
          - name: app
            image: registry.example.com/multicloud-app:v1.2.3
            ports:
            - containerPort: 8080
            resources:
              requests:
                memory: "256Mi"
                cpu: "250m"
              limits:
                memory: "512Mi"
                cpu: "500m"
  
  # è°ƒåº¦ç­–ç•¥ - å†³å®šåœ¨å“ªäº›é›†ç¾¤è¿è¡Œ
  placement:
    clusters:
    - name: aws-cluster-us-east-1
    - name: azure-cluster-westeurope
    - name: gcp-cluster-asia-east1
  
  # å·®å¼‚åŒ–é…ç½® - æŒ‰é›†ç¾¤è¦†ç›–ç‰¹å®šè®¾ç½®
  overrides:
  # AWSé›†ç¾¤: ä½¿ç”¨æ›´å¤šå‰¯æœ¬æœåŠ¡åŒ—ç¾ç”¨æˆ·
  - clusterName: aws-cluster-us-east-1
    clusterOverrides:
    - path: "/spec/replicas"
      value: 5
    - path: "/spec/template/spec/containers/0/resources/requests/cpu"
      value: "500m"
      
  # Azureé›†ç¾¤: ä¸­ç­‰é…ç½®æœåŠ¡æ¬§æ´²ç”¨æˆ·
  - clusterName: azure-cluster-westeurope
    clusterOverrides:
    - path: "/spec/replicas"
      value: 3
      
  # GCPé›†ç¾¤: è¾ƒå°‘å‰¯æœ¬æœåŠ¡äºšæ´²ç”¨æˆ·
  - clusterName: gcp-cluster-asia-east1
    clusterOverrides:
    - path: "/spec/replicas"
      value: 2

---
# ========== è”é‚¦æœåŠ¡é…ç½® ==========
apiVersion: types.kubefed.io/v1beta1
kind: FederatedService
metadata:
  name: multicloud-app-service
  namespace: production
spec:
  template:
    metadata:
      labels:
        app: multicloud-app
    spec:
      type: LoadBalancer
      selector:
        app: multicloud-app
      ports:
      - protocol: TCP
        port: 80
        targetPort: 8080
  
  placement:
    clusters:
    - name: aws-cluster-us-east-1
    - name: azure-cluster-westeurope
    - name: gcp-cluster-asia-east1

---
# ========== è”é‚¦é…ç½®åŒæ­¥ ==========
apiVersion: types.kubefed.io/v1beta1
kind: FederatedConfigMap
metadata:
  name: app-config
  namespace: production
spec:
  template:
    data:
      # å…¨å±€é…ç½®
      LOG_LEVEL: "info"
      FEATURE_FLAG_NEW_UI: "true"
  
  placement:
    clusters:
    - name: aws-cluster-us-east-1
    - name: azure-cluster-westeurope
    - name: gcp-cluster-asia-east1
  
  overrides:
  # AWSé›†ç¾¤ç‰¹å®šé…ç½®
  - clusterName: aws-cluster-us-east-1
    clusterOverrides:
    - path: "/data/REGION"
      value: "us-east-1"
    - path: "/data/DATABASE_ENDPOINT"
      value: "db.us-east-1.aws.example.com"
      
  # Azureé›†ç¾¤ç‰¹å®šé…ç½®
  - clusterName: azure-cluster-westeurope
    clusterOverrides:
    - path: "/data/REGION"
      value: "westeurope"
    - path: "/data/DATABASE_ENDPOINT"
      value: "db.westeurope.azure.example.com"
      
  # GCPé›†ç¾¤ç‰¹å®šé…ç½®
  - clusterName: gcp-cluster-asia-east1
    clusterOverrides:
    - path: "/data/REGION"
      value: "asia-east1"
    - path: "/data/DATABASE_ENDPOINT"
      value: "db.asia-east1.gcp.example.com"
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: è”é‚¦èƒ½è§£å†³æ‰€æœ‰å¤šé›†ç¾¤é—®é¢˜** | è”é‚¦ä¸»è¦è§£å†³"èµ„æºåŒæ­¥"é—®é¢˜,ä¸èƒ½è‡ªåŠ¨è§£å†³ç½‘ç»œäº’é€šã€å­˜å‚¨å…±äº«ç­‰é—®é¢˜ | è”é‚¦+æœåŠ¡ç½‘æ ¼+ç»Ÿä¸€å­˜å‚¨æ–¹æ¡ˆç»„åˆä½¿ç”¨ |
| **è¯¯åŒº2: è”é‚¦æ§åˆ¶å¹³é¢å•ç‚¹æ•…éšœ** | æ˜¯çš„,è”é‚¦æ§åˆ¶å¹³é¢å®•æœºä¼šå½±å“ç®¡ç†èƒ½åŠ›,ä½†æˆå‘˜é›†ç¾¤ä»æ­£å¸¸è¿è¡Œ | è”é‚¦æ§åˆ¶å¹³é¢HAéƒ¨ç½²+å®šæœŸå¤‡ä»½æˆå‘˜é›†ç¾¤é…ç½® |
| **è¯¯åŒº3: æ‰€æœ‰èµ„æºéƒ½åº”è¯¥è”é‚¦åŒ–** | è”é‚¦å¢åŠ å¤æ‚åº¦,åªåº”ç”¨äºéœ€è¦è·¨é›†ç¾¤ç®¡ç†çš„èµ„æº | æ ¸å¿ƒåº”ç”¨è”é‚¦åŒ–,é›†ç¾¤ç‰¹å®šèµ„æºç›´æ¥ç®¡ç† |
| **è¯¯åŒº4: è”é‚¦èƒ½è‡ªåŠ¨åšè·¨é›†ç¾¤è´Ÿè½½å‡è¡¡** | è”é‚¦åªè´Ÿè´£èµ„æºåˆ†å‘,éœ€è¦é…åˆGlobal Load Balanceråšæµé‡åˆ†å‘ | ä½¿ç”¨AWS Global Acceleratorã€Azure Traffic Managerç­‰ |

**å…³é”®å†³ç­–ç‚¹:**
- **ä½•æ—¶ä½¿ç”¨è”é‚¦**: éœ€è¦è·¨å¤šä¸ªé›†ç¾¤éƒ¨ç½²ç›¸åŒåº”ç”¨ã€ç»Ÿä¸€ç®¡ç†é…ç½®æ—¶
- **ä½•æ—¶ä¸ç”¨è”é‚¦**: é›†ç¾¤å®Œå…¨ç‹¬ç«‹ã€åº”ç”¨ä¸éœ€è¦è·¨é›†ç¾¤åŒæ­¥æ—¶,ç”¨ç‹¬ç«‹ç®¡ç†æ›´ç®€å•
- **è”é‚¦å·¥å…·é€‰æ‹©**: KubeFed(å¼€æºæ ‡å‡†)ã€Karmada(åä¸ºå¼€æº)ã€Rancher Multi-Cluster(å•†ä¸š)

### 1.6 ç»Ÿä¸€ç®¡ç†å¹³é¢ (Unified Control Plane)

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**èˆªç©ºç®¡åˆ¶ä¸­å¿ƒ** â€” è™½ç„¶æ¯ä¸ªæœºåœº(é›†ç¾¤)æœ‰è‡ªå·±çš„å¡”å°(æ§åˆ¶å¹³é¢),ä½†å›½å®¶èˆªç©ºç®¡åˆ¶ä¸­å¿ƒæä¾›ç»Ÿä¸€çš„è°ƒåº¦è§†å›¾å’Œåè°ƒèƒ½åŠ›ã€‚ç»Ÿä¸€ç®¡ç†å¹³é¢è®©ä½ åœ¨ä¸€ä¸ªç•Œé¢ç®¡ç†æ‰€æœ‰é›†ç¾¤,æ— éœ€åˆ‡æ¢ä¸Šä¸‹æ–‡ã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[ç»Ÿä¸€ç®¡ç†å¹³é¢<br/>Unified Control Plane] --> B[èº«ä»½è®¤è¯å±‚<br/>SSO/RBAC]
    A --> C[èµ„æºç®¡ç†å±‚<br/>Multi-Cluster API]
    A --> D[å¯è§‚æµ‹å±‚<br/>Metrics/Logs/Traces]
    
    C --> E[AWSé›†ç¾¤]
    C --> F[Azureé›†ç¾¤]
    C --> G[GCPé›†ç¾¤]
    
    D --> H[Prometheusè”é‚¦]
    D --> I[ç»Ÿä¸€æ—¥å¿—]
    D --> J[åˆ†å¸ƒå¼è¿½è¸ª]
    
    K[è¿ç»´äººå‘˜] --> B
    B --> A
    
    style A fill:#e3f2fd
    style K fill:#fff3e0
```

**ä¸é›†ç¾¤è”é‚¦çš„åŒºåˆ«:**

| ç‰¹æ€§ | é›†ç¾¤è”é‚¦ | ç»Ÿä¸€ç®¡ç†å¹³é¢ |
|------|----------|--------------|
| **èµ„æºåŒæ­¥** | è‡ªåŠ¨åŒæ­¥èµ„æºåˆ°æˆå‘˜é›†ç¾¤ | èšåˆæŸ¥è¯¢,ä¸å¼ºåˆ¶åŒæ­¥ |
| **æ§åˆ¶æ¨¡å¼** | ä¸»åŠ¨æ¨é€ (Push) | æŒ‰éœ€æ‹‰å– (Pull) |
| **é›†ç¾¤ç‹¬ç«‹æ€§** | å—è”é‚¦æ§åˆ¶å¹³é¢å½±å“ | å®Œå…¨ç‹¬ç«‹,å¹³é¢åªè¯» |
| **é€‚ç”¨åœºæ™¯** | éœ€è¦å¼ºä¸€è‡´æ€§ã€ç»Ÿä¸€éƒ¨ç½² | éœ€è¦ç»Ÿä¸€è§†å›¾ã€æ¾è€¦åˆ |

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== Rancherç»Ÿä¸€ç®¡ç†å¹³é¢ç¤ºä¾‹ ==========
# æ³¨å†Œç®¡ç†é›†ç¾¤
apiVersion: management.cattle.io/v3
kind: Cluster
metadata:
  name: aws-production-cluster
spec:
  displayName: "AWSç”Ÿäº§ç¯å¢ƒ (us-east-1)"
  description: "AWS EKSç”Ÿäº§é›†ç¾¤"
  
  # é›†ç¾¤è¿æ¥é…ç½®
  eksConfig:
    amazonCredentialSecret: aws-credentials
    region: us-east-1
    displayName: aws-production-cluster
    
  # ç»Ÿä¸€ç›‘æ§é…ç½®
  enableClusterMonitoring: true
  clusterMonitoringInput:
    answers:
      prometheus.retention: "15d"
      grafana.persistence.enabled: "true"
      
  # ç»Ÿä¸€å‘Šè­¦é…ç½®
  enableClusterAlerting: true
  
  # ç»Ÿä¸€æ—¥å¿—é…ç½®
  enableClusterLogging: true
  clusterLoggingInput:
    elasticsearchConfig:
      endpoint: "https://elasticsearch.example.com:9200"
      indexPrefix: "aws-cluster"

---
apiVersion: management.cattle.io/v3
kind: Cluster
metadata:
  name: azure-production-cluster
spec:
  displayName: "Azureç”Ÿäº§ç¯å¢ƒ (westeurope)"
  description: "Azure AKSç”Ÿäº§é›†ç¾¤"
  
  aksConfig:
    azureCredentialSecret: azure-credentials
    resourceGroup: production-rg
    resourceLocation: westeurope
    
  enableClusterMonitoring: true
  enableClusterAlerting: true
  enableClusterLogging: true

---
# ========== ç»Ÿä¸€RBACç­–ç•¥ ==========
apiVersion: management.cattle.io/v3
kind: GlobalRole
metadata:
  name: multicloud-admin
rules:
# å¯ä»¥ç®¡ç†æ‰€æœ‰é›†ç¾¤
- apiGroups: ["management.cattle.io"]
  resources: ["clusters"]
  verbs: ["*"]
# å¯ä»¥æŸ¥çœ‹æ‰€æœ‰é¡¹ç›®
- apiGroups: ["management.cattle.io"]
  resources: ["projects"]
  verbs: ["get", "list", "watch"]

---
apiVersion: management.cattle.io/v3
kind: GlobalRoleBinding
metadata:
  name: alice-multicloud-admin
globalRoleName: multicloud-admin
userName: alice@example.com

---
# ========== è·¨é›†ç¾¤åº”ç”¨éƒ¨ç½²(éè”é‚¦æ¨¡å¼) ==========
# ä½¿ç”¨Rancher Fleetè¿›è¡ŒGitOps
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: multicloud-apps
  namespace: fleet-default
spec:
  repo: https://github.com/company/multicloud-apps.git
  branch: main
  paths:
  - apps/production
  
  # ç›®æ ‡é›†ç¾¤é€‰æ‹©å™¨
  targets:
  - name: aws-production
    clusterSelector:
      matchLabels:
        provider: aws
        env: production
        
  - name: azure-production
    clusterSelector:
      matchLabels:
        provider: azure
        env: production
        
  - name: gcp-production
    clusterSelector:
      matchLabels:
        provider: gcp
        env: production

---
# ========== ç»Ÿä¸€ç›‘æ§æŸ¥è¯¢é…ç½® ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: unified-monitoring-queries
  namespace: cattle-monitoring-system
data:
  # è·¨é›†ç¾¤CPUä½¿ç”¨ç‡æŸ¥è¯¢
  multicluster-cpu.promql: |
    avg by(cluster_name) (
      rate(node_cpu_seconds_total{mode!="idle"}[5m])
    ) * 100
    
  # è·¨é›†ç¾¤å†…å­˜ä½¿ç”¨ç‡æŸ¥è¯¢
  multicluster-memory.promql: |
    (1 - avg by(cluster_name) (
      node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes
    )) * 100
    
  # è·¨é›†ç¾¤Podæ•°é‡
  multicluster-pods.promql: |
    sum by(cluster_name) (kube_pod_info)
    
  # è·¨é›†ç¾¤åº”ç”¨é”™è¯¯ç‡
  multicluster-error-rate.promql: |
    sum by(cluster_name, app) (
      rate(http_requests_total{code=~"5.."}[5m])
    ) / 
    sum by(cluster_name, app) (
      rate(http_requests_total[5m])
    ) * 100

---
# ========== ç»Ÿä¸€å‘Šè­¦è·¯ç”± ==========
apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: multicloud-alerting
  namespace: cattle-monitoring-system
spec:
  route:
    receiver: default
    groupBy: ['cluster_name', 'alertname']
    groupWait: 30s
    groupInterval: 5m
    repeatInterval: 12h
    
    routes:
    # ç”Ÿäº§ç¯å¢ƒå‘Šè­¦ -> PagerDuty
    - match:
        severity: critical
        env: production
      receiver: pagerduty-critical
      continue: true
      
    # æ‰€æœ‰å‘Šè­¦ -> Slack
    - match:
        severity: warning|critical
      receiver: slack-alerts
      
  receivers:
  - name: default
    emailConfigs:
    - to: ops-team@example.com
      
  - name: pagerduty-critical
    pagerdutyConfigs:
    - serviceKey:
        name: pagerduty-secret
        key: service-key
      details:
        cluster: '{{ .GroupLabels.cluster_name }}'
        
  - name: slack-alerts
    slackConfigs:
    - apiURL:
        name: slack-secret
        key: webhook-url
      channel: '#k8s-alerts'
      title: '[{{ .GroupLabels.cluster_name }}] {{ .GroupLabels.alertname }}'
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: ç®¡ç†å¹³é¢èƒ½æ§åˆ¶æ‰€æœ‰é›†ç¾¤æ“ä½œ** | ç®¡ç†å¹³é¢é€šå¸¸åªæä¾›ç»Ÿä¸€è§†å›¾å’Œéƒ¨åˆ†æ“ä½œ,ä¸èƒ½å®Œå…¨æ›¿ä»£é›†ç¾¤åŸç”ŸAPI | å…³é”®æ“ä½œä½¿ç”¨åŸç”Ÿå·¥å…·,æ—¥å¸¸æŸ¥çœ‹ç”¨ç®¡ç†å¹³é¢ |
| **è¯¯åŒº2: ç®¡ç†å¹³é¢å®•æœºä¼šå½±å“é›†ç¾¤è¿è¡Œ** | ä¸ä¼š,ç®¡ç†å¹³é¢åªæ˜¯"ä»ªè¡¨ç›˜",é›†ç¾¤å®Œå…¨ç‹¬ç«‹è¿è¡Œ | ç®¡ç†å¹³é¢å¯ç”¨æ€§è¦æ±‚ç›¸å¯¹è¾ƒä½ |
| **è¯¯åŒº3: ä¸€ä¸ªç®¡ç†å¹³é¢å°±å¤Ÿäº†** | åº”è¯¥åˆ†å±‚ç®¡ç†:è¿ç»´ç®¡ç†å¹³é¢+å¼€å‘è‡ªåŠ©å¹³å°+ç›‘æ§å¹³å° | æŒ‰è§’è‰²é€‰æ‹©åˆé€‚çš„ç®¡ç†å·¥å…· |
| **è¯¯åŒº4: å…è´¹å¼€æºæ–¹æ¡ˆåŠŸèƒ½ä¸€å®šå¼±** | Rancherã€Lensç­‰å¼€æºå·¥å…·åŠŸèƒ½å¼ºå¤§,å•†ä¸šç‰ˆä¸»è¦æä¾›æ”¯æŒå’Œé«˜çº§ç‰¹æ€§ | å…ˆç”¨å¼€æºç‰ˆæœ¬éªŒè¯,æŒ‰éœ€å‡çº§å•†ä¸šç‰ˆ |

**å…³é”®å†³ç­–ç‚¹:**

| éœ€æ±‚åœºæ™¯ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
|----------|----------|------|
| **ç»Ÿä¸€éƒ¨ç½²ç›¸åŒåº”ç”¨** | é›†ç¾¤è”é‚¦ (KubeFed/Karmada) | éœ€è¦å¼ºä¸€è‡´æ€§åŒæ­¥ |
| **å¤šé›†ç¾¤ç»Ÿä¸€è§†å›¾** | ç®¡ç†å¹³é¢ (Rancher/Lens) | åªéœ€è¦æŸ¥çœ‹å’ŒåŸºç¡€æ“ä½œ |
| **GitOpså¤šé›†ç¾¤éƒ¨ç½²** | ArgoCD ApplicationSet | å£°æ˜å¼ã€ç‰ˆæœ¬æ§åˆ¶å‹å¥½ |
| **ä¸´æ—¶æŸ¥çœ‹å¤šé›†ç¾¤** | kubectl + kubectxæ’ä»¶ | è½»é‡çº§ã€æ— é¢å¤–ç»„ä»¶ |
| **ä¼ä¸šçº§å®Œæ•´æ–¹æ¡ˆ** | Rancher/OpenShift | é›†æˆRBACã€ç›‘æ§ã€æ—¥å¿—ã€å¸‚åœº |



---

## 5. ç¾å¤‡ä¸å®¹ç¾

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: å¤šäº‘æ¶æ„çš„æœ€å¤§ä¼˜åŠ¿ä¹‹ä¸€å°±æ˜¯å¤©ç„¶çš„ç¾å¤‡èƒ½åŠ›ã€‚ä½†éœ€è¦è§„åˆ’:RPO(èƒ½ä¸¢å¤šå°‘æ•°æ®)å’ŒRTO(å¤šå¿«æ¢å¤)å†³å®šäº†ç¾å¤‡æ–¹æ¡ˆçš„å¤æ‚åº¦å’Œæˆæœ¬ã€‚

### 5.1 å¤šäº‘ç¾å¤‡ç­–ç•¥

| ç¾å¤‡æ¨¡å¼ | RTOç›®æ ‡ | RPOç›®æ ‡ | å®æ–½å¤æ‚åº¦ | æˆæœ¬è€ƒè™‘ | é€‚ç”¨åœºæ™¯ |
|----------|---------|---------|------------|----------|----------|
| **çƒ­å¤‡æ¨¡å¼** | < 5åˆ†é’Ÿ | < 1åˆ†é’Ÿ | â­â­â­â­â­ | æœ€é«˜ | é‡‘èã€ç”µå•†æ ¸å¿ƒä¸šåŠ¡ |
| **æ¸©å¤‡æ¨¡å¼** | < 30åˆ†é’Ÿ | < 15åˆ†é’Ÿ | â­â­â­â­ | ä¸­é«˜ | é‡è¦ä¸šåŠ¡ç³»ç»Ÿ |
| **å†·å¤‡æ¨¡å¼** | < 4å°æ—¶ | < 1å°æ—¶ | â­â­â­ | ä¸­ä½ | ä¸€èˆ¬ä¸šåŠ¡ç³»ç»Ÿ |
| **æ··åˆæ¨¡å¼** | åˆ†å±‚è®¾å®š | åˆ†å±‚è®¾å®š | â­â­â­â­ | ä¸­ç­‰ | åˆ†å±‚ä¸šåŠ¡æ¶æ„ |
| **å¤šåœ°å¤šæ´»** | < 1åˆ†é’Ÿ | < 1åˆ†é’Ÿ | â­â­â­â­â­ | æœ€é«˜ | å…¨çƒåŒ–æœåŠ¡ |

### 5.2 ç¾å¤‡è‡ªåŠ¨åŒ–å®æ–½

```yaml
# ========== å¤šäº‘ç¾å¤‡é…ç½® ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: disaster-recovery-config
  namespace: platform-ops
data:
  dr-strategy.yaml: |
    # ç¾å¤‡ç­–ç•¥å®šä¹‰
    disaster_recovery:
      primary_region: "us-east-1"
      backup_regions:
        - "us-west-2"
        - "eu-west-1"
        - "ap-southeast-1"
      
      failover_triggers:
        - health_check_failure_count: 3
        - api_server_unreachable_duration: "5m"
        - etcd_quorum_lost: true
        - critical_service_unavailable: true
        
      failback_conditions:
        - primary_site_stable_duration: "1h"
        - data_consistency_verified: true
        - service_performance_restored: true
        
      testing_schedule: "0 2 * * 0"  # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹

---
# ========== è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œåˆ‡æ¢ ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dr-orchestrator
  namespace: platform-ops
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dr-orchestrator
  template:
    metadata:
      labels:
        app: dr-orchestrator
    spec:
      serviceAccountName: dr-operator
      containers:
      - name: orchestrator
        image: platform/dr-orchestrator:latest
        env:
        - name: PRIMARY_CLUSTER_ENDPOINT
          value: "https://primary-cluster-api.example.com"
        - name: BACKUP_CLUSTER_ENDPOINTS
          value: "https://backup1.example.com,https://backup2.example.com"
        - name: HEALTH_CHECK_INTERVAL
          value: "30s"
        - name: FAILOVER_THRESHOLD
          value: "3"
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30

---
# ========== æ•°æ®åŒæ­¥å’Œå¤‡ä»½ç­–ç•¥ ==========
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cross-cloud-backup
  namespace: platform-ops
spec:
  schedule: "0 */6 * * *"  # æ¯6å°æ—¶æ‰§è¡Œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backup-operator
          containers:
          - name: backup-runner
            image: platform/backup-tools:latest
            env:
            - name: AWS_BUCKET
              value: "company-backups-primary"
            - name: AZURE_CONTAINER
              value: "company-backups-secondary"
            - name: GCP_BUCKET
              value: "company-backups-tertiary"
            command:
            - /bin/sh
            - -c
            - |
              #!/bin/bash
              set -euo pipefail
              
              echo "å¼€å§‹è·¨äº‘å¤‡ä»½ä»»åŠ¡..."
              
              # 1. etcdå¤‡ä»½
              echo "æ‰§è¡Œetcdå¤‡ä»½..."
              etcdctl snapshot save /tmp/etcd-snapshot.db \
                --endpoints=https://etcd-client:2379 \
                --cert=/etc/etcd/ssl/etcd.pem \
                --key=/etc/etcd/ssl/etcd-key.pem \
                --cacert=/etc/etcd/ssl/ca.pem
              
              # 2. åº”ç”¨æ•°æ®å¤‡ä»½
              echo "æ‰§è¡Œåº”ç”¨æ•°æ®å¤‡ä»½..."
              velero backup create multicloud-backup-$(date +%Y%m%d-%H%M%S) \
                --include-namespaces production,staging \
                --exclude-resources events,replicasets,pods \
                --snapshot-volumes \
                --ttl 168h0m0s
              
              # 3. è·¨äº‘å­˜å‚¨åŒæ­¥
              echo "åŒæ­¥å¤‡ä»½åˆ°å„äº‘å­˜å‚¨..."
              
              # AWS S3åŒæ­¥
              aws s3 sync /backups s3://${AWS_BUCKET}/backups/$(date +%Y/%m/%d)/
              
              # Azure BlobåŒæ­¥
              az storage blob upload-batch \
                --destination ${AZURE_CONTAINER} \
                --source /backups \
                --destination-path backups/$(date +%Y/%m/%d)/
              
              # GCP StorageåŒæ­¥
              gsutil -m rsync -r /backups gs://${GCP_BUCKET}/backups/$(date +%Y/%m/%d)/
              
              # 4. éªŒè¯å¤‡ä»½å®Œæ•´æ€§
              echo "éªŒè¯å¤‡ä»½å®Œæ•´æ€§..."
              BACKUP_SIZE=$(du -sh /backups | cut -f1)
              echo "å¤‡ä»½æ€»å¤§å°: ${BACKUP_SIZE}"
              
              # 5. æ¸…ç†æ—§å¤‡ä»½
              echo "æ¸…ç†7å¤©å‰çš„æ—§å¤‡ä»½..."
              find /backups -type f -mtime +7 -delete
              
              echo "è·¨äº‘å¤‡ä»½ä»»åŠ¡å®Œæˆ"
          restartPolicy: OnFailure

---
# ========== ç¾å¤‡æ¼”ç»ƒè‡ªåŠ¨åŒ– ==========
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-drill-automation
  namespace: platform-ops
spec:
  schedule: "0 2 * * 0"  # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: dr-tester
          containers:
          - name: drill-runner
            image: platform/dr-testing:latest
            command:
            - /bin/sh
            - -c
            - |
              #!/bin/bash
              set -euo pipefail
              
              echo "å¼€å§‹ç¾å¤‡æ¼”ç»ƒ..."
              
              # 1. å‰ç½®æ£€æŸ¥
              echo "æ‰§è¡Œå‰ç½®å¥åº·æ£€æŸ¥..."
              python3 /scripts/pre-drill-check.py --validate-environment
              
              # 2. æ•…éšœæ¨¡æ‹Ÿ
              echo "æ¨¡æ‹Ÿæ•…éšœåœºæ™¯..."
              python3 /scripts/fault-injector.py --scenario network-partition --duration 300
              
              # 3. æ•…éšœæ£€æµ‹éªŒè¯
              echo "éªŒè¯æ•…éšœæ£€æµ‹æœºåˆ¶..."
              python3 /scripts/detection-verifier.py --expected-trigger-time 60
              
              # 4. è‡ªåŠ¨æ•…éšœåˆ‡æ¢
              echo "æ‰§è¡Œè‡ªåŠ¨æ•…éšœåˆ‡æ¢..."
              python3 /scripts/failover-orchestrator.py --simulate-failover
              
              # 5. æœåŠ¡æ¢å¤éªŒè¯
              echo "éªŒè¯æœåŠ¡æ¢å¤..."
              python3 /scripts/recovery-validator.py --check-service-availability
              
              # 6. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
              echo "æ‰§è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥..."
              python3 /scripts/data-consistency-check.py --compare-primary-backup
              
              # 7. æ•…éšœæ¢å¤
              echo "æ‰§è¡Œæ•…éšœæ¢å¤..."
              python3 /scripts/fault-recovery.py --restore-primary-site
              
              # 8. ç”Ÿæˆæ¼”ç»ƒæŠ¥å‘Š
              echo "ç”Ÿæˆæ¼”ç»ƒæŠ¥å‘Š..."
              python3 /scripts/drill-report-generator.py --output-format markdown
              
              echo "ç¾å¤‡æ¼”ç»ƒå®Œæˆ"
          restartPolicy: OnFailure
```

### 5.4 è·¨äº‘ç¾å¤‡

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**å¼‚åœ°å¤‡ä»½ä¿é™©ç®±** â€” é‡è¦æ–‡ä»¶ä¸èƒ½åªæ”¾å®¶é‡Œä¸€ä¸ªä¿é™©ç®±,è¦åœ¨é“¶è¡Œå†å­˜ä¸€ä»½ã€‚è·¨äº‘ç¾å¤‡å°±æ˜¯åœ¨ä¸åŒäº‘å‚å•†ä¹‹é—´å¤‡ä»½æ•°æ®å’Œé…ç½®,å³ä½¿ä¸€ä¸ªäº‘å®Œå…¨ä¸å¯ç”¨,ä¹Ÿèƒ½ä»å¦ä¸€ä¸ªäº‘æ¢å¤ã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[ä¸»é›†ç¾¤<br/>AWS us-east-1] --> B[åº”ç”¨æœåŠ¡]
    A --> C[æ•°æ®åº“]
    A --> D[å¯¹è±¡å­˜å‚¨]
    
    B -->|å®æ—¶åŒæ­¥| E[å¤‡ä»½é›†ç¾¤<br/>Azure westeurope]
    C -->|å®šæœŸå¤‡ä»½| F[Azure Blob Storage]
    D -->|è·¨äº‘å¤åˆ¶| G[GCP Cloud Storage]
    
    E --> H[å¾…å‘½çŠ¶æ€]
    F --> I[æ•°æ®å¿«ç…§]
    G --> J[ç¾å¤‡å‰¯æœ¬]
    
    K{æ•…éšœæ£€æµ‹} -->|ä¸»é›†ç¾¤å®•æœº| L[è‡ªåŠ¨åˆ‡æ¢]
    L --> E
    L --> M[DNSæ›´æ–°]
    M --> N[æµé‡è·¯ç”±åˆ°å¤‡ä»½é›†ç¾¤]
    
    style A fill:#e3f2fd
    style E fill:#fff3e0
    style K fill:#ffebee
```

**è·¨äº‘ç¾å¤‡å…³é”®æŒ‡æ ‡:**

| æŒ‡æ ‡ | å®šä¹‰ | ç›®æ ‡è®¾å®š | å®ç°æ–¹å¼ |
|------|------|----------|----------|
| **RPO** (Recovery Point Objective) | èƒ½å®¹å¿ä¸¢å¤±å¤šå°‘æ•°æ® | é‡‘è:0ç§’,ç”µå•†:5åˆ†é’Ÿ,å†…éƒ¨ç³»ç»Ÿ:1å°æ—¶ | å®æ—¶åŒæ­¥/å®šæœŸå¤‡ä»½ |
| **RTO** (Recovery Time Objective) | å¤šå¿«æ¢å¤æœåŠ¡ | å…³é”®ä¸šåŠ¡:5åˆ†é’Ÿ,ä¸€èˆ¬ä¸šåŠ¡:1å°æ—¶ | çƒ­å¤‡/æ¸©å¤‡/å†·å¤‡ |
| **æ•°æ®ä¸€è‡´æ€§** | ä¸»å¤‡æ•°æ®æ˜¯å¦ä¸€è‡´ | å¼ºä¸€è‡´æ€§/æœ€ç»ˆä¸€è‡´æ€§ | åŒæ­¥/å¼‚æ­¥å¤åˆ¶ |

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== Veleroè·¨äº‘å¤‡ä»½é…ç½® ==========
# Veleroæ˜¯CNCFå¤‡ä»½å·¥å…·,æ”¯æŒè·¨äº‘å­˜å‚¨
apiVersion: v1
kind: Namespace
metadata:
  name: velero

---
# 1. AWSé›†ç¾¤å¤‡ä»½é…ç½®
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: aws-primary
  namespace: velero
spec:
  provider: aws
  objectStorage:
    bucket: company-k8s-backups-aws
    prefix: primary-cluster
  config:
    region: us-east-1
    
---
# 2. Azureå¤‡ä»½å­˜å‚¨ä½ç½®(è·¨äº‘å¤‡ä»½)
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: azure-dr
  namespace: velero
spec:
  provider: azure
  objectStorage:
    bucket: company-k8s-backups-azure
    prefix: dr-backups
  config:
    resourceGroup: velero-backups-rg
    storageAccount: velerostorageaccount
    
---
# 3. è‡ªåŠ¨å¤‡ä»½ç­–ç•¥
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: multicloud-backup
  namespace: velero
spec:
  # æ¯6å°æ—¶å¤‡ä»½ä¸€æ¬¡
  schedule: "0 */6 * * *"
  
  template:
    # å¤‡ä»½èŒƒå›´
    includedNamespaces:
    - production
    - staging
    
    # æ’é™¤çš„èµ„æº
    excludedResources:
    - events
    - replicasets
    
    # åŒæ—¶å¤‡ä»½åˆ°ä¸¤ä¸ªäº‘
    storageLocations:
    - aws-primary
    - azure-dr
    
    # å¤‡ä»½PVå¿«ç…§
    snapshotVolumes: true
    
    # ä¿ç•™30å¤©
    ttl: 720h0m0s
    
    # å¤‡ä»½é’©å­(å¤‡ä»½å‰æ‰§è¡Œçš„å‘½ä»¤)
    hooks:
      resources:
      - name: database-backup-hook
        includedNamespaces:
        - production
        labelSelector:
          matchLabels:
            app: postgresql
        pre:
        - exec:
            container: postgres
            command:
            - /bin/bash
            - -c
            - pg_dump mydb > /backup/db.sql
            onError: Fail
            timeout: 10m

---
# 4. è·¨äº‘æ¢å¤ç¤ºä¾‹
# ä»Azureå¤‡ä»½æ¢å¤åˆ°GCPé›†ç¾¤
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: disaster-recovery-restore
  namespace: velero
spec:
  # æŒ‡å®šå¤‡ä»½åç§°
  backupName: multicloud-backup-20260210-120000
  
  # æ¢å¤åˆ°çš„å‘½åç©ºé—´æ˜ å°„
  namespaceMapping:
    production: production-restored
    
  # åªæ¢å¤ç‰¹å®šèµ„æº
  includedResources:
  - deployments
  - services
  - configmaps
  - secrets
  - persistentvolumeclaims
  
  # æ¢å¤PV
  restorePVs: true
  
  # ä¿ç•™å·²å­˜åœ¨çš„èµ„æº
  existingResourcePolicy: update
  
---
# 5. æ•°æ®åº“è·¨äº‘åŒæ­¥é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-replication-config
  namespace: production
data:
  # PostgreSQLé€»è¾‘å¤åˆ¶é…ç½®
  replication.conf: |
    # ä¸»åº“é…ç½®(AWS)
    wal_level = logical
    max_wal_senders = 10
    max_replication_slots = 10
    
    # å‘å¸ƒé…ç½®
    CREATE PUBLICATION multicloud_pub FOR ALL TABLES;
    
    # ä»åº“é…ç½®(Azure)
    # åˆ›å»ºè®¢é˜…
    CREATE SUBSCRIPTION multicloud_sub
      CONNECTION 'host=primary-db.aws port=5432 dbname=mydb user=replicator password=xxx'
      PUBLICATION multicloud_pub;
      
---
# 6. å¯¹è±¡å­˜å‚¨è·¨äº‘åŒæ­¥
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cross-cloud-storage-sync
  namespace: platform-ops
spec:
  schedule: "0 */12 * * *"  # æ¯12å°æ—¶åŒæ­¥
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: sync-tool
            image: amazon/aws-cli:latest
            command:
            - /bin/bash
            - -c
            - |
              #!/bin/bash
              set -euo pipefail
              
              echo "å¼€å§‹è·¨äº‘å­˜å‚¨åŒæ­¥..."
              
              # 1. AWS S3 -> Azure Blob
              echo "åŒæ­¥AWS S3åˆ°Azure Blob..."
              aws s3 sync s3://company-data-aws/ /tmp/data/
              az storage blob upload-batch \
                --destination company-data-azure \
                --source /tmp/data/
              
              # 2. AWS S3 -> GCP Storage
              echo "åŒæ­¥AWS S3åˆ°GCP Storage..."
              gsutil -m rsync -r s3://company-data-aws gs://company-data-gcp
              
              # 3. éªŒè¯åŒæ­¥å®Œæ•´æ€§
              echo "éªŒè¯æ•°æ®ä¸€è‡´æ€§..."
              aws_count=$(aws s3 ls s3://company-data-aws/ --recursive | wc -l)
              azure_count=$(az storage blob list -c company-data-azure --query "length(@)")
              gcp_count=$(gsutil ls gs://company-data-gcp/** | wc -l)
              
              echo "AWSå¯¹è±¡æ•°: $aws_count"
              echo "Azureå¯¹è±¡æ•°: $azure_count"
              echo "GCPå¯¹è±¡æ•°: $gcp_count"
              
              if [ "$aws_count" -eq "$azure_count" ] && [ "$aws_count" -eq "$gcp_count" ]; then
                echo "è·¨äº‘åŒæ­¥æˆåŠŸ,æ•°æ®ä¸€è‡´"
              else
                echo "è­¦å‘Š:æ•°æ®ä¸ä¸€è‡´,éœ€è¦äººå·¥æ£€æŸ¥"
                exit 1
              fi
          restartPolicy: OnFailure
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: åªå¤‡ä»½ä¸æµ‹è¯•æ¢å¤** | å¤‡ä»½å¯èƒ½å¤±è´¥,ä¸æµ‹è¯•ç­‰äºæ²¡å¤‡ä»½ | æ¯æœˆç¾å¤‡æ¼”ç»ƒ,éªŒè¯RTO/RPO |
| **è¯¯åŒº2: è·¨äº‘å¤åˆ¶å®æ—¶æ€§å‡è®¾** | è·¨äº‘ä¼ è¾“æœ‰å»¶è¿Ÿ,ä¸æ˜¯å®æ—¶åŒæ­¥ | è¯„ä¼°ç½‘ç»œå»¶è¿Ÿ,è®¾ç½®åˆç†RPO |
| **è¯¯åŒº3: å¿½ç•¥è·¨äº‘æˆæœ¬** | è·¨äº‘æ•°æ®ä¼ è¾“è´¹æ˜‚è´µ | å‹ç¼©æ•°æ®,å¢é‡å¤‡ä»½,é™åˆ¶é¢‘ç‡ |
| **è¯¯åŒº4: é…ç½®ä¸ä¸€èµ·å¤‡ä»½** | åªå¤‡ä»½æ•°æ®,ConfigMap/Secretä¸¢å¤±ä¹Ÿæ— æ³•æ¢å¤ | ä½¿ç”¨Veleroå¤‡ä»½å®Œæ•´é›†ç¾¤çŠ¶æ€ |

### 5.5 æ•…éšœåˆ‡æ¢

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**å¤‡ç”¨å‘ç”µæœº** â€” åœç”µæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å‘ç”µæœºä¾›ç”µã€‚æ•…éšœåˆ‡æ¢å°±æ˜¯ä¸»é›†ç¾¤æ•…éšœæ—¶,è‡ªåŠ¨å°†æµé‡åˆ‡æ¢åˆ°å¤‡ä»½é›†ç¾¤,ç”¨æˆ·æ— æ„ŸçŸ¥æˆ–çŸ­æš‚ä¸­æ–­ã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[ç”¨æˆ·è¯·æ±‚] --> B[Global Load Balancer<br/>AWS Route53/Cloudflare]
    
    B -->|å¥åº·æ£€æŸ¥é€šè¿‡| C[ä¸»é›†ç¾¤<br/>AWS]
    B -->|ä¸»é›†ç¾¤æ•…éšœ| D[å¤‡ä»½é›†ç¾¤<br/>Azure]
    
    E[å¥åº·æ£€æŸ¥<br/>æ¯30ç§’] --> C
    E --> D
    
    F{æ•…éšœæ£€æµ‹} -->|3æ¬¡è¿ç»­å¤±è´¥| G[è‡ªåŠ¨åˆ‡æ¢]
    G --> H[æ›´æ–°DNSè®°å½•]
    H --> I[æµé‡è·¯ç”±åˆ°å¤‡ä»½]
    
    style F fill:#ffebee
    style G fill:#fff3e0
```

**æ•…éšœåˆ‡æ¢ç­–ç•¥:**

| ç­–ç•¥ | åˆ‡æ¢æ—¶é—´ | æ•°æ®ä¸€è‡´æ€§ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|----------|------------|------|----------|
| **DNSæ•…éšœè½¬ç§»** | 1-5åˆ†é’Ÿ(TTLé™åˆ¶) | æœ€ç»ˆä¸€è‡´ | ä½ | ä¸€èˆ¬Webåº”ç”¨ |
| **Global Load Balancer** | <1åˆ†é’Ÿ | æœ€ç»ˆä¸€è‡´ | ä¸­ | å…¬ç½‘æœåŠ¡ |
| **æ•°æ®åº“ä¸»å¤‡åˆ‡æ¢** | 30ç§’-5åˆ†é’Ÿ | å¯é…ç½® | é«˜ | æœ‰çŠ¶æ€æœåŠ¡ |
| **Active-Active** | 0ç§’(æ— åˆ‡æ¢) | å¼ºä¸€è‡´(å¤æ‚) | æœ€é«˜ | é‡‘èäº¤æ˜“ |

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== AWS Route53å¥åº·æ£€æŸ¥å’Œæ•…éšœè½¬ç§» ==========
resource "aws_route53_health_check" "primary" {
  fqdn              = "app.primary.example.com"
  port              = 443
  type              = "HTTPS"
  resource_path     = "/health"
  failure_threshold = 3
  request_interval  = 30
  
  tags = {
    Name = "primary-cluster-health"
  }
}

resource "aws_route53_health_check" "secondary" {
  fqdn              = "app.secondary.example.com"
  port              = 443
  type              = "HTTPS"
  resource_path     = "/health"
  failure_threshold = 3
  request_interval  = 30
  
  tags = {
    Name = "secondary-cluster-health"
  }
}

# ä¸»è®°å½•(primary failover)
resource "aws_route53_record" "primary" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "app.example.com"
  type    = "A"
  
  failover_routing_policy {
    type = "PRIMARY"
  }
  
  set_identifier = "primary"
  health_check_id = aws_route53_health_check.primary.id
  ttl            = 60
  
  records = [aws_lb.primary.ip_address]
}

# å¤‡ç”¨è®°å½•(secondary failover)
resource "aws_route53_record" "secondary" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "app.example.com"
  type    = "A"
  
  failover_routing_policy {
    type = "SECONDARY"
  }
  
  set_identifier = "secondary"
  health_check_id = aws_route53_health_check.secondary.id
  ttl            = 60
  
  records = [azurerm_public_ip.backup.ip_address]
}

---
# ========== Kubernetesè‡ªåŠ¨æ•…éšœåˆ‡æ¢é…ç½® ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: failover-automation
  namespace: platform-ops
data:
  failover-script.sh: |
    #!/bin/bash
    set -euo pipefail
    
    # æ•…éšœåˆ‡æ¢è„šæœ¬
    PRIMARY_CLUSTER="aws-us-east-1"
    BACKUP_CLUSTER="azure-westeurope"
    HEALTH_CHECK_URL="https://app.example.com/health"
    
    check_health() {
      local cluster=$1
      local response=$(curl -s -o /dev/null -w "%{http_code}" $HEALTH_CHECK_URL)
      if [ "$response" = "200" ]; then
        return 0
      else
        return 1
      fi
    }
    
    failover() {
      echo "æ£€æµ‹åˆ°ä¸»é›†ç¾¤æ•…éšœ,å¼€å§‹æ•…éšœåˆ‡æ¢..."
      
      # 1. æ›´æ–°DNSæŒ‡å‘å¤‡ä»½é›†ç¾¤
      echo "æ›´æ–°DNSè®°å½•..."
      aws route53 change-resource-record-sets \
        --hosted-zone-id Z1234567890ABC \
        --change-batch file:///tmp/failover-dns.json
      
      # 2. æ‰©å®¹å¤‡ä»½é›†ç¾¤
      echo "æ‰©å®¹å¤‡ä»½é›†ç¾¤..."
      kubectl --context=$BACKUP_CLUSTER scale deployment -n production --all --replicas=5
      
      # 3. å‘é€å‘Šè­¦é€šçŸ¥
      echo "å‘é€æ•…éšœåˆ‡æ¢é€šçŸ¥..."
      curl -X POST https://hooks.slack.com/services/xxx \
        -d '{"text": "æ•…éšœåˆ‡æ¢å®Œæˆ: æµé‡å·²åˆ‡æ¢åˆ°å¤‡ä»½é›†ç¾¤ '$BACKUP_CLUSTER'"}'
      
      echo "æ•…éšœåˆ‡æ¢å®Œæˆ"
    }
    
    # ä¸»å¾ªç¯
    FAILURE_COUNT=0
    while true; do
      if check_health $PRIMARY_CLUSTER; then
        echo "$(date): ä¸»é›†ç¾¤å¥åº·"
        FAILURE_COUNT=0
      else
        FAILURE_COUNT=$((FAILURE_COUNT + 1))
        echo "$(date): ä¸»é›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥ ($FAILURE_COUNT/3)"
        
        if [ $FAILURE_COUNT -ge 3 ]; then
          failover
          break
        fi
      fi
      
      sleep 30
    done
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: DNS TTLè®¾ç½®è¿‡å¤§** | TTL=3600æ„å‘³ç€æœ€å¤š1å°æ—¶æ‰èƒ½åˆ‡æ¢ | å…³é”®åº”ç”¨TTLè®¾ç½®60ç§’ |
| **è¯¯åŒº2: å¤‡ä»½é›†ç¾¤å†·å¯åŠ¨** | åˆ‡æ¢åå‘ç°å¤‡ä»½é›†ç¾¤èµ„æºä¸è¶³ | ä¿æŒå¤‡ä»½é›†ç¾¤æœ€å°è§„æ¨¡è¿è¡Œ |
| **è¯¯åŒº3: åªåˆ‡æ¢åº”ç”¨ä¸åˆ‡æ¢æ•°æ®åº“** | åº”ç”¨åˆ‡æ¢åä»è¿æ¥ä¸»é›†ç¾¤æ•°æ®åº“ | æ•°æ®åº“ä¹Ÿè¦æœ‰ä¸»å¤‡åˆ‡æ¢æ–¹æ¡ˆ |
| **è¯¯åŒº4: ä¸é€šçŸ¥ç”¨æˆ·** | ç”¨æˆ·ä¸çŸ¥é“å‘ç”Ÿäº†ä»€ä¹ˆ | çŠ¶æ€é¡µé¢+é‚®ä»¶é€šçŸ¥ |

**æ•…éšœåˆ‡æ¢checklist:**
- [ ] DNS/LBå¥åº·æ£€æŸ¥é…ç½®æ­£ç¡®
- [ ] å¤‡ä»½é›†ç¾¤èµ„æºå……è¶³(è‡³å°‘50%ä¸»é›†ç¾¤è§„æ¨¡)
- [ ] æ•°æ®å·²åŒæ­¥åˆ°å¤‡ä»½é›†ç¾¤(æ£€æŸ¥RPO)
- [ ] åˆ‡æ¢è„šæœ¬ç»è¿‡æ¼”ç»ƒéªŒè¯
- [ ] ç›‘æ§å‘Šè­¦å·²é…ç½®
- [ ] å›åˆ‡æ–¹æ¡ˆå·²å‡†å¤‡(ä¸»é›†ç¾¤æ¢å¤åå¦‚ä½•åˆ‡å›)



---

## 6. å¤šäº‘æ²»ç†æ¡†æ¶

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: æ²»ç†æ¡†æ¶è§£å†³"è°èƒ½åšä»€ä¹ˆã€åœ¨å“ªåšã€æ€ä¹ˆåš"çš„é—®é¢˜ã€‚åŒ…æ‹¬ç»Ÿä¸€çš„å‘½åè§„èŒƒã€æ ‡ç­¾ç­–ç•¥ã€å®‰å…¨åŸºçº¿ã€æˆæœ¬åˆ†æ‘Šå’Œåˆè§„è¦æ±‚ã€‚

### 6.1 å¤šäº‘æ²»ç†åŸåˆ™

| æ²»ç†ç»´åº¦ | æ ¸å¿ƒåŸåˆ™ | å®æ–½è¦ç‚¹ | æ²»ç†å·¥å…· | åˆè§„è¦æ±‚ |
|----------|----------|----------|----------|----------|
| **ç»Ÿä¸€èº«ä»½** | å•ç‚¹ç™»å½•ã€ç»Ÿä¸€è®¤è¯ | SSOé›†æˆã€RBACç»Ÿä¸€ | Keycloakã€AAD | SOC2ã€ISO27001 |
| **èµ„æºç®¡ç†** | æ ‡å‡†åŒ–å‘½åã€æ ‡ç­¾æ²»ç† | å‘½åè§„èŒƒã€æˆæœ¬æ ‡ç­¾ | Terraformã€Crossplane | å†…éƒ¨æ²»ç†è¦æ±‚ |
| **å®‰å…¨åˆè§„** | ç­–ç•¥ç»Ÿä¸€ã€å®¡è®¡é›†ä¸­ | ç­–ç•¥å¼•æ“ã€åˆè§„æ‰«æ | OPAã€Falco | ç­‰ä¿ã€GDPR |
| **æˆæœ¬æ§åˆ¶** | é¢„ç®—ç®¡ç†ã€æˆæœ¬åˆ†æ‘Š | é¢„ç®—å‘Šè­¦ã€æˆæœ¬åˆ†æ | Kubecostã€CloudHealth | è´¢åŠ¡ç®¡æ§è¦æ±‚ |
| **å˜æ›´ç®¡ç†** | æµç¨‹æ ‡å‡†åŒ–ã€å®¡æ‰¹è‡ªåŠ¨åŒ– | GitOpsã€å˜æ›´çª—å£ | ArgoCDã€Spinnaker | å˜æ›´ç®¡ç†æµç¨‹ |

### 6.2 å¤šäº‘æ²»ç†å®æ–½

```yaml
# ========== å¤šäº‘æ²»ç†ç­–ç•¥ ==========
apiVersion: governance.example.com/v1
kind: MulticloudGovernancePolicy
metadata:
  name: enterprise-governance
  namespace: platform-governance
spec:
  # ç»Ÿä¸€èº«ä»½è®¤è¯
  identity_management:
    sso_provider: "keycloak"
    identity_federation:
      enabled: true
      providers:
        - name: "aws-sso"
          type: "saml"
          metadata_url: "https://sso.us-east-1.amazonaws.com/idp/metadata"
        - name: "azure-ad"
          type: "oidc"
          issuer_url: "https://login.microsoftonline.com/common/v2.0"
        - name: "gcp-identity"
          type: "oidc"
          issuer_url: "https://accounts.google.com"
    
    role_mapping:
      admin_role: "platform-admin"
      developer_role: "app-developer"
      auditor_role: "security-auditor"
      
  # èµ„æºå‘½åå’Œæ ‡ç­¾è§„èŒƒ
  resource_governance:
    naming_standards:
      cluster_pattern: "{environment}-{purpose}-{region}-{sequence}"
      namespace_pattern: "{team}-{application}-{environment}"
      resource_pattern: "{application}-{component}-{environment}"
      
    tagging_requirements:
      mandatory_tags:
        - "Environment"
        - "Team"
        - "CostCenter"
        - "Owner"
        - "ComplianceLevel"
      recommended_tags:
        - "Project"
        - "BusinessUnit"
        - "CreateDate"
        
  # å®‰å…¨ç­–ç•¥ç»Ÿä¸€
  security_policies:
    cluster_hardening:
      pod_security_standards: "restricted"
      network_policies_required: true
      image_scanning_mandatory: true
      
    data_protection:
      encryption_at_rest: true
      encryption_in_transit: true
      data_classification_required: true
      
    compliance_frameworks:
      - name: "soc2-type2"
        controls:
          - "access-logging"
          - "change-management"
          - "incident-response"
      - name: "iso27001"
        controls:
          - "asset-management"
          - "access-control"
          - "cryptography"

---
# ========== æ²»ç†ç›‘æ§å’ŒæŠ¥å‘Š ==========
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: governance-compliance-rules
  namespace: platform-governance
spec:
  groups:
  - name: governance.compliance.rules
    rules:
    # èµ„æºæ ‡ç­¾åˆè§„æ£€æŸ¥
    - alert: MissingMandatoryTags
      expr: |
        count by(namespace, resource) (
          kube_resource_labels{label_environment="", label_team="", label_costcenter=""}
        ) > 0
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "èµ„æºç¼ºå°‘å¼ºåˆ¶æ ‡ç­¾"
        description: "æ£€æµ‹åˆ°èµ„æºç¼ºå°‘Environmentã€Teamæˆ–CostCenteræ ‡ç­¾"
        
    # å®‰å…¨ç­–ç•¥åˆè§„æ£€æŸ¥
    - alert: SecurityPolicyViolation
      expr: |
        count(opa_policy_violations_total{severity="high"}) > 0
      for: 30m
      labels:
        severity: critical
      annotations:
        summary: "å®‰å…¨ç­–ç•¥è¿è§„"
        description: "æ£€æµ‹åˆ°é«˜ä¸¥é‡æ€§å®‰å…¨ç­–ç•¥è¿è§„"
        
    # æˆæœ¬æ²»ç†æ£€æŸ¥
    - alert: BudgetGovernanceViolation
      expr: |
        sum by(team) (rate(cloud_cost_hourly_total[1h])) > 
        on(team) group_left budget_hourly_limit
      for: 1h
      labels:
        severity: critical
      annotations:
        summary: "é¢„ç®—æ²»ç†è¿è§„"
        description: "å›¢é˜Ÿ {{ $labels.team }} çš„å°æ—¶æˆæœ¬è¶…å‡ºé¢„ç®—é™åˆ¶"

---
# ========== æ²»ç†ä»ªè¡¨æ¿ ==========
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: multicloud-governance-dashboard
  namespace: platform-governance
spec:
  json: |
    {
      "dashboard": {
        "title": "å¤šäº‘æ²»ç†ä»ªè¡¨æ¿",
        "panels": [
          {
            "title": "æ²»ç†åˆè§„çŠ¶æ€",
            "type": "stat",
            "targets": [
              {"expr": "governance_compliance_score", "legendFormat": "æ•´ä½“åˆè§„è¯„åˆ†"},
              {"expr": "count(governance_policy_violations_total)", "legendFormat": "è¿è§„é¡¹æ•°é‡"}
            ]
          },
          {
            "title": "å„ç»´åº¦åˆè§„ç‡",
            "type": "barchart",
            "targets": [
              {"expr": "governance_dimension_compliance{dimension=\"identity\"}", "legendFormat": "èº«ä»½æ²»ç†"},
              {"expr": "governance_dimension_compliance{dimension=\"security\"}", "legendFormat": "å®‰å…¨æ²»ç†"},
              {"expr": "governance_dimension_compliance{dimension=\"cost\"}", "legendFormat": "æˆæœ¬æ²»ç†"},
              {"expr": "governance_dimension_compliance{dimension=\"resource\"}", "legendFormat": "èµ„æºæ²»ç†"}
            ]
          },
          {
            "title": "è¿è§„è¶‹åŠ¿åˆ†æ",
            "type": "graph",
            "targets": [
              {"expr": "increase(governance_policy_violations_total[1h])", "legendFormat": "æ¯å°æ—¶æ–°å¢è¿è§„"}
            ]
          },
          {
            "title": "æ²»ç†æˆæœ¬åˆ†æ",
            "type": "table",
            "targets": [
              {
                "expr": "sum by(team) (cloud_cost_monthly_total)",
                "format": "table"
              }
            ]
          }
        ]
      }
    }
```

### 2.4 è·¨äº‘æˆæœ¬å¯¹æ¯”

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**æ¯”ä»·è´­ç‰©** â€” å°±åƒä¹°åŒä¸€å•†å“è¦åœ¨ä¸åŒè¶…å¸‚æ¯”ä»·,äº‘èµ„æºåœ¨ä¸åŒäº‘å‚å•†çš„ä»·æ ¼å’Œè®¡è´¹æ¨¡å¼å·®å¼‚å¾ˆå¤§ã€‚æ‡‚å¾—å¦‚ä½•å¯¹æ¯”,æ‰èƒ½åšå‡ºæœ€ä¼˜å†³ç­–ã€‚å…³é”®ä¸æ˜¯çœ‹æ ‡ä»·,è€Œæ˜¯çœ‹**æ€»æ‹¥æœ‰æˆæœ¬(TCO)**ã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph LR
    A[æˆæœ¬å¯¹æ¯”ç»´åº¦] --> B[è®¡ç®—æˆæœ¬]
    A --> C[å­˜å‚¨æˆæœ¬]
    A --> D[ç½‘ç»œæˆæœ¬]
    A --> E[æ‰˜ç®¡æœåŠ¡æˆæœ¬]
    
    B --> B1[æŒ‰éœ€å®ä¾‹]
    B --> B2[é¢„ç•™å®ä¾‹]
    B --> B3[Spot/æŠ¢å å¼]
    
    C --> C1[å—å­˜å‚¨]
    C --> C2[å¯¹è±¡å­˜å‚¨]
    C --> C3[å½’æ¡£å­˜å‚¨]
    
    D --> D1[è·¨åŒºåŸŸä¼ è¾“]
    D --> D2[å…¬ç½‘å‡ºç«™]
    D --> D3[è´Ÿè½½å‡è¡¡]
    
    E --> E1[K8sæ‰˜ç®¡è´¹]
    E --> E2[æ•°æ®åº“æ‰˜ç®¡]
    E --> E3[ç›‘æ§æ—¥å¿—]
    
    style A fill:#e3f2fd
```

**æˆæœ¬å¯¹æ¯”å…³é”®åŸåˆ™:**
1. **ä¸æ­¢çœ‹å•ä»·,ç®—æ€»æˆæœ¬**: åŒ…æ‹¬æ•°æ®ä¼ è¾“ã€å­˜å‚¨ã€å¤‡ä»½ã€ç›‘æ§ç­‰éšè—æˆæœ¬
2. **è€ƒè™‘æŠ˜æ‰£ç­–ç•¥**: é¢„ç•™å®ä¾‹ã€æ‰¿è¯ºä½¿ç”¨æŠ˜æ‰£(CUD)ã€ä¼ä¸šåè®®(EA)
3. **è®¡ç®—åŒºåŸŸå·®å¼‚**: åŒä¸€æœåŠ¡åœ¨ä¸åŒåŒºåŸŸä»·æ ¼å¯èƒ½å·®30%+
4. **è¯„ä¼°æ‰˜ç®¡æœåŠ¡æº¢ä»·**: æ‰˜ç®¡Kubernetesæ¯”è‡ªå»ºè´µ,ä½†çœè¿ç»´äººåŠ›

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== è·¨äº‘æˆæœ¬å¯¹æ¯”é…ç½® ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: multicloud-cost-comparison
  namespace: platform-ops
data:
  # è®¡ç®—å®ä¾‹æˆæœ¬å¯¹æ¯”(2024å¹´å‚è€ƒä»·æ ¼,å•ä½:USD/æœˆ)
  compute-cost-comparison.yaml: |
    # 4vCPU 16GBå†…å­˜å®ä¾‹å¯¹æ¯”
    instance_comparison:
      aws:
        instance_type: "m5.xlarge"
        on_demand_monthly: 140.16  # $0.192/å°æ—¶ * 730å°æ—¶
        reserved_1yr_monthly: 84.67  # 40%æŠ˜æ‰£
        reserved_3yr_monthly: 56.57  # 60%æŠ˜æ‰£
        spot_monthly: 42.05  # 70%æŠ˜æ‰£ (ä»·æ ¼æ³¢åŠ¨)
        
      azure:
        instance_type: "Standard_D4s_v3"
        on_demand_monthly: 131.40  # $0.18/å°æ—¶ * 730å°æ—¶
        reserved_1yr_monthly: 84.67  # 35%æŠ˜æ‰£
        reserved_3yr_monthly: 52.56  # 60%æŠ˜æ‰£
        spot_monthly: 39.42  # 70%æŠ˜æ‰£
        
      gcp:
        instance_type: "n2-standard-4"
        on_demand_monthly: 121.17  # $0.166/å°æ—¶ * 730å°æ—¶
        committed_1yr_monthly: 84.82  # 30%æŠ˜æ‰£
        committed_3yr_monthly: 66.65  # 45%æŠ˜æ‰£
        preemptible_monthly: 36.35  # 70%æŠ˜æ‰£
        
    # å…³é”®è§‚å¯Ÿ:
    # 1. GCPæŒ‰éœ€æœ€ä¾¿å®œ,ä½†é¢„ç•™æŠ˜æ‰£åŠ›åº¦æœ€å°
    # 2. AWSå’ŒAzureé¢„ç•™æŠ˜æ‰£æ›´æ¿€è¿›
    # 3. Spot/Preemptibleä»·æ ¼ç›¸è¿‘,ä½†å¯ç”¨æ€§ä¸åŒ
    
  # å­˜å‚¨æˆæœ¬å¯¹æ¯”
  storage-cost-comparison.yaml: |
    # 1TB SSDå—å­˜å‚¨ (ç±»ä¼¼EBS gp3/Azure Premium SSD/GCP PD-SSD)
    block_storage_1tb:
      aws_ebs_gp3: 80.00  # $0.08/GB/æœˆ
      azure_premium_ssd_p30: 135.17  # å›ºå®š512GBå®¹é‡å®šä»·
      gcp_pd_ssd: 170.00  # $0.17/GB/æœˆ
      
    # 1TBå¯¹è±¡å­˜å‚¨(æ ‡å‡†å±‚)
    object_storage_1tb_standard:
      aws_s3_standard: 23.55  # $0.023/GB/æœˆ
      azure_blob_hot: 18.40  # $0.018/GB/æœˆ
      gcp_storage_standard: 20.48  # $0.020/GB/æœˆ
      
    # 100TBå¯¹è±¡å­˜å‚¨(å½’æ¡£å±‚)
    object_storage_100tb_archive:
      aws_s3_glacier_deep_archive: 102.40  # $0.00099/GB/æœˆ
      azure_blob_archive: 204.80  # $0.00199/GB/æœˆ
      gcp_archive: 122.88  # $0.0012/GB/æœˆ
      
    # å…³é”®è§‚å¯Ÿ:
    # 1. å—å­˜å‚¨: AWSæœ€ä¾¿å®œ,GCPæœ€è´µ(å·®2å€)
    # 2. å¯¹è±¡å­˜å‚¨æ ‡å‡†å±‚: Azureç•¥ä¾¿å®œ
    # 3. å½’æ¡£å­˜å‚¨: AWSæœ€ä¾¿å®œ,ä½†æ¢å¤è´¹ç”¨éœ€è€ƒè™‘
    
  # ç½‘ç»œæˆæœ¬å¯¹æ¯”
  network-cost-comparison.yaml: |
    # æ•°æ®ä¼ å‡ºè´¹ç”¨(å…¬ç½‘å‡ºç«™, USD/GB)
    egress_pricing:
      # å‰10TB/æœˆçš„ä»·æ ¼
      aws_first_10tb: 0.09
      azure_first_10tb: 0.087
      gcp_first_10tb: 0.12  # GCPå‡ºç«™æœ€è´µ!
      
      # 150TB+/æœˆçš„ä»·æ ¼
      aws_over_150tb: 0.05
      azure_over_150tb: 0.05
      gcp_over_150tb: 0.08
      
    # è·¨åŒºåŸŸä¼ è¾“(åŒäº‘ä¸åŒåŒºåŸŸ)
    inter_region_transfer:
      aws_cross_region: 0.02  # ç¾å›½åŒºåŸŸé—´
      azure_cross_region: 0.02
      gcp_cross_region: 0.01  # GCPåŒºåŸŸé—´æœ€ä¾¿å®œ
      
    # K8sæ‰˜ç®¡è´¹ç”¨
    managed_k8s_control_plane:
      aws_eks: 73.00  # $0.10/å°æ—¶ * 730å°æ—¶
      azure_aks: 0.00  # å…è´¹! åªä»˜èŠ‚ç‚¹è´¹
      gcp_gke: 73.00  # $0.10/å°æ—¶ * 730å°æ—¶
      gcp_gke_autopilot: 0.00  # Autopilotæ¨¡å¼å…è´¹æ§åˆ¶å¹³é¢
      
    # å…³é”®è§‚å¯Ÿ:
    # 1. GCPå…¬ç½‘å‡ºç«™è´¹æœ€è´µ,æ•°æ®å¯†é›†å‹åº”ç”¨éœ€æ³¨æ„
    # 2. Azure AKSæ§åˆ¶å¹³é¢å…è´¹æ˜¯ä¼˜åŠ¿
    # 3. è·¨åŒºåŸŸä¼ è¾“æˆæœ¬éœ€è€ƒè™‘å¤šåŒºåŸŸæ¶æ„

---
# ========== è‡ªåŠ¨åŒ–æˆæœ¬å¯¹æ¯”å·¥å…· ==========
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cost-comparison-analyzer
  namespace: platform-ops
spec:
  schedule: "0 0 * * 0"  # æ¯å‘¨æ—¥è¿è¡Œ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: cost-analyzer
          containers:
          - name: analyzer
            image: platform/cost-comparison:latest
            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key-id
            - name: AZURE_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: azure-credentials
                  key: client-id
            - name: GCP_PROJECT_ID
              value: "my-gcp-project"
            command:
            - /bin/sh
            - -c
            - |
              #!/bin/bash
              set -euo pipefail
              
              echo "å¼€å§‹å¤šäº‘æˆæœ¬å¯¹æ¯”åˆ†æ..."
              
              # 1. è·å–å½“å‰å·¥ä½œè´Ÿè½½èµ„æºä½¿ç”¨æƒ…å†µ
              echo "åˆ†æå½“å‰èµ„æºä½¿ç”¨..."
              python3 /scripts/resource-profiler.py \
                --output /tmp/current-usage.json
              
              # 2. è°ƒç”¨å„äº‘å®šä»·APIè¿›è¡Œå¯¹æ¯”
              echo "æŸ¥è¯¢å„äº‘å®šä»·..."
              python3 /scripts/pricing-fetcher.py \
                --usage-profile /tmp/current-usage.json \
                --clouds aws,azure,gcp \
                --output /tmp/pricing-comparison.json
              
              # 3. è®¡ç®—TCO(æ€»æ‹¥æœ‰æˆæœ¬)
              echo "è®¡ç®—æ€»æ‹¥æœ‰æˆæœ¬..."
              python3 /scripts/tco-calculator.py \
                --pricing-data /tmp/pricing-comparison.json \
                --include-hidden-costs \
                --output /tmp/tco-report.json
              
              # 4. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
              echo "ç”Ÿæˆæˆæœ¬å¯¹æ¯”æŠ¥å‘Š..."
              python3 /scripts/report-generator.py \
                --data /tmp/tco-report.json \
                --format html \
                --output /reports/cost-comparison-$(date +%Y%m%d).html
              
              # 5. è¯†åˆ«æˆæœ¬ä¼˜åŒ–æœºä¼š
              echo "è¯†åˆ«ä¼˜åŒ–æœºä¼š..."
              python3 /scripts/optimization-recommender.py \
                --current-cloud aws \
                --comparison-data /tmp/tco-report.json \
                --output /tmp/recommendations.txt
              
              cat /tmp/recommendations.txt
              
              echo "æˆæœ¬å¯¹æ¯”åˆ†æå®Œæˆ"
          restartPolicy: OnFailure
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: åªæ¯”è¾ƒè™šæ‹Ÿæœºå•ä»·** | å®é™…æˆæœ¬åŒ…æ‹¬ç½‘ç»œã€å­˜å‚¨ã€ç›‘æ§ã€æ•°æ®ä¼ è¾“ç­‰å¤šé¡¹è´¹ç”¨ | ä½¿ç”¨TCOè®¡ç®—å™¨è®¡ç®—å…¨æ ˆæˆæœ¬ |
| **è¯¯åŒº2: æœ€ä¾¿å®œçš„äº‘å°±æ˜¯æœ€ä¼˜é€‰æ‹©** | éœ€è¦è€ƒè™‘è¿ç»´æˆæœ¬ã€å›¢é˜Ÿç†Ÿæ‚‰åº¦ã€ç”Ÿæ€é›†æˆç­‰å› ç´  | ç»¼åˆè¯„ä¼°æŠ€æœ¯æˆæœ¬+äººåŠ›æˆæœ¬ |
| **è¯¯åŒº3: å®šä»·æ˜¯å›ºå®šçš„** | äº‘å‚å•†å®šä»·ç»å¸¸è°ƒæ•´,æœ‰ä¼ä¸šåè®®(EA)å¯è°ˆåˆ¤ | å®šæœŸå®¡æŸ¥,åˆ©ç”¨æŠ˜æ‰£è®¡åˆ’ |
| **è¯¯åŒº4: Spot/Preemptibleæ€»æ˜¯åˆ’ç®—** | é¢‘ç¹ä¸­æ–­å¯¼è‡´çš„ä»»åŠ¡é‡è¯•å¯èƒ½æŠµæ¶ˆæˆæœ¬èŠ‚çœ | åªç”¨äºå®¹é”™æ€§é«˜çš„æ‰¹å¤„ç†ä»»åŠ¡ |
| **è¯¯åŒº5: å¿½ç•¥éšè—æˆæœ¬** | æ•°æ®ä¼ è¾“è´¹ã€APIè°ƒç”¨è´¹ã€æ—¥å¿—å­˜å‚¨è´¹å¯èƒ½è¶…è¿‡è®¡ç®—æˆæœ¬ | ç›‘æ§æ‰€æœ‰æˆæœ¬ç»´åº¦,è®¾ç½®é¢„ç®—å‘Šè­¦ |

**å®æˆ˜å†³ç­–æŒ‡å—:**

| åœºæ™¯ | æˆæœ¬è€ƒé‡ | æ¨èæ–¹æ¡ˆ |
|------|----------|----------|
| **è®¡ç®—å¯†é›†å‹åº”ç”¨** | CPU/å†…å­˜ä»·æ ¼,é¢„ç•™å®ä¾‹æŠ˜æ‰£ | AWS/Azureé¢„ç•™å®ä¾‹,GCPæ‰¿è¯ºä½¿ç”¨æŠ˜æ‰£ |
| **å­˜å‚¨å¯†é›†å‹åº”ç”¨** | å­˜å‚¨å•ä»·,æ•°æ®ä¼ è¾“è´¹ | AWS S3æ€§ä»·æ¯”é«˜,æ³¨æ„å‡ºç«™è´¹ |
| **æ•°æ®å‡ºç«™å¤§** | å…¬ç½‘æµé‡è´¹ç”¨ | é¿å…GCP,æˆ–ä½¿ç”¨CDNé™ä½ç›´æ¥å‡ºç«™ |
| **K8sæ‰˜ç®¡** | æ§åˆ¶å¹³é¢è´¹ç”¨,èŠ‚ç‚¹æŠ˜æ‰£ | Azure AKS(å…è´¹æ§åˆ¶å¹³é¢)æˆ–GCP Autopilot |
| **æ‰¹å¤„ç†ä»»åŠ¡** | Spotå®ä¾‹å¯ç”¨æ€§å’Œä»·æ ¼ | AWS Spot Fleet(å¯ç”¨æ€§æœ€ç¨³å®š) |
| **å¤šåŒºåŸŸéƒ¨ç½²** | è·¨åŒºåŸŸä¼ è¾“è´¹ | GCPåŒºåŸŸé—´ä¼ è¾“ä¾¿å®œ,é€‚åˆå…¨çƒéƒ¨ç½² |

### 2.5 èµ„æºå³sizing (Right Sizing)

> **ğŸ”° åˆå­¦è€…ç†è§£**: ç±»æ¯”**æŒ‰éœ€é€‰è¡£æœå°ºç ** â€” ç©¿è¿‡å¤§çš„è¡£æœæµªè´¹å¸ƒæ–™,è¿‡å°çš„è¡£æœä¸èˆ’é€‚ã€‚äº‘èµ„æºä¹Ÿä¸€æ ·,é…ç½®è¿‡é«˜æµªè´¹æˆæœ¬,è¿‡ä½å½±å“æ€§èƒ½ã€‚Right Sizingå°±æ˜¯æ‰¾åˆ°"åˆšåˆšå¥½"çš„é…ç½®ã€‚

#### ğŸ”§ å·¥ä½œåŸç†

```mermaid
graph TB
    A[èµ„æºç›‘æ§] --> B[æ”¶é›†ä½¿ç”¨æ•°æ®<br/>CPU/å†…å­˜/ç½‘ç»œ/ç£ç›˜]
    B --> C[åˆ†æå†å²è¶‹åŠ¿<br/>å³°å€¼/å¹³å‡/å­£èŠ‚æ€§]
    C --> D{èµ„æºåˆ©ç”¨ç‡è¯„ä¼°}
    
    D -->|åˆ©ç”¨ç‡<20%| E[è¿‡åº¦é…ç½®<br/>Over-Provisioned]
    D -->|20-70%| F[é…ç½®åˆç†<br/>Well-Sized]
    D -->|>80%| G[é…ç½®ä¸è¶³<br/>Under-Provisioned]
    
    E --> H[é™ä½é…ç½®<br/>downsize]
    F --> I[ä¿æŒç°çŠ¶]
    G --> J[å¢åŠ é…ç½®<br/>upsize]
    
    H --> K[éªŒè¯æ€§èƒ½]
    J --> K
    K --> L[æŒç»­ç›‘æ§]
    
    style E fill:#ffebee
    style F fill:#e8f5e9
    style G fill:#fff3e0
```

**Right Sizingæ ¸å¿ƒæŒ‡æ ‡:**

| æŒ‡æ ‡ | å¥åº·èŒƒå›´ | è¿‡åº¦é…ç½® | é…ç½®ä¸è¶³ | ä¼˜åŒ–åŠ¨ä½œ |
|------|----------|----------|----------|----------|
| **CPUåˆ©ç”¨ç‡** | 40-70% | <20% | >80% | è°ƒæ•´request/limit |
| **å†…å­˜åˆ©ç”¨ç‡** | 50-80% | <30% | >90% | è°ƒæ•´å†…å­˜é…é¢ |
| **ç£ç›˜I/Oåˆ©ç”¨ç‡** | 30-60% | <15% | >75% | æ›´æ¢å­˜å‚¨ç±»å‹ |
| **ç½‘ç»œå¸¦å®½** | 30-60% | <10% | >70% | è°ƒæ•´å®ä¾‹ç±»å‹ |

#### ğŸ“ æœ€å°ç¤ºä¾‹

```yaml
# ========== Right Sizingåˆ†æå·¥å…·é…ç½® ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: rightsizing-config
  namespace: platform-ops
data:
  # Right Sizingç­–ç•¥å®šä¹‰
  sizing-policy.yaml: |
    # èµ„æºåˆ†æå‘¨æœŸ
    analysis_period:
      lookback_days: 30  # åˆ†ææœ€è¿‘30å¤©æ•°æ®
      peak_percentile: 95  # åŸºäºP95å³°å€¼sizing
      
    # èµ„æºåˆ©ç”¨ç‡é˜ˆå€¼
    utilization_thresholds:
      cpu:
        over_provisioned: 20  # CPUå¹³å‡åˆ©ç”¨ç‡<20%ä¸ºè¿‡åº¦é…ç½®
        under_provisioned: 80  # CPUå³°å€¼>80%ä¸ºé…ç½®ä¸è¶³
        target_range: [40, 70]  # ç›®æ ‡èŒƒå›´
        
      memory:
        over_provisioned: 30
        under_provisioned: 85
        target_range: [50, 80]
        
      storage:
        over_provisioned: 40
        under_provisioned: 85
        target_range: [50, 75]
        
    # è°ƒæ•´ç­–ç•¥
    adjustment_strategy:
      # ä¿å®ˆæ¨¡å¼:å°æ­¥è°ƒæ•´,é€æ­¥éªŒè¯
      mode: "conservative"  # å¯é€‰: aggressive, conservative
      
      # æ¯æ¬¡è°ƒæ•´å¹…åº¦
      step_size:
        cpu: 25  # æ¯æ¬¡è°ƒæ•´25% (å¦‚2æ ¸->1.5æ ¸)
        memory: 20  # æ¯æ¬¡è°ƒæ•´20%
        
      # å®‰å…¨é˜ˆå€¼
      min_cpu_cores: 0.25  # æœ€å°ä¸ä½äº250m
      min_memory_mb: 128  # æœ€å°ä¸ä½äº128MB
      
      # æ’é™¤è§„åˆ™
      exclusions:
        # ç”Ÿäº§å…³é”®åº”ç”¨ä¸è‡ªåŠ¨è°ƒæ•´
        - namespace: "production"
          labels:
            tier: "critical"
        # æœ‰çŠ¶æ€åº”ç”¨è°¨æ…è°ƒæ•´
        - kind: "StatefulSet"
          
    # æˆæœ¬ä¼°ç®—
    cost_calculation:
      # å„äº‘CPU/å†…å­˜å•ä»·(USD/æœˆ)
      aws:
        cpu_per_core: 20.00
        memory_per_gb: 5.00
      azure:
        cpu_per_core: 18.50
        memory_per_gb: 4.80
      gcp:
        cpu_per_core: 17.00
        memory_per_gb: 4.50

---
# ========== èµ„æºä½¿ç”¨ç›‘æ§æŸ¥è¯¢ ==========
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: rightsizing-queries
  namespace: platform-ops
spec:
  groups:
  - name: rightsizing.metrics
    interval: 5m
    rules:
    # å®¹å™¨CPUåˆ©ç”¨ç‡
    - record: container:cpu_usage_ratio:30d_avg
      expr: |
        avg_over_time(
          (rate(container_cpu_usage_seconds_total{container!=""}[5m])
          / 
          on(container, pod, namespace) group_left
          kube_pod_container_resource_requests{resource="cpu"})[30d:5m]
        )
      
    # å®¹å™¨å†…å­˜åˆ©ç”¨ç‡
    - record: container:memory_usage_ratio:30d_avg
      expr: |
        avg_over_time(
          (container_memory_working_set_bytes{container!=""}
          / 
          on(container, pod, namespace) group_left
          kube_pod_container_resource_requests{resource="memory"})[30d:5m]
        )
      
    # è¿‡åº¦é…ç½®æ£€æµ‹(CPUé•¿æœŸä½åˆ©ç”¨)
    - record: container:cpu_overprovisioned:30d
      expr: |
        container:cpu_usage_ratio:30d_avg < 0.2
      
    # é…ç½®ä¸è¶³æ£€æµ‹(å†…å­˜æ¥è¿‘é™åˆ¶)
    - record: container:memory_underprovisioned:30d
      expr: |
        container:memory_usage_ratio:30d_avg > 0.85

---
# ========== Right Sizingè‡ªåŠ¨åŒ–ä»»åŠ¡ ==========
apiVersion: batch/v1
kind: CronJob
metadata:
  name: rightsizing-analyzer
  namespace: platform-ops
spec:
  schedule: "0 2 * * 1"  # æ¯å‘¨ä¸€å‡Œæ™¨2ç‚¹åˆ†æ
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: rightsizing-operator
          containers:
          - name: analyzer
            image: platform/rightsizing-tool:latest
            command:
            - /bin/sh
            - -c
            - |
              #!/bin/bash
              set -euo pipefail
              
              echo "å¼€å§‹Right Sizingåˆ†æ..."
              
              # 1. ä»PrometheusæŸ¥è¯¢å†å²æ•°æ®
              echo "æŸ¥è¯¢èµ„æºä½¿ç”¨å†å²æ•°æ®..."
              python3 /scripts/metrics-fetcher.py \
                --prometheus-url http://prometheus:9090 \
                --lookback-days 30 \
                --output /tmp/usage-data.json
              
              # 2. åˆ†æè¿‡åº¦é…ç½®çš„å·¥ä½œè´Ÿè½½
              echo "åˆ†æè¿‡åº¦é…ç½®çš„èµ„æº..."
              python3 /scripts/overprovisioned-analyzer.py \
                --usage-data /tmp/usage-data.json \
                --threshold-cpu 0.20 \
                --threshold-memory 0.30 \
                --output /tmp/overprovisioned.json
              
              # 3. åˆ†æé…ç½®ä¸è¶³çš„å·¥ä½œè´Ÿè½½
              echo "åˆ†æé…ç½®ä¸è¶³çš„èµ„æº..."
              python3 /scripts/underprovisioned-analyzer.py \
                --usage-data /tmp/usage-data.json \
                --threshold-cpu 0.80 \
                --threshold-memory 0.85 \
                --output /tmp/underprovisioned.json
              
              # 4. ç”Ÿæˆè°ƒæ•´å»ºè®®
              echo "ç”ŸæˆRight Sizingå»ºè®®..."
              python3 /scripts/recommendations-generator.py \
                --overprovisioned /tmp/overprovisioned.json \
                --underprovisioned /tmp/underprovisioned.json \
                --policy /config/sizing-policy.yaml \
                --output /tmp/recommendations.yaml
              
              # 5. è®¡ç®—æˆæœ¬èŠ‚çœé¢„ä¼°
              echo "è®¡ç®—æ½œåœ¨æˆæœ¬èŠ‚çœ..."
              python3 /scripts/savings-calculator.py \
                --recommendations /tmp/recommendations.yaml \
                --pricing-config /config/sizing-policy.yaml \
                --output /tmp/savings-report.txt
              
              cat /tmp/savings-report.txt
              
              # 6. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
              echo "ç”ŸæˆRight SizingæŠ¥å‘Š..."
              REPORT_DATE=$(date +%Y%m%d)
              python3 /scripts/report-generator.py \
                --recommendations /tmp/recommendations.yaml \
                --savings /tmp/savings-report.txt \
                --format html \
                --output /reports/rightsizing-report-${REPORT_DATE}.html
              
              # 7. å‘é€é€šçŸ¥
              echo "å‘é€ä¼˜åŒ–å»ºè®®é€šçŸ¥..."
              curl -X POST \
                -H "Content-Type: application/json" \
                -d "{\"text\": \"æœ¬å‘¨Right Sizingåˆ†æå®Œæˆ,é¢„è®¡å¯èŠ‚çœæˆæœ¬: $(cat /tmp/savings-report.txt | grep 'Total Savings' | awk '{print $3}')\"}" \
                ${SLACK_WEBHOOK_URL}
              
              echo "Right Sizingåˆ†æå®Œæˆ"
          restartPolicy: OnFailure

---
# ========== Right Sizingå»ºè®®ç¤ºä¾‹ ==========
# è‡ªåŠ¨ç”Ÿæˆçš„è°ƒæ•´å»ºè®®æ ¼å¼
apiVersion: v1
kind: ConfigMap
metadata:
  name: rightsizing-recommendations-example
  namespace: platform-ops
data:
  recommendations.yaml: |
    # Right Sizingå»ºè®®æŠ¥å‘Š
    generated_at: "2026-02-10T02:00:00Z"
    analysis_period: "2026-01-11 to 2026-02-10"
    
    # è¿‡åº¦é…ç½®å·¥ä½œè´Ÿè½½
    overprovisioned:
    - namespace: development
      workload: deployment/frontend-dev
      current_resources:
        cpu_request: "2000m"
        memory_request: "4Gi"
      average_usage:
        cpu: "250m"  # 12.5%åˆ©ç”¨ç‡
        memory: "800Mi"  # 20%åˆ©ç”¨ç‡
      recommended_resources:
        cpu_request: "500m"
        memory_request: "1Gi"
      estimated_monthly_savings: "$45.00"
      confidence: "high"  # æ•°æ®å……åˆ†,å»ºè®®å¯é 
      
    - namespace: staging
      workload: deployment/api-staging
      current_resources:
        cpu_request: "4000m"
        memory_request: "8Gi"
      average_usage:
        cpu: "600m"  # 15%åˆ©ç”¨ç‡
        memory: "1.5Gi"  # 19%åˆ©ç”¨ç‡
      recommended_resources:
        cpu_request: "1000m"
        memory_request: "2Gi"
      estimated_monthly_savings: "$120.00"
      confidence: "high"
      
    # é…ç½®ä¸è¶³å·¥ä½œè´Ÿè½½
    underprovisioned:
    - namespace: production
      workload: deployment/api-prod
      current_resources:
        cpu_request: "1000m"
        memory_request: "2Gi"
      peak_usage:
        cpu: "950m"  # 95%åˆ©ç”¨ç‡
        memory: "1.8Gi"  # 90%åˆ©ç”¨ç‡
      recommended_resources:
        cpu_request: "1500m"
        memory_request: "3Gi"
      risk: "medium"  # å¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜
      confidence: "high"
      
    # æ€»è®¡
    summary:
      total_workloads_analyzed: 127
      overprovisioned_count: 34
      underprovisioned_count: 8
      well_sized_count: 85
      estimated_monthly_savings: "$1,250.00"
      estimated_annual_savings: "$15,000.00"
```

#### âš ï¸ å¸¸è§è¯¯åŒº

| è¯¯åŒº | çœŸç›¸ | æ¨èåšæ³• |
|------|------|----------|
| **è¯¯åŒº1: åŸºäºå¹³å‡å€¼sizing** | åº”è¯¥åŸºäºP95æˆ–P99å³°å€¼,å¦åˆ™é«˜å³°æœŸä¼šæ€§èƒ½ä¸è¶³ | ä½¿ç”¨95thç™¾åˆ†ä½å€¼+20%ä½™é‡ |
| **è¯¯åŒº2: ä¸€æ¬¡æ€§å¤§å¹…è°ƒæ•´** | å¤§å¹…é™é…å¯èƒ½å¯¼è‡´æœåŠ¡ä¸ç¨³å®š | é€æ­¥è°ƒæ•´,æ¯æ¬¡20-30%,è§‚å¯Ÿ1å‘¨ |
| **è¯¯åŒº3: æ‰€æœ‰åº”ç”¨éƒ½èƒ½ä¼˜åŒ–** | æŸäº›åº”ç”¨éœ€è¦é¢„ç•™èµ„æº(å¦‚JVMå †å†…å­˜) | åŒºåˆ†å¯¹å¾…,å…³é”®åº”ç”¨ä¿å®ˆsizing |
| **è¯¯åŒº4: å¿½ç•¥çªå‘æµé‡** | åŸºäºå†å²æ•°æ®å¯èƒ½å¿½ç•¥ä¿ƒé”€ç­‰ç‰¹æ®Šäº‹ä»¶ | ç»“åˆä¸šåŠ¡æ—¥å†,é¢„ç•™å¼¹æ€§ç©ºé—´ |
| **è¯¯åŒº5: åªçœ‹CPU/å†…å­˜** | ç£ç›˜I/Oã€ç½‘ç»œå¸¦å®½ä¹Ÿæ˜¯é‡è¦ç“¶é¢ˆ | å…¨é¢è¯„ä¼°æ‰€æœ‰èµ„æºç»´åº¦ |

**å®æˆ˜æ“ä½œæŒ‡å—:**

```bash
# 1. æŸ¥çœ‹Podèµ„æºä½¿ç”¨æƒ…å†µ
kubectl top pods -n production --sort-by=cpu

# 2. åˆ†æå†å²èµ„æºä½¿ç”¨(éœ€è¦Prometheus)
kubectl port-forward -n monitoring svc/prometheus 9090:9090

# è®¿é—® http://localhost:9090 æŸ¥è¯¢:
# CPUåˆ©ç”¨ç‡: rate(container_cpu_usage_seconds_total[5m]) / on() kube_pod_container_resource_requests
# å†…å­˜åˆ©ç”¨ç‡: container_memory_working_set_bytes / on() kube_pod_container_resource_requests

# 3. ä½¿ç”¨Vertical Pod Autoscaler(VPA)æ¨è
kubectl apply -f https://github.com/kubernetes/autoscaler/releases/download/vpa-0.14.0/vpa-v0.14-crd.yaml

# åˆ›å»ºVPAèµ„æº(æ¨èæ¨¡å¼,ä¸è‡ªåŠ¨åº”ç”¨)
cat <<EOF | kubectl apply -f -
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: my-app
  updateMode: "Off"  # åªæ¨è,ä¸è‡ªåŠ¨åº”ç”¨
EOF

# æŸ¥çœ‹VPAæ¨è
kubectl describe vpa my-app-vpa -n production

# 4. åº”ç”¨Right Sizingå»ºè®®
kubectl set resources deployment my-app -n production \
  --requests=cpu=500m,memory=1Gi \
  --limits=cpu=1000m,memory=2Gi

# 5. ç›‘æ§è°ƒæ•´åçš„æ•ˆæœ
watch kubectl top pods -n production -l app=my-app
```

**æˆæœ¬èŠ‚çœæ¡ˆä¾‹:**
- **æ¡ˆä¾‹1**: æŸå…¬å¸å¯¹200+å¾®æœåŠ¡åšRight Sizing,å¹³å‡CPU requesté™ä½40%,**å¹´èŠ‚çœ$180,000**
- **æ¡ˆä¾‹2**: å‘ç°å¼€å‘/æµ‹è¯•ç¯å¢ƒè¿‡åº¦é…ç½®,æ™šä¸Šå’Œå‘¨æœ«è‡ªåŠ¨ç¼©å®¹,**å¹´èŠ‚çœ$75,000**
- **æ¡ˆä¾‹3**: è¯†åˆ«"åƒµå°¸èµ„æº"(é•¿æœŸæœªä½¿ç”¨çš„PVã€é—²ç½®çš„Load Balancer),æ¸…ç†å**å¹´èŠ‚çœ$30,000**

---

## 6. å¤šäº‘æ²»ç†æ¡†æ¶

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: æ²»ç†æ¡†æ¶è§£å†³"è°èƒ½åšä»€ä¹ˆã€åœ¨å“ªåšã€æ€ä¹ˆåš"çš„é—®é¢˜ã€‚åŒ…æ‹¬ç»Ÿä¸€çš„å‘½åè§„èŒƒã€æ ‡ç­¾ç­–ç•¥ã€å®‰å…¨åŸºçº¿ã€æˆæœ¬åˆ†æ‘Šå’Œåˆè§„è¦æ±‚ã€‚

### 6.1 å¤šäº‘æ²»ç†åŸåˆ™

| æ²»ç†ç»´åº¦ | æ ¸å¿ƒåŸåˆ™ | å®æ–½è¦ç‚¹ | æ²»ç†å·¥å…· | åˆè§„è¦æ±‚ |
|----------|----------|----------|----------|----------|
| **ç»Ÿä¸€èº«ä»½** | å•ç‚¹ç™»å½•ã€ç»Ÿä¸€è®¤è¯ | SSOé›†æˆã€RBACç»Ÿä¸€ | Keycloakã€AAD | SOC2ã€ISO27001 |
| **èµ„æºç®¡ç†** | æ ‡å‡†åŒ–å‘½åã€æ ‡ç­¾æ²»ç† | å‘½åè§„èŒƒã€æˆæœ¬æ ‡ç­¾ | Terraformã€Crossplane | å†…éƒ¨æ²»ç†è¦æ±‚ |
| **å®‰å…¨åˆè§„** | ç­–ç•¥ç»Ÿä¸€ã€å®¡è®¡é›†ä¸­ | ç­–ç•¥å¼•æ“ã€åˆè§„æ‰«æ | OPAã€Falco | ç­‰ä¿ã€GDPR |
| **æˆæœ¬æ§åˆ¶** | é¢„ç®—ç®¡ç†ã€æˆæœ¬åˆ†æ‘Š | é¢„ç®—å‘Šè­¦ã€æˆæœ¬åˆ†æ | Kubecostã€CloudHealth | è´¢åŠ¡ç®¡æ§è¦æ±‚ |
| **å˜æ›´ç®¡ç†** | æµç¨‹æ ‡å‡†åŒ–ã€å®¡æ‰¹è‡ªåŠ¨åŒ– | GitOpsã€å˜æ›´çª—å£ | ArgoCDã€Spinnaker | å˜æ›´ç®¡ç†æµç¨‹ |

### 6.2 å¤šäº‘æ²»ç†å®æ–½

```yaml
# ========== å¤šäº‘æ²»ç†ç­–ç•¥ ==========
apiVersion: governance.example.com/v1
kind: MulticloudGovernancePolicy
metadata:
  name: enterprise-governance
  namespace: platform-governance
spec:
  # ç»Ÿä¸€èº«ä»½è®¤è¯
  identity_management:
    sso_provider: "keycloak"
    identity_federation:
      enabled: true
      providers:
        - name: "aws-sso"
          type: "saml"
          metadata_url: "https://sso.us-east-1.amazonaws.com/idp/metadata"
        - name: "azure-ad"
          type: "oidc"
          issuer_url: "https://login.microsoftonline.com/common/v2.0"
        - name: "gcp-identity"
          type: "oidc"
          issuer_url: "https://accounts.google.com"
    
    role_mapping:
      admin_role: "platform-admin"
      developer_role: "app-developer"
      auditor_role: "security-auditor"
      
  # èµ„æºå‘½åå’Œæ ‡ç­¾è§„èŒƒ
  resource_governance:
    naming_standards:
      cluster_pattern: "{environment}-{purpose}-{region}-{sequence}"
      namespace_pattern: "{team}-{application}-{environment}"
      resource_pattern: "{application}-{component}-{environment}"
      
    tagging_requirements:
      mandatory_tags:
        - "Environment"
        - "Team"
        - "CostCenter"
        - "Owner"
        - "ComplianceLevel"
      recommended_tags:
        - "Project"
        - "BusinessUnit"
        - "CreateDate"
        
  # å®‰å…¨ç­–ç•¥ç»Ÿä¸€
  security_policies:
    cluster_hardening:
      pod_security_standards: "restricted"
      network_policies_required: true
      image_scanning_mandatory: true
      
    data_protection:
      encryption_at_rest: true
      encryption_in_transit: true
      data_classification_required: true
      
    compliance_frameworks:
      - name: "soc2-type2"
        controls:
          - "access-logging"
          - "change-management"
          - "incident-response"
      - name: "iso27001"
        controls:
          - "asset-management"
          - "access-control"
          - "cryptography"

---
# ========== æ²»ç†ç›‘æ§å’ŒæŠ¥å‘Š ==========
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: governance-compliance-rules
  namespace: platform-governance
spec:
  groups:
  - name: governance.compliance.rules
    rules:
    # èµ„æºæ ‡ç­¾åˆè§„æ£€æŸ¥
    - alert: MissingMandatoryTags
      expr: |
        count by(namespace, resource) (
          kube_resource_labels{label_environment="", label_team="", label_costcenter=""}
        ) > 0
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "èµ„æºç¼ºå°‘å¼ºåˆ¶æ ‡ç­¾"
        description: "æ£€æµ‹åˆ°èµ„æºç¼ºå°‘Environmentã€Teamæˆ–CostCenteræ ‡ç­¾"
        
    # å®‰å…¨ç­–ç•¥åˆè§„æ£€æŸ¥
    - alert: SecurityPolicyViolation
      expr: |
        count(opa_policy_violations_total{severity="high"}) > 0
      for: 30m
      labels:
        severity: critical
      annotations:
        summary: "å®‰å…¨ç­–ç•¥è¿è§„"
        description: "æ£€æµ‹åˆ°é«˜ä¸¥é‡æ€§å®‰å…¨ç­–ç•¥è¿è§„"
        
    # æˆæœ¬æ²»ç†æ£€æŸ¥
    - alert: BudgetGovernanceViolation
      expr: |
        sum by(team) (rate(cloud_cost_hourly_total[1h])) > 
        on(team) group_left budget_hourly_limit
      for: 1h
      labels:
        severity: critical
      annotations:
        summary: "é¢„ç®—æ²»ç†è¿è§„"
        description: "å›¢é˜Ÿ {{ $labels.team }} çš„å°æ—¶æˆæœ¬è¶…å‡ºé¢„ç®—é™åˆ¶"

---
# ========== æ²»ç†ä»ªè¡¨æ¿ ==========
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: multicloud-governance-dashboard
  namespace: platform-governance
spec:
  json: |
    {
      "dashboard": {
        "title": "å¤šäº‘æ²»ç†ä»ªè¡¨æ¿",
        "panels": [
          {
            "title": "æ²»ç†åˆè§„çŠ¶æ€",
            "type": "stat",
            "targets": [
              {"expr": "governance_compliance_score", "legendFormat": "æ•´ä½“åˆè§„è¯„åˆ†"},
              {"expr": "count(governance_policy_violations_total)", "legendFormat": "è¿è§„é¡¹æ•°é‡"}
            ]
          },
          {
            "title": "å„ç»´åº¦åˆè§„ç‡",
            "type": "barchart",
            "targets": [
              {"expr": "governance_dimension_compliance{dimension=\"identity\"}", "legendFormat": "èº«ä»½æ²»ç†"},
              {"expr": "governance_dimension_compliance{dimension=\"security\"}", "legendFormat": "å®‰å…¨æ²»ç†"},
              {"expr": "governance_dimension_compliance{dimension=\"cost\"}", "legendFormat": "æˆæœ¬æ²»ç†"},
              {"expr": "governance_dimension_compliance{dimension=\"resource\"}", "legendFormat": "èµ„æºæ²»ç†"}
            ]
          },
          {
            "title": "è¿è§„è¶‹åŠ¿åˆ†æ",
            "type": "graph",
            "targets": [
              {"expr": "increase(governance_policy_violations_total[1h])", "legendFormat": "æ¯å°æ—¶æ–°å¢è¿è§„"}
            ]
          },
          {
            "title": "æ²»ç†æˆæœ¬åˆ†æ",
            "type": "table",
            "targets": [
              {
                "expr": "sum by(team) (cloud_cost_monthly_total)",
                "format": "table"
              }
            ]
          }
        ]
      }
    }
```

---

**è¡¨æ ¼åº•éƒ¨æ ‡è®°**: Kusheet Project | ä½œè€…: Allen Galler (allengaller@gmail.com) | æœ€åæ›´æ–°: 2026-02 | ç‰ˆæœ¬: v1.25-v1.32 | è´¨é‡ç­‰çº§: â­â­â­â­â­ ä¸“å®¶çº§
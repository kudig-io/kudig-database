# 03 - Kubernetes æ€§èƒ½è°ƒä¼˜ä¸“å®¶æŒ‡å—

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25-v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä½œè€…**: Allen Galler | **è´¨é‡ç­‰çº§**: â­â­â­â­â­ ä¸“å®¶çº§

> **æ€§èƒ½ä¼˜åŒ–å®æˆ˜å®å…¸**: åŸºäºä¸‡çº§èŠ‚ç‚¹é›†ç¾¤æ€§èƒ½ä¼˜åŒ–ç»éªŒï¼Œæ¶µç›–ä»ç³»ç»Ÿè°ƒä¼˜åˆ°åº”ç”¨ä¼˜åŒ–çš„å…¨æ–¹ä½æ€§èƒ½æå‡æ–¹æ¡ˆ

---

## ç›®å½•

- [1. ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆè¯†åˆ«](#1-ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆè¯†åˆ«)
- [2. èµ„æºä¼˜åŒ–ç­–ç•¥](#2-èµ„æºä¼˜åŒ–ç­–ç•¥)
- [3. è°ƒåº¦å™¨è°ƒä¼˜å‚æ•°](#3-è°ƒåº¦å™¨è°ƒä¼˜å‚æ•°)
- [4. ç½‘ç»œæ€§èƒ½ä¼˜åŒ–](#4-ç½‘ç»œæ€§èƒ½ä¼˜åŒ–)
- [5. å­˜å‚¨IOè°ƒä¼˜](#5-å­˜å‚¨ioè°ƒä¼˜)
- [6. åº”ç”¨å±‚æ€§èƒ½ä¼˜åŒ–](#6-åº”ç”¨å±‚æ€§èƒ½ä¼˜åŒ–)
- [7. ç›‘æ§ä¸åŸºå‡†æµ‹è¯•](#7-ç›‘æ§ä¸åŸºå‡†æµ‹è¯•)

---

## 1. ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆè¯†åˆ«

### 1.1 æ€§èƒ½ç“¶é¢ˆåˆ†ç±»çŸ©é˜µ

| ç“¶é¢ˆç±»å‹ | å…¸å‹ç—‡çŠ¶ | æ£€æµ‹æŒ‡æ ‡ | å½±å“ç¨‹åº¦ | ä¼˜åŒ–ä¼˜å…ˆçº§ |
|---------|---------|---------|---------|-----------|
| **CPUç“¶é¢ˆ** | åº”ç”¨å“åº”æ…¢ã€è°ƒåº¦å»¶è¿Ÿ | CPUä½¿ç”¨ç‡>80%ã€Load Averageé«˜ | é«˜ | P0 |
| **å†…å­˜ç“¶é¢ˆ** | OOMKilledã€é¢‘ç¹GC | å†…å­˜ä½¿ç”¨ç‡>85%ã€Page Faultå¤š | é«˜ | P0 |
| **ç£ç›˜IOç“¶é¢ˆ** | è¯»å†™å»¶è¿Ÿé«˜ã€ååé‡ä½ | IOPSé¥±å’Œã€Awaitæ—¶é—´é•¿ | ä¸­ | P1 |
| **ç½‘ç»œç“¶é¢ˆ** | é€šä¿¡å»¶è¿Ÿã€ä¸¢åŒ… | å¸¦å®½åˆ©ç”¨ç‡>70%ã€RTTé«˜ | ä¸­ | P1 |
| **API Serverç“¶é¢ˆ** | è¯·æ±‚è¶…æ—¶ã€é™æµ | QPSè¿‡é«˜ã€å»¶è¿Ÿå¢åŠ  | é«˜ | P0 |
| **etcdç“¶é¢ˆ** | æ•°æ®è¯»å†™æ…¢ã€leaderåˆ‡æ¢ | WALå»¶è¿Ÿã€fsyncæ—¶é—´é•¿ | é«˜ | P0 |

### 1.2 æ€§èƒ½è¯Šæ–­å·¥å…·é“¾

```bash
#!/bin/bash
# ========== æ€§èƒ½ç»¼åˆè¯Šæ–­è„šæœ¬ ==========
set -euo pipefail

NODE_NAME=${1:-"all-nodes"}
OUTPUT_DIR="/tmp/performance-analysis-$(date +%Y%m%d-%H%M%S)"

mkdir -p ${OUTPUT_DIR}
echo "æ€§èƒ½åˆ†ææŠ¥å‘Šç”Ÿæˆä¸­: ${OUTPUT_DIR}"

# 1. ç³»ç»Ÿçº§åˆ«æ€§èƒ½æ•°æ®æ”¶é›†
collect_system_metrics() {
    echo "=== ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡æ”¶é›† ==="
    
    # CPUä½¿ç”¨æƒ…å†µ
    echo "CPUä½¿ç”¨ç‡ç»Ÿè®¡:"
    kubectl top nodes | tee ${OUTPUT_DIR}/cpu-usage.txt
    
    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    echo -e "\nå†…å­˜ä½¿ç”¨ç»Ÿè®¡:"
    kubectl top pods -A --sort-by=memory | head -20 | tee ${OUTPUT_DIR}/memory-usage.txt
    
    # èŠ‚ç‚¹èµ„æºå‹åŠ›
    echo -e "\nèŠ‚ç‚¹èµ„æºå‹åŠ›:"
    kubectl describe nodes | grep -E "(memory|cpu).*pressure" | tee ${OUTPUT_DIR}/resource-pressure.txt
}

# 2. ç½‘ç»œæ€§èƒ½æ£€æµ‹
check_network_performance() {
    echo -e "\n=== ç½‘ç»œæ€§èƒ½æ£€æµ‹ ==="
    
    # Podé—´ç½‘ç»œå»¶è¿Ÿæµ‹è¯•
    kubectl run netperf-test --image=networkstatic/netperf --restart=Never \
      --overrides='{"spec":{"hostNetwork":true}}' -- \
      netperf -H 8.8.8.8 -t TCP_RR -- -r 64
    
    # DNSè§£ææ€§èƒ½
    kubectl run dns-test --image=busybox --restart=Never -- \
      sh -c "for i in \$(seq 1 10); do time nslookup kubernetes.default; done" \
      2>&1 | tee ${OUTPUT_DIR}/dns-performance.txt
}

# 3. å­˜å‚¨æ€§èƒ½æµ‹è¯•
test_storage_performance() {
    echo -e "\n=== å­˜å‚¨æ€§èƒ½æµ‹è¯• ==="
    
    # åˆ›å»ºå­˜å‚¨æ€§èƒ½æµ‹è¯•Pod
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: storage-perf-test
spec:
  containers:
  - name: fio-test
    image: ljishen/fio
    command: ["fio"]
    args:
    - "--name=test"
    - "--rw=randrw"
    - "--bs=4k"
    - "--iodepth=16"
    - "--size=1g"
    - "--direct=1"
    - "--runtime=60"
    - "--time_based"
    volumeMounts:
    - name: test-volume
      mountPath: /data
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: perf-test-pvc
EOF
    
    # ç­‰å¾…æµ‹è¯•å®Œæˆ
    kubectl wait --for=condition=Ready pod/storage-perf-test --timeout=90s
    kubectl logs storage-perf-test > ${OUTPUT_DIR}/storage-performance.txt
    kubectl delete pod/storage-perf-test
}

# 4. API Serveræ€§èƒ½åˆ†æ
analyze_api_server() {
    echo -e "\n=== API Serveræ€§èƒ½åˆ†æ ==="
    
    # API ServeræŒ‡æ ‡æ”¶é›†
    kubectl get --raw /metrics | grep -E "(apiserver_request_|etcd_|rest_client_)" \
      > ${OUTPUT_DIR}/api-server-metrics.txt
    
    # è¯·æ±‚å»¶è¿Ÿåˆ†æ
    echo "API Serverå»¶è¿Ÿåˆ†å¸ƒ:"
    kubectl get --raw /metrics | grep apiserver_request_duration_seconds_bucket \
      | awk '{print $1}' | sort -n | tail -10 >> ${OUTPUT_DIR}/api-latency.txt
}

# 5. åº”ç”¨æ€§èƒ½å‰–æ
profile_application() {
    echo -e "\n=== åº”ç”¨æ€§èƒ½å‰–æ ==="
    
    # Javaåº”ç”¨å †æ ˆåˆ†æ
    kubectl get pods -n production -l app=java-app -o name | head -1 | \
      xargs -I {} kubectl exec {} -n production -- jstack 1 > ${OUTPUT_DIR}/java-thread-dump.txt
    
    # Goåº”ç”¨pprofåˆ†æ
    kubectl port-forward svc/go-app-service 6060:6060 -n production &
    sleep 5
    curl -s http://localhost:6060/debug/pprof/profile?seconds=30 > ${OUTPUT_DIR}/go-profile.pb.gz
    kill %1
}

# æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
collect_system_metrics
check_network_performance
test_storage_performance
analyze_api_server
profile_application

echo -e "\næ€§èƒ½åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šä½ç½®: ${OUTPUT_DIR}"
ls -la ${OUTPUT_DIR}
```

### 1.3 æ€§èƒ½ç“¶é¢ˆè¯†åˆ«æµç¨‹

```mermaid
graph TD
    A[æ€§èƒ½é—®é¢˜å‘ç°] --> B{ç¡®å®šç“¶é¢ˆç±»å‹}
    B --> C[CPUç“¶é¢ˆ?]
    B --> D[å†…å­˜ç“¶é¢ˆ?]
    B --> E[IOç“¶é¢ˆ?]
    B --> F[ç½‘ç»œç“¶é¢ˆ?]
    
    C --> C1[æ£€æŸ¥CPUä½¿ç”¨ç‡]
    C --> C2[åˆ†æè¿›ç¨‹CPUæ¶ˆè€—]
    C --> C3[æ£€æŸ¥è°ƒåº¦å»¶è¿Ÿ]
    
    D --> D1[æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡]
    D --> D2[åˆ†æå†…å­˜åˆ†é…æ¨¡å¼]
    D --> D3[æ£€æŸ¥é¡µé¢äº¤æ¢]
    
    E --> E1[æ£€æŸ¥ç£ç›˜IOç­‰å¾…]
    E --> E2[åˆ†æå­˜å‚¨ç±»å‹æ€§èƒ½]
    E --> E3[æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿç¼“å­˜]
    
    F --> F1[æ£€æŸ¥ç½‘ç»œå¸¦å®½ä½¿ç”¨]
    F --> F2[åˆ†æç½‘ç»œå»¶è¿Ÿ]
    F --> F3[æ£€æŸ¥è¿æ¥æ•°é™åˆ¶]
    
    C1 --> G[å®šä½çƒ­ç‚¹å‡½æ•°]
    D2 --> H[ä¼˜åŒ–å†…å­˜åˆ†é…]
    E2 --> I[è°ƒæ•´å­˜å‚¨é…ç½®]
    F2 --> J[ä¼˜åŒ–ç½‘ç»œç­–ç•¥]
    
    G --> K[åº”ç”¨å±‚ä¼˜åŒ–]
    H --> K
    I --> K
    J --> K
    
    K --> L[éªŒè¯ä¼˜åŒ–æ•ˆæœ]
    L --> M{æ€§èƒ½è¾¾æ ‡?}
    M -->|æ˜¯| N[ä¼˜åŒ–å®Œæˆ]
    M -->|å¦| A
```

---

## 2. èµ„æºä¼˜åŒ–ç­–ç•¥

### 2.1 CPUä¼˜åŒ–é…ç½®

```yaml
# ========== CPUä¼˜åŒ–é…ç½®æ¨¡æ¿ ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-optimized-app
  namespace: production
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: app:v1.0
        resources:
          requests:
            # åŸºäºå®é™…ä½¿ç”¨é‡çš„95ç™¾åˆ†ä½
            cpu: "300m"
          limits:
            # åˆç†çš„ä¸Šé™ï¼Œé¿å…è¿‡åº¦é™åˆ¶
            cpu: "1500m"
            
        # CPUäº²å’Œæ€§è®¾ç½®
        env:
        - name: GOMAXPROCS
          value: "2"  # é™åˆ¶Goè¿è¡Œæ—¶ä½¿ç”¨çš„CPUæ ¸å¿ƒæ•°
        - name: JAVA_TOOL_OPTIONS
          value: >
            -XX:ActiveProcessorCount=2
            -XX:+UseContainerSupport
            -XX:ParallelGCThreads=2
            -XX:ConcGCThreads=1
            
        # CPUè°ƒåº¦ä¼˜å…ˆçº§
        securityContext:
          # è®¾ç½®CPUè°ƒåº¦ç­–ç•¥
          sysctls:
          - name: kernel.sched_min_granularity_ns
            value: "10000000"  # 10ms
          - name: kernel.sched_latency_ns
            value: "24000000"  # 24ms

---
# ========== CPUç»‘æ ¸é…ç½® ==========
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: cpu-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: cpu-manager
  template:
    metadata:
      labels:
        name: cpu-manager
    spec:
      # å¯ç”¨é™æ€CPUç®¡ç†ç­–ç•¥
      kubeletConfig:
        cpuManagerPolicy: static
        reservedSystemCPUs: "0,1"  # ä¸ºç³»ç»Ÿä¿ç•™CPUæ ¸å¿ƒ
        
      containers:
      - name: cpu-manager
        image: k8s.gcr.io/cpu-manager:v1.0
        command:
        - /cpu-manager
        - --policy=static
        - --reserved-cpus=0,1
        volumeMounts:
        - name: sysfs
          mountPath: /sys
        securityContext:
          privileged: true
          
      volumes:
      - name: sysfs
        hostPath:
          path: /sys
```

### 2.2 å†…å­˜ä¼˜åŒ–é…ç½®

```yaml
# ========== å†…å­˜ä¼˜åŒ–é…ç½®æ¨¡æ¿ ==========
apiVersion: v1
kind: Pod
metadata:
  name: memory-optimized-app
  namespace: production
spec:
  containers:
  - name: app
    image: app:v1.0
    resources:
      requests:
        # åŸºäºç¨³æ€ä½¿ç”¨é‡çš„1.2å€
        memory: "512Mi"
      limits:
        # requestsçš„1.5-2å€ï¼Œå…è®¸åˆç†çªå‘
        memory: "1Gi"
        
    # å†…å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡
    env:
    # Javaåº”ç”¨å†…å­˜ä¼˜åŒ–
    - name: JAVA_OPTS
      value: >
        -Xmx768m
        -Xms512m
        -XX:+UseG1GC
        -XX:MaxGCPauseMillis=200
        -XX:+UnlockExperimentalVMOptions
        -XX:+UseCGroupMemoryLimitForHeap
        -XX:MaxRAMPercentage=75.0
        
    # Goåº”ç”¨å†…å­˜ä¼˜åŒ–
    - name: GOMEMLIMIT
      value: "800MiB"  # Go 1.19+ å†…å­˜è½¯é™åˆ¶
    - name: GOGC
      value: "20"      # åƒåœ¾å›æ”¶è§¦å‘æ¯”ä¾‹
      
    # å†…å­˜å®‰å…¨è®¾ç½®
    securityContext:
      # å¯ç”¨å†…å­˜ä¿æŠ¤
      sysctls:
      - name: vm.overcommit_memory
        value: "1"  # å¯ç”¨å†…å­˜è¶…é¢åˆ†é…
      - name: vm.swappiness
        value: "1"  # é™ä½äº¤æ¢å€¾å‘

---
# ========== HugePagesé…ç½® ==========
apiVersion: v1
kind: Pod
metadata:
  name: hugepages-app
  namespace: production
spec:
  containers:
  - name: app
    image: database:v1.0
    resources:
      requests:
        memory: "2Gi"
        hugepages-2Mi: "1Gi"
      limits:
        memory: "2Gi"
        hugepages-2Mi: "1Gi"
        
    volumeMounts:
    - name: hugepage-2mi
      mountPath: /hugepages-2Mi
      
  volumes:
  - name: hugepage-2mi
    emptyDir:
      medium: HugePages-2Mi
```

### 2.3 èµ„æºé…é¢ä¼˜åŒ–

```yaml
# ========== å‘½åç©ºé—´èµ„æºé…é¢ ==========
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # CPUé…é¢
    requests.cpu: "20"
    limits.cpu: "40"
    # å†…å­˜é…é¢
    requests.memory: "40Gi"
    limits.memory: "80Gi"
    # å­˜å‚¨é…é¢
    requests.storage: "2Ti"
    persistentvolumeclaims: "100"
    # å¯¹è±¡æ•°é‡é™åˆ¶
    pods: "1000"
    services: "50"
    secrets: "100"
    
  # ä½œç”¨åŸŸé€‰æ‹©å™¨
  scopeSelector:
    matchExpressions:
    - scopeName: PriorityClass
      operator: In
      values: ["high-priority", "system-node-critical"]

---
# ========== LimitRangeé…ç½® ==========
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limits
  namespace: production
spec:
  limits:
  # å®¹å™¨é»˜è®¤é™åˆ¶
  - type: Container
    default:
      cpu: "500m"
      memory: "1Gi"
    defaultRequest:
      cpu: "100m"
      memory: "256Mi"
    max:
      cpu: "4"
      memory: "16Gi"
    min:
      cpu: "10m"
      memory: "32Mi"
      
  # Podçº§åˆ«é™åˆ¶
  - type: Pod
    max:
      cpu: "8"
      memory: "32Gi"
```

---

## 3. è°ƒåº¦å™¨è°ƒä¼˜å‚æ•°

### 3.1 è°ƒåº¦å™¨æ€§èƒ½è°ƒä¼˜

```yaml
# ========== è°ƒåº¦å™¨é«˜çº§é…ç½® ==========
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration
metadata:
  name: scheduler-config
profiles:
- schedulerName: default-scheduler
  plugins:
    # é¢„é€‰é˜¶æ®µä¼˜åŒ–
    filter:
      disabled:
      - name: "NodeResourcesFit"  # å¦‚æœä¸éœ€è¦ä¸¥æ ¼çš„èµ„æºæ£€æŸ¥
      enabled:
      - name: "NodeResourcesBalancedAllocation"
        weight: 2
        
    # ä¼˜é€‰é˜¶æ®µä¼˜åŒ–
    score:
      enabled:
      - name: "NodeResourcesLeastAllocated"
        weight: 1
      - name: "InterPodAffinity"
        weight: 2
      - name: "NodeAffinity"
        weight: 1
        
  pluginConfig:
  # è°ƒåº¦å™¨æ€§èƒ½å‚æ•°
  - name: "NodeResourcesFit"
    args:
      scoringStrategy:
        type: LeastAllocated
        resources:
        - name: cpu
          weight: 1
        - name: memory
          weight: 1
          
  # æ‰¹é‡è°ƒåº¦ä¼˜åŒ–
  - name: "VolumeBinding"
    args:
      bindTimeoutSeconds: 30
      
# è°ƒåº¦å™¨å…¨å±€é…ç½®
extenders:
- urlPrefix: "http://scheduler-extender.example.com"
  filterVerb: "filter"
  prioritizeVerb: "prioritize"
  weight: 1
  enableHttps: false
  nodeCacheCapable: true

---
# ========== è°ƒåº¦å™¨èµ„æºé™åˆ¶ ==========
apiVersion: v1
kind: Pod
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - name: kube-scheduler
    image: k8s.gcr.io/kube-scheduler:v1.32.0
    resources:
      requests:
        cpu: "200m"
        memory: "256Mi"
      limits:
        cpu: "1000m"
        memory: "1Gi"
        
    # è°ƒåº¦å™¨æ€§èƒ½å‚æ•°
    command:
    - kube-scheduler
    - --address=0.0.0.0
    - --leader-elect=true
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=0.0.0.0
    - --secure-port=10259
    - --profiling=false  # ç”Ÿäº§ç¯å¢ƒç¦ç”¨æ€§èƒ½åˆ†æ
    
    # æ€§èƒ½ä¼˜åŒ–å‚æ•°
    - --percentage-of-nodes-to-score=50  # è¯„åˆ†èŠ‚ç‚¹æ¯”ä¾‹
    - --pod-max-in-unschedulable-pods-duration=60s  # æ— æ³•è°ƒåº¦Podçš„æœ€å¤§ç­‰å¾…æ—¶é—´
    - --scheduler-name=default-scheduler
```

### 3.2 è°ƒåº¦ç­–ç•¥ä¼˜åŒ–

```yaml
# ========== è‡ªå®šä¹‰è°ƒåº¦ç­–ç•¥ ==========
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: high-performance-scheduler
  plugins:
    preFilter:
      enabled:
      - name: "NodeResourcesFit"
    filter:
      enabled:
      - name: "NodeUnschedulable"
      - name: "NodeAffinity"
      - name: "NodeResourcesFit"
      - name: "VolumeRestrictions"
      - name: "TaintToleration"
    postFilter:
      enabled:
      - name: "DefaultPreemption"
    preScore:
      enabled:
      - name: "InterPodAffinity"
    score:
      enabled:
      - name: "NodeResourcesBalancedAllocation"
        weight: 2
      - name: "ImageLocality"
        weight: 1
      - name: "InterPodAffinity"
        weight: 1
      - name: "NodeAffinity"
        weight: 1
      - name: "NodePreferAvoidPods"
        weight: 10000
      - name: "NodeResourcesLeastAllocated"
        weight: 1
      - name: "TaintToleration"
        weight: 1

---
# ========== æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦ ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: topology-aware-app
  namespace: production
spec:
  replicas: 6
  template:
    spec:
      # æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: topology-aware-app
            
      # èŠ‚ç‚¹äº²å’Œæ€§
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - us-west-1
                
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: topology-aware-app
              topologyKey: kubernetes.io/hostname
```

---

## 4. ç½‘ç»œæ€§èƒ½ä¼˜åŒ–

### 4.1 CNIæ’ä»¶ä¼˜åŒ–

```yaml
# ========== Calicoç½‘ç»œä¼˜åŒ–é…ç½® ==========
apiVersion: crd.projectcalico.org/v1
kind: FelixConfiguration
metadata:
  name: default
spec:
  # æ€§èƒ½ä¼˜åŒ–å‚æ•°
  bpfLogLevel: ""
  bpfEnabled: true  # å¯ç”¨eBPFæ•°æ®å¹³é¢
  floatingIPs: Disabled
  healthPort: 9099
  logSeverityScreen: Info
  
  # è¿æ¥è·Ÿè¸ªä¼˜åŒ–
  netlinkTimeoutSecs: 10
  reportingIntervalSecs: 0
  
  # è·¯ç”±ä¼˜åŒ–
  routeRefreshIntervalSecs: 90
  vxlanVNI: 4096

---
# ========== Ciliumé«˜æ€§èƒ½é…ç½® ==========
apiVersion: cilium.io/v2
kind: CiliumConfig
metadata:
  name: cilium-config
  namespace: kube-system
spec:
  # å¯ç”¨é«˜æ€§èƒ½ç‰¹æ€§
  enable-bpf-clock-probe: true
  enable-bpf-tproxy: true
  enable-host-firewall: false  # å¦‚ä¸éœ€è¦å¯å…³é—­æå‡æ€§èƒ½
  enable-ipv4-masquerade: true
  enable-ipv6-masquerade: false
  
  # è´Ÿè½½å‡è¡¡ä¼˜åŒ–
  kube-proxy-replacement: strict
  enable-health-check-nodeport: true
  node-port-bind-addr: "0.0.0.0"
  
  # ç›‘æ§å’Œè°ƒè¯•
  monitor-aggregation: medium
  monitor-aggregation-flags: all
  monitor-aggregation-interval: 5s
```

### 4.2 Serviceæ€§èƒ½ä¼˜åŒ–

```yaml
# ========== Headless Serviceä¼˜åŒ– ==========
apiVersion: v1
kind: Service
metadata:
  name: high-performance-service
  namespace: production
spec:
  clusterIP: None  # Headless Serviceå‡å°‘DNSæŸ¥è¯¢
  selector:
    app: backend
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    protocol: TCP

---
# ========== ExternalTrafficPolicyä¼˜åŒ– ==========
apiVersion: v1
kind: Service
metadata:
  name: external-service
  namespace: production
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local  # ä¿æŒå®¢æˆ·ç«¯æºIPï¼Œå‡å°‘SNAT
  selector:
    app: frontend
  ports:
  - port: 80
    targetPort: 8080

---
# ========== Session Affinityé…ç½® ==========
apiVersion: v1
kind: Service
metadata:
  name: session-affinity-service
  namespace: production
spec:
  selector:
    app: app-with-session
  ports:
  - port: 80
    targetPort: 8080
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 3å°æ—¶ä¼šè¯ä¿æŒ
```

### 4.3 ç½‘ç»œç­–ç•¥ä¼˜åŒ–

```yaml
# ========== é«˜æ€§èƒ½ç½‘ç»œç­–ç•¥ ==========
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: optimized-network-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: high-performance-app
  policyTypes:
  - Ingress
  - Egress
  
  # ä¼˜åŒ–çš„å…¥å£è§„åˆ™
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    - podSelector:
        matchLabels:
          app: api-gateway
    ports:
    - protocol: TCP
      port: 8080
      
  # ä¼˜åŒ–çš„å‡ºå£è§„åˆ™
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
    - protocol: TCP
      port: 3306
      
  # å…è®¸å¿…è¦çš„åŸºç¡€è®¾æ–½é€šä¿¡
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53  # DNS
    - protocol: TCP
      port: 53  # DNS
```

---

## 5. å­˜å‚¨IOè°ƒä¼˜

### 5.1 å­˜å‚¨æ€§èƒ½é…ç½®

```yaml
# ========== é«˜æ€§èƒ½StorageClassé…ç½® ==========
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  fsType: ext4
  iops: "3000"      # IOPSæ€§èƒ½
  throughput: "125" # ååé‡(MB/s)
reclaimPolicy: Retain
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer

---
# ========== æœ¬åœ°å­˜å‚¨ä¼˜åŒ– ==========
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-fast
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: false

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-fast
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-fast
  local:
    path: /mnt/fast-disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-1
```

### 5.2 åº”ç”¨å±‚å­˜å‚¨ä¼˜åŒ–

```yaml
# ========== å­˜å‚¨ä¼˜åŒ–çš„Podé…ç½® ==========
apiVersion: v1
kind: Pod
metadata:
  name: io-optimized-app
  namespace: production
spec:
  containers:
  - name: app
    image: database:v1.0
    volumeMounts:
    - name: data-volume
      mountPath: /var/lib/mysql
      # IOä¼˜åŒ–æŒ‚è½½é€‰é¡¹
      mountPropagation: None
      
    # IOè°ƒåº¦ä¼˜åŒ–
    env:
    - name: MYSQLD_OPTS
      value: >
        --innodb-flush-method=O_DIRECT
        --innodb-io-capacity=2000
        --innodb-read-io-threads=8
        --innodb-write-io-threads=8
        
  volumes:
  - name: data-volume
    persistentVolumeClaim:
      claimName: mysql-pvc
      
---
# ========== ç¼“å­˜ä¼˜åŒ–é…ç½® ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: cache-config
  namespace: production
data:
  redis.conf: |
    # å†…å­˜ä¼˜åŒ–
    maxmemory 2gb
    maxmemory-policy allkeys-lru
    
    # ç½‘ç»œä¼˜åŒ–
    tcp-keepalive 300
    timeout 0
    
    # æŒä¹…åŒ–ä¼˜åŒ–
    save 900 1
    save 300 10
    save 60 10000
    
    # æ€§èƒ½ä¼˜åŒ–
    lazyfree-lazy-eviction yes
    lazyfree-lazy-expire yes
    lazyfree-lazy-server-del yes
```

### 5.3 å­˜å‚¨ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•

```bash
#!/bin/bash
# ========== å­˜å‚¨æ€§èƒ½åŸºå‡†æµ‹è¯• ==========
set -euo pipefail

TEST_NAMESPACE=${1:-"storage-test"}
STORAGE_CLASS=${2:-"fast-ssd"}

echo "å¼€å§‹å­˜å‚¨æ€§èƒ½æµ‹è¯•..."

# 1. åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: ${TEST_NAMESPACE}
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: perf-test-pvc
  namespace: ${TEST_NAMESPACE}
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: ${STORAGE_CLASS}
  resources:
    requests:
      storage: 10Gi
EOF

# 2. éƒ¨ç½²FIOæµ‹è¯•
kubectl run fio-test --image=ljishen/fio -n ${TEST_NAMESPACE} \
  --overrides='{
    "spec": {
      "containers": [{
        "name": "fio-test",
        "command": ["fio"],
        "args": [
          "--name=test",
          "--rw=randrw",
          "--bs=4k",
          "--iodepth=16",
          "--size=2g",
          "--direct=1",
          "--runtime=120",
          "--time_based",
          "--group_reporting",
          "--output-format=json"
        ],
        "volumeMounts": [{
          "name": "test-volume",
          "mountPath": "/data"
        }]
      }],
      "volumes": [{
        "name": "test-volume",
        "persistentVolumeClaim": {
          "claimName": "perf-test-pvc"
        }
      }]
    }
  }'

# 3. ç­‰å¾…æµ‹è¯•å®Œæˆå¹¶æ”¶é›†ç»“æœ
kubectl wait --for=condition=Ready pod/fio-test -n ${TEST_NAMESPACE} --timeout=150s
kubectl logs pod/fio-test -n ${TEST_NAMESPACE} > /tmp/storage-benchmark-results.json

# 4. è§£ææµ‹è¯•ç»“æœ
echo "=== å­˜å‚¨æ€§èƒ½æµ‹è¯•ç»“æœ ==="
jq '.jobs[].read' /tmp/storage-benchmark-results.json
jq '.jobs[].write' /tmp/storage-benchmark-results.json

# 5. æ¸…ç†æµ‹è¯•èµ„æº
kubectl delete namespace ${TEST_NAMESPACE}

echo "å­˜å‚¨æ€§èƒ½æµ‹è¯•å®Œæˆ"
```

---

## 6. åº”ç”¨å±‚æ€§èƒ½ä¼˜åŒ–

### 6.1 JVMåº”ç”¨ä¼˜åŒ–

```yaml
# ========== JVMæ€§èƒ½ä¼˜åŒ–é…ç½® ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: java-app-optimized
  namespace: production
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: java-app:v1.0
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
            
        # JVMæ€§èƒ½ä¼˜åŒ–å‚æ•°
        env:
        - name: JAVA_OPTS
          value: >
            -server
            -Xmx1536m
            -Xms1024m
            -XX:+UseG1GC
            -XX:MaxGCPauseMillis=200
            -XX:+UnlockExperimentalVMOptions
            -XX:+UseCGroupMemoryLimitForHeap
            -XX:MaxRAMPercentage=75.0
            -XX:+UseContainerSupport
            -XX:ActiveProcessorCount=2
            -XX:ParallelGCThreads=2
            -XX:ConcGCThreads=1
            -XX:+PrintGC
            -XX:+PrintGCDetails
            -XX:+PrintGCTimeStamps
            -Xloggc:/var/log/gc.log
            -XX:+UseGCLogFileRotation
            -XX:NumberOfGCLogFiles=5
            -XX:GCLogFileSize=100M
            
        # JVMå¯åŠ¨ä¼˜åŒ–
        startupProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - |
              curl -f http://localhost:8080/actuator/health || exit 1
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
```

### 6.2 Goåº”ç”¨ä¼˜åŒ–

```yaml
# ========== Goåº”ç”¨æ€§èƒ½ä¼˜åŒ– ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-app-optimized
  namespace: production
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: go-app:v1.0
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "1000m"
            
        # Goè¿è¡Œæ—¶ä¼˜åŒ–
        env:
        - name: GOMEMLIMIT
          value: "460MiB"  # 90% of limit
        - name: GOGC
          value: "20"      # æ›´é¢‘ç¹çš„GC
        - name: GOMAXPROCS
          value: "2"       # é™åˆ¶CPUæ ¸å¿ƒæ•°
        - name: GOTRACEBACK
          value: "crash"   # å´©æºƒæ—¶æ‰“å°å †æ ˆ
          
        # æ€§èƒ½ç›‘æ§
        ports:
        - name: pprof
          containerPort: 6060
          protocol: TCP
          
        # å¯åŠ¨ä¼˜åŒ–
        startupProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 20
```

### 6.3 Pythonåº”ç”¨ä¼˜åŒ–

```yaml
# ========== Pythonåº”ç”¨æ€§èƒ½ä¼˜åŒ– ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: python-app-optimized
  namespace: production
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: python-app:v1.0
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "2000m"
            
        # Pythonæ€§èƒ½ä¼˜åŒ–
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: PYTHONDONTWRITEBYTECODE
          value: "1"
        - name: PYTHONHASHSEED
          value: "random"
        - name: UVICORN_WORKERS
          value: "4"  # æ ¹æ®CPUæ ¸å¿ƒè°ƒæ•´
        - name: UVICORN_THREADS
          value: "1"
          
        # å¯åŠ¨å‘½ä»¤ä¼˜åŒ–
        command:
        - uvicorn
        - main:app
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
        - --workers
        - "4"
        - --http
        - "h11"
        - --loop
        - "uvloop"
        - --interface
        - "asgi3"
```

---

## 7. ç›‘æ§ä¸åŸºå‡†æµ‹è¯•

### 7.1 æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿

```yaml
# ========== Grafanaæ€§èƒ½ç›‘æ§é¢æ¿ ==========
apiVersion: integreatly.org/v1alpha1
kind: GrafanaDashboard
metadata:
  name: k8s-performance-dashboard
  namespace: monitoring
spec:
  json: |
    {
      "dashboard": {
        "title": "Kubernetes Performance Dashboard",
        "panels": [
          {
            "title": "é›†ç¾¤CPUä½¿ç”¨ç‡",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(container_cpu_usage_seconds_total[5m])) by (node)",
                "legendFormat": "{{node}}"
              }
            ]
          },
          {
            "title": "å†…å­˜ä½¿ç”¨è¶‹åŠ¿",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(container_memory_working_set_bytes) by (namespace)",
                "legendFormat": "{{namespace}}"
              }
            ]
          },
          {
            "title": "API Serverå»¶è¿Ÿ",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.99, rate(apiserver_request_duration_seconds_bucket[5m]))",
                "legendFormat": "99th percentile"
              }
            ]
          },
          {
            "title": "etcdæ€§èƒ½",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.99, etcd_disk_backend_commit_duration_seconds_bucket)",
                "legendFormat": "fsync 99th"
              }
            ]
          }
        ]
      }
    }
```

### 7.2 è‡ªåŠ¨åŒ–æ€§èƒ½æµ‹è¯•

```bash
#!/bin/bash
# ========== è‡ªåŠ¨åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶ ==========
set -euo pipefail

TEST_SUITE=${1:-"full"}
RESULTS_DIR="/tmp/performance-benchmarks-$(date +%Y%m%d-%H%M%S)"

mkdir -p ${RESULTS_DIR}
echo "å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•: ${TEST_SUITE}"

# åŸºå‡†æµ‹è¯•é…ç½®
declare -A TEST_CONFIGS=(
    ["cpu"]="stress-ng --cpu 4 --timeout 60s"
    ["memory"]="stress-ng --vm 2 --vm-bytes 1G --timeout 60s"
    ["disk"]="fio --name=test --rw=randrw --bs=4k --iodepth=16 --size=1g --runtime=60"
    ["network"]="iperf3 -c benchmark-server -t 60"
)

# æ‰§è¡ŒåŸºå‡†æµ‹è¯•
run_benchmark() {
    local test_type=$1
    local test_cmd=${TEST_CONFIGS[$test_type]}
    
    echo "æ‰§è¡Œ${test_type}åŸºå‡†æµ‹è¯•..."
    
    case $test_type in
        "cpu")
            kubectl run cpu-bench --image=alexeiled/stress-ng --restart=Never \
              -- ${test_cmd}
            ;;
        "memory")
            kubectl run mem-bench --image=alexeiled/stress-ng --restart=Never \
              -- ${test_cmd}
            ;;
        "disk")
            # åˆ›å»ºå­˜å‚¨æµ‹è¯•ç¯å¢ƒ
            cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: disk-bench
spec:
  containers:
  - name: fio-test
    image: ljishen/fio
    command: ["sh", "-c"]
    args:
    - "${test_cmd} --output-format=json > /results/fio-results.json"
    volumeMounts:
    - name: results
      mountPath: /results
    - name: test-volume
      mountPath: /data
  volumes:
  - name: results
    emptyDir: {}
  - name: test-volume
    persistentVolumeClaim:
      claimName: bench-pvc
EOF
            ;;
    esac
    
    # ç­‰å¾…æµ‹è¯•å®Œæˆ
    kubectl wait --for=condition=Ready pod/${test_type}-bench --timeout=90s 2>/dev/null || true
    
    # æ”¶é›†ç»“æœ
    if kubectl get pod/${test_type}-bench >/dev/null 2>&1; then
        kubectl logs pod/${test_type}-bench > ${RESULTS_DIR}/${test_type}-results.txt
        kubectl delete pod/${test_type}-bench
    fi
}

# æ ¹æ®æµ‹è¯•å¥—ä»¶æ‰§è¡Œç›¸åº”æµ‹è¯•
case ${TEST_SUITE} in
    "quick")
        run_benchmark "cpu"
        run_benchmark "memory"
        ;;
    "full")
        run_benchmark "cpu"
        run_benchmark "memory"
        run_benchmark "disk"
        ;;
    "network")
        run_benchmark "network"
        ;;
esac

# ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
cat > ${RESULTS_DIR}/benchmark-report.md <<EOF
# Kubernetesæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•ä¿¡æ¯
- æµ‹è¯•æ—¶é—´: $(date)
- æµ‹è¯•å¥—ä»¶: ${TEST_SUITE}
- Kubernetesç‰ˆæœ¬: $(kubectl version --short | grep Server | awk '{print $3}')

## æµ‹è¯•ç»“æœæ‘˜è¦

### CPUæ€§èƒ½
$(cat ${RESULTS_DIR}/cpu-results.txt 2>/dev/null || echo "æ— æ•°æ®")

### å†…å­˜æ€§èƒ½
$(cat ${RESULTS_DIR}/memory-results.txt 2>/dev/null || echo "æ— æ•°æ®")

### ç£ç›˜IOæ€§èƒ½
$(cat ${RESULTS_DIR}/disk-results.txt 2>/dev/null || echo "æ— æ•°æ®")

### ç½‘ç»œæ€§èƒ½
$(cat ${RESULTS_DIR}/network-results.txt 2>/dev/null || echo "æ— æ•°æ®")

## å»ºè®®ä¼˜åŒ–æªæ–½
- æ ¹æ®æµ‹è¯•ç»“æœè°ƒæ•´èµ„æºé…ç½®
- ä¼˜åŒ–åº”ç”¨æ€§èƒ½å‚æ•°
- è€ƒè™‘ç¡¬ä»¶å‡çº§éœ€æ±‚
EOF

echo "åŸºå‡†æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: ${RESULTS_DIR}"
ls -la ${RESULTS_DIR}
```

### 7.3 æŒç»­æ€§èƒ½ç›‘æ§

```yaml
# ========== æŒç»­æ€§èƒ½ç›‘æ§é…ç½® ==========
apiVersion: batch/v1
kind: CronJob
metadata:
  name: performance-monitoring
  namespace: monitoring
spec:
  schedule: "*/30 * * * *"  # æ¯30åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: perf-collector
            image: perf-tools:latest
            command:
            - /scripts/collect-performance-metrics.sh
            env:
            - name: SLACK_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: monitoring-secrets
                  key: slack-webhook-url
            volumeMounts:
            - name: scripts
              mountPath: /scripts
          volumes:
          - name: scripts
            configMap:
              name: perf-scripts
          restartPolicy: OnFailure

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: perf-scripts
  namespace: monitoring
data:
  collect-performance-metrics.sh: |
    #!/bin/bash
    set -euo pipefail
    
    # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
    COLLECT_TIME=$(date -Iseconds)
    
    # CPUä½¿ç”¨ç‡
    CPU_USAGE=$(kubectl top nodes | awk 'NR>1 {sum+=$3} END {print sum/NR}')
    
    # å†…å­˜ä½¿ç”¨ç‡
    MEM_USAGE=$(kubectl top nodes | awk 'NR>1 {sum+=$5} END {print sum/NR}')
    
    # API Serverå»¶è¿Ÿ
    API_LATENCY=$(kubectl get --raw /metrics | grep apiserver_request_duration_seconds | \
      awk '/quantile="0.99"/ {print $2}' | head -1)
    
    # ç”ŸæˆæŠ¥å‘Š
    cat <<REPORT
    {
      "timestamp": "${COLLECT_TIME}",
      "cpu_usage_percent": ${CPU_USAGE},
      "memory_usage_percent": ${MEM_USAGE},
      "api_server_latency_99th_ms": ${API_LATENCY},
      "cluster_health": "$(if (( $(echo "${CPU_USAGE} < 80" | bc -l) )) && (( $(echo "${MEM_USAGE} < 85" | bc -l) )); then echo "healthy"; else echo "warning"; fi)"
    }
    REPORT
```

---
## 8. æ€§èƒ½ä¼˜åŒ–å®æˆ˜æ¡ˆä¾‹åº“

### 8.1 å¤§è§„æ¨¡é›†ç¾¤æ€§èƒ½ä¼˜åŒ–æ¡ˆä¾‹

#### æ¡ˆä¾‹1: API Serveræ€§èƒ½ç“¶é¢ˆçªç ´
```markdown
**ä¼˜åŒ–èƒŒæ™¯**: 
ä¸‡çº§èŠ‚ç‚¹é›†ç¾¤API Server QPSè¾¾åˆ°2000+æ—¶å‡ºç°æ˜æ˜¾å»¶è¿Ÿï¼Œå½±å“é›†ç¾¤ç®¡ç†æ•ˆç‡

**é—®é¢˜è¯Šæ–­**:
```bash
# ç›‘æ§API Serveræ€§èƒ½æŒ‡æ ‡
kubectl get --raw /metrics | grep apiserver_request_duration_seconds | \
  awk '/quantile="0.99"/ {print $2}'
# å‘ç°99th percentileå»¶è¿Ÿè¾¾åˆ°2.5ç§’

# åˆ†æè¯·æ±‚ç±»å‹åˆ†å¸ƒ
kubectl get --raw /metrics | grep apiserver_request_total | \
  awk '{print $1}' | cut -d'_' -f4- | sort | uniq -c | sort -nr
# å‘ç°list/watchè¯·æ±‚å æ¯”è¾ƒé«˜

# æ£€æŸ¥etcdæ€§èƒ½
kubectl exec -n kube-system etcd-master1 -- etcdctl check perf
# å‘ç°etcdå†™å…¥å»¶è¿Ÿè¾ƒé«˜
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:

1. **API Serverè°ƒä¼˜**:
```yaml
# kube-apiserverä¼˜åŒ–é…ç½®
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
spec:
  containers:
  - name: kube-apiserver
    command:
    - kube-apiserver
    - --max-requests-inflight=1200
    - --max-mutating-requests-inflight=600
    - --request-timeout=2m
    - --min-request-timeout=1800
    - --enable-aggregator-routing=true
    - --storage-backend=etcd3
    - --etcd-servers=https://etcd1:2379,https://etcd2:2379,https://etcd3:2379
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --profiling=false  # ç”Ÿäº§ç¯å¢ƒå…³é—­æ€§èƒ½åˆ†æ
```

2. **etcdä¼˜åŒ–**:
```bash
# etcdæ€§èƒ½è°ƒä¼˜å‚æ•°
ETCD_HEARTBEAT_INTERVAL=100
ETCD_ELECTION_TIMEOUT=1000
ETCD_QUOTA_BACKEND_BYTES=8589934592  # 8GB
ETCD_AUTO_COMPACTION_RETENTION=1
ETCD_AUTO_COMPACTION_MODE=periodic
ETCD_SNAPSHOT_COUNT=10000
```

3. **å®¢æˆ·ç«¯ä¼˜åŒ–**:
```go
// Goå®¢æˆ·ç«¯è¿æ¥æ± ä¼˜åŒ–
config := &rest.Config{
    Host: "https://kubernetes:6443",
    QPS:   100,     // æ¯ç§’æŸ¥è¯¢é€Ÿç‡
    Burst: 200,     // çªå‘æŸ¥è¯¢æ•°
}
clientset, err := kubernetes.NewForConfig(config)
```

**ä¼˜åŒ–æ•ˆæœ**:
- API Server 99thå»¶è¿Ÿä»2.5ç§’é™è‡³0.8ç§’
- QPSæ‰¿è½½èƒ½åŠ›æå‡è‡³5000+
- etcdå†™å…¥å»¶è¿Ÿé™ä½60%
```

#### æ¡ˆä¾‹2: èŠ‚ç‚¹èµ„æºåˆ©ç”¨ç‡ä¼˜åŒ–
```markdown
**ä¼˜åŒ–èƒŒæ™¯**: 
é›†ç¾¤èŠ‚ç‚¹CPUå¹³å‡ä½¿ç”¨ç‡ä»…35%ï¼Œå†…å­˜ä½¿ç”¨ç‡45%ï¼Œèµ„æºæµªè´¹ä¸¥é‡

**é—®é¢˜åˆ†æ**:
```bash
# åˆ†æèŠ‚ç‚¹èµ„æºåˆ†å¸ƒ
kubectl top nodes | awk 'NR>1 {print $1,$3,$5}' | \
  awk '{cpu[$1]=$2; mem[$1]=$3} END {
    for(node in cpu) {
      print node, cpu[node], mem[node]
    }
  }' | sort -k2 -n

# æ£€æŸ¥Podèµ„æºè¯·æ±‚è®¾ç½®
kubectl get pods -A -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].resources.requests.cpu}{"\t"}{.spec.containers[*].resources.limits.cpu}{"\n"}{end}' | \
  head -20

# åˆ†æèµ„æºç¢ç‰‡åŒ–ç¨‹åº¦
kubectl describe nodes | grep -E "(Allocated|Requests)" | \
  awk '{print $2,$4}' | sort | uniq -c
```

**ä¼˜åŒ–ç­–ç•¥**:

1. **å‚ç›´Podè‡ªåŠ¨æ‰©ç¼©å®¹(VPA)**:
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa-optimizer
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: high-traffic-app
  updatePolicy:
    updateMode: "Initial"  # é¦–æ¬¡ä¼˜åŒ–åæ‰‹åŠ¨è°ƒæ•´
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2000m
        memory: 4Gi
```

2. **æ°´å¹³Podè‡ªåŠ¨æ‰©ç¼©å®¹ä¼˜åŒ–**:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: smart-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: traffic-sensitive-app
  minReplicas: 3
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60  # é™ä½è§¦å‘é˜ˆå€¼
  - type: External
    external:
      metric:
        name: custom.business.metric
      target:
        type: Value
        value: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        value: 4
        periodSeconds: 60
```

3. **èŠ‚ç‚¹èµ„æºä¼˜åŒ–è„šæœ¬**:
```bash
#!/bin/bash
# èŠ‚ç‚¹èµ„æºä¼˜åŒ–è‡ªåŠ¨åŒ–è„šæœ¬

optimize_node_resources() {
    local node_name=$1
    
    # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
    node_info=$(kubectl describe node ${node_name})
    
    # è®¡ç®—èµ„æºä½¿ç”¨ç‡
    allocatable_cpu=$(echo "${node_info}" | grep "cpu " | awk '{print $2}')
    allocatable_memory=$(echo "${node_info}" | grep "memory " | awk '{print $2}')
    
    # åˆ†æPodèµ„æºè¯·æ±‚
    pod_requests=$(kubectl get pods -o jsonpath='{range .items[?(@.spec.nodeName=="'${node_name}'")]}{.spec.containers[*].resources.requests.cpu}{"\n"}{end}' | \
                   awk '{sum+=$1} END {print sum}')
    
    # è®¡ç®—ä¼˜åŒ–å»ºè®®
    cpu_utilization=$(echo "scale=2; ${pod_requests}/${allocatable_cpu}*100" | bc)
    
    if (( $(echo "${cpu_utilization} < 40" | bc -l) )); then
        echo "èŠ‚ç‚¹${node_name} CPUåˆ©ç”¨ç‡åä½(${cpu_utilization}%)ï¼Œå»ºè®®ä¼˜åŒ–"
        # å®æ–½ä¼˜åŒ–æªæ–½
        optimize_workload_distribution ${node_name}
    fi
}

optimize_workload_distribution() {
    local node_name=$1
    
    # é‡æ–°å¹³è¡¡Podåˆ†å¸ƒ
    kubectl patch deployment app-deployment -p '{
        "spec": {
            "template": {
                "spec": {
                    "affinity": {
                        "podAntiAffinity": {
                            "preferredDuringSchedulingIgnoredDuringExecution": [
                                {
                                    "weight": 100,
                                    "podAffinityTerm": {
                                        "labelSelector": {
                                            "matchExpressions": [
                                                {
                                                    "key": "app",
                                                    "operator": "In",
                                                    "values": ["high-cpu-app"]
                                                }
                                            ]
                                        },
                                        "topologyKey": "kubernetes.io/hostname"
                                    }
                                }
                            ]
                        }
                    }
                }
            }
        }
    }'
}
```

**ä¼˜åŒ–æˆæœ**:
- èŠ‚ç‚¹CPUå¹³å‡åˆ©ç”¨ç‡æå‡è‡³65%
- å†…å­˜åˆ©ç”¨ç‡æå‡è‡³70%
- é›†ç¾¤æ•´ä½“èµ„æºæˆæœ¬é™ä½30%
- åº”ç”¨æ€§èƒ½ç¨³å®šæ€§æ˜¾è‘—æ”¹å–„

### 8.2 æ€§èƒ½ç›‘æ§æœ€ä½³å®è·µ

#### æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ç›‘æ§ä½“ç³»

| ç›‘æ§ç»´åº¦ | å…³é”®æŒ‡æ ‡ | å‘Šè­¦é˜ˆå€¼ | æ£€æµ‹é¢‘ç‡ | å“åº”ç­–ç•¥ |
|---------|---------|---------|---------|---------|
| **API Server** | QPSã€99thå»¶è¿Ÿã€é”™è¯¯ç‡ | QPS>3000ã€å»¶è¿Ÿ>1sã€é”™è¯¯ç‡>1% | 30ç§’ | è‡ªåŠ¨æ‰©å®¹ã€é™æµé™çº§ |
| **etcd** | WALå»¶è¿Ÿã€fsyncæ—¶é—´ã€å­˜å‚¨ä½¿ç”¨ç‡ | WAL>100msã€fsync>50msã€ä½¿ç”¨ç‡>80% | 15ç§’ | å­˜å‚¨æ‰©å®¹ã€æ€§èƒ½è°ƒä¼˜ |
| **èŠ‚ç‚¹èµ„æº** | CPUä½¿ç”¨ç‡ã€å†…å­˜ä½¿ç”¨ç‡ã€ç£ç›˜IO | CPU>85%ã€å†…å­˜>90%ã€IOç­‰å¾…>30% | 60ç§’ | èµ„æºè°ƒåº¦ã€èŠ‚ç‚¹æ‰©å®¹ |
| **ç½‘ç»œæ€§èƒ½** | å¸¦å®½åˆ©ç”¨ç‡ã€ä¸¢åŒ…ç‡ã€å»¶è¿Ÿ | åˆ©ç”¨ç‡>70%ã€ä¸¢åŒ…>0.1%ã€å»¶è¿Ÿ>100ms | 30ç§’ | ç½‘ç»œä¼˜åŒ–ã€è´Ÿè½½å‡è¡¡ |
| **åº”ç”¨æ€§èƒ½** | å“åº”æ—¶é—´ã€ååé‡ã€é”™è¯¯ç‡ | RT>500msã€QPSä¸‹é™30%ã€é”™è¯¯ç‡>0.5% | å®æ—¶ | è‡ªåŠ¨æ‰©ç¼©å®¹ã€æ•…éšœè½¬ç§» |

#### æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶

```yaml
# æ€§èƒ½åŸºå‡†æµ‹è¯•é…ç½®
apiVersion: batch/v1
kind: Job
metadata:
  name: performance-benchmark
spec:
  template:
    spec:
      containers:
      - name: benchmark
        image: k8s.gcr.io/benchmark-runner:latest
        command:
        - /benchmark
        - --duration=300s
        - --concurrency=100
        - --target=qps-test
        - --metrics-output=prometheus
        env:
        - name: TARGET_SERVICE
          value: "http://test-app.production.svc.cluster.local"
        - name: PROMETHEUS_ENDPOINT
          value: "http://prometheus.monitoring.svc:9090"
      restartPolicy: Never
  backoffLimit: 3
```

#### æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

- [ ] å®šæœŸå®¡æŸ¥èµ„æºè¯·æ±‚å’Œé™åˆ¶è®¾ç½®
- [ ] ç›‘æ§å¹¶ä¼˜åŒ–Podåˆ†å¸ƒç­–ç•¥
- [ ] å®æ–½åˆé€‚çš„è‡ªåŠ¨æ‰©ç¼©å®¹ç­–ç•¥
- [ ] ä¼˜åŒ–å­˜å‚¨IOæ€§èƒ½é…ç½®
- [ ] è°ƒæ•´ç½‘ç»œæ’ä»¶å‚æ•°
- [ ] å®šæœŸæ¸…ç†æ— ç”¨èµ„æº
- [ ] ä¼˜åŒ–é•œåƒæ‹‰å–ç­–ç•¥
- [ ] å®æ–½æœ‰æ•ˆçš„ç¼“å­˜ç­–ç•¥

---

## 8. é«˜çº§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### 8.1 å†…æ ¸çº§æ€§èƒ½è°ƒä¼˜

#### Linuxå†…æ ¸å‚æ•°ä¼˜åŒ–çŸ©é˜µ
| å‚æ•°ç±»åˆ« | å‚æ•°åç§° | æ¨èå€¼ | ä½œç”¨è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|---------|---------|--------|----------|----------|
| **ç½‘ç»œæ ˆä¼˜åŒ–** | net.core.somaxconn | 65535 | å¢å¤§TCPè¿æ¥é˜Ÿåˆ— | é«˜å¹¶å‘æœåŠ¡ |
| **ç½‘ç»œæ ˆä¼˜åŒ–** | net.ipv4.tcp_fin_timeout | 30 | ç¼©çŸ­FIN_WAITè¶…æ—¶ | çŸ­è¿æ¥åœºæ™¯ |
| **ç½‘ç»œæ ˆä¼˜åŒ–** | net.ipv4.tcp_tw_reuse | 1 | å…è®¸TIME_WAITé‡ç”¨ | é«˜é¢‘çŸ­è¿æ¥ |
| **å†…å­˜ç®¡ç†** | vm.swappiness | 1 | é™ä½äº¤æ¢å€¾å‘ | å†…å­˜å……è¶³ç¯å¢ƒ |
| **å†…å­˜ç®¡ç†** | vm.dirty_ratio | 15 | è°ƒæ•´è„é¡µæ¯”ä¾‹ | å†™å¯†é›†å‹åº”ç”¨ |
| **æ–‡ä»¶ç³»ç»Ÿ** | fs.file-max | 2097152 | å¢å¤§æ–‡ä»¶å¥æŸ„é™åˆ¶ | é«˜å¹¶å‘æ–‡ä»¶æ“ä½œ |
| **æ–‡ä»¶ç³»ç»Ÿ** | fs.inotify.max_user_watches | 1048576 | å¢å¤§æ–‡ä»¶ç›‘å¬é™åˆ¶ | å¤§é‡æ–‡ä»¶ç›‘æ§ |

#### å†…æ ¸è°ƒä¼˜å®æ–½è„šæœ¬
```bash
#!/bin/bash
# ========== ç”Ÿäº§ç¯å¢ƒå†…æ ¸æ€§èƒ½è°ƒä¼˜è„šæœ¬ ==========
set -euo pipefail

# å¤‡ä»½åŸå§‹é…ç½®
cp /etc/sysctl.conf /etc/sysctl.conf.backup.$(date +%Y%m%d)

# ç½‘ç»œæ€§èƒ½ä¼˜åŒ–
cat >> /etc/sysctl.conf << 'EOF'

# ===== ç½‘ç»œæ€§èƒ½è°ƒä¼˜ =====
# å¢å¤§TCPè¿æ¥é˜Ÿåˆ—
net.core.somaxconn = 65535
net.core.netdev_max_backlog = 5000

# TCPçª—å£å’Œç¼“å†²åŒºä¼˜åŒ–
net.core.rmem_default = 262144
net.core.wmem_default = 262144
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216

# TCPæ‹¥å¡æ§åˆ¶ç®—æ³•
net.ipv4.tcp_congestion_control = bbr
net.ipv4.tcp_allowed_congestion_control = bbr cubic reno

# è¿æ¥å¤ç”¨å’Œè¶…æ—¶ä¼˜åŒ–
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_keepalive_time = 1200

# ===== å†…å­˜ç®¡ç†ä¼˜åŒ– =====
vm.swappiness = 1
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
vm.overcommit_memory = 1
vm.overcommit_ratio = 100

# ===== æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ– =====
fs.file-max = 2097152
fs.inotify.max_user_watches = 1048576
fs.inotify.max_user_instances = 8192

# ===== ç½‘ç»œå®‰å…¨ä¼˜åŒ– =====
net.ipv4.tcp_syncookies = 1
net.ipv4.ip_forward = 1
EOF

# åº”ç”¨é…ç½®
sysctl -p

echo "âœ… å†…æ ¸æ€§èƒ½è°ƒä¼˜å®Œæˆ"
echo "ğŸ“‹ å»ºè®®é‡å¯ç³»ç»Ÿä½¿æ‰€æœ‰ä¼˜åŒ–ç”Ÿæ•ˆ"
```

### 8.2 å®¹å™¨è¿è¡Œæ—¶æ€§èƒ½ä¼˜åŒ–

#### Containerdé«˜çº§é…ç½®ä¼˜åŒ–
```toml
# ========== Containerdæ€§èƒ½ä¼˜åŒ–é…ç½® ==========
version = 2

[plugins."io.containerd.grpc.v1.cri"]
  # é•œåƒæ‹‰å–ä¼˜åŒ–
  [plugins."io.containerd.grpc.v1.cri".registry]
    config_path = "/etc/containerd/certs.d"
    
  [plugins."io.containerd.grpc.v1.cri".containerd]
    # ä½¿ç”¨overlayfså¿«ç…§å™¨è·å¾—æ›´å¥½æ€§èƒ½
    snapshotter = "overlayfs"
    default_runtime_name = "runc"
    
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
      runtime_type = "io.containerd.runc.v2"
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
        # å¯ç”¨systemd cgroupé©±åŠ¨
        SystemdCgroup = true
        # å¯ç”¨ç‰¹æƒæ¨¡å¼ä¼˜åŒ–
        NoPivotRoot = false
        
  # é•œåƒåƒåœ¾å›æ”¶ä¼˜åŒ–
  [plugins."io.containerd.grpc.v1.cri".image_decryption]
    key_model = "node"

[plugins."io.containerd.internal.v1.opt"]
  path = "/opt/containerd"

[plugins."io.containerd.grpc.v1.cri".cni]
  bin_dir = "/opt/cni/bin"
  conf_dir = "/etc/cni/net.d"

# æ€§èƒ½ç›¸å…³çš„å…¨å±€é…ç½®
[grpc]
  address = "/run/containerd/containerd.sock"
  # å¢å¤§gRPCæœ€å¤§æ¶ˆæ¯å¤§å°
  max_recv_message_size = 16777216
  max_send_message_size = 16777216

[debug]
  # ç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­debug
  level = "info"
```

#### Docker Engineæ€§èƒ½è°ƒä¼˜
```json
{
  "experimental": false,
  "features": {
    "buildkit": true
  },
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "default-ulimits": {
    "nofile": {
      "Name": "nofile",
      "Hard": 65536,
      "Soft": 65536
    }
  },
  "live-restore": true,
  "iptables": false,
  "ip-forward": true,
  "userland-proxy": false,
  "userns-remap": "default"
}
```

### 8.3 å¾®æœåŠ¡æ€§èƒ½ä¼˜åŒ–æ¨¡å¼

#### æœåŠ¡ç½‘æ ¼æ€§èƒ½ä¼˜åŒ–
```yaml
# ========== Istioæ€§èƒ½ä¼˜åŒ–é…ç½® ==========
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-performance-optimized
spec:
  components:
    pilot:
      k8s:
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        env:
        - name: PILOT_PUSH_THROTTLE
          value: "100"
        - name: PILOT_TRACE_SAMPLING
          value: "1.0"
          
    ingressGateways:
    - name: istio-ingressgateway
      enabled: true
      k8s:
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 2000m
            memory: 2Gi
        service:
          ports:
          - port: 80
            targetPort: 8080
            name: http2
          - port: 443
            targetPort: 8443
            name: https

  values:
    global:
      proxy:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 1Gi
            
    pilot:
      autoscaleEnabled: true
      autoscaleMin: 2
      autoscaleMax: 10
      cpu:
        targetAverageUtilization: 80
        
    gateways:
      istio-ingressgateway:
        autoscaleEnabled: true
        autoscaleMin: 2
        autoscaleMax: 10
        cpu:
          targetAverageUtilization: 80
          
    telemetry:
      v2:
        prometheus:
          enabled: true
          configOverride:
            inboundSidecar:
              debug: false
              stat_prefix: istio
```

#### gRPCæœåŠ¡æ€§èƒ½ä¼˜åŒ–é…ç½®
```yaml
# ========== gRPCæœåŠ¡æ€§èƒ½ä¼˜åŒ– ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grpc-service-optimized
spec:
  replicas: 3
  selector:
    matchLabels:
      app: grpc-service
  template:
    metadata:
      labels:
        app: grpc-service
    spec:
      containers:
      - name: grpc-server
        image: grpc-service:latest
        ports:
        - containerPort: 50051
          name: grpc
        env:
        # gRPCæ€§èƒ½ä¼˜åŒ–å‚æ•°
        - name: GRPC_SERVER_KEEPALIVE_TIME_MS
          value: "600000"  # 10åˆ†é’Ÿ
        - name: GRPC_SERVER_KEEPALIVE_TIMEOUT_MS
          value: "20000"   # 20ç§’
        - name: GRPC_SERVER_MAX_CONNECTION_IDLE_MS
          value: "300000"  # 5åˆ†é’Ÿ
        - name: GRPC_SERVER_MAX_CONCURRENT_STREAMS
          value: "1000"
        - name: GRPC_SERVER_HTTP2_MAX_PINGS_WITHOUT_DATA
          value: "0"
        - name: GRPC_SERVER_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_MS
          value: "300000"  # 5åˆ†é’Ÿ
          
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 2000m
            memory: 2Gi
            
        # å¥åº·æ£€æŸ¥ä¼˜åŒ–
        livenessProbe:
          grpc:
            port: 50051
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          
        readinessProbe:
          grpc:
            port: 50051
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
```

### 8.4 æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦æœ€ä½³å®è·µ

#### Prometheusé«˜çº§æŸ¥è¯¢ä¼˜åŒ–
```yaml
# ========== Prometheusæ€§èƒ½ä¼˜åŒ–é…ç½® ==========
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  
# æŸ¥è¯¢ä¼˜åŒ–é…ç½®
query:
  max_concurrency: 20
  timeout: 2m
  lookback_delta: 5m
  
# å­˜å‚¨ä¼˜åŒ–
storage:
  tsdb:
    retention.time: 15d
    wal-compression: true
    # å¢å¤§æ‰¹é‡å†™å…¥å¤§å°
    out_of_order_time_window: 30m
    
rule_files:
  - "performance.rules.yml"

alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - alertmanager.monitoring.svc:9093
```

#### å…³é”®æ€§èƒ½æŒ‡æ ‡å‘Šè­¦è§„åˆ™
```yaml
# ========== æ€§èƒ½å‘Šè­¦è§„åˆ™ ==========
groups:
- name: performance.alerts
  rules:
  # API Serveræ€§èƒ½å‘Šè­¦
  - alert: APIServerHighLatency
    expr: histogram_quantile(0.99, rate(apiserver_request_duration_seconds_bucket[5m])) > 1
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "API Server 99th percentileå»¶è¿Ÿè¶…è¿‡1ç§’"
      description: "å½“å‰å»¶è¿Ÿ: {{ $value }}ç§’ï¼Œå¯èƒ½å½±å“é›†ç¾¤æ“ä½œ"
      
  # etcdæ€§èƒ½å‘Šè­¦
  - alert: EtcdHighWalFsyncDuration
    expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) > 0.1
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "etcd WAL fsyncå»¶è¿Ÿè¿‡é«˜"
      description: "å½“å‰fsyncå»¶è¿Ÿ: {{ $value }}ç§’ï¼Œå¯èƒ½å½±å“æ•°æ®æŒä¹…åŒ–"
      
  # èŠ‚ç‚¹èµ„æºå‘Šè­¦
  - alert: NodeHighCPUUsage
    expr: (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) * 100 > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "èŠ‚ç‚¹CPUä½¿ç”¨ç‡è¿‡é«˜"
      description: "èŠ‚ç‚¹ {{ $labels.instance }} CPUä½¿ç”¨ç‡è¾¾åˆ° {{ $value }}%"
      
  # ç½‘ç»œæ€§èƒ½å‘Šè­¦
  - alert: HighNetworkPacketLoss
    expr: rate(node_network_receive_drop_total[5m]) / rate(node_network_receive_packets_total[5m]) > 0.001
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: "ç½‘ç»œä¸¢åŒ…ç‡å¼‚å¸¸"
      description: "èŠ‚ç‚¹ {{ $labels.instance }} ç½‘ç»œä¸¢åŒ…ç‡: {{ $value }}"
      
  # åº”ç”¨æ€§èƒ½å‘Šè­¦
  - alert: ApplicationHighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "åº”ç”¨é”™è¯¯ç‡è¿‡é«˜"
      description: "æœåŠ¡ {{ $labels.job }} é”™è¯¯ç‡è¾¾åˆ° {{ $value | humanizePercentage }}"
```

---

**è¡¨æ ¼åº•éƒ¨æ ‡è®°**: Kusheet Project | ä½œè€…: Allen Galler (allengaller@gmail.com) | æœ€åæ›´æ–°: 2026-02 | ç‰ˆæœ¬: v1.25-v1.32 | è´¨é‡ç­‰çº§: â­â­â­â­â­ ä¸“å®¶çº§
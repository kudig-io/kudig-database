# 13 - å®¹é‡è§„åˆ’ä¸èµ„æºé¢„æµ‹

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25-v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä½œè€…**: Allen Galler | **è´¨é‡ç­‰çº§**: â­â­â­â­â­ ä¸“å®¶çº§

> **ç”Ÿäº§ç¯å¢ƒå®æˆ˜ç»éªŒæ€»ç»“**: åŸºäºä¸‡çº§èŠ‚ç‚¹é›†ç¾¤å®¹é‡ç®¡ç†ç»éªŒï¼Œæ¶µç›–ä»èµ„æºé¢„æµ‹åˆ°æˆæœ¬ä¼˜åŒ–çš„å…¨æ–¹ä½æœ€ä½³å®è·µ

---

## çŸ¥è¯†åœ°å›¾

| å±æ€§ | è¯´æ˜ |
|------|------|
| **æ–‡ä»¶è§’è‰²** | å®¹é‡è§„åˆ’ä¸èµ„æºé¢„æµ‹ â€” K8sé›†ç¾¤èµ„æºç®¡ç†çš„å®Œæ•´æ–¹æ³•è®º |
| **é€‚åˆè¯»è€…** | è¿ç»´(èµ„æºç®¡ç†) â†’ å¹³å°å·¥ç¨‹(å®¹é‡è§„åˆ’) â†’ CTO/VP(æˆæœ¬å†³ç­–) |
| **å‰ç½®çŸ¥è¯†** | 01(è¿ç»´å®è·µ)ã€03(æ€§èƒ½è°ƒä¼˜) |
| **å…³è”æ–‡ä»¶** | 03(æ€§èƒ½)ã€10(å¤šäº‘æˆæœ¬)ã€15(SLI/SLO) |

---

## ç›®å½•

- [1. å®¹é‡è§„åˆ’æ¡†æ¶](#1-å®¹é‡è§„åˆ’æ¡†æ¶)
- [2. èµ„æºä½¿ç”¨åˆ†æ](#2-èµ„æºä½¿ç”¨åˆ†æ)
- [3. å®¹é‡é¢„æµ‹æ¨¡å‹](#3-å®¹é‡é¢„æµ‹æ¨¡å‹)
- [4. é›†ç¾¤æ‰©å®¹ç­–ç•¥](#4-é›†ç¾¤æ‰©å®¹ç­–ç•¥)
- [5. èµ„æºé…é¢ç®¡ç†](#5-èµ„æºé…é¢ç®¡ç†)
- [6. æˆæœ¬ä¼˜åŒ–å®è·µ](#6-æˆæœ¬ä¼˜åŒ–å®è·µ)
- [7. å®¹é‡ç›‘æ§ä¸å‘Šè­¦](#7-å®¹é‡ç›‘æ§ä¸å‘Šè­¦)
- [8. å®æˆ˜æ¡ˆä¾‹åˆ†æ](#8-å®æˆ˜æ¡ˆä¾‹åˆ†æ)

---

## 1. å®¹é‡è§„åˆ’æ¡†æ¶

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: å®¹é‡è§„åˆ’å°±æ˜¯å›ç­”"æˆ‘éœ€è¦å¤šå°‘èµ„æº"å’Œ"ä»€ä¹ˆæ—¶å€™éœ€è¦æ‰©å®¹"ã€‚å°±åƒåŸå¸‚è§„åˆ’å¸ˆé¢„æµ‹æœªæ¥5å¹´çš„ç”¨æ°´ç”¨ç”µéœ€æ±‚ä¸€æ ·â€”â€”æ—¢ä¸èƒ½æµªè´¹(è¿‡åº¦é…ç½®)ï¼Œä¹Ÿä¸èƒ½ä¸å¤Ÿ(å½±å“ä¸šåŠ¡)ã€‚

### 1.1 å®¹é‡è§„åˆ’ç”Ÿå‘½å‘¨æœŸ

```mermaid
graph LR
    A[éœ€æ±‚æ”¶é›†] --> B[å½“å‰çŠ¶æ€è¯„ä¼°]
    B --> C[å®¹é‡é¢„æµ‹]
    C --> D[è§„åˆ’æ–¹æ¡ˆåˆ¶å®š]
    D --> E[é¢„ç®—å®¡æ‰¹]
    E --> F[èµ„æºé‡‡è´­]
    F --> G[éƒ¨ç½²å®æ–½]
    G --> H[æŒç»­ç›‘æ§]
    H --> I{å®¹é‡å……è¶³?}
    I -->|å¦| A
    I -->|æ˜¯| H
    
    subgraph è§„åˆ’å‘¨æœŸ
        A
        B
        C
        D
    end
    
    subgraph æ‰§è¡Œå‘¨æœŸ
        E
        F
        G
    end
    
    subgraph è¿è¥å‘¨æœŸ
        H
        I
    end
```

### 1.2 å®¹é‡è§„åˆ’ç»´åº¦

| ç»´åº¦ | å…³é”®æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯„ä¼°å‘¨æœŸ | æ•°æ®æ¥æº |
|------|----------|--------|----------|----------|
| **è®¡ç®—èµ„æº** | CPU åˆ©ç”¨ç‡ã€å¯è°ƒåº¦å®¹é‡ | 60-75% | æ¯å‘¨ | Prometheusã€Metrics Server |
| **å†…å­˜èµ„æº** | å†…å­˜åˆ©ç”¨ç‡ã€OOM é¢‘ç‡ | 65-80% | æ¯å‘¨ | cAdvisorã€Node Exporter |
| **å­˜å‚¨èµ„æº** | ç£ç›˜ä½¿ç”¨ç‡ã€IOPSã€å¸¦å®½ | <80% | æ¯å¤© | CSI Metricsã€Node Exporter |
| **ç½‘ç»œèµ„æº** | å¸¦å®½ä½¿ç”¨ç‡ã€PPSã€è¿æ¥æ•° | <70% | æ¯å¤© | CNI Metricsã€Node Exporter |
| **Pod å¯†åº¦** | æ¯èŠ‚ç‚¹ Pod æ•°ã€è°ƒåº¦å¤±è´¥ç‡ | <80% max-pods | æ¯å‘¨ | Kube-Scheduler Metrics |
| **etcd å®¹é‡** | DB å¤§å°ã€è¯·æ±‚å»¶è¿Ÿã€ååé‡ | <8GB, <100ms | æ¯å¤© | etcd Metrics |

### 1.3 å®¹é‡è§„åˆ’æ–¹æ³•è®º

#### æ·±å…¥ç†è§£ï¼šå®¹é‡è§„åˆ’æ–¹æ³•è®º

**å®¹é‡è§„åˆ’çš„æ ¸å¿ƒç†å¿µ**ï¼šå°±åƒåŸå¸‚ç”¨æ°´è§„åˆ’ä¸€æ ·â€”â€”è§„åˆ’éƒ¨é—¨éœ€è¦æ ¹æ®äººå£å¢é•¿ã€å­£èŠ‚å˜åŒ–ã€å·¥ä¸šå‘å±•é¢„æµ‹æœªæ¥çš„ç”¨æ°´éœ€æ±‚ã€‚Kubernetes å®¹é‡è§„åˆ’ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œéœ€è¦é¢„æµ‹ä¸šåŠ¡å¢é•¿ã€æµé‡æ³¢åŠ¨ã€èµ„æºæ¶ˆè€—è¶‹åŠ¿ã€‚

**å››å¤§é¢„æµ‹æ–¹æ³•å¯¹æ¯”**ï¼š

| æ–¹æ³• | ç±»æ¯” | é€‚ç”¨åœºæ™¯ | å‡†ç¡®åº¦ | æ•°æ®è¦æ±‚ |
|------|------|----------|--------|----------|
| **å†å²è¶‹åŠ¿åˆ†æ** | æ ¹æ®è¿‡å»5å¹´äººå£å¢é•¿è¶‹åŠ¿é¢„æµ‹æœªæ¥ | ç¨³å®šå¢é•¿çš„ä¸šåŠ¡ | 70-85% | è‡³å°‘6ä¸ªæœˆæ•°æ® |
| **ä¸šåŠ¡é©±åŠ¨è§„åˆ’** | æ ¹æ®æ–°å·¥å‚å»ºè®¾è®¡åˆ’é¢„æµ‹å·¥ä¸šç”¨æ°´ | æ–°ä¸šåŠ¡ä¸Šçº¿ã€å¤§ä¿ƒ | 60-75% | ä¸šåŠ¡æŒ‡æ ‡ |
| **è´Ÿè½½æµ‹è¯•éªŒè¯** | åœ¨å®éªŒå®¤æ¨¡æ‹Ÿç”¨æ°´é«˜å³° | å…³é”®ç³»ç»Ÿã€é«˜å¹¶å‘ | 85-95% | æµ‹è¯•ç¯å¢ƒ |
| **æœºå™¨å­¦ä¹ é¢„æµ‹** | AI åˆ†æå¤æ‚çš„ç”¨æ°´æ¨¡å¼ | å¤§è§„æ¨¡å¤æ‚åœºæ™¯ | 80-90% | è‡³å°‘12ä¸ªæœˆæ•°æ® |

**æ–¹æ³•1ï¼šå†å²è¶‹åŠ¿åˆ†æ** â€” ç±»æ¯”å¤©æ°”é¢„æŠ¥

å°±åƒå¤©æ°”é¢„æŠ¥æ ¹æ®è¿‡å»çš„æ°”è±¡æ•°æ®é¢„æµ‹æœªæ¥å¤©æ°”ï¼Œå†å²è¶‹åŠ¿åˆ†æé€šè¿‡ Prometheus é‡‡é›†è¿‡å» 6-12 ä¸ªæœˆçš„èµ„æºä½¿ç”¨æ•°æ®ï¼Œç»˜åˆ¶è¶‹åŠ¿çº¿ï¼Œé¢„æµ‹æœªæ¥ 3-6 ä¸ªæœˆçš„èµ„æºéœ€æ±‚ã€‚

**å…³é”®æ­¥éª¤**ï¼š
1. é‡‡é›†å†å²æ•°æ®ï¼šCPUã€å†…å­˜ã€å­˜å‚¨ã€ç½‘ç»œä½¿ç”¨é‡
2. æ¸…æ´—å¼‚å¸¸æ•°æ®ï¼šå»é™¤çªå‘äº‹ä»¶ï¼ˆå¦‚æ”»å‡»ã€æ•…éšœï¼‰å¯¼è‡´çš„æ•°æ®å°–å³°
3. ç»˜åˆ¶è¶‹åŠ¿çº¿ï¼šä½¿ç”¨çº¿æ€§å›å½’æˆ–å¤šé¡¹å¼æ‹Ÿåˆ
4. åŠ å…¥å®‰å…¨ç³»æ•°ï¼šé¢„ç•™ 20-30% bufferï¼ˆå°±åƒå¤©æ°”é¢„æŠ¥è¯´"å¯èƒ½ä¸‹é›¨"ï¼‰

**ä¼˜ç‚¹**ï¼šç®€å•ã€å¿«é€Ÿã€ä¸éœ€è¦å¤æ‚å·¥å…·  
**ç¼ºç‚¹**ï¼šæ— æ³•é¢„æµ‹çªå‘äº‹ä»¶ï¼ˆå¦‚ä¸šåŠ¡çˆ†å‘å¢é•¿ã€æ–°åŠŸèƒ½ä¸Šçº¿ï¼‰

---

**æ–¹æ³•2ï¼šä¸šåŠ¡é©±åŠ¨è§„åˆ’** â€” ç±»æ¯”æ–°å·¥å‚å»ºè®¾

å½“åŸå¸‚è§„åˆ’å»ºè®¾æ–°å·¥å‚æ—¶ï¼Œè§„åˆ’éƒ¨é—¨ä¸ä¼šç­‰å·¥å‚å»ºå¥½å†è€ƒè™‘ä¾›æ°´â€”â€”è€Œæ˜¯æå‰æ ¹æ®å·¥å‚è§„æ¨¡ã€ç”¨æ°´å¼ºåº¦è®¡ç®—éœ€æ±‚ã€‚åŒç†ï¼Œå½“ä¸šåŠ¡å›¢é˜Ÿå‘Šè¯‰ä½ "ä¸‹ä¸ªæœˆä¸Šçº¿æ–°åŠŸèƒ½ï¼Œé¢„è®¡ç”¨æˆ·å¢é•¿ 3 å€"æ—¶ï¼Œä½ éœ€è¦æå‰è®¡ç®—èµ„æºéœ€æ±‚ã€‚

**å…³é”®å‚æ•°**ï¼š
- **é¢„æœŸç”¨æˆ·å¢é•¿**ï¼šDAU ä» 100 ä¸‡å¢é•¿åˆ° 500 ä¸‡
- **å•ç”¨æˆ·èµ„æºæ¶ˆè€—**ï¼šæ¯ç”¨æˆ·å¹³å‡æ¶ˆè€— 0.01 æ ¸ CPUã€50MB å†…å­˜
- **å³°å€¼å€æ•°**ï¼šæ—¥å‡æµé‡çš„ 5-10 å€ï¼ˆå¦‚ç§’æ€æ´»åŠ¨ï¼‰
- **å†—ä½™ç³»æ•°**ï¼š1.3-1.5ï¼ˆé¢„ç•™ 30-50% bufferï¼‰

**è®¡ç®—å…¬å¼**ï¼š
```
æ€»éœ€æ±‚ = é¢„æœŸç”¨æˆ·æ•° Ã— å•ç”¨æˆ·èµ„æº Ã— å³°å€¼å€æ•° Ã— å†—ä½™ç³»æ•°
```

**å®é™…æ¡ˆä¾‹**ï¼š
å‡è®¾å¤§ä¿ƒæ´»åŠ¨ï¼š
- æ—¥å¸¸ DAU: 100 ä¸‡
- å¤§ä¿ƒé¢„æœŸ DAU: 500 ä¸‡
- å³°å€¼å€æ•°: 10xï¼ˆå‡Œæ™¨ç§’æ€ï¼‰
- å•ç”¨æˆ· CPU: 0.01 æ ¸
- å†—ä½™ç³»æ•°: 1.5

```
å³°å€¼å¹¶å‘ç”¨æˆ· = 5,000,000 Ã— 10 / 24 = 2,083,333
æ‰€éœ€ CPU = 2,083,333 Ã— 0.01 Ã— 1.5 = 31,250 æ ¸
```

**ä¼˜ç‚¹**ï¼šèƒ½é¢„æµ‹è®¡åˆ’å†…çš„å¤§è§„æ¨¡å˜åŒ–  
**ç¼ºç‚¹**ï¼šä¾èµ–ä¸šåŠ¡å›¢é˜Ÿçš„å‡†ç¡®é¢„æµ‹ï¼Œå®é™…ç”¨æˆ·è¡Œä¸ºå¯èƒ½åå·®

```yaml
# å®¹é‡è§„åˆ’æœ€ä½³å®è·µ
capacityPlanningMethods:
  
  # æ–¹æ³•1: å†å²è¶‹åŠ¿åˆ†æ
  trend-based-forecasting:
    description: "åŸºäºå†å²æ•°æ®é¢„æµ‹æœªæ¥éœ€æ±‚"
    é€‚ç”¨åœºæ™¯: "ç¨³å®šå¢é•¿çš„ä¸šåŠ¡"
    æ•°æ®çª—å£: "è‡³å°‘6ä¸ªæœˆå†å²æ•°æ®"
    é¢„æµ‹å‘¨æœŸ: "æœªæ¥3-6ä¸ªæœˆ"
    å‡†ç¡®åº¦: "70-85%"
    å·¥å…·: "Prometheus + Grafana + ML æ¨¡å‹"
    
  # æ–¹æ³•2: ä¸šåŠ¡é©±åŠ¨è§„åˆ’
  business-driven-planning:
    description: "æ ¹æ®ä¸šåŠ¡ç›®æ ‡åæ¨èµ„æºéœ€æ±‚"
    é€‚ç”¨åœºæ™¯: "æ–°ä¸šåŠ¡ä¸Šçº¿ã€å¤§ä¿ƒæ´»åŠ¨"
    è¾“å…¥å‚æ•°:
      - "é¢„æœŸç”¨æˆ·å¢é•¿"
      - "å•ç”¨æˆ·èµ„æºæ¶ˆè€—"
      - "å³°å€¼å€æ•°"
    è®¡ç®—å…¬å¼: "æ€»éœ€æ±‚ = æ—¥å‡ç”¨æˆ· Ã— å•ç”¨æˆ·èµ„æº Ã— å³°å€¼å€æ•° Ã— å†—ä½™ç³»æ•°"
    å‡†ç¡®åº¦: "60-75%"
    
  # æ–¹æ³•3: è´Ÿè½½æµ‹è¯•éªŒè¯
  load-testing-validation:
    description: "é€šè¿‡å‹åŠ›æµ‹è¯•éªŒè¯å®¹é‡å‡è®¾"
    é€‚ç”¨åœºæ™¯: "å…³é”®ä¸šåŠ¡ã€é«˜å¹¶å‘åœºæ™¯"
    æµ‹è¯•å·¥å…·: "K6ã€JMeterã€Locust"
    æµ‹è¯•ç»´åº¦:
      - "å¹¶å‘ç”¨æˆ·æ•°"
      - "è¯·æ±‚å“åº”æ—¶é—´"
      - "èµ„æºä½¿ç”¨ç‡"
      - "é”™è¯¯ç‡"
    å‡†ç¡®åº¦: "85-95%"
    
  # æ–¹æ³•4: æœºå™¨å­¦ä¹ é¢„æµ‹
  ml-based-forecasting:
    description: "ä½¿ç”¨ ML æ¨¡å‹é¢„æµ‹èµ„æºéœ€æ±‚"
    é€‚ç”¨åœºæ™¯: "å¤§è§„æ¨¡é›†ç¾¤ã€å¤æ‚ä¸šåŠ¡æ¨¡å¼"
    æ¨¡å‹ç±»å‹:
      - "æ—¶é—´åºåˆ—é¢„æµ‹ (ARIMA, Prophet)"
      - "å›å½’æ¨¡å‹ (Linear Regression)"
      - "æ·±åº¦å­¦ä¹  (LSTM)"
    è®­ç»ƒæ•°æ®: "è‡³å°‘12ä¸ªæœˆå†å²æ•°æ®"
    å‡†ç¡®åº¦: "80-90%"
    å·¥å…·: "Python + scikit-learn + TensorFlow"
```

---

## 2. èµ„æºä½¿ç”¨åˆ†æ

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: åˆ†æå½“å‰èµ„æºä½¿ç”¨æƒ…å†µæ˜¯å®¹é‡è§„åˆ’çš„ç¬¬ä¸€æ­¥ã€‚é€šè¿‡Prometheusé‡‡é›†CPUã€å†…å­˜ã€å­˜å‚¨ã€ç½‘ç»œçš„ä½¿ç”¨æ•°æ®ï¼Œæ‰¾å‡ºç“¶é¢ˆå’Œæµªè´¹ç‚¹ã€‚å°±åƒ"å…ˆä½“æ£€å†å¼€è¯æ–¹"ã€‚

### 2.1 é›†ç¾¤èµ„æºå…¨æ™¯è§†å›¾

```bash
#!/bin/bash
# cluster-capacity-report.sh - ç”Ÿæˆé›†ç¾¤å®¹é‡æŠ¥å‘Š

echo "========== Kubernetes é›†ç¾¤å®¹é‡æŠ¥å‘Š =========="
echo "ç”Ÿæˆæ—¶é—´: $(date)"
echo ""

# 1. èŠ‚ç‚¹æ€»è§ˆ
echo "=== 1. èŠ‚ç‚¹èµ„æºæ€»è§ˆ ==="
kubectl get nodes -o custom-columns=\
NAME:.metadata.name,\
STATUS:.status.conditions[-1].type,\
ROLE:.metadata.labels.node-role\\.kubernetes\\.io/master,\
CPU_CAPACITY:.status.capacity.cpu,\
CPU_ALLOCATABLE:.status.allocatable.cpu,\
MEM_CAPACITY:.status.capacity.memory,\
MEM_ALLOCATABLE:.status.allocatable.memory,\
PODS_CAPACITY:.status.capacity.pods

echo ""
echo "=== 2. é›†ç¾¤æ€»å®¹é‡æ±‡æ€» ==="
kubectl get nodes -o json | jq -r '
.items | 
"æ€»èŠ‚ç‚¹æ•°: " + (length | tostring) + "\n" +
"æ€» CPU æ ¸å¿ƒæ•°: " + ([.[].status.capacity.cpu | tonumber] | add | tostring) + "\n" +
"å¯åˆ†é… CPU: " + ([.[].status.allocatable.cpu | tonumber] | add | tostring) + "\n" +
"æ€»å†…å­˜ (GB): " + ([.[].status.capacity.memory | rtrimstr("Ki") | tonumber] | add / 1024 / 1024 | tostring) + "\n" +
"å¯åˆ†é…å†…å­˜ (GB): " + ([.[].status.allocatable.memory | rtrimstr("Ki") | tonumber] | add / 1024 / 1024 | tostring)
'

echo ""
echo "=== 3. å‘½åç©ºé—´èµ„æºä½¿ç”¨ TOP 10 ==="
kubectl get pods --all-namespaces -o json | jq -r '
.items | 
group_by(.metadata.namespace) | 
map({
  namespace: .[0].metadata.namespace,
  pod_count: length,
  cpu_requests: [.[].spec.containers[].resources.requests.cpu // "0" | gsub("m"; "") | tonumber] | add,
  mem_requests: [.[].spec.containers[].resources.requests.memory // "0" | gsub("Mi"; "") | tonumber] | add
}) | 
sort_by(.cpu_requests) | 
reverse | 
.[:10] | 
.[] | 
"\(.namespace)\t\(.pod_count)\t\(.cpu_requests)m\t\(.mem_requests)Mi"
' | column -t -s $'\t' -N "å‘½åç©ºé—´,Podæ•°,CPUè¯·æ±‚,å†…å­˜è¯·æ±‚"

echo ""
echo "=== 4. èµ„æºåˆ©ç”¨ç‡ï¼ˆå®æ—¶ï¼‰ ==="
kubectl top nodes | awk 'NR==1 {print; next} {
  cpu_pct=$3; mem_pct=$5;
  gsub("%", "", cpu_pct); gsub("%", "", mem_pct);
  total_cpu += cpu_pct; total_mem += mem_pct; count++
}
END {
  print "å¹³å‡ CPU åˆ©ç”¨ç‡:", total_cpu/count "%"
  print "å¹³å‡å†…å­˜åˆ©ç”¨ç‡:", total_mem/count "%"
}'

echo ""
echo "=== 5. ä¸å¯è°ƒåº¦èŠ‚ç‚¹æ£€æŸ¥ ==="
kubectl get nodes -o json | jq -r '
.items[] | 
select(.spec.unschedulable == true or 
       (.status.conditions[] | select(.type == "Ready" and .status != "True"))) |
.metadata.name
' | while read node; do
  echo "âš ï¸  èŠ‚ç‚¹ $node ä¸å¯è°ƒåº¦"
done

echo ""
echo "=== 6. èµ„æºç¢ç‰‡åŒ–åˆ†æ ==="
kubectl describe nodes | awk '
/^Name:/ {node=$2}
/cpu.*requests/ {
  split($0, a, /[()]/);
  cpu_allocated=a[2];
  gsub("%", "", cpu_allocated);
}
/memory.*requests/ {
  split($0, a, /[()]/);
  mem_allocated=a[2];
  gsub("%", "", mem_allocated);
  if (cpu_allocated < 50 && mem_allocated < 50) {
    print node ": CPU=" cpu_allocated "%, MEM=" mem_allocated "% (ä½åˆ©ç”¨ç‡)"
  }
}'
```

### 2.2 èµ„æºä½¿ç”¨è¶‹åŠ¿åˆ†æ

```promql
# Prometheus æŸ¥è¯¢ - èµ„æºä½¿ç”¨è¶‹åŠ¿

## 1. CPU ä½¿ç”¨ç‡è¶‹åŠ¿ï¼ˆè¿‡å»7å¤©ï¼‰
avg(rate(container_cpu_usage_seconds_total[5m])) by (namespace)

## 2. å†…å­˜ä½¿ç”¨è¶‹åŠ¿ï¼ˆè¿‡å»7å¤©ï¼‰
avg(container_memory_working_set_bytes) by (namespace) / 1024 / 1024 / 1024

## 3. Pod æ•°é‡å¢é•¿è¶‹åŠ¿
count(kube_pod_info) by (namespace)

## 4. èŠ‚ç‚¹ CPU å¯è°ƒåº¦å®¹é‡å‰©ä½™
sum(kube_node_status_allocatable{resource="cpu"}) - 
sum(kube_pod_container_resource_requests{resource="cpu"})

## 5. å†…å­˜å¯è°ƒåº¦å®¹é‡å‰©ä½™ (GB)
(sum(kube_node_status_allocatable{resource="memory"}) - 
 sum(kube_pod_container_resource_requests{resource="memory"})) / 1024 / 1024 / 1024

## 6. å­˜å‚¨ä½¿ç”¨ç‡
(node_filesystem_size_bytes{mountpoint="/"} - 
 node_filesystem_avail_bytes{mountpoint="/"}) / 
 node_filesystem_size_bytes{mountpoint="/"} * 100

## 7. ç½‘ç»œå¸¦å®½ä½¿ç”¨
rate(node_network_receive_bytes_total[5m]) / 1024 / 1024

## 8. etcd æ•°æ®åº“å¤§å°
etcd_mvcc_db_total_size_in_bytes / 1024 / 1024 / 1024
```

---

## 3. å®¹é‡é¢„æµ‹æ¨¡å‹

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: é¢„æµ‹æœªæ¥èµ„æºéœ€æ±‚çš„æ–¹æ³•æœ‰ï¼šçº¿æ€§å›å½’(ç®€å•è¶‹åŠ¿)ã€å­£èŠ‚æ€§åˆ†æ(å‘¨æœŸæ€§å˜åŒ–)ã€æœºå™¨å­¦ä¹ (å¤æ‚æ¨¡å¼)ã€‚ç”Ÿäº§ç¯å¢ƒæœ€å¸¸ç”¨çº¿æ€§å›å½’+å®‰å…¨ç³»æ•°(å¦‚é¢„ç•™30%buffer)ã€‚

### 3.1 çº¿æ€§å›å½’é¢„æµ‹æ¨¡å‹

#### æ·±å…¥ç†è§£ï¼šçº¿æ€§å›å½’é¢„æµ‹ â€” æ ¹æ®è¿‡å»èµ°åŠ¿ç”»è¶‹åŠ¿çº¿

**ä»€ä¹ˆæ˜¯çº¿æ€§å›å½’é¢„æµ‹**ï¼Ÿ

å°±åƒæ ¹æ®è¿‡å» 6 ä¸ªæœˆçš„è‚¡ç¥¨èµ°åŠ¿é¢„æµ‹æœªæ¥ä»·æ ¼â€”â€”åœ¨å›¾è¡¨ä¸Šç”»ä¸€æ¡"æœ€ä½³æ‹Ÿåˆçº¿"ï¼Œå»¶ä¼¸åˆ°æœªæ¥ã€‚å¯¹äºèµ„æºé¢„æµ‹ï¼Œæˆ‘ä»¬æ ¹æ®å†å²çš„ CPUã€å†…å­˜ä½¿ç”¨æ•°æ®ï¼Œç»˜åˆ¶è¶‹åŠ¿çº¿ï¼Œé¢„æµ‹æœªæ¥çš„èµ„æºéœ€æ±‚ã€‚

**çº¿æ€§å›å½’çš„æ ¸å¿ƒå‡è®¾**ï¼š
- èµ„æºä½¿ç”¨é‡éšæ—¶é—´å‘ˆçº¿æ€§å¢é•¿ï¼ˆæˆ–ç¨³å®šï¼‰
- æœªæ¥çš„å¢é•¿æ¨¡å¼ä¸è¿‡å»ä¸€è‡´
- æ²¡æœ‰çªå‘çš„ä¸šåŠ¡å˜åŒ–

**é€‚ç”¨åœºæ™¯**ï¼š
âœ… æˆç†Ÿç¨³å®šçš„ä¸šåŠ¡ï¼ˆå¦‚ SaaS äº§å“ã€å†…éƒ¨ç³»ç»Ÿï¼‰  
âœ… ç”¨æˆ·å¢é•¿å¹³ç¨³çš„åº”ç”¨  
âœ… èµ„æºä½¿ç”¨è¶‹åŠ¿æ˜æ˜¾çš„é›†ç¾¤  

âŒ ä¸é€‚ç”¨ï¼šæ–°ä¸šåŠ¡ä¸Šçº¿ã€æµé‡æ³¢åŠ¨å‰§çƒˆã€å­£èŠ‚æ€§æ˜æ˜¾çš„åœºæ™¯

---

**é¢„æµ‹æ­¥éª¤è¯¦è§£**ï¼š

1. **æ•°æ®å‡†å¤‡**ï¼šé‡‡é›†è¿‡å» 6-12 ä¸ªæœˆçš„èµ„æºä½¿ç”¨æ•°æ®
   ```python
   # ä» Prometheus å¯¼å‡ºå†å²æ•°æ®
   dates = pd.date_range(start='2025-08-01', end='2026-01-31', freq='D')
   cpu_usage = [100, 102, 105, 107, ...]  # æ¯å¤©çš„ CPU ä½¿ç”¨é‡ï¼ˆæ ¸ï¼‰
   ```

2. **æ•°æ®æ¸…æ´—**ï¼šå»é™¤å¼‚å¸¸å€¼ï¼ˆå¦‚æ•…éšœå¯¼è‡´çš„å°–å³°ï¼‰
   ```python
   # å»é™¤è¶…è¿‡å¹³å‡å€¼ 3 å€æ ‡å‡†å·®çš„å¼‚å¸¸ç‚¹
   mean = np.mean(cpu_usage)
   std = np.std(cpu_usage)
   clean_data = [x for x in cpu_usage if abs(x - mean) < 3 * std]
   ```

3. **æ¨¡å‹è®­ç»ƒ**ï¼šæ‹Ÿåˆçº¿æ€§å›å½’æ¨¡å‹
   ```python
   model = LinearRegression()
   model.fit(X=days, y=cpu_usage)  # X æ˜¯å¤©æ•°ï¼Œy æ˜¯ CPU ä½¿ç”¨é‡
   ```

4. **æœªæ¥é¢„æµ‹**ï¼šé¢„æµ‹æœªæ¥ 90 å¤©çš„èµ„æºéœ€æ±‚
   ```python
   future_days = np.arange(180, 270)  # æœªæ¥ 90 å¤©
   predicted_cpu = model.predict(future_days)
   ```

5. **åŠ å…¥å®‰å…¨ç¼“å†²**ï¼šé¢„ç•™ 25-30% buffer
   ```python
   safe_prediction = predicted_cpu * 1.25  # åŠ  25% å®‰å…¨ç³»æ•°
   ```

---

**å®é™…æ¡ˆä¾‹**ï¼š

å‡è®¾æŸé›†ç¾¤è¿‡å» 6 ä¸ªæœˆçš„ CPU ä½¿ç”¨æ•°æ®ï¼š
- 2025-08: 100 æ ¸
- 2025-09: 110 æ ¸
- 2025-10: 120 æ ¸
- 2025-11: 130 æ ¸
- 2025-12: 140 æ ¸
- 2026-01: 150 æ ¸

çº¿æ€§å›å½’æ¨¡å‹é¢„æµ‹ï¼š
- **å¢é•¿é€Ÿç‡**ï¼šæ¯æœˆ +10 æ ¸
- **3 ä¸ªæœˆåï¼ˆ2026-04ï¼‰**ï¼š180 æ ¸
- **åŠ  25% buffer**ï¼š225 æ ¸

**æ‰©å®¹å†³ç­–**ï¼š
- å½“å‰é›†ç¾¤æ€»å®¹é‡ï¼š200 æ ¸
- é¢„æµ‹ 3 ä¸ªæœˆåéœ€æ±‚ï¼š225 æ ¸
- **ç»“è®º**ï¼šéœ€è¦åœ¨ 2 ä¸ªæœˆå†…å¢åŠ  25 æ ¸ï¼ˆçº¦ 4 å° 8 æ ¸æœåŠ¡å™¨ï¼‰

---

**æ¨¡å‹å‡†ç¡®åº¦è¯„ä¼°**ï¼š

```python
# è®¡ç®— RÂ² åˆ†æ•°ï¼ˆè¶Šæ¥è¿‘ 1 è¶Šå‡†ç¡®ï¼‰
r2_score = model.score(X, y)
print(f"æ¨¡å‹å‡†ç¡®åº¦ RÂ²: {r2_score:.2f}")

# RÂ² è§£è¯»
# 0.90-1.00: ä¼˜ç§€ï¼Œè¶‹åŠ¿éå¸¸æ˜ç¡®
# 0.70-0.89: è‰¯å¥½ï¼Œå¯ç”¨äºé¢„æµ‹
# 0.50-0.69: ä¸€èˆ¬ï¼Œéœ€è°¨æ…ä½¿ç”¨
# < 0.50: å·®ï¼Œæ•°æ®æ³¢åŠ¨å¤ªå¤§ï¼Œä¸é€‚åˆçº¿æ€§é¢„æµ‹
```

**ä¼˜ç‚¹**ï¼š
- ç®€å•æ˜“æ‡‚ï¼Œå®æ–½å¿«é€Ÿ
- ä¸éœ€è¦å¤æ‚çš„æ•°å­¦çŸ¥è¯†
- é€‚åˆå¤§å¤šæ•°ç¨³å®šä¸šåŠ¡åœºæ™¯

**å±€é™æ€§**ï¼š
- æ— æ³•é¢„æµ‹çªå‘å˜åŒ–ï¼ˆå¦‚ç—…æ¯’å¼å¢é•¿ã€çªç„¶æ‰é‡ï¼‰
- å¿½ç•¥å­£èŠ‚æ€§å› ç´ ï¼ˆå¦‚èŠ‚å‡æ—¥æµé‡é«˜å³°ï¼‰
- å‡è®¾æœªæ¥è¶‹åŠ¿ä¸è¿‡å»ä¸€è‡´ï¼ˆå®é™…å¯èƒ½æ”¹å˜ï¼‰

---

#### æ·±å…¥ç†è§£ï¼šå­£èŠ‚æ€§åˆ†æ â€” èŠ‚å‡æ—¥å®¢æµé‡é¢„æµ‹

**ä»€ä¹ˆæ˜¯å­£èŠ‚æ€§åˆ†æ**ï¼Ÿ

å°±åƒå•†åœºé¢„æµ‹èŠ‚å‡æ—¥å®¢æµé‡â€”â€”çŸ¥é“æ¯å¹´æ˜¥èŠ‚ã€åŒ11ã€é»‘äº”ä¼šæœ‰æµé‡é«˜å³°ã€‚å­£èŠ‚æ€§åˆ†æè¯†åˆ«å‘¨æœŸæ€§çš„èµ„æºä½¿ç”¨æ¨¡å¼ï¼Œé¢„æµ‹æœªæ¥çš„å³°è°·ã€‚

**å¸¸è§å­£èŠ‚æ€§æ¨¡å¼**ï¼š

| å‘¨æœŸ | ç¤ºä¾‹åœºæ™¯ | ç‰¹å¾ |
|------|----------|------|
| **æ¯æ—¥** | ç”µå•†ç½‘ç«™ | ç™½å¤©æµé‡é«˜ï¼Œå‡Œæ™¨ä½ |
| **æ¯å‘¨** | B2B SaaS | å·¥ä½œæ—¥é«˜ï¼Œå‘¨æœ«ä½ |
| **æ¯æœˆ** | é‡‘èç³»ç»Ÿ | æœˆåˆæœˆæœ«é«˜ï¼ˆå·¥èµ„ã€è´¦å•ï¼‰ |
| **æ¯å¹´** | ç”µå•†å¹³å° | æ˜¥èŠ‚ã€åŒ11ã€é»‘äº”é«˜å³° |

**å­£èŠ‚æ€§åˆ†è§£**ï¼š

èµ„æºä½¿ç”¨é‡ = **è¶‹åŠ¿** + **å­£èŠ‚æ€§** + **éšæœºå™ªå£°**

```
ä¾‹å¦‚æŸç”µå•† CPU ä½¿ç”¨é‡ï¼š
- åŸºç¡€è¶‹åŠ¿ï¼šæ¯æœˆå¢é•¿ 5 æ ¸ï¼ˆä¸šåŠ¡å¢é•¿ï¼‰
- æ¯å‘¨å­£èŠ‚æ€§ï¼šå‘¨æœ« +20%ï¼ˆæµé‡é«˜å³°ï¼‰
- å¹´åº¦å­£èŠ‚æ€§ï¼šåŒ11 +300%ï¼ˆä¿ƒé”€æ´»åŠ¨ï¼‰
```

**Prophet æ¨¡å‹ç¤ºä¾‹**ï¼š

Facebook Prophet æ˜¯ä¸“é—¨å¤„ç†å­£èŠ‚æ€§æ—¶é—´åºåˆ—çš„å·¥å…·ï¼Œè‡ªåŠ¨è¯†åˆ«å‘¨æœŸæ€§æ¨¡å¼ã€‚

```python
from prophet import Prophet

# å‡†å¤‡æ•°æ®ï¼ˆProphet è¦æ±‚ç‰¹å®šæ ¼å¼ï¼‰
df = pd.DataFrame({
    'ds': dates,  # æ—¶é—´æˆ³
    'y': cpu_usage  # CPU ä½¿ç”¨é‡
})

# åˆ›å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨è¯†åˆ«å¹´åº¦ã€å‘¨åº¦å­£èŠ‚æ€§ï¼‰
model = Prophet(
    yearly_seasonality=True,  # è¯†åˆ«å¹´åº¦å­£èŠ‚æ€§
    weekly_seasonality=True,  # è¯†åˆ«å‘¨åº¦å­£èŠ‚æ€§
    daily_seasonality=False   # å…³é—­æ—¥åº¦å­£èŠ‚æ€§
)

# è®­ç»ƒå’Œé¢„æµ‹
model.fit(df)
future = model.make_future_dataframe(periods=90)  # é¢„æµ‹ 90 å¤©
forecast = model.predict(future)
```

**Prophet çš„ä¼˜åŠ¿**ï¼š
- è‡ªåŠ¨è¯†åˆ«å¤šç§å­£èŠ‚æ€§æ¨¡å¼
- å¤„ç†ç¼ºå¤±æ•°æ®å’Œå¼‚å¸¸å€¼
- æä¾›ç½®ä¿¡åŒºé—´ï¼ˆé¢„æµ‹ä¸Šä¸‹é™ï¼‰

**å®é™…åº”ç”¨**ï¼š

å‡è®¾æŸç”µå•†å¹³å°ï¼ŒProphet åˆ†æå‘ç°ï¼š
- **åŸºç¡€è¶‹åŠ¿**ï¼šæ¯æœˆå¢é•¿ 10%
- **å‘¨åº¦å­£èŠ‚æ€§**ï¼šå‘¨æœ«æµé‡æ¯”å¹³æ—¥é«˜ 30%
- **å¹´åº¦å­£èŠ‚æ€§**ï¼šåŒ11 æµé‡æ˜¯å¹³æ—¥çš„ 5 å€

**æ‰©å®¹ç­–ç•¥**ï¼š
- æ—¥å¸¸ï¼šæŒ‰åŸºç¡€è¶‹åŠ¿æ¯æœˆæ‰©å®¹ 10%
- æ¯å‘¨äº”ä¸‹åˆï¼šæå‰æ‰©å®¹ 30% åº”å¯¹å‘¨æœ«é«˜å³°
- åŒ11 å‰ 2 å‘¨ï¼šæ‰©å®¹è‡³ 5 å€å®¹é‡ï¼Œæ´»åŠ¨åç¼©å®¹

---

**å­£èŠ‚æ€§åˆ†æçš„å…³é”®ä»·å€¼**ï¼š

âœ… **é¿å…è¿‡åº¦é…ç½®**ï¼šå¹³æ—¥ä¸éœ€è¦æŒ‰åŒ11å³°å€¼é…ç½®èµ„æº  
âœ… **æå‰å‡†å¤‡**ï¼šåœ¨é«˜å³°æ¥ä¸´å‰ 1-2 å‘¨å®Œæˆæ‰©å®¹ï¼ˆé¿å…ä¸´æ—¶æŠ±ä½›è„šï¼‰  
âœ… **æˆæœ¬ä¼˜åŒ–**ï¼šåˆ©ç”¨äº‘å‚å•† Spot å®ä¾‹åº”å¯¹ä¸´æ—¶é«˜å³°

```python
#!/usr/bin/env python3
# capacity_forecasting.py - èµ„æºå®¹é‡é¢„æµ‹å·¥å…·

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

class CapacityForecaster:
    """Kubernetes é›†ç¾¤å®¹é‡é¢„æµ‹å™¨"""
    
    def __init__(self, metric_data):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            metric_data: DataFrameï¼ŒåŒ…å«æ—¶é—´æˆ³å’Œèµ„æºä½¿ç”¨é‡
                        åˆ—: ['timestamp', 'cpu_usage', 'memory_usage', 'pod_count']
        """
        self.data = metric_data
        self.model_cpu = LinearRegression()
        self.model_memory = LinearRegression()
        
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºæ•°å€¼ï¼ˆå¤©æ•°ï¼‰
        self.data['days'] = (self.data['timestamp'] - self.data['timestamp'].min()).dt.days
        
        # ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = self.data[['days']].values
        y_cpu = self.data['cpu_usage'].values
        y_memory = self.data['memory_usage'].values
        
        return X, y_cpu, y_memory
    
    def train(self):
        """è®­ç»ƒé¢„æµ‹æ¨¡å‹"""
        X, y_cpu, y_memory = self.prepare_data()
        
        # è®­ç»ƒ CPU é¢„æµ‹æ¨¡å‹
        self.model_cpu.fit(X, y_cpu)
        
        # è®­ç»ƒå†…å­˜é¢„æµ‹æ¨¡å‹
        self.model_memory.fit(X, y_memory)
        
        print(f"CPU æ¨¡å‹ RÂ² åˆ†æ•°: {self.model_cpu.score(X, y_cpu):.4f}")
        print(f"å†…å­˜æ¨¡å‹ RÂ² åˆ†æ•°: {self.model_memory.score(X, y_memory):.4f}")
    
    def forecast(self, days_ahead=90):
        """
        é¢„æµ‹æœªæ¥èµ„æºéœ€æ±‚
        
        Args:
            days_ahead: é¢„æµ‹æœªæ¥å¤šå°‘å¤©
            
        Returns:
            DataFrame: é¢„æµ‹ç»“æœ
        """
        last_day = self.data['days'].max()
        future_days = np.arange(last_day + 1, last_day + days_ahead + 1).reshape(-1, 1)
        
        # é¢„æµ‹
        cpu_forecast = self.model_cpu.predict(future_days)
        memory_forecast = self.model_memory.predict(future_days)
        
        # æ„å»ºç»“æœ DataFrame
        forecast_df = pd.DataFrame({
            'days_from_now': future_days.flatten() - last_day,
            'predicted_cpu_usage': cpu_forecast,
            'predicted_memory_usage': memory_forecast
        })
        
        return forecast_df
    
    def calculate_capacity_needs(self, forecast_df, cluster_cpu_total, cluster_memory_total, buffer=0.25):
        """
        è®¡ç®—éœ€è¦æ‰©å®¹çš„èµ„æºé‡
        
        Args:
            forecast_df: é¢„æµ‹ç»“æœ
            cluster_cpu_total: å½“å‰é›†ç¾¤æ€» CPU æ ¸å¿ƒæ•°
            cluster_memory_total: å½“å‰é›†ç¾¤æ€»å†…å­˜ (GB)
            buffer: å®‰å…¨ç¼“å†²ï¼ˆé»˜è®¤25%ï¼‰
            
        Returns:
            dict: æ‰©å®¹å»ºè®®
        """
        # æ‰¾åˆ°é¢„æµ‹å€¼è¶…è¿‡å½“å‰å®¹é‡çš„æ—¶é—´ç‚¹
        cpu_threshold = cluster_cpu_total * (1 - buffer)
        memory_threshold = cluster_memory_total * (1 - buffer)
        
        cpu_exceed = forecast_df[forecast_df['predicted_cpu_usage'] > cpu_threshold]
        memory_exceed = forecast_df[forecast_df['predicted_memory_usage'] > memory_threshold]
        
        result = {
            'current_capacity': {
                'cpu': cluster_cpu_total,
                'memory': cluster_memory_total
            },
            'cpu_scale_recommendation': None,
            'memory_scale_recommendation': None
        }
        
        if not cpu_exceed.empty:
            days_until_cpu_full = cpu_exceed['days_from_now'].min()
            additional_cpu_needed = cpu_exceed['predicted_cpu_usage'].max() - cpu_threshold
            result['cpu_scale_recommendation'] = {
                'days_until_full': int(days_until_cpu_full),
                'additional_cores_needed': int(np.ceil(additional_cpu_needed)),
                'recommended_action': f"åœ¨ {days_until_cpu_full} å¤©å†…å¢åŠ  {int(np.ceil(additional_cpu_needed))} æ ¸ CPU"
            }
        
        if not memory_exceed.empty:
            days_until_memory_full = memory_exceed['days_from_now'].min()
            additional_memory_needed = memory_exceed['predicted_memory_usage'].max() - memory_threshold
            result['memory_scale_recommendation'] = {
                'days_until_full': int(days_until_memory_full),
                'additional_gb_needed': int(np.ceil(additional_memory_needed)),
                'recommended_action': f"åœ¨ {days_until_memory_full} å¤©å†…å¢åŠ  {int(np.ceil(additional_memory_needed))} GB å†…å­˜"
            }
        
        return result
    
    def plot_forecast(self, forecast_df, cluster_cpu_total, cluster_memory_total):
        """ç»˜åˆ¶é¢„æµ‹å›¾è¡¨"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # CPU é¢„æµ‹
        ax1.plot(self.data['days'], self.data['cpu_usage'], label='å†å² CPU ä½¿ç”¨', marker='o')
        ax1.plot(forecast_df['days_from_now'] + self.data['days'].max(), 
                 forecast_df['predicted_cpu_usage'], 
                 label='é¢„æµ‹ CPU ä½¿ç”¨', linestyle='--', marker='x')
        ax1.axhline(y=cluster_cpu_total, color='r', linestyle='-', label='é›†ç¾¤æ€»å®¹é‡')
        ax1.axhline(y=cluster_cpu_total * 0.75, color='orange', linestyle=':', label='75% è­¦æˆ’çº¿')
        ax1.set_xlabel('å¤©æ•°')
        ax1.set_ylabel('CPU æ ¸å¿ƒæ•°')
        ax1.set_title('CPU å®¹é‡é¢„æµ‹')
        ax1.legend()
        ax1.grid(True)
        
        # å†…å­˜é¢„æµ‹
        ax2.plot(self.data['days'], self.data['memory_usage'], label='å†å²å†…å­˜ä½¿ç”¨', marker='o')
        ax2.plot(forecast_df['days_from_now'] + self.data['days'].max(), 
                 forecast_df['predicted_memory_usage'], 
                 label='é¢„æµ‹å†…å­˜ä½¿ç”¨', linestyle='--', marker='x')
        ax2.axhline(y=cluster_memory_total, color='r', linestyle='-', label='é›†ç¾¤æ€»å®¹é‡')
        ax2.axhline(y=cluster_memory_total * 0.75, color='orange', linestyle=':', label='75% è­¦æˆ’çº¿')
        ax2.set_xlabel('å¤©æ•°')
        ax2.set_ylabel('å†…å­˜ (GB)')
        ax2.set_title('å†…å­˜å®¹é‡é¢„æµ‹')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('capacity_forecast.png', dpi=300)
        print("é¢„æµ‹å›¾è¡¨å·²ä¿å­˜è‡³ capacity_forecast.png")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡æ‹Ÿå†å²æ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶ä» Prometheus è·å–ï¼‰
    dates = pd.date_range(start='2025-08-01', end='2026-01-31', freq='D')
    np.random.seed(42)
    
    data = pd.DataFrame({
        'timestamp': dates,
        'cpu_usage': np.linspace(100, 180, len(dates)) + np.random.normal(0, 10, len(dates)),
        'memory_usage': np.linspace(500, 800, len(dates)) + np.random.normal(0, 50, len(dates)),
        'pod_count': np.linspace(1000, 1500, len(dates)) + np.random.normal(0, 100, len(dates))
    })
    
    # åˆ›å»ºé¢„æµ‹å™¨
    forecaster = CapacityForecaster(data)
    forecaster.train()
    
    # é¢„æµ‹æœªæ¥ 90 å¤©
    forecast = forecaster.forecast(days_ahead=90)
    print("\næœªæ¥ 90 å¤©é¢„æµ‹:")
    print(forecast.head(10))
    
    # è®¡ç®—æ‰©å®¹éœ€æ±‚
    recommendations = forecaster.calculate_capacity_needs(
        forecast, 
        cluster_cpu_total=200, 
        cluster_memory_total=1000
    )
    
    print("\næ‰©å®¹å»ºè®®:")
    print(f"CPU: {recommendations['cpu_scale_recommendation']}")
    print(f"å†…å­˜: {recommendations['memory_scale_recommendation']}")
    
    # ç»˜åˆ¶å›¾è¡¨
    forecaster.plot_forecast(forecast, 200, 1000)
```

### 3.2 æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆProphetï¼‰

```python
#!/usr/bin/env python3
# capacity_forecasting_prophet.py - ä½¿ç”¨ Facebook Prophet è¿›è¡Œå®¹é‡é¢„æµ‹

from prophet import Prophet
import pandas as pd
import matplotlib.pyplot as plt

class ProphetCapacityForecaster:
    """åŸºäº Prophet çš„å®¹é‡é¢„æµ‹å™¨"""
    
    def __init__(self, metric_data):
        self.data = metric_data
        self.model = Prophet(
            changepoint_prior_scale=0.05,  # æ§åˆ¶è¶‹åŠ¿çµæ´»æ€§
            seasonality_prior_scale=10.0,  # æ§åˆ¶å­£èŠ‚æ€§å¼ºåº¦
            yearly_seasonality=True,
            weekly_seasonality=True,
            daily_seasonality=False
        )
    
    def prepare_data(self, metric_column):
        """å‡†å¤‡ Prophet æ ¼å¼æ•°æ®"""
        df = pd.DataFrame({
            'ds': self.data['timestamp'],
            'y': self.data[metric_column]
        })
        return df
    
    def train_and_forecast(self, metric_column, periods=90):
        """è®­ç»ƒæ¨¡å‹å¹¶é¢„æµ‹"""
        df = self.prepare_data(metric_column)
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(df)
        
        # åˆ›å»ºæœªæ¥æ—¶é—´æ¡†æ¶
        future = self.model.make_future_dataframe(periods=periods)
        
        # é¢„æµ‹
        forecast = self.model.predict(future)
        
        return forecast
    
    def plot_components(self, forecast):
        """ç»˜åˆ¶è¶‹åŠ¿å’Œå­£èŠ‚æ€§ç»„ä»¶"""
        fig = self.model.plot_components(forecast)
        plt.savefig('prophet_components.png', dpi=300)
        print("Prophet ç»„ä»¶å›¾å·²ä¿å­˜")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    dates = pd.date_range(start='2024-02-01', end='2026-01-31', freq='D')
    np.random.seed(42)
    
    data = pd.DataFrame({
        'timestamp': dates,
        'cpu_usage': (np.linspace(100, 180, len(dates)) + 
                      20 * np.sin(np.arange(len(dates)) * 2 * np.pi / 365) +  # å¹´åº¦å­£èŠ‚æ€§
                      10 * np.sin(np.arange(len(dates)) * 2 * np.pi / 7) +   # å‘¨å­£èŠ‚æ€§
                      np.random.normal(0, 5, len(dates)))
    })
    
    forecaster = ProphetCapacityForecaster(data)
    forecast = forecaster.train_and_forecast('cpu_usage', periods=90)
    
    print("é¢„æµ‹ç»“æœï¼ˆæœªæ¥10å¤©ï¼‰:")
    print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(10))
    
    forecaster.plot_components(forecast)
```

---

## 4. é›†ç¾¤æ‰©å®¹ç­–ç•¥

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: æ‰©å®¹ç­–ç•¥å†³å®š"ä»€ä¹ˆæ—¶å€™åŠ æœºå™¨ã€åŠ å¤šå°‘"ã€‚è‡ªåŠ¨æ‰©å®¹(Cluster Autoscaler)æ ¹æ®pending Podè‡ªåŠ¨æ·»åŠ èŠ‚ç‚¹ï¼›æ‰‹åŠ¨æ‰©å®¹é€‚åˆæœ‰è®¡åˆ’çš„å¤§è§„æ¨¡æ‰©å±•ã€‚å…³é”®åŸåˆ™ï¼šæå‰æ‰©å®¹è€Œéç­‰åˆ°å‘Šæ€¥ã€‚

### 4.1 æ°´å¹³æ‰©å®¹å†³ç­–çŸ©é˜µ

#### æ·±å…¥ç†è§£ï¼šCluster Autoscaler â€” è‡ªåŠ¨å«å‡ºç§Ÿè½¦

**ä»€ä¹ˆæ˜¯ Cluster Autoscaler**ï¼Ÿ

å°±åƒåœ¨é«˜å³°æ—¶æ®µè‡ªåŠ¨å«å‡ºç§Ÿè½¦â€”â€”å½“ç³»ç»Ÿæ£€æµ‹åˆ°æœ‰ä¹˜å®¢ï¼ˆPodï¼‰ç­‰å¾…æ—¶ï¼Œè‡ªåŠ¨å¢åŠ è½¦è¾†ï¼ˆèŠ‚ç‚¹ï¼‰ï¼›å½“è½¦è¾†ç©ºé—²å¤ªä¹…æ—¶ï¼Œè‡ªåŠ¨å‡å°‘è½¦è¾†èŠ‚çœæˆæœ¬ã€‚

**å·¥ä½œåŸç†**ï¼š

1. **æ‰©å®¹è§¦å‘æ¡ä»¶**ï¼š
   - æœ‰ Pod å› èµ„æºä¸è¶³å¤„äº Pending çŠ¶æ€
   - æŒç»­æ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼ˆå¦‚ 10 åˆ†é’Ÿï¼‰
   - ç°æœ‰èŠ‚ç‚¹æ— æ³•æ»¡è¶³ Pod çš„ requests è¦æ±‚

2. **æ‰©å®¹å†³ç­–**ï¼š
   - è®¡ç®—éœ€è¦å¤šå°‘èŠ‚ç‚¹æ‰èƒ½è°ƒåº¦æ‰€æœ‰ Pending Pod
   - é€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹æ± ï¼ˆå¦‚ CPU å¯†é›†å‹ã€å†…å­˜å¯†é›†å‹ï¼‰
   - è°ƒç”¨äº‘å‚å•† API åˆ›å»ºæ–°èŠ‚ç‚¹

3. **ç¼©å®¹è§¦å‘æ¡ä»¶**ï¼š
   - èŠ‚ç‚¹åˆ©ç”¨ç‡æŒç»­ä½äº 50%ï¼ˆå¯é…ç½®ï¼‰
   - èŠ‚ç‚¹ä¸Šçš„ Pod å¯ä»¥è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹
   - èŠ‚ç‚¹ç©ºé—²è¶…è¿‡ 10 åˆ†é’Ÿï¼ˆå¯é…ç½®ï¼‰

---

**Cluster Autoscaler vs æ‰‹åŠ¨æ‰©å®¹**ï¼š

| å¯¹æ¯”ç»´åº¦ | Cluster Autoscaler | æ‰‹åŠ¨æ‰©å®¹ |
|----------|---------------------|----------|
| **å“åº”é€Ÿåº¦** | 10-15 åˆ†é’Ÿï¼ˆæ£€æµ‹ + åˆ›å»ºèŠ‚ç‚¹ï¼‰ | æå‰è§„åˆ’ï¼Œæ— å»¶è¿Ÿ |
| **æˆæœ¬æ•ˆç‡** | é«˜ï¼ˆæŒ‰éœ€æ‰©ç¼©ï¼‰ | å¯èƒ½æµªè´¹ï¼ˆè¿‡åº¦é…ç½®ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | æµé‡æ³¢åŠ¨ã€çªå‘è´Ÿè½½ | è®¡åˆ’å†…å¤§è§„æ¨¡æ‰©å®¹ |
| **é£é™©** | æ‰©å®¹ä¸åŠæ—¶å¯èƒ½å½±å“ä¸šåŠ¡ | é¢„æµ‹å¤±è¯¯å¯¼è‡´èµ„æºæµªè´¹ |

**æœ€ä½³å®è·µ**ï¼š
- **æ—¥å¸¸è¿ç»´**ï¼šä½¿ç”¨ Cluster Autoscaler åº”å¯¹æµé‡æ³¢åŠ¨
- **å¤§ä¿ƒæ´»åŠ¨**ï¼šæå‰ 2 å‘¨æ‰‹åŠ¨æ‰©å®¹åˆ°é¢„æœŸå®¹é‡ï¼Œé¿å…ä¸´æ—¶æ‰©å®¹å»¶è¿Ÿ
- **æ··åˆç­–ç•¥**ï¼šä¿æŒåŸºç¡€å®¹é‡ï¼ˆæ‰‹åŠ¨ï¼‰ï¼Œå³°å€¼å®¹é‡ï¼ˆè‡ªåŠ¨æ‰©å®¹ï¼‰

---

**å®é™…æ¡ˆä¾‹**ï¼š

æŸç”µå•†å¹³å°çš„æ‰©å®¹ç­–ç•¥ï¼š
- **æ—¥å¸¸**ï¼š
  - åŸºç¡€èŠ‚ç‚¹ï¼š200 å°ï¼ˆå›ºå®šï¼‰
  - Cluster Autoscaler èŒƒå›´ï¼š200-500 å°
  - æ‰©å®¹é˜ˆå€¼ï¼šPod Pending > 10 åˆ†é’Ÿ
  
- **å¤§ä¿ƒï¼ˆåŒ11ï¼‰**ï¼š
  - æå‰ 2 å‘¨æ‰‹åŠ¨æ‰©å®¹è‡³ 800 å°
  - Cluster Autoscaler èŒƒå›´ï¼š800-1200 å°
  - æ‰©å®¹é˜ˆå€¼ï¼šPod Pending > 3 åˆ†é’Ÿï¼ˆæ›´æ¿€è¿›ï¼‰
  
- **æ´»åŠ¨ç»“æŸå**ï¼š
  - è§‚å¯Ÿ 3 å¤©æµé‡è¶‹åŠ¿
  - é€æ­¥ç¼©å®¹è‡³ 300 å°ï¼ˆä¿ç•™ç¼“å†²ï¼‰
  - 1 å‘¨åæ¢å¤è‡³ 200 å°åŸºçº¿

---

**Cluster Autoscaler é…ç½®å…³é”®å‚æ•°**ï¼š

```yaml
# æ‰©å®¹ç­–ç•¥
scaleUpDelay: 10m          # æ£€æµ‹åˆ° Pending Pod åç­‰å¾… 10 åˆ†é’Ÿæ‰æ‰©å®¹
                           # é¿å…å› çŸ­æš‚æµé‡å°–å³°è€Œæ‰©å®¹
cpuThreshold: 0.75         # èŠ‚ç‚¹ CPU åˆ©ç”¨ç‡è¶…è¿‡ 75% è§¦å‘æ‰©å®¹
memoryThreshold: 0.80      # èŠ‚ç‚¹å†…å­˜åˆ©ç”¨ç‡è¶…è¿‡ 80% è§¦å‘æ‰©å®¹

# ç¼©å®¹ç­–ç•¥
scaleDownDelay: 10m        # èŠ‚ç‚¹ç©ºé—² 10 åˆ†é’Ÿåæ‰è€ƒè™‘ç¼©å®¹
nodeUtilizationThreshold: 0.50  # èŠ‚ç‚¹åˆ©ç”¨ç‡ä½äº 50% æ‰ç¼©å®¹
scaleDownUnneededTime: 10m # ç¡®è®¤èŠ‚ç‚¹"ä¸éœ€è¦"æŒç»­ 10 åˆ†é’Ÿåæ‰ç¼©å®¹
```

**å‚æ•°è°ƒä¼˜å»ºè®®**ï¼š

- **ä¿å®ˆç­–ç•¥**ï¼ˆé‡‘èã€åŒ»ç–—ï¼‰ï¼š
  - `scaleUpDelay: 5m`ï¼ˆå¿«é€Ÿæ‰©å®¹ï¼‰
  - `scaleDownDelay: 30m`ï¼ˆè°¨æ…ç¼©å®¹ï¼‰
  - `nodeUtilizationThreshold: 0.40`ï¼ˆä¿ç•™æ›´å¤šç¼“å†²ï¼‰

- **æ¿€è¿›ç­–ç•¥**ï¼ˆå¼€å‘æµ‹è¯•ç¯å¢ƒï¼‰ï¼š
  - `scaleUpDelay: 15m`ï¼ˆå®¹å¿çŸ­æš‚ç­‰å¾…ï¼‰
  - `scaleDownDelay: 5m`ï¼ˆå¿«é€Ÿç¼©å®¹èŠ‚çœæˆæœ¬ï¼‰
  - `nodeUtilizationThreshold: 0.60`ï¼ˆæ›´é«˜çš„åˆ©ç”¨ç‡é˜ˆå€¼ï¼‰

---

#### æ·±å…¥ç†è§£ï¼šèŠ‚ç‚¹æ± ç®¡ç† â€” åœè½¦åœºåˆ†åŒº

**ä»€ä¹ˆæ˜¯èŠ‚ç‚¹æ± **ï¼Ÿ

å°±åƒåœè½¦åœºåˆ†ä¸º"å°æ±½è½¦åŒº""è´§è½¦åŒº""ç”µåŠ¨è½¦åŒº"â€”â€”ä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½éœ€è¦ä¸åŒé…ç½®çš„èŠ‚ç‚¹ã€‚èŠ‚ç‚¹æ± ï¼ˆNode Poolï¼‰æ˜¯ä¸€ç»„ç›¸åŒé…ç½®çš„èŠ‚ç‚¹é›†åˆã€‚

**å¸¸è§èŠ‚ç‚¹æ± åˆ’åˆ†**ï¼š

| èŠ‚ç‚¹æ±  | æœºå‹ | ç”¨é€” | æ‰©å®¹ç­–ç•¥ |
|--------|------|------|----------|
| **é€šç”¨å‹** | 8C16G | æ— çŠ¶æ€ Web æœåŠ¡ | è‡ªåŠ¨æ‰©ç¼©å®¹ |
| **è®¡ç®—å¯†é›†å‹** | 32C64G | è§†é¢‘è½¬ç ã€AI æ¨ç† | å®šæœŸè¯„ä¼° |
| **å†…å­˜å¯†é›†å‹** | 8C128G | ç¼“å­˜ã€æ•°æ®åº“ | æ‰‹åŠ¨æ‰©å®¹ |
| **GPU** | 8C32G + A100 | AI è®­ç»ƒ | æŒ‰éœ€ç”³è¯· |
| **Spot å®ä¾‹** | 8C16G (ç«ä»·) | æ‰¹å¤„ç†ã€éå…³é”®ä»»åŠ¡ | æ¿€è¿›æ‰©ç¼©å®¹ |

**èŠ‚ç‚¹æ± çš„æ ¸å¿ƒä»·å€¼**ï¼š

1. **èµ„æºéš”ç¦»**ï¼šGPU ä»»åŠ¡ä¸ä¼šå’Œæ™®é€šä»»åŠ¡æŠ¢èµ„æº
2. **æˆæœ¬ä¼˜åŒ–**ï¼šæ™®é€šä»»åŠ¡ç”¨ä¾¿å®œçš„é€šç”¨å‹ï¼ŒAI ä»»åŠ¡ç”¨ GPU
3. **æ‰©å®¹çµæ´»**ï¼šå¯ä»¥é’ˆå¯¹ä¸åŒç±»å‹åˆ†åˆ«è®¾ç½®æ‰©å®¹ç­–ç•¥

**å®é™…æ¡ˆä¾‹**ï¼š

æŸ AI å…¬å¸çš„èŠ‚ç‚¹æ± è®¾è®¡ï¼š

```yaml
nodePools:
  # æ±  1: é€šç”¨ Web æœåŠ¡
  - name: general-purpose
    instanceType: c5.2xlarge  # 8C16G
    minNodes: 50
    maxNodes: 500
    autoscaling: true
    labels:
      workload: web
    taints: []
    
  # æ±  2: AI æ¨ç†æœåŠ¡
  - name: inference
    instanceType: g4dn.xlarge  # 4C16G + T4 GPU
    minNodes: 10
    maxNodes: 100
    autoscaling: true
    labels:
      workload: inference
    taints:
      - key: nvidia.com/gpu
        value: "true"
        effect: NoSchedule
    
  # æ±  3: AI è®­ç»ƒ
  - name: training
    instanceType: p4d.24xlarge  # 96C1152G + 8x A100
    minNodes: 0
    maxNodes: 20
    autoscaling: false  # æ‰‹åŠ¨ç”³è¯·
    labels:
      workload: training
    taints:
      - key: training
        value: "true"
        effect: NoSchedule
    
  # æ±  4: Spot å®ä¾‹ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰
  - name: spot-batch
    instanceType: c5.2xlarge
    instanceLifecycle: spot  # ç«ä»·å®ä¾‹
    minNodes: 0
    maxNodes: 200
    autoscaling: true
    labels:
      workload: batch
    taints:
      - key: spot
        value: "true"
        effect: PreferNoSchedule
```

**Pod å¦‚ä½•é€‰æ‹©èŠ‚ç‚¹æ± **ï¼š

```yaml
# Web æœåŠ¡ â†’ é€šç”¨èŠ‚ç‚¹æ± 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  template:
    spec:
      nodeSelector:
        workload: web  # åŒ¹é…é€šç”¨èŠ‚ç‚¹æ± 
      containers:
      - name: app
        resources:
          requests:
            cpu: 500m
            memory: 1Gi

---
# AI è®­ç»ƒä»»åŠ¡ â†’ GPU èŠ‚ç‚¹æ± 
apiVersion: batch/v1
kind: Job
metadata:
  name: training-job
spec:
  template:
    spec:
      nodeSelector:
        workload: training  # åŒ¹é… GPU èŠ‚ç‚¹æ± 
      tolerations:
      - key: training
        operator: Equal
        value: "true"
        effect: NoSchedule
      containers:
      - name: trainer
        resources:
          requests:
            nvidia.com/gpu: 8  # è¯·æ±‚ 8 ä¸ª GPU
```

**èŠ‚ç‚¹æ± ç®¡ç†æœ€ä½³å®è·µ**ï¼š

1. **æŒ‰å·¥ä½œè´Ÿè½½ç±»å‹åˆ†æ± **ï¼šä¸è¦æŠŠæ‰€æœ‰ä»»åŠ¡æ··åœ¨ä¸€ä¸ªæ± é‡Œ
2. **è®¾ç½®åˆç†çš„ taints**ï¼šé˜²æ­¢é”™è¯¯è°ƒåº¦ï¼ˆå¦‚æ™®é€šä»»åŠ¡è°ƒåº¦åˆ° GPU èŠ‚ç‚¹ï¼‰
3. **æˆæœ¬ä¼˜åŒ–**ï¼š
   - æ‰¹å¤„ç†ä»»åŠ¡ç”¨ Spot å®ä¾‹ï¼ˆèŠ‚çœ 60-90% æˆæœ¬ï¼‰
   - å…³é”®ä»»åŠ¡ç”¨æŒ‰éœ€å®ä¾‹ï¼ˆä¿éšœç¨³å®šæ€§ï¼‰
4. **ç›‘æ§æ¯ä¸ªæ± çš„åˆ©ç”¨ç‡**ï¼šé¿å…æŸä¸ªæ± è¿‡è½½è€Œå…¶ä»–æ± ç©ºé—²

```yaml
# Cluster Autoscaler é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-config
  namespace: kube-system
data:
  # æ‰©å®¹ç­–ç•¥
  scale-up-config: |
    # CPU é˜ˆå€¼
    cpuThreshold: 0.75
    # å†…å­˜é˜ˆå€¼
    memoryThreshold: 0.80
    # æ‰©å®¹å»¶è¿Ÿï¼ˆé¿å…é¢‘ç¹æ‰©å®¹ï¼‰
    scaleUpDelay: 10m
    # æœ€å°èŠ‚ç‚¹æ•°
    minNodes: 10
    # æœ€å¤§èŠ‚ç‚¹æ•°
    maxNodes: 1000
    # æ‰©å®¹æ­¥é•¿
    scaleUpIncrement: 5
    
  # ç¼©å®¹ç­–ç•¥
  scale-down-config: |
    # èŠ‚ç‚¹ç©ºé—²é˜ˆå€¼
    nodeUtilizationThreshold: 0.50
    # ç¼©å®¹å»¶è¿Ÿï¼ˆç¡®ä¿çœŸçš„ç©ºé—²ï¼‰
    scaleDownDelay: 10m
    # ä¸å¯ç¼©å®¹æ—¶é—´çª—å£
    scaleDownUnneededTime: 10m
    # ç¼©å®¹æ­¥é•¿
    scaleDownDecrement: 2

---
# HPA é…ç½®ç¤ºä¾‹
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: production-app
  minReplicas: 10
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 75
  - type: Pods
    pods:
      metric:
        name: custom_metric_qps
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 10
        periodSeconds: 60
      selectPolicy: Max
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      selectPolicy: Min
```

---

## 5. èµ„æºé…é¢ç®¡ç†

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: ResourceQuotaå’ŒLimitRangeæ˜¯K8sçš„"ç”¨ç”µé¢åº¦"â€”â€”é˜²æ­¢æŸä¸ªå›¢é˜Ÿ/åº”ç”¨å ç”¨è¿‡å¤šèµ„æºå½±å“ä»–äººã€‚å¤šç§Ÿæˆ·ç¯å¢ƒå¿…é¡»é…ç½®ï¼Œå¦åˆ™ä¸€ä¸ªå›¢é˜Ÿå¯èƒ½è€—å°½æ‰€æœ‰é›†ç¾¤èµ„æºã€‚

### 5.1 å‘½åç©ºé—´èµ„æºé…é¢

#### æ·±å…¥ç†è§£ï¼šResourceQuota â€” æ°´ç”µé…é¢åˆ¶åº¦

**ä»€ä¹ˆæ˜¯ ResourceQuota**ï¼Ÿ

å°±åƒå°åŒºç‰©ä¸šç»™æ¯æˆ·åˆ†é…ç”¨æ°´ç”¨ç”µé¢åº¦â€”â€”é˜²æ­¢æŸä¸€æˆ·è¿‡åº¦ä½¿ç”¨å¯¼è‡´å…¶ä»–ä½æˆ·æ–­æ°´æ–­ç”µã€‚åœ¨ Kubernetes å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼ŒResourceQuota é™åˆ¶æ¯ä¸ªå‘½åç©ºé—´ï¼ˆå›¢é˜Ÿï¼‰èƒ½ä½¿ç”¨çš„èµ„æºæ€»é‡ã€‚

**ResourceQuota è§£å†³çš„æ ¸å¿ƒé—®é¢˜**ï¼š

âŒ **æ²¡æœ‰ ResourceQuota çš„æ··ä¹±åœºæ™¯**ï¼š
- å¼€å‘å›¢é˜Ÿéƒ¨ç½²äº† 1000 ä¸ªæµ‹è¯• Podï¼Œå æ»¡äº†æ•´ä¸ªé›†ç¾¤
- æŸä¸ªåº”ç”¨æ²¡è®¾ç½® limitsï¼Œå†…å­˜æ³„æ¼è€—å°½äº†èŠ‚ç‚¹èµ„æº
- æ–°ä¸šåŠ¡ä¸Šçº¿æ— æ³•è°ƒåº¦ï¼Œå› ä¸ºèµ„æºè¢«å…¶ä»–å›¢é˜Ÿå ç”¨

âœ… **æœ‰ ResourceQuota çš„ç§©åº**ï¼š
- æ¯ä¸ªå›¢é˜Ÿæœ‰æ˜ç¡®çš„èµ„æºé¢åº¦
- è¶…è¿‡é¢åº¦çš„ Pod æ— æ³•åˆ›å»ºï¼ˆæå‰æ‹’ç»ï¼Œè€Œéäº‹åæ•…éšœï¼‰
- èµ„æºä½¿ç”¨å…¬å¹³ï¼Œä¸ä¼šäº’ç›¸å½±å“

---

**ResourceQuota çš„å…³é”®é…ç½®**ï¼š

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # è®¡ç®—èµ„æºï¼ˆæœ€æ ¸å¿ƒï¼‰
    requests.cpu: "500"           # è¯¥å‘½åç©ºé—´æ‰€æœ‰ Pod çš„ CPU requests æ€»å’Œä¸èƒ½è¶…è¿‡ 500 æ ¸
    requests.memory: "1000Gi"     # å†…å­˜ requests æ€»å’Œä¸èƒ½è¶…è¿‡ 1000GB
    limits.cpu: "1000"            # CPU limits æ€»å’Œä¸èƒ½è¶…è¿‡ 1000 æ ¸
    limits.memory: "2000Gi"       # å†…å­˜ limits æ€»å’Œä¸èƒ½è¶…è¿‡ 2000GB
    
    # å¯¹è±¡æ•°é‡ï¼ˆé˜²æ­¢æ— é™åˆ›å»ºï¼‰
    pods: "500"                   # æœ€å¤š 500 ä¸ª Pod
    services: "100"               # æœ€å¤š 100 ä¸ª Service
    persistentvolumeclaims: "50"  # æœ€å¤š 50 ä¸ª PVC
    
    # å­˜å‚¨
    requests.storage: "10Ti"      # å­˜å‚¨æ€»é‡ä¸èƒ½è¶…è¿‡ 10TB
```

**requests vs limits çš„åŒºåˆ«**ï¼š

| ç±»å‹ | å«ä¹‰ | å½±å“ |
|------|------|------|
| **requests** | ä¿è¯åˆ†é…çš„èµ„æºï¼ˆé¢„ç•™ï¼‰ | å½±å“è°ƒåº¦ï¼ˆèŠ‚ç‚¹æ˜¯å¦æœ‰è¶³å¤Ÿçš„ requests ç©ºé—´ï¼‰ |
| **limits** | æœ€å¤§å¯ä½¿ç”¨çš„èµ„æºï¼ˆä¸Šé™ï¼‰ | å½±å“è¿è¡Œæ—¶ï¼ˆè¶…è¿‡ limits ä¼šè¢«é™æµæˆ– OOM Killï¼‰ |

**ä¸ºä»€ä¹ˆè¦åŒæ—¶é™åˆ¶ requests å’Œ limits**ï¼Ÿ

- **åªé™åˆ¶ requests**ï¼šå¯èƒ½å¯¼è‡´ Pod å®é™…ä½¿ç”¨è¿œè¶… requestsï¼ˆå¦‚è®¾ç½® requests=1 æ ¸ï¼Œlimits=100 æ ¸ï¼‰
- **åªé™åˆ¶ limits**ï¼šå¯èƒ½å¯¼è‡´è¿‡åº¦è°ƒåº¦ï¼ˆå¦‚æ‰€æœ‰ Pod çš„ requests æ€»å’Œè¶…è¿‡é›†ç¾¤å®¹é‡ï¼‰
- **åŒæ—¶é™åˆ¶**ï¼šæ—¢ä¿è¯è°ƒåº¦åˆç†ï¼Œåˆé˜²æ­¢è¿è¡Œæ—¶èµ„æºäº‰æŠ¢

---

**å®é™…æ¡ˆä¾‹**ï¼šæŸå…¬å¸çš„å¤šç§Ÿæˆ·é…é¢è®¾è®¡

```yaml
# å›¢é˜ŸAï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰ï¼šæ ¸å¿ƒä¸šåŠ¡ï¼Œåˆ†é…æœ€å¤šèµ„æº
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-a-quota
  namespace: team-a-prod
spec:
  hard:
    requests.cpu: "500"
    requests.memory: "1000Gi"
    limits.cpu: "1000"
    limits.memory: "2000Gi"
    pods: "500"

---
# å›¢é˜ŸBï¼ˆå¼€å‘ç¯å¢ƒï¼‰ï¼šåˆ†é…è¾ƒå°‘èµ„æº
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-b-quota
  namespace: team-b-dev
spec:
  hard:
    requests.cpu: "100"
    requests.memory: "200Gi"
    limits.cpu: "200"
    limits.memory: "400Gi"
    pods: "100"

---
# å›¢é˜ŸCï¼ˆæµ‹è¯•ç¯å¢ƒï¼‰ï¼šèµ„æºå—é™
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-c-quota
  namespace: team-c-test
spec:
  hard:
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "100"
    limits.memory: "200Gi"
    pods: "50"
```

**é…é¢åˆ†é…åŸåˆ™**ï¼š
- **ç”Ÿäº§ç¯å¢ƒ**ï¼šä¼˜å…ˆçº§æœ€é«˜ï¼Œåˆ†é…å……è¶³èµ„æº
- **é¢„å‘å¸ƒç¯å¢ƒ**ï¼šæ¬¡ä¼˜å…ˆçº§ï¼Œåˆ†é…ä¸­ç­‰èµ„æº
- **å¼€å‘æµ‹è¯•ç¯å¢ƒ**ï¼šæœ€ä½ä¼˜å…ˆçº§ï¼Œåˆ†é…æœ‰é™èµ„æº

---

#### æ·±å…¥ç†è§£ï¼šLimitRange â€” å•å“é™è´­åˆ¶åº¦

**ä»€ä¹ˆæ˜¯ LimitRange**ï¼Ÿ

å¦‚æœ ResourceQuota æ˜¯"æ¯æˆ·æ€»ç”¨ç”µé‡"ï¼Œé‚£ä¹ˆ LimitRange å°±æ˜¯"å•ä¸ªç”µå™¨çš„åŠŸç‡é™åˆ¶"â€”â€”é˜²æ­¢æŸä¸€ä¸ª Pod æˆ– Container ç‹¬å èµ„æºã€‚

**LimitRange è§£å†³çš„æ ¸å¿ƒé—®é¢˜**ï¼š

âŒ **æ²¡æœ‰ LimitRange çš„é—®é¢˜**ï¼š
- å¼€å‘è€…åˆ›å»º Pod æ—¶ä¸è®¾ç½® requests/limitsï¼ˆ"å¿˜äº†"æˆ–"ä¸çŸ¥é“æ€ä¹ˆè®¾ç½®"ï¼‰
- æŸä¸ª Pod è¯·æ±‚ 100 æ ¸ CPUï¼Œå ç”¨å¤§é‡èµ„æºé…é¢
- èµ„æºç¢ç‰‡åŒ–ï¼šæœ‰çš„ Pod è¦ 10m CPUï¼Œæœ‰çš„è¦ 10 æ ¸ CPUï¼Œéš¾ä»¥è°ƒåº¦

âœ… **æœ‰ LimitRange çš„å¥½å¤„**ï¼š
- è‡ªåŠ¨ä¸ºæ²¡è®¾ç½® requests/limits çš„ Pod åˆ†é…é»˜è®¤å€¼
- é™åˆ¶å•ä¸ª Pod/Container çš„èµ„æºèŒƒå›´
- å¼ºåˆ¶æœ€ä½³å®è·µï¼ˆå¦‚ CPU limits ä¸èƒ½è¶…è¿‡ requests çš„ 10 å€ï¼‰

---

**LimitRange çš„å…³é”®é…ç½®**ï¼š

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limit-range
  namespace: production
spec:
  limits:
  # Container çº§åˆ«é™åˆ¶ï¼ˆæœ€å¸¸ç”¨ï¼‰
  - type: Container
    default:                    # å¦‚æœ Pod æ²¡è®¾ç½® limitsï¼Œè‡ªåŠ¨åˆ†é…è¿™ä¸ªå€¼
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:             # å¦‚æœ Pod æ²¡è®¾ç½® requestsï¼Œè‡ªåŠ¨åˆ†é…è¿™ä¸ªå€¼
      cpu: "100m"
      memory: "128Mi"
    max:                        # å•ä¸ª Container çš„æœ€å¤§å€¼
      cpu: "4"
      memory: "8Gi"
    min:                        # å•ä¸ª Container çš„æœ€å°å€¼
      cpu: "50m"
      memory: "64Mi"
    maxLimitRequestRatio:       # limits ä¸èƒ½è¶…è¿‡ requests çš„ N å€
      cpu: "10"                 # CPU limits æœ€å¤šæ˜¯ requests çš„ 10 å€
      memory: "2"               # å†…å­˜ limits æœ€å¤šæ˜¯ requests çš„ 2 å€
      
  # Pod çº§åˆ«é™åˆ¶
  - type: Pod
    max:
      cpu: "8"
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
```

**é…ç½®å‚æ•°è¯¦è§£**ï¼š

| å‚æ•° | ä½œç”¨ | ç¤ºä¾‹åœºæ™¯ |
|------|------|----------|
| **default** | æœªè®¾ç½® limits æ—¶çš„é»˜è®¤å€¼ | å¼€å‘è€…"å¿˜äº†"è®¾ç½®ï¼Œè‡ªåŠ¨è¡¥å…… |
| **defaultRequest** | æœªè®¾ç½® requests æ—¶çš„é»˜è®¤å€¼ | ç¡®ä¿æ‰€æœ‰ Pod éƒ½æœ‰ requestsï¼ˆå¯è°ƒåº¦ï¼‰ |
| **max** | å•ä¸ªèµ„æºçš„æœ€å¤§å€¼ | é˜²æ­¢"è´ªå¿ƒ" Podï¼ˆå¦‚è¯¯å†™ 100 æ ¸ï¼‰ |
| **min** | å•ä¸ªèµ„æºçš„æœ€å°å€¼ | å¼ºåˆ¶æœ€ä½é…ç½®ï¼ˆå¦‚å†…å­˜è‡³å°‘ 64Miï¼‰ |
| **maxLimitRequestRatio** | limits/requests çš„æœ€å¤§æ¯”ä¾‹ | é˜²æ­¢è¿‡åº¦è¶…å–ï¼ˆå¦‚ requests=1m, limits=100 æ ¸ï¼‰ |

---

**å®é™…æ¡ˆä¾‹**ï¼šæŸå…¬å¸çš„ LimitRange æœ€ä½³å®è·µ

```yaml
# ç”Ÿäº§ç¯å¢ƒï¼šä¸¥æ ¼é™åˆ¶
apiVersion: v1
kind: LimitRange
metadata:
  name: prod-limits
  namespace: production
spec:
  limits:
  - type: Container
    default:
      cpu: "1"
      memory: "1Gi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
    max:
      cpu: "8"
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    maxLimitRequestRatio:
      cpu: "4"          # limits æœ€å¤šæ˜¯ requests çš„ 4 å€
      memory: "2"       # å†…å­˜ limits æœ€å¤šæ˜¯ requests çš„ 2 å€

---
# å¼€å‘ç¯å¢ƒï¼šå®½æ¾é™åˆ¶
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limits
  namespace: development
spec:
  limits:
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    max:
      cpu: "4"
      memory: "8Gi"
    min:
      cpu: "10m"        # å…è®¸æ›´å°çš„ requestsï¼ˆå¦‚æµ‹è¯•å°åº”ç”¨ï¼‰
      memory: "32Mi"
    maxLimitRequestRatio:
      cpu: "10"         # å…è®¸æ›´å¤§çš„æ¯”ä¾‹ï¼ˆå¼€å‘ç¯å¢ƒèµ„æºå……è¶³ï¼‰
      memory: "4"
```

---

**ResourceQuota å’Œ LimitRange çš„é…åˆä½¿ç”¨**ï¼š

| ç»´åº¦ | ResourceQuota | LimitRange |
|------|---------------|------------|
| **ä½œç”¨èŒƒå›´** | æ•´ä¸ªå‘½åç©ºé—´ï¼ˆæ€»é¢åº¦ï¼‰ | å•ä¸ª Pod/Containerï¼ˆå•å“é™åˆ¶ï¼‰ |
| **ç±»æ¯”** | å®¶åº­æ€»ç”¨ç”µé‡ | å•ä¸ªç”µå™¨åŠŸç‡ |
| **å…¸å‹é™åˆ¶** | "è¿™ä¸ªå‘½åç©ºé—´æœ€å¤š 500 æ ¸ CPU" | "å•ä¸ª Pod æœ€å¤š 8 æ ¸ CPU" |
| **é…åˆæ•ˆæœ** | æ—¢æ§åˆ¶æ€»é‡ï¼Œåˆæ§åˆ¶å•ä¸ªèµ„æºå¤§å° | é˜²æ­¢èµ„æºé…é¢è¢«å°‘æ•°"è´ªå¿ƒ" Pod å æ»¡ |

**æœ€ä½³å®è·µç»„åˆ**ï¼š

```yaml
# å‘½åç©ºé—´æ€»é…é¢ï¼š500 æ ¸ CPU
ResourceQuota: requests.cpu = 500

# å•ä¸ª Container é™åˆ¶ï¼šæœ€å¤§ 4 æ ¸ CPU
LimitRange: max.cpu = 4

# ç»“æœï¼š
# âœ… æœ€å¤šå¯ä»¥è¿è¡Œ 125 ä¸ª 4 æ ¸ Podï¼ˆ500/4ï¼‰
# âœ… æˆ–è€… 500 ä¸ª 1 æ ¸ Pod
# âŒ æ— æ³•åˆ›å»ºè¶…è¿‡ 4 æ ¸çš„"è¶…çº§ Pod"
```

```yaml
# ç”Ÿäº§ç¯å¢ƒèµ„æºé…é¢æ¨¡æ¿
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # è®¡ç®—èµ„æºé™åˆ¶
    requests.cpu: "500"           # æ€» CPU è¯·æ±‚
    requests.memory: "1000Gi"     # æ€»å†…å­˜è¯·æ±‚
    limits.cpu: "1000"            # æ€» CPU é™åˆ¶
    limits.memory: "2000Gi"       # æ€»å†…å­˜é™åˆ¶
    
    # å¯¹è±¡æ•°é‡é™åˆ¶
    pods: "500"                   # æœ€å¤§ Pod æ•°
    services: "100"               # æœ€å¤§ Service æ•°
    persistentvolumeclaims: "50"  # æœ€å¤§ PVC æ•°
    configmaps: "200"             # æœ€å¤§ ConfigMap æ•°
    secrets: "200"                # æœ€å¤§ Secret æ•°
    
    # å­˜å‚¨é™åˆ¶
    requests.storage: "10Ti"      # æ€»å­˜å‚¨è¯·æ±‚
    
---
# LimitRange é…ç½®
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limit-range
  namespace: production
spec:
  limits:
  # Pod çº§åˆ«é™åˆ¶
  - type: Pod
    max:
      cpu: "8"
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
      
  # Container çº§åˆ«é™åˆ¶
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    max:
      cpu: "4"
      memory: "8Gi"
    min:
      cpu: "50m"
      memory: "64Mi"
      
  # PVC é™åˆ¶
  - type: PersistentVolumeClaim
    max:
      storage: "100Gi"
    min:
      storage: "1Gi"
```

### 5.2 èµ„æºé…é¢ç›‘æ§

```promql
# Prometheus æŸ¥è¯¢ - èµ„æºé…é¢ç›‘æ§

## 1. å‘½åç©ºé—´é…é¢ä½¿ç”¨ç‡
kube_resourcequota{type="used"} / kube_resourcequota{type="hard"} * 100

## 2. CPU é…é¢ä½¿ç”¨ç‡
kube_resourcequota{resource="requests.cpu", type="used"} / 
kube_resourcequota{resource="requests.cpu", type="hard"} * 100

## 3. å†…å­˜é…é¢ä½¿ç”¨ç‡
kube_resourcequota{resource="requests.memory", type="used"} / 
kube_resourcequota{resource="requests.memory", type="hard"} * 100

## 4. å³å°†è¶…é…é¢çš„å‘½åç©ºé—´ï¼ˆä½¿ç”¨ç‡ > 90%ï¼‰
(kube_resourcequota{type="used"} / kube_resourcequota{type="hard"}) > 0.9
```

---

## 6. æˆæœ¬ä¼˜åŒ–å®è·µ

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: æˆæœ¬ä¼˜åŒ–çš„æ ¸å¿ƒæ˜¯"ä¸æµªè´¹"â€”â€”è¯†åˆ«å¹¶å›æ”¶é—²ç½®èµ„æºã€åˆç†è®¾ç½®requests/limitsã€åˆ©ç”¨Spotå®ä¾‹ã€‚80%çš„æˆæœ¬ä¼˜åŒ–æ¥è‡ªåˆç†çš„èµ„æºè¯·æ±‚è®¾ç½®ã€‚

### 6.1 æˆæœ¬å½’å› åˆ†æ

```bash
#!/bin/bash
# cost-attribution.sh - æˆæœ¬å½’å› åˆ†æè„šæœ¬

echo "=== Kubernetes é›†ç¾¤æˆæœ¬å½’å› åˆ†æ ==="

# å‡è®¾å•ä¸ª CPU æ ¸å¿ƒæˆæœ¬ $30/æœˆï¼Œ1GB å†…å­˜æˆæœ¬ $5/æœˆ

CPU_COST_PER_CORE=30
MEMORY_COST_PER_GB=5

kubectl get namespaces -o json | jq -r '.items[].metadata.name' | while read ns; do
  echo ""
  echo "å‘½åç©ºé—´: $ns"
  
  # è·å– CPU å’Œå†…å­˜è¯·æ±‚æ€»é‡
  cpu_total=$(kubectl get pods -n $ns -o json | jq -r '
    [.items[].spec.containers[].resources.requests.cpu // "0" | 
     gsub("m"; "") | tonumber] | add / 1000
  ')
  
  mem_total=$(kubectl get pods -n $ns -o json | jq -r '
    [.items[].spec.containers[].resources.requests.memory // "0" | 
     gsub("Mi"; "") | tonumber] | add / 1024
  ')
  
  # è®¡ç®—æˆæœ¬
  cpu_cost=$(echo "$cpu_total * $CPU_COST_PER_CORE" | bc)
  mem_cost=$(echo "$mem_total * $MEMORY_COST_PER_GB" | bc)
  total_cost=$(echo "$cpu_cost + $mem_cost" | bc)
  
  echo "  CPU è¯·æ±‚: ${cpu_total} æ ¸ â†’ æˆæœ¬: \$$cpu_cost/æœˆ"
  echo "  å†…å­˜è¯·æ±‚: ${mem_total} GB â†’ æˆæœ¬: \$$mem_cost/æœˆ"
  echo "  æ€»æˆæœ¬: \$$total_cost/æœˆ"
done
```

### 6.2 æˆæœ¬ä¼˜åŒ–å»ºè®®

| ä¼˜åŒ–ç­–ç•¥ | èŠ‚çœæ½œåŠ› | å®æ–½éš¾åº¦ | é£é™©ç­‰çº§ | é€‚ç”¨åœºæ™¯ |
|----------|----------|----------|----------|----------|
| **ä½¿ç”¨ Spot å®ä¾‹** | 60-90% | â­â­â­ | ä¸­ | æ— çŠ¶æ€å·¥ä½œè´Ÿè½½ |
| **Right-sizing** | 20-40% | â­â­ | ä½ | æ‰€æœ‰å·¥ä½œè´Ÿè½½ |
| **é¢„ç•™å®ä¾‹** | 30-50% | â­ | ä½ | ç¨³å®šå·¥ä½œè´Ÿè½½ |
| **è‡ªåŠ¨ç¼©å®¹** | 10-30% | â­â­â­ | ä½ | æ³¢åŠ¨æ€§å·¥ä½œè´Ÿè½½ |
| **å­˜å‚¨åˆ†å±‚** | 15-35% | â­â­ | ä½ | å¤§é‡å­˜å‚¨éœ€æ±‚ |
| **å¤šç§Ÿæˆ·å…±äº«** | 20-40% | â­â­â­â­ | ä¸­ | å¼€å‘/æµ‹è¯•ç¯å¢ƒ |

---

## 7. å®¹é‡ç›‘æ§ä¸å‘Šè­¦

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: å®¹é‡å‘Šè­¦åº”è¯¥æ˜¯"é¢„è­¦"è€Œé"æŠ¥è­¦"â€”â€”åœ¨èµ„æºè€—å°½ä¹‹å‰å°±é€šçŸ¥ä½ ã€‚è®¾ç½®å¤šçº§æ°´ä½çº¿ï¼š70%é¢„è­¦â†’85%å‘Šè­¦â†’95%ç´§æ€¥ï¼Œç»™è¿ç»´ç•™å‡ºæ‰©å®¹æ—¶é—´ã€‚

### 7.1 å®¹é‡å‘Šè­¦è§„åˆ™

```yaml
# Prometheus å‘Šè­¦è§„åˆ™ - å®¹é‡ç®¡ç†
groups:
- name: capacity_alerts
  interval: 1m
  rules:
  
  # é›†ç¾¤ CPU å®¹é‡å‘Šè­¦
  - alert: ClusterCPUCapacityLow
    expr: |
      (sum(kube_node_status_allocatable{resource="cpu"}) - 
       sum(kube_pod_container_resource_requests{resource="cpu"})) < 50
    for: 30m
    labels:
      severity: warning
      category: capacity
    annotations:
      summary: "é›†ç¾¤ CPU å¯ç”¨å®¹é‡ä¸è¶³"
      description: "é›†ç¾¤å‰©ä½™å¯è°ƒåº¦ CPU < 50 æ ¸ï¼Œéœ€è¦è€ƒè™‘æ‰©å®¹"
      
  # é›†ç¾¤å†…å­˜å®¹é‡å‘Šè­¦
  - alert: ClusterMemoryCapacityLow
    expr: |
      (sum(kube_node_status_allocatable{resource="memory"}) - 
       sum(kube_pod_container_resource_requests{resource="memory"})) / 1024 / 1024 / 1024 < 200
    for: 30m
    labels:
      severity: warning
      category: capacity
    annotations:
      summary: "é›†ç¾¤å†…å­˜å¯ç”¨å®¹é‡ä¸è¶³"
      description: "é›†ç¾¤å‰©ä½™å¯è°ƒåº¦å†…å­˜ < 200GBï¼Œéœ€è¦è€ƒè™‘æ‰©å®¹"
      
  # èµ„æºé…é¢å³å°†è€—å°½
  - alert: NamespaceQuotaAlmostExhausted
    expr: |
      kube_resourcequota{type="used"} / kube_resourcequota{type="hard"} > 0.90
    for: 15m
    labels:
      severity: warning
      category: quota
    annotations:
      summary: "å‘½åç©ºé—´ {{ $labels.namespace }} èµ„æºé…é¢å³å°†è€—å°½"
      description: "èµ„æº {{ $labels.resource }} ä½¿ç”¨ç‡ {{ $value | humanizePercentage }}"
      
  # Pod å¯†åº¦è¿‡é«˜
  - alert: NodePodDensityHigh
    expr: |
      kube_node_status_capacity{resource="pods"} - 
      kube_node_status_allocatable{resource="pods"} < 20
    for: 1h
    labels:
      severity: info
      category: capacity
    annotations:
      summary: "èŠ‚ç‚¹ {{ $labels.node }} Pod å¯†åº¦è¿‡é«˜"
      description: "å‰©ä½™å¯è°ƒåº¦ Pod æ•° < 20"
      
  # etcd æ•°æ®åº“å®¹é‡å‘Šè­¦
  - alert: EtcdDatabaseSizeLarge
    expr: |
      etcd_mvcc_db_total_size_in_bytes / 1024 / 1024 / 1024 > 6
    for: 10m
    labels:
      severity: critical
      category: capacity
    annotations:
      summary: "etcd æ•°æ®åº“å¤§å°è¶…è¿‡ 6GB"
      description: "å½“å‰å¤§å°: {{ $value | humanize }}GBï¼Œéœ€è¦æ‰§è¡Œç¢ç‰‡æ•´ç†æˆ–æ‰©å®¹"
```

---

## 8. å®æˆ˜æ¡ˆä¾‹åˆ†æ

> **ğŸ”° åˆå­¦è€…å¯¼è¯»**: çœŸå®çš„å®¹é‡è§„åˆ’æ¡ˆä¾‹å¸®ä½ ç†è§£ç†è®ºå¦‚ä½•åº”ç”¨åˆ°å®è·µã€‚æ¯ä¸ªæ¡ˆä¾‹åŒ…å«ï¼šèƒŒæ™¯â†’æŒ‘æˆ˜â†’æ–¹æ¡ˆâ†’ç»“æœâ†’æ•™è®­ã€‚

### 8.1 æ¡ˆä¾‹1ï¼šç”µå•†å¤§ä¿ƒå®¹é‡è§„åˆ’

**èƒŒæ™¯**
- æ—¥å¸¸ DAU: 100 ä¸‡
- å¤§ä¿ƒé¢„æœŸ DAU: 500 ä¸‡
- å³°å€¼å€æ•°: 10x
- æ´»åŠ¨æ—¶é•¿: 24 å°æ—¶

**å®¹é‡è®¡ç®—**
```python
# å®¹é‡éœ€æ±‚è®¡ç®—
daily_users = 5_000_000
peak_multiplier = 10
avg_user_cpu = 0.01  # å•ç”¨æˆ·å¹³å‡æ¶ˆè€— 0.01 æ ¸ CPU
avg_user_memory = 50  # å•ç”¨æˆ·å¹³å‡æ¶ˆè€— 50MB å†…å­˜
redundancy_factor = 1.5  # å†—ä½™ç³»æ•°

peak_users = daily_users * peak_multiplier / 24  # å³°å€¼å¹¶å‘ç”¨æˆ·
required_cpu = peak_users * avg_user_cpu * redundancy_factor
required_memory = peak_users * avg_user_memory * redundancy_factor / 1024  # GB

print(f"å³°å€¼å¹¶å‘ç”¨æˆ·: {peak_users:,.0f}")
print(f"æ‰€éœ€ CPU æ ¸å¿ƒæ•°: {required_cpu:,.0f}")
print(f"æ‰€éœ€å†…å­˜ (GB): {required_memory:,.0f}")

# è¾“å‡º:
# å³°å€¼å¹¶å‘ç”¨æˆ·: 2,083,333
# æ‰€éœ€ CPU æ ¸å¿ƒæ•°: 31,250
# æ‰€éœ€å†…å­˜ (GB): 1,526
```

**æ‰©å®¹æ–¹æ¡ˆ**
| èµ„æºç±»å‹ | å½“å‰å®¹é‡ | å¤§ä¿ƒéœ€æ±‚ | æ‰©å®¹é‡ | æ‰©å®¹æ–¹å¼ | æˆæœ¬ |
|----------|----------|----------|--------|----------|------|
| CPU | 10,000 æ ¸ | 31,250 æ ¸ | +21,250 æ ¸ | Spot å®ä¾‹ | $12,750/å¤© |
| å†…å­˜ | 50 TB | 1.5 TB | +100 TB | æŒ‰éœ€å®ä¾‹ | $500/å¤© |
| å­˜å‚¨ | 100 TB | 150 TB | +50 TB | SSD | $2,500/æœˆ |

### 8.2 æ¡ˆä¾‹2ï¼šAI è®­ç»ƒé›†ç¾¤å®¹é‡è§„åˆ’

**èƒŒæ™¯**
- GPU ç±»å‹: NVIDIA A100 (40GB)
- è®­ç»ƒä»»åŠ¡: å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒ
- æ¨¡å‹å¤§å°: 70B å‚æ•°
- å¹¶å‘è®­ç»ƒä»»åŠ¡: 10 ä¸ª

**GPU éœ€æ±‚è®¡ç®—**
```yaml
gpu-capacity-planning:
  model-size: 70B å‚æ•°
  gpu-memory-per-model: 280GB  # FP16 æ ¼å¼
  gpus-per-task: 8  # 8-way æ¨¡å‹å¹¶è¡Œ
  concurrent-tasks: 10
  total-gpus-needed: 80
  
  redundancy:
    æ•…éšœå†—ä½™: 10%
    è°ƒåº¦ç¼“å†²: 15%
    
  final-gpu-count: 100  # 80 * 1.25
  
  infrastructure:
    nodes: 13  # æ¯èŠ‚ç‚¹ 8 GPU
    node-type: "p4d.24xlarge"
    networking: "800 Gbps EFA"
    storage: "FSx for Lustre (100 TB)"
```

---

**è¡¨æ ¼åº•éƒ¨æ ‡è®°**: Kusheet Project | ä½œè€…: Allen Galler (allengaller@gmail.com) | æœ€åæ›´æ–°: 2026-02 | ç‰ˆæœ¬: v1.25-v1.32 | è´¨é‡ç­‰çº§: â­â­â­â­â­ ä¸“å®¶çº§

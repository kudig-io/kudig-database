# 01 - Kubernetes ç”Ÿäº§ç¯å¢ƒè¿ç»´æœ€ä½³å®è·µå­—å…¸

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25-v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä½œè€…**: Allen Galler | **è´¨é‡ç­‰çº§**: â­â­â­â­â­ ä¸“å®¶çº§

> **ç”Ÿäº§ç¯å¢ƒå®æˆ˜ç»éªŒæ€»ç»“**: åŸºäºä¸‡çº§èŠ‚ç‚¹é›†ç¾¤è¿ç»´ç»éªŒï¼Œæ¶µç›–ä»æ•…éšœå¤„ç†åˆ°æ€§èƒ½ä¼˜åŒ–çš„å…¨æ–¹ä½æœ€ä½³å®è·µ

---

## ç›®å½•

- [1. ç”Ÿäº§ç¯å¢ƒé…ç½®æ ‡å‡†](#1-ç”Ÿäº§ç¯å¢ƒé…ç½®æ ‡å‡†)
- [2. é«˜å¯ç”¨æ¶æ„æ¨¡å¼](#2-é«˜å¯ç”¨æ¶æ„æ¨¡å¼)
- [3. å®‰å…¨åŠ å›ºæŒ‡å—](#3-å®‰å…¨åŠ å›ºæŒ‡å—)
- [4. ç›‘æ§å‘Šè­¦æœ€ä½³å®è·µ](#4-ç›‘æ§å‘Šè­¦æœ€ä½³å®è·µ)
- [5. ç¾å¤‡æ¢å¤æ–¹æ¡ˆ](#5-ç¾å¤‡æ¢å¤æ–¹æ¡ˆ)
- [6. è‡ªåŠ¨åŒ–è¿ç»´ç­–ç•¥](#6-è‡ªåŠ¨åŒ–è¿ç»´ç­–ç•¥)
- [7. æˆæœ¬ä¼˜åŒ–å®è·µ](#7-æˆæœ¬ä¼˜åŒ–å®è·µ)
- [8. å¤šé›†ç¾¤ç®¡ç†è§„èŒƒ](#8-å¤šé›†ç¾¤ç®¡ç†è§„èŒƒ)

---

## 1. ç”Ÿäº§ç¯å¢ƒé…ç½®æ ‡å‡†

### 1.1 é›†ç¾¤é…ç½®åŸºçº¿

| é…ç½®é¡¹ | æ¨èå€¼ | è¯´æ˜ | é£é™©ç­‰çº§ |
|-------|--------|------|---------|
| **API Serverå¹¶å‘é™åˆ¶** | `--max-requests-inflight=400` | æ§åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡ | ä¸­ |
| | `--max-mutating-requests-inflight=200` | å†™æ“ä½œå¹¶å‘é™åˆ¶ | ä¸­ |
| **etcdå­˜å‚¨é…é¢** | `--quota-backend-bytes=8GB` | å­˜å‚¨ç©ºé—´é™åˆ¶ | é«˜ |
| **äº‹ä»¶ä¿ç•™æ—¶é—´** | `--event-ttl=1h` | å‡å°‘etcdå­˜å‚¨å‹åŠ› | ä½ |
| **èŠ‚ç‚¹æœ€å¤§Podæ•°** | `--max-pods=110` | æ ‡å‡†ç¯å¢ƒé…ç½® | ä¸­ |
| | `--max-pods=500` | AWSäº‘ç¯å¢ƒé…ç½® | é«˜ |
| **é•œåƒåƒåœ¾å›æ”¶** | `--image-gc-high-threshold=85` | é«˜æ°´ä½è§¦å‘GC | ä¸­ |
| | `--image-gc-low-threshold=80` | ä½æ°´ä½åœæ­¢GC | ä¸­ |

### 1.2 èµ„æºé…ç½®æ ‡å‡†æ¨¡æ¿

```yaml
# ========== ç”Ÿäº§ç¯å¢ƒDeploymentæ ‡å‡†é…ç½® ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: production-app-standard
  namespace: production
  labels:
    app: production-app
    tier: backend
    version: v1.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: production-app
  template:
    metadata:
      labels:
        app: production-app
        version: v1.0
      annotations:
        # æ³¨å…¥æ„å»ºä¿¡æ¯
        build.timestamp: "2026-02-05T10:30:00Z"
        build.commit: "a1b2c3d4"
    spec:
      # ä¼˜å…ˆçº§è®¾ç½®
      priorityClassName: high-priority
      
      # èŠ‚ç‚¹é€‰æ‹©ç­–ç•¥
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - production-app
              topologyKey: kubernetes.io/hostname
      
      # å®¹å¿æ±¡ç‚¹
      tolerations:
      - key: dedicated
        operator: Equal
        value: production
        effect: NoSchedule
        
      containers:
      - name: app
        image: registry.example.com/app:v1.0
        imagePullPolicy: Always
        
        # æ ¸å¿ƒèµ„æºé…ç½®
        resources:
          requests:
            cpu: "250m"
            memory: "512Mi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
            
        # å¥åº·æ£€æŸ¥é…ç½®
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
          
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
          
        # å¯åŠ¨æ¢é’ˆï¼ˆK8s 1.18+ï¼‰
        startupProbe:
          httpGet:
            path: /startup
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 30
          
        # ç¯å¢ƒå˜é‡é…ç½®
        env:
        - name: LOG_LEVEL
          value: "INFO"
        - name: JAVA_OPTS
          value: "-Xmx768m -Xms512m -XX:+UseG1GC"
        - name: GOMEMLIMIT
          value: "800MiB"
          
        # å®‰å…¨ä¸Šä¸‹æ–‡
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          
        # æŒ‚è½½å·
        volumeMounts:
        - name: tmp-volume
          mountPath: /tmp
        - name: logs-volume
          mountPath: /var/log/app
          
      volumes:
      - name: tmp-volume
        emptyDir: {}
      - name: logs-volume
        persistentVolumeClaim:
          claimName: app-logs-pvc
```

### 1.3 ç½‘ç»œç­–ç•¥æ ‡å‡†

```yaml
# ========== é»˜è®¤æ‹’ç»ç½‘ç»œç­–ç•¥ ==========
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# ========== å…è®¸DNSæŸ¥è¯¢ç­–ç•¥ ==========
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-access
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53

---
# ========== åº”ç”¨é—´é€šä¿¡ç­–ç•¥ ==========
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-communication-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: backend
    ports:
    - protocol: TCP
      port: 8080
```

---

## 2. é«˜å¯ç”¨æ¶æ„æ¨¡å¼

### 2.1 æ§åˆ¶å¹³é¢é«˜å¯ç”¨

```yaml
# ========== ç”Ÿäº§ç¯å¢ƒæ§åˆ¶å¹³é¢é…ç½® ==========
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
metadata:
  name: production-cluster
networking:
  serviceSubnet: "10.96.0.0/12"
  podSubnet: "10.244.0.0/16"
  dnsDomain: "cluster.local"
etcd:
  local:
    extraArgs:
      listen-client-urls: "https://0.0.0.0:2379"
      advertise-client-urls: "https://ETCD_IP:2379"
      initial-cluster-token: "etcd-cluster-1"
      initial-cluster-state: "new"
      auto-compaction-mode: "periodic"
      auto-compaction-retention: "1"
    serverCertSANs:
    - "etcd01.example.com"
    - "etcd02.example.com"
    - "etcd03.example.com"
apiServer:
  certSANs:
  - "k8s-api.example.com"
  - "10.0.0.100"  # Load Balancer VIP
  extraArgs:
    authorization-mode: "Node,RBAC"
    enable-bootstrap-token-auth: "true"
    encryption-provider-config: "/etc/kubernetes/encryption-config.yaml"
controllerManager:
  extraArgs:
    cluster-signing-cert-file: "/etc/kubernetes/pki/ca.crt"
    cluster-signing-key-file: "/etc/kubernetes/pki/ca.key"
scheduler:
  extraArgs:
    bind-address: "0.0.0.0"
```

### 2.2 åº”ç”¨å±‚é¢é«˜å¯ç”¨

```yaml
# ========== å¤šåŒºåŸŸéƒ¨ç½²ç­–ç•¥ ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-region-app
  namespace: production
spec:
  replicas: 6
  selector:
    matchLabels:
      app: multi-region-app
  template:
    metadata:
      labels:
        app: multi-region-app
    spec:
      affinity:
        # è·¨å¯ç”¨åŒºåˆ†å¸ƒ
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - multi-region-app
            topologyKey: topology.kubernetes.io/zone
            
        # èŠ‚ç‚¹äº²å’Œæ€§
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - us-west-1
                - us-east-1
                
      # æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: multi-region-app
```

---

## 3. å®‰å…¨åŠ å›ºæŒ‡å—

### 3.1 Podå®‰å…¨æ ‡å‡†

```yaml
# ========== ç”Ÿäº§ç¯å¢ƒPodå®‰å…¨é…ç½® ==========
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
  namespace: production
spec:
  # æœåŠ¡è´¦æˆ·
  serviceAccountName: app-service-account
  
  # å®‰å…¨ä¸Šä¸‹æ–‡
  securityContext:
    runAsNonRoot: true
    runAsUser: 10001
    fsGroup: 2000
    supplementalGroups: [3000]
    
  containers:
  - name: app
    image: registry.example.com/secure-app:v1.0
    securityContext:
      # å®¹å™¨å®‰å…¨è®¾ç½®
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 10001
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE  # å¦‚éœ€ç»‘å®šä½ç«¯å£
        
    # åªè¯»æŒ‚è½½é‡è¦ç›®å½•
    volumeMounts:
    - name: tmpfs
      mountPath: /tmp
    - name: app-config
      mountPath: /config
      readOnly: true
      
  volumes:
  - name: tmpfs
    emptyDir:
      medium: Memory
  - name: app-config
    configMap:
      name: app-config
```

### 3.2 ç½‘ç»œå®‰å…¨ç­–ç•¥

```yaml
# ========== ç”Ÿäº§ç½‘ç»œå®‰å…¨ç­–ç•¥ ==========
apiVersion: security.k8s.io/v1
kind: PodSecurityPolicy
metadata:
  name: production-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - emptyDir
  - projected
  - secret
  - downwardAPI
  - persistentVolumeClaim
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: true

---
# ========== RBACæœ€å°æƒé™åŸåˆ™ ==========
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: app-developer-role
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-developer-binding
  namespace: production
subjects:
- kind: User
  name: developer@example.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: app-developer-role
  apiGroup: rbac.authorization.k8s.io
```

---

## 4. ç›‘æ§å‘Šè­¦æœ€ä½³å®è·µ

### 4.1 æ ¸å¿ƒç›‘æ§æŒ‡æ ‡

```yaml
# ========== Prometheusæ ¸å¿ƒå‘Šè­¦è§„åˆ™ ==========
groups:
- name: kubernetes.system.rules
  rules:
  # API Serverç›‘æ§
  - alert: APIServerDown
    expr: up{job="apiserver"} == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "API Serverå®ä¾‹ {{ $labels.instance }} ä¸å¯ç”¨"
      description: "API Serverå·²ç»å®•æœºè¶…è¿‡2åˆ†é’Ÿï¼Œè¯·ç«‹å³å¤„ç†"

  - alert: APIServerLatencyHigh
    expr: histogram_quantile(0.99, rate(apiserver_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "API Serverå“åº”å»¶è¿Ÿè¿‡é«˜"
      description: "99thç™¾åˆ†ä½å“åº”æ—¶é—´è¶…è¿‡1ç§’"

  # etcdç›‘æ§
  - alert: EtcdNoLeader
    expr: etcd_server_has_leader == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "etcdé›†ç¾¤æ— é¢†å¯¼è€…"
      description: "etcdé›†ç¾¤å·²å¤±å»é¢†å¯¼è€…è¶…è¿‡1åˆ†é’Ÿ"

  - alert: EtcdHighFsyncDuration
    expr: histogram_quantile(0.99, etcd_disk_backend_commit_duration_seconds_bucket) > 0.5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "etcdç£ç›˜åŒæ­¥å»¶è¿Ÿé«˜"
      description: "99thç™¾åˆ†ä½fsyncå»¶è¿Ÿè¶…è¿‡500ms"

  # èŠ‚ç‚¹ç›‘æ§
  - alert: NodeNotReady
    expr: kube_node_status_condition{condition="Ready",status="true"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "èŠ‚ç‚¹ {{ $labels.node }} ä¸å¯ç”¨"
      description: "èŠ‚ç‚¹å·²å¤„äºNotReadyçŠ¶æ€è¶…è¿‡5åˆ†é’Ÿ"

  - alert: NodeMemoryPressure
    expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "èŠ‚ç‚¹ {{ $labels.node }} å†…å­˜å‹åŠ›å¤§"
      description: "èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡è¾¾åˆ°è­¦å‘Šé˜ˆå€¼"

  # Podç›‘æ§
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0.2
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} é¢‘ç¹é‡å¯"
      description: "Podé‡å¯é¢‘ç‡è¶…è¿‡æ¯åˆ†é’Ÿ0.2æ¬¡"

  - alert: PodNotReady
    expr: kube_pod_status_ready{condition="true"} == 0
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} æœªå°±ç»ª"
      description: "Podé•¿æ—¶é—´æœªè¿›å…¥ReadyçŠ¶æ€"
```

### 4.2 åº”ç”¨ç›‘æ§é…ç½®

```yaml
# ========== ServiceMonitoré…ç½® ==========
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: app-monitor
  namespace: monitoring
  labels:
    team: sre
spec:
  selector:
    matchLabels:
      app: production-app
  namespaceSelector:
    matchNames:
    - production
  endpoints:
  - port: http-metrics
    interval: 30s
    path: /metrics
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_service_name]
      targetLabel: service
    
  # è‡ªå®šä¹‰æŒ‡æ ‡é‡‡é›†
  - port: http-app
    interval: 60s
    path: /actuator/prometheus
    params:
      include: ["jvm.memory.used", "http.server.requests"]
```

---

## 5. ç¾å¤‡æ¢å¤æ–¹æ¡ˆ

### 5.1 etcdå¤‡ä»½ç­–ç•¥

```bash
#!/bin/bash
# ========== etcdå¤‡ä»½è„šæœ¬ ==========
set -euo pipefail

BACKUP_DIR="/backup/etcd"
DATE=$(date +%Y%m%d_%H%M%S)
ETCDCTL_API=3

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p ${BACKUP_DIR}/${DATE}

# æ‰§è¡Œå¿«ç…§å¤‡ä»½
etcdctl --endpoints=https://127.0.0.1:2379 \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  snapshot save ${BACKUP_DIR}/${DATE}/etcd-snapshot.db

# éªŒè¯å¤‡ä»½å®Œæ•´æ€§
etcdctl --write-out=table snapshot status ${BACKUP_DIR}/${DATE}/etcd-snapshot.db

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
tar -czf ${BACKUP_DIR}/${DATE}.tar.gz -C ${BACKUP_DIR} ${DATE}

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
find ${BACKUP_DIR} -name "*.tar.gz" -mtime +7 -delete
find ${BACKUP_DIR} -mindepth 1 -maxdepth 1 -type d -empty -delete

echo "etcd backup completed: ${BACKUP_DIR}/${DATE}.tar.gz"
```

### 5.2 åº”ç”¨æ•°æ®å¤‡ä»½

```yaml
# ========== Veleroå¤‡ä»½é…ç½® ==========
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-backup
  namespace: velero
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨2ç‚¹
  template:
    includedNamespaces:
    - production
    - staging
    excludedNamespaces:
    - kube-system
    - monitoring
    includedResources:
    - deployments
    - services
    - configmaps
    - secrets
    - persistentvolumeclaims
    labelSelector:
      matchLabels:
        backup: enabled
    snapshotVolumes: true
    ttl: 168h  # ä¿ç•™7å¤©

---
# ========== ç¾éš¾æ¢å¤æ¼”ç»ƒé…ç½® ==========
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: dr-test-restore
  namespace: velero
spec:
  backupName: daily-backup-20260205020000
  includedNamespaces:
  - production-dr-test
  restorePVs: true
  preserveNodePorts: true
```

---

## 6. è‡ªåŠ¨åŒ–è¿ç»´ç­–ç•¥

### 6.1 GitOpsæµæ°´çº¿

```yaml
# ========== ArgoCDåº”ç”¨é…ç½® ==========
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: production-app
  namespace: argocd
spec:
  project: production
  source:
    repoURL: https://github.com/company/production-app.git
    targetRevision: HEAD
    path: k8s/overlays/production
    helm:
      valueFiles:
      - values-production.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
    syncOptions:
    - CreateNamespace=true
    - PruneLast=true

---
# ========== å¤šç¯å¢ƒé…ç½®ç®¡ç† ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-environment-config
  namespace: production
data:
  # ç”Ÿäº§ç¯å¢ƒç‰¹å®šé…ç½®
  DATABASE_URL: "postgresql://prod-db:5432/app"
  LOG_LEVEL: "WARN"
  CACHE_TTL: "300"
  ENABLE_DEBUG: "false"
  MAX_CONNECTIONS: "100"
  
  # å®‰å…¨é…ç½®
  TLS_MIN_VERSION: "TLS1.2"
  HSTS_MAX_AGE: "31536000"
  CORS_ALLOWED_ORIGINS: "https://app.example.com"
```

### 6.2 è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®

```yaml
# ========== HPAé«˜çº§é…ç½® ==========
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      selectPolicy: Max

---
# ========== VPAé…ç½® ==========
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 256Mi
      maxAllowed:
        cpu: 2000m
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
```

---

## 7. æˆæœ¬ä¼˜åŒ–å®è·µ

### 7.1 èµ„æºä¼˜åŒ–ç­–ç•¥

```yaml
# ========== æˆæœ¬ä¼˜åŒ–èµ„æºé…ç½® ==========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cost-optimized-app
  namespace: production
spec:
  replicas: 3
  template:
    spec:
      # Spotå®ä¾‹å®¹å¿
      tolerations:
      - key: spot-instance
        operator: Equal
        value: "true"
        effect: NoSchedule
        
      # èŠ‚ç‚¹äº²å’Œæ€§ - ä¼˜å…ˆä½¿ç”¨æˆæœ¬è¾ƒä½çš„å®ä¾‹
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values:
                - t3.medium
                - t3.large
          - weight: 50
            preference:
              matchExpressions:
              - key: cloud.google.com/gke-preemptible
                operator: In
                values:
                - "true"
                
      containers:
      - name: app
        image: app:v1.0
        resources:
          requests:
            # åŸºäºå®é™…ä½¿ç”¨é‡ç²¾ç¡®é…ç½®
            cpu: "150m"
            memory: "384Mi"
          limits:
            # åˆç†çš„ä¸Šé™ï¼Œé¿å…æµªè´¹
            cpu: "500m"
            memory: "768Mi"
            
        # åº”ç”¨å±‚ä¼˜åŒ–
        env:
        - name: JAVA_OPTS
          value: "-Xmx640m -Xms384m -XX:MaxRAMPercentage=80.0"
        - name: GOMEMLIMIT
          value: "680MiB"
```

### 7.2 æˆæœ¬ç›‘æ§å‘Šè­¦

```yaml
# ========== æˆæœ¬ç›‘æ§å‘Šè­¦è§„åˆ™ ==========
groups:
- name: cost.monitoring.rules
  rules:
  - alert: HighResourceUtilizationCost
    expr: avg(rate(container_cpu_usage_seconds_total[1h])) by (namespace) * 100 > 80
    for: 1h
    labels:
      severity: warning
    annotations:
      summary: "å‘½åç©ºé—´ {{ $labels.namespace }} CPUä½¿ç”¨ç‡è¿‡é«˜"
      description: "å¹³å‡CPUä½¿ç”¨ç‡è¶…è¿‡80%ï¼Œå¯èƒ½å­˜åœ¨èµ„æºé…ç½®è¿‡åº¦"

  - alert: MemoryOverProvisioned
    expr: (kube_pod_container_resource_limits_memory_bytes - container_memory_working_set_bytes) / kube_pod_container_resource_limits_memory_bytes * 100 > 50
    for: 6h
    labels:
      severity: info
    annotations:
      summary: "å†…å­˜è¿‡åº¦é…ç½®"
      description: "Podå†…å­˜é¢„ç•™é‡è¶…è¿‡å®é™…ä½¿ç”¨é‡50%ä»¥ä¸Š"

  - alert: UnusedPersistentVolumes
    expr: kube_persistentvolume_status_phase{phase="Available"} == 1
    for: 24h
    labels:
      severity: warning
    annotations:
      summary: "å­˜åœ¨æœªä½¿ç”¨çš„æŒä¹…å·"
      description: "æ£€æµ‹åˆ°é—²ç½®çš„PVï¼Œå»ºè®®æ¸…ç†ä»¥é™ä½æˆæœ¬"
```

---

## 8. å¤šé›†ç¾¤ç®¡ç†è§„èŒƒ

### 8.1 é›†ç¾¤è”é‚¦é…ç½®

```yaml
# ========== Cluster APIé…ç½® ==========
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: production-cluster-us-west
  namespace: capi-system
spec:
  clusterNetwork:
    services:
      cidrBlocks: ["10.128.0.0/12"]
    pods:
      cidrBlocks: ["10.0.0.0/8"]
    serviceDomain: "cluster.local"
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AWSCluster
    name: production-cluster-us-west
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: production-cluster-us-west-control-plane

---
# ========== å¤šé›†ç¾¤æœåŠ¡å‘ç° ==========
apiVersion: multicluster.x-k8s.io/v1alpha1
kind: ServiceExport
metadata:
  name: global-service
  namespace: production
spec: {}

---
apiVersion: multicluster.x-k8s.io/v1alpha1
kind: ServiceImport
metadata:
  name: global-service
  namespace: production
spec:
  type: ClusterSetIP
  ports:
  - name: http
    protocol: TCP
    port: 80
```

### 8.2 ç»Ÿä¸€ç›‘æ§é…ç½®

```yaml
# ========== Thanoså¤šé›†ç¾¤ç›‘æ§ ==========
apiVersion: v1
kind: Service
metadata:
  name: thanos-sidecar
  namespace: monitoring
  labels:
    app: thanos-sidecar
spec:
  ports:
  - name: grpc
    port: 10901
    targetPort: 10901
  - name: http
    port: 10902
    targetPort: 10902
  clusterIP: None

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: thanos-query
  namespace: monitoring
spec:
  serviceName: thanos-query
  replicas: 2
  selector:
    matchLabels:
      app: thanos-query
  template:
    metadata:
      labels:
        app: thanos-query
    spec:
      containers:
      - name: thanos-query
        image: quay.io/thanos/thanos:v0.32.0
        args:
        - query
        - --grpc-address=0.0.0.0:10901
        - --http-address=0.0.0.0:10902
        - --store=dnssrv+_grpc._tcp.thanos-sidecar.monitoring.svc.cluster.local
        - --query.replica-label=replica
        ports:
        - name: grpc
          containerPort: 10901
        - name: http
          containerPort: 10902
```

---

## 9. ç”Ÿäº§ç¯å¢ƒæ•…éšœåº”æ€¥å“åº”

### 9.1 æ•…éšœåˆ†çº§å“åº”æœºåˆ¶

| æ•…éšœç­‰çº§ | å“åº”æ—¶é—´ | é€šçŸ¥èŒƒå›´ | å¤„ç†æµç¨‹ | è®°å½•è¦æ±‚ |
|---------|---------|---------|---------|---------|
| **P0 - æ ¸å¿ƒæœåŠ¡ä¸­æ–­** | 5åˆ†é’Ÿå†…å“åº” | å…¨ä½“æŠ€æœ¯å›¢é˜Ÿ+ç®¡ç†å±‚ | ç«‹å³ç»„å»ºåº”æ€¥å°ç»„ï¼Œå¯åŠ¨åº”æ€¥é¢„æ¡ˆ | è¯¦ç»†æ•…éšœæ—¶é—´çº¿è®°å½• |
| **P1 - é‡è¦åŠŸèƒ½å¼‚å¸¸** | 30åˆ†é’Ÿå†…å“åº” | ç›¸å…³æŠ€æœ¯å›¢é˜Ÿ | æŒ‡å®šè´Ÿè´£äººå¤„ç†ï¼Œå®šæœŸåŒæ­¥è¿›å±• | æ•…éšœåˆ†ææŠ¥å‘Šå¿…å¡« |
| **P2 - ä¸€èˆ¬æ€§é—®é¢˜** | 2å°æ—¶å†…å“åº” | å¯¹åº”æ¨¡å—è´Ÿè´£äºº | æŒ‰æ­£å¸¸æµç¨‹å¤„ç†ï¼Œçº³å…¥å‘¨æŠ¥ | é—®é¢˜è·Ÿè¸ªè®°å½• |
| **P3 - ä¼˜åŒ–å»ºè®®ç±»** | ä¸‹ä¸€å·¥ä½œæ—¥å¤„ç† | ç›¸å…³äººå‘˜ | çº³å…¥æ”¹è¿›è®¡åˆ’ | éœ€æ±‚æ± ç®¡ç† |

### 9.2 åº”æ€¥å“åº”æ ‡å‡†æ“ä½œç¨‹åº(SOP)

```bash
#!/bin/bash
# ========== ç”Ÿäº§ç¯å¢ƒåº”æ€¥å“åº”è„šæœ¬ ==========
set -euo pipefail

INCIDENT_ID=$(date +%Y%m%d_%H%M%S)_${RANDOM}
INCIDENT_DIR="/var/incidents/${INCIDENT_ID}"
mkdir -p ${INCIDENT_DIR}

log_incident() {
    local severity=$1
    local component=$2
    local description=$3
    
    echo "$(date '+%Y-%m-%d %H:%M:%S') [${severity}] ${component}: ${description}" | \
        tee -a ${INCIDENT_DIR}/incident.log
    
    # å‘é€å‘Šè­¦é€šçŸ¥
    case ${severity} in
        "P0")
            # ç´§æ€¥é€šçŸ¥æ‰€æœ‰ç›¸å…³äººå‘˜
            send_emergency_alert "${description}"
            ;;
        "P1")
            # é€šçŸ¥ç›¸å…³æŠ€æœ¯å›¢é˜Ÿ
            send_team_alert "${component}" "${description}"
            ;;
    esac
}

# æ•…éšœè¯Šæ–­å‡½æ•°
diagnose_cluster_health() {
    echo "=== é›†ç¾¤å¥åº·çŠ¶æ€è¯Šæ–­ ===" > ${INCIDENT_DIR}/diagnosis.txt
    
    # æ£€æŸ¥æ§åˆ¶å¹³é¢çŠ¶æ€
    kubectl get componentstatuses >> ${INCIDENT_DIR}/diagnosis.txt 2>&1
    
    # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
    kubectl get nodes -o wide >> ${INCIDENT_DIR}/diagnosis.txt 2>&1
    
    # æ£€æŸ¥å…³é”®ç³»ç»ŸPodçŠ¶æ€
    kubectl get pods -n kube-system >> ${INCIDENT_DIR}/diagnosis.txt 2>&1
    
    # æ£€æŸ¥äº‹ä»¶æ—¥å¿—
    kubectl get events --sort-by='.lastTimestamp' -A | tail -20 >> ${INCIDENT_DIR}/diagnosis.txt
}

# è‡ªåŠ¨åŒ–æ¢å¤å°è¯•
attempt_auto_recovery() {
    local component=$1
    
    case ${component} in
        "coredns")
            echo "å°è¯•é‡å¯CoreDNS..."
            kubectl rollout restart deployment coredns -n kube-system
            ;;
        "kube-proxy")
            echo "å°è¯•é‡å¯kube-proxy DaemonSet..."
            kubectl delete pods -n kube-system -l k8s-app=kube-proxy
            ;;
        *)
            echo "ç»„ä»¶${component}æš‚æ— è‡ªåŠ¨æ¢å¤ç­–ç•¥"
            return 1
            ;;
    esac
}

# ä½¿ç”¨ç¤ºä¾‹
# log_incident "P0" "API Server" "API Serverå“åº”è¶…æ—¶ï¼Œå½±å“é›†ç¾¤ç®¡ç†"
# diagnose_cluster_health
# attempt_auto_recovery "coredns"
```

### 9.3 æ•…éšœå¤ç›˜ä¸æ”¹è¿›

```yaml
# ========== æ•…éšœå¤ç›˜æ¨¡æ¿ ==========
apiVersion: incident.review/v1
kind: PostMortemReport
metadata:
  name: incident-${INCIDENT_ID}
spec:
  incidentDetails:
    startTime: "2026-02-05T14:30:00Z"
    endTime: "2026-02-05T15:45:00Z"
    duration: "1h15m"
    severity: "P0"
    affectedServices:
    - name: user-api-service
      impact: "50%è¯·æ±‚å¤±è´¥"
    - name: order-processing
      impact: "å®Œå…¨ä¸å¯ç”¨"
  
  timeline:
  - time: "14:30"
    event: "ç›‘æ§ç³»ç»Ÿå‘Šè­¦ï¼šAPI Serverå“åº”æ—¶é—´è¶…è¿‡é˜ˆå€¼"
    actor: "Prometheus Alertmanager"
  - time: "14:32"
    event: "å€¼ç­å·¥ç¨‹å¸ˆç¡®è®¤é—®é¢˜å¹¶é€šçŸ¥SREå›¢é˜Ÿ"
    actor: "on-call engineer"
  - time: "14:35"
    event: "å¯åŠ¨åº”æ€¥å“åº”æµç¨‹ï¼Œåˆ›å»ºæ•…éšœå·¥å•"
    actor: "incident commander"
  - time: "14:40"
    event: "åˆæ­¥è¯Šæ–­å‘ç°etcdé›†ç¾¤å‡ºç°ç½‘ç»œåˆ†åŒº"
    actor: "SRE team"
  - time: "15:10"
    event: "æ‰§è¡Œetcdé›†ç¾¤æ¢å¤æ“ä½œ"
    actor: "database specialist"
  - time: "15:30"
    event: "æœåŠ¡æ¢å¤æ­£å¸¸ï¼Œå¼€å§‹éªŒè¯"
    actor: "QA team"
  - time: "15:45"
    event: "ç¡®è®¤æœåŠ¡ç¨³å®šï¼Œå…³é—­æ•…éšœå·¥å•"
    actor: "incident commander"
  
  rootCauseAnalysis:
    primaryCause: "etcdé›†ç¾¤ç½‘ç»œåˆ†åŒºå¯¼è‡´è„‘è£‚"
    contributingFactors:
    - ç½‘ç»œè®¾å¤‡å›ºä»¶bug
    - ç¼ºä¹ç½‘ç»œå¥åº·æ£€æŸ¥æœºåˆ¶
    - æ•…éšœè½¬ç§»æµ‹è¯•ä¸å……åˆ†
    
  correctiveActions:
  - immediate:
    - ä¿®å¤ç½‘ç»œè®¾å¤‡å›ºä»¶
    - å¢åŠ etcdå¥åº·æ£€æŸ¥é¢‘ç‡
    - å®Œå–„æ•…éšœè½¬ç§»æµ‹è¯•æµç¨‹
  - longTerm:
    - éƒ¨ç½²ç½‘ç»œç›‘æ§ç³»ç»Ÿ
    - å»ºç«‹å¤šåœ°åŸŸetcdé›†ç¾¤
    - å®Œå–„ç¾éš¾æ¢å¤é¢„æ¡ˆ
  
  lessonsLearned:
  - ç½‘ç»œåŸºç¡€è®¾æ–½çš„å¯é æ€§ç›´æ¥å½±å“é›†ç¾¤ç¨³å®šæ€§
  - éœ€è¦å»ºç«‹æ›´å®Œå–„çš„ç›‘æ§å‘Šè­¦ä½“ç³»
  - å®šæœŸè¿›è¡Œæ•…éšœæ¼”ç»ƒçš„é‡è¦æ€§
```

---

## 10. ç”Ÿäº§ç¯å¢ƒå®‰å…¨æœ€ä½³å®è·µ

### 10.1 é›¶ä¿¡ä»»å®‰å…¨å®æ–½æ¡†æ¶

```yaml
# ========== ç”Ÿäº§ç¯å¢ƒé›¶ä¿¡ä»»å®‰å…¨é…ç½® ==========
apiVersion: security.k8s.io/v1
kind: PodSecurityPolicy
metadata:
  name: production-restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - configMap
  - emptyDir
  - projected
  - secret
  - downwardAPI
  - persistentVolumeClaim
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: MustRunAsNonRoot
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: MustRunAs
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: true

---
# ========== ç½‘ç»œç­–ç•¥å®æ–½ ==========
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 9090
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53

---
# ========== RBACæœ€å°æƒé™é…ç½® ==========
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: app-developer-role
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log", "services", "configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-developer-binding
  namespace: production
subjects:
- kind: User
  name: developer@example.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: app-developer-role
  apiGroup: rbac.authorization.k8s.io
```

### 10.2 å®‰å…¨ç›‘æ§ä¸å‘Šè­¦ç­–ç•¥

| å®‰å…¨ç»´åº¦ | ç›‘æ§æŒ‡æ ‡ | å‘Šè­¦é˜ˆå€¼ | å“åº”åŠ¨ä½œ | å¤„ç†æ—¶æ•ˆ |
|---------|---------|---------|---------|---------|
| **èº«ä»½è®¤è¯** | å¼‚å¸¸ç™»å½•å°è¯•ã€ä»¤ç‰Œæ³„éœ² | >5æ¬¡å¤±è´¥ç™»å½•/å°æ—¶ | ç«‹å³é”å®šè´¦æˆ· | 5åˆ†é’Ÿ |
| **æƒé™å˜æ›´** | RBACè§„åˆ™ä¿®æ”¹ã€ServiceAccountå˜æ›´ | ä»»ä½•æœªæˆæƒå˜æ›´ | å®‰å…¨å®¡è®¡ã€å›æ»šå˜æ›´ | 30åˆ†é’Ÿ |
| **ç½‘ç»œè®¿é—®** | å¼‚å¸¸ç«¯å£è®¿é—®ã€å¤–éƒ¨è¿æ¥ | è¿æ¥åˆ°é»‘åå•IP | é˜»æ–­æµé‡ã€å®‰å…¨è°ƒæŸ¥ | 15åˆ†é’Ÿ |
| **é•œåƒå®‰å…¨** | æ¼æ´æ‰«æç»“æœã€åŸºçº¿ä¸ç¬¦åˆ | Critical/Highæ¼æ´ | é˜»æ–­éƒ¨ç½²ã€ç´§æ€¥ä¿®å¤ | 1å°æ—¶ |
| **è¿è¡Œæ—¶å®‰å…¨** | å¼‚å¸¸ç³»ç»Ÿè°ƒç”¨ã€æ–‡ä»¶ä¿®æ”¹ | è¿åå®‰å…¨ç­–ç•¥ | éš”ç¦»å®¹å™¨ã€å‘Šè­¦é€šçŸ¥ | 10åˆ†é’Ÿ |

### 10.3 åˆè§„æ€§è‡ªåŠ¨åŒ–æ£€æŸ¥

```bash
#!/bin/bash
# ========== Kuberneteså®‰å…¨åˆè§„æ£€æŸ¥è„šæœ¬ ==========
set -euo pipefail

COMPLIANCE_REPORT="/var/reports/compliance-$(date +%Y%m%d).txt"
echo "Kuberneteså®‰å…¨åˆè§„æ£€æŸ¥æŠ¥å‘Š - $(date)" > ${COMPLIANCE_REPORT}

# CISåŸºå‡†æ£€æŸ¥
check_cis_benchmark() {
    echo "=== CIS Kubernetes Benchmark æ£€æŸ¥ ===" >> ${COMPLIANCE_REPORT}
    
    # æ£€æŸ¥API Serveré…ç½®
    if kubectl get pod -n kube-system -l component=kube-apiserver -o jsonpath='{.items[*].spec.containers[*].command}' | \
       grep -q "anonymous-auth=false"; then
        echo "âœ… API ServeråŒ¿åè®¤è¯å·²ç¦ç”¨" >> ${COMPLIANCE_REPORT}
    else
        echo "âŒ API ServeråŒ¿åè®¤è¯æœªç¦ç”¨" >> ${COMPLIANCE_REPORT}
    fi
    
    # æ£€æŸ¥etcdåŠ å¯†
    if kubectl get pod -n kube-system -l component=etcd -o jsonpath='{.items[*].spec.containers[*].command}' | \
       grep -q "auto-tls=true"; then
        echo "âœ… etcdè‡ªåŠ¨TLSå·²å¯ç”¨" >> ${COMPLIANCE_REPORT}
    else
        echo "âŒ etcdè‡ªåŠ¨TLSæœªå¯ç”¨" >> ${COMPLIANCE_REPORT}
    fi
    
    # æ£€æŸ¥Podå®‰å…¨ç­–ç•¥
    psp_count=$(kubectl get psp --no-headers | wc -l)
    if [ ${psp_count} -gt 0 ]; then
        echo "âœ… å·²é…ç½®${psp_count}ä¸ªPodå®‰å…¨ç­–ç•¥" >> ${COMPLIANCE_REPORT}
    else
        echo "âŒ æœªé…ç½®Podå®‰å…¨ç­–ç•¥" >> ${COMPLIANCE_REPORT}
    fi
}

# GDPRåˆè§„æ£€æŸ¥
check_gdpr_compliance() {
    echo -e "\n=== GDPRåˆè§„æ£€æŸ¥ ===" >> ${COMPLIANCE_REPORT}
    
    # æ£€æŸ¥æ•°æ®åŠ å¯†
    secrets_encrypted=$(kubectl get secrets -A --no-headers | wc -l)
    echo "ğŸ”’ åŠ å¯†Secretæ•°é‡: ${secrets_encrypted}" >> ${COMPLIANCE_REPORT}
    
    # æ£€æŸ¥æ—¥å¿—ä¿ç•™ç­–ç•¥
    log_retention_days=$(kubectl get cm -n kube-system kube-proxy -o jsonpath='{.data.config\.yaml}' | \
                        grep -o "log-flush-frequency=[0-9]*" | cut -d'=' -f2 || echo "æœªé…ç½®")
    echo "ğŸ“ æ—¥å¿—åˆ·æ–°é¢‘ç‡: ${log_retention_days}s" >> ${COMPLIANCE_REPORT}
}

check_cis_benchmark
check_gdpr_compliance

echo -e "\nåˆè§„æ£€æŸ¥å®Œæˆï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹: ${COMPLIANCE_REPORT}"
```

## 11. æˆæœ¬ä¼˜åŒ–ä¸èµ„æºç®¡ç†

### 11.1 èµ„æºé…é¢ä¸é™åˆ¶ç®¡ç†

```yaml
# ========== ç”Ÿäº§ç¯å¢ƒèµ„æºé…é¢é…ç½® ==========
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    # è®¡ç®—èµ„æºé…é¢
    requests.cpu: "100"
    requests.memory: "200Gi"
    limits.cpu: "200"
    limits.memory: "400Gi"
    
    # å­˜å‚¨èµ„æºé…é¢
    requests.storage: "10Ti"
    persistentvolumeclaims: "1000"
    
    # å¯¹è±¡æ•°é‡é…é¢
    pods: "10000"
    services: "500"
    secrets: "1000"
    configmaps: "1000"

---
# ========== LimitRangeé…ç½® ==========
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limits
  namespace: production
spec:
  limits:
  - type: Container
    default:
      cpu: "1"
      memory: "1Gi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    max:
      cpu: "8"
      memory: "16Gi"
    min:
      cpu: "10m"
      memory: "16Mi"
  - type: Pod
    max:
      cpu: "16"
      memory: "32Gi"
```

### 11.2 æˆæœ¬ç›‘æ§ä¸ä¼˜åŒ–ç­–ç•¥

| ä¼˜åŒ–ç»´åº¦ | ç›‘æ§æŒ‡æ ‡ | ä¼˜åŒ–ç­–ç•¥ | é¢„æœŸæ”¶ç›Š | å®æ–½å¤æ‚åº¦ |
|---------|---------|---------|---------|-----------|
| **èŠ‚ç‚¹èµ„æº** | CPU/å†…å­˜åˆ©ç”¨ç‡ã€èŠ‚ç‚¹ç©ºé—²ç‡ | æ°´å¹³æ‰©ç¼©å®¹ã€èŠ‚ç‚¹æ± ä¼˜åŒ– | 20-40%æˆæœ¬èŠ‚çº¦ | â­â­ |
| **å­˜å‚¨æˆæœ¬** | PVCä½¿ç”¨ç‡ã€å¿«ç…§ä¿ç•™ | ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€å†·çƒ­æ•°æ®åˆ†ç¦» | 30-50%å­˜å‚¨èŠ‚çº¦ | â­â­â­ |
| **ç½‘ç»œè´¹ç”¨** | æµé‡ä½¿ç”¨ã€è·¨åŒºåŸŸä¼ è¾“ | CDNä¼˜åŒ–ã€å°±è¿‘éƒ¨ç½² | 25-35%ç½‘ç»œèŠ‚çº¦ | â­â­ |
| **Spotå®ä¾‹** | æŒ‰éœ€/ç«ä»·å®ä¾‹æ¯”ä¾‹ | æ™ºèƒ½è°ƒåº¦ç­–ç•¥ | 50-80%è®¡ç®—èŠ‚çº¦ | â­â­â­â­ |
| **é•œåƒç¼“å­˜** | é•œåƒæ‹‰å–æ¬¡æ•°ã€ç¼“å­˜å‘½ä¸­ç‡ | é•œåƒé¢„çƒ­ã€æœ¬åœ°ç¼“å­˜ | 15-25%æ‹‰å–èŠ‚çº¦ | â­â­ |

### 11.3 æˆæœ¬ä¼˜åŒ–è‡ªåŠ¨åŒ–è„šæœ¬

```bash
#!/bin/bash
# ========== Kubernetesæˆæœ¬ä¼˜åŒ–åˆ†æè„šæœ¬ ==========
set -euo pipefail

COST_ANALYSIS_DIR="/var/cost-analysis/$(date +%Y%m%d)"
mkdir -p ${COST_ANALYSIS_DIR}

analyze_cluster_costs() {
    echo "=== é›†ç¾¤æˆæœ¬åˆ†ææŠ¥å‘Š ===" > ${COST_ANALYSIS_DIR}/cost-report.txt
    
    # èŠ‚ç‚¹æˆæœ¬åˆ†æ
    echo "èŠ‚ç‚¹æˆæœ¬åˆ†å¸ƒ:" >> ${COST_ANALYSIS_DIR}/cost-report.txt
    kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.capacity.cpu}{"\t"}{.status.capacity.memory}{"\n"}{end}' | \
    while read node cpu mem; do
        # åŸºäºå®ä¾‹ç±»å‹çš„ä¼°ç®—æˆæœ¬ï¼ˆç¤ºä¾‹ä»·æ ¼ï¼‰
        case ${node} in
            *m5.large*) hourly_cost=0.096 ;;
            *m5.xlarge*) hourly_cost=0.192 ;;
            *m5.2xlarge*) hourly_cost=0.384 ;;
            *) hourly_cost=0.200 ;;  # é»˜è®¤ä»·æ ¼
        esac
        monthly_cost=$(echo "${hourly_cost} * 730" | bc -l)
        echo "${node}: $${monthly_cost}/æœˆ (${cpu}vCPU, ${mem})" >> ${COST_ANALYSIS_DIR}/cost-report.txt
    done
    
    # Podèµ„æºä½¿ç”¨åˆ†æ
    echo -e "\nPodèµ„æºä½¿ç”¨æ•ˆç‡:" >> ${COST_ANALYSIS_DIR}/cost-report.txt
    kubectl top pods -A --no-headers | \
    awk '{
        cpu_req=$3+0; mem_req=$4+0;
        cpu_util=$5+0; mem_util=$6+0;
        cpu_efficiency = (cpu_util/cpu_req)*100;
        mem_efficiency = (mem_util/mem_req)*100;
        if(cpu_efficiency < 30 || mem_efficiency < 30) {
            print $1"/"$2": CPUæ•ˆç‡="cpu_efficiency"% Memoryæ•ˆç‡="mem_efficiency"%"
        }
    }' >> ${COST_ANALYSIS_DIR}/cost-report.txt
    
    # å­˜å‚¨æˆæœ¬åˆ†æ
    echo -e "\nå­˜å‚¨æˆæœ¬åˆ†æ:" >> ${COST_ANALYSIS_DIR}/cost-report.txt
    kubectl get pvc -A -o jsonpath='{range .items[*]}{.metadata.namespace}{"\t"}{.metadata.name}{"\t"}{.spec.resources.requests.storage}{"\n"}{end}' | \
    while read ns pvc size; do
        # åŸºäºå­˜å‚¨ç±»å‹çš„ä¼°ç®—æˆæœ¬
        storage_cost=$(echo "${size%Gi} * 0.10" | bc -l)  # $0.10/GiB/æœˆ
        echo "${ns}/${pvc}: ${size} ($${storage_cost}/æœˆ)" >> ${COST_ANALYSIS_DIR}/cost-report.txt
    done
}

generate_optimization_recommendations() {
    echo -e "\n=== ä¼˜åŒ–å»ºè®® ===" >> ${COST_ANALYSIS_DIR}/cost-report.txt
    
    # ä½æ•ˆPodæ¨è
    echo "å»ºè®®ä¼˜åŒ–çš„Pod:" >> ${COST_ANALYSIS_DIR}/cost-report.txt
    kubectl top pods -A --no-headers | \
    awk '$5 < 30 || $6 < 30 {print $1"/"$2" - èµ„æºä½¿ç”¨ç‡ä½"}' >> ${COST_ANALYSIS_DIR}/cost-report.txt
    
    # èŠ‚ç‚¹ä¼˜åŒ–å»ºè®®
    echo -e "\nèŠ‚ç‚¹ä¼˜åŒ–å»ºè®®:" >> ${COST_ANALYSIS_DIR}/cost-report.txt
    kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.allocatable.cpu}{"\n"}{end}' | \
    while read node allocatable; do
        pod_count=$(kubectl get pods --field-selector spec.nodeName=${node} --no-headers | wc -l)
        pods_per_core=$(echo "${pod_count}/${allocatable}" | bc -l)
        if (( $(echo "${pods_per_core} < 2" | bc -l) )); then
            echo "${node}: CPUåˆ©ç”¨ç‡ä½ï¼Œè€ƒè™‘ç¼©å°å®ä¾‹è§„æ ¼" >> ${COST_ANALYSIS_DIR}/cost-report.txt
        fi
    done
}

analyze_cluster_costs
generate_optimization_recommendations

echo "æˆæœ¬åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: ${COST_ANALYSIS_DIR}/cost-report.txt"
```

## 12. å˜æ›´ç®¡ç†ä¸å‘å¸ƒç­–ç•¥

### 12.1 GitOpsæµæ°´çº¿æœ€ä½³å®è·µ

```yaml
# ========== ArgoCDåº”ç”¨é…ç½®æ¨¡æ¿ ==========
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: production-app-template
  namespace: argocd
spec:
  project: production
  source:
    repoURL: https://github.com/company/production-app.git
    targetRevision: HEAD
    path: k8s/overlays/production
    helm:
      valueFiles:
      - values-production.yaml
      parameters:
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
    syncOptions:
    - CreateNamespace=true
    - PruneLast=true
    - RespectIgnoreDifferences=true
    - ApplyOutOfSyncOnly=true

---
# ========== å¤šç¯å¢ƒé…ç½®ç®¡ç† ==========
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-environment-config
  namespace: production
data:
  # ç”Ÿäº§ç¯å¢ƒç‰¹å®šé…ç½®
  DATABASE_URL: "postgresql://prod-db.cluster.local:5432/app"
  REDIS_URL: "redis://prod-redis.cluster.local:6379"
  LOG_LEVEL: "WARN"
  CACHE_TTL: "300"
  ENABLE_DEBUG: "false"
  MAX_CONNECTIONS: "100"
  
  # å®‰å…¨é…ç½®
  TLS_MIN_VERSION: "TLS1.3"
  HSTS_MAX_AGE: "31536000"
  CORS_ALLOWED_ORIGINS: "https://app.example.com"
  SECURITY_HEADERS: |
    Strict-Transport-Security: max-age=31536000; includeSubDomains
    X-Content-Type-Options: nosniff
    X-Frame-Options: DENY
    Content-Security-Policy: default-src 'self'
```

### 12.2 æ¸è¿›å¼å‘å¸ƒç­–ç•¥

| å‘å¸ƒç­–ç•¥ | å®æ–½æ–¹å¼ | é£é™©æ§åˆ¶ | ç›‘æ§æŒ‡æ ‡ | å›æ»šæœºåˆ¶ |
|---------|---------|---------|---------|---------|
| **è“ç»¿éƒ¨ç½²** | ç»´æŠ¤ä¸¤å¥—å®Œæ•´ç¯å¢ƒ | é›¶åœæœºæ—¶é—´ | å¥åº·æ£€æŸ¥ã€æ€§èƒ½æŒ‡æ ‡ | ä¸€é”®åˆ‡æ¢å›æ—§ç¯å¢ƒ |
| **é‡‘ä¸é›€å‘å¸ƒ** | é€æ­¥å¢åŠ æ–°ç‰ˆæœ¬æµé‡ | é™åˆ¶å½±å“èŒƒå›´ | é”™è¯¯ç‡ã€å»¶è¿ŸæŒ‡æ ‡ | è‡ªåŠ¨å›æ»šåˆ°ç¨³å®šç‰ˆæœ¬ |
| **æ»šåŠ¨æ›´æ–°** | é€ä¸ªæ›¿æ¢Podå®ä¾‹ | åŸåœ°å‡çº§ | å°±ç»ªæ¢é’ˆã€å­˜æ´»æ¢é’ˆ | å¤±è´¥æ—¶æš‚åœå¹¶å›æ»š |
| **åŠŸèƒ½å¼€å…³** | ä»£ç å±‚é¢æ§åˆ¶åŠŸèƒ½ | ç²¾ç¡®æ§åˆ¶èŒƒå›´ | ä¸šåŠ¡æŒ‡æ ‡ã€ç”¨æˆ·åé¦ˆ | åŠ¨æ€å¼€å¯/å…³é—­åŠŸèƒ½ |

### 12.3 å˜æ›´å®¡æ‰¹ä¸å®¡è®¡æµç¨‹

```yaml
# ========== å˜æ›´ç®¡ç†æµç¨‹é…ç½® ==========
apiVersion: changemanagement.example.com/v1
kind: ChangeRequest
metadata:
  name: cr-20260205-001
spec:
  changeType: "Production Deployment"
  priority: "High"
  affectedSystems:
  - name: "user-service"
    environment: "production"
    criticality: "Business Critical"
  
  approvalWorkflow:
    reviewers:
    - role: "SRE Team Lead"
      required: true
    - role: "Security Officer"
      required: true
    - role: "Product Owner"
      required: false
    
    approvalConditions:
    - type: "Automated Tests"
      status: "Passed"
      required: true
    - type: "Security Scan"
      status: "Clean"
      required: true
    - type: "Performance Test"
      status: "Within Threshold"
      required: true
  
  rollbackPlan:
    triggerConditions:
    - metric: "error_rate"
      threshold: "5%"
      duration: "5m"
    - metric: "response_time"
      threshold: "2s"
      duration: "10m"
    - metric: "business_impact"
      threshold: "significant_degradation"
      duration: "immediate"
    
    rollbackActions:
    - action: "argo_rollout_undo"
      target: "user-service"
      timeout: "300s"
    - action: "notification_slack"
      target: "#production-alerts"
      message: "Automatic rollback triggered for user-service"
```

---

**è¡¨æ ¼åº•éƒ¨æ ‡è®°**: Kusheet Project | ä½œè€…: Allen Galler (allengaller@gmail.com) | æœ€åæ›´æ–°: 2026-02 | ç‰ˆæœ¬: v1.25-v1.32 | è´¨é‡ç­‰çº§: â­â­â­â­â­ ä¸“å®¶çº§
# MongoDB Enterprise Database Operations æ·±åº¦å®è·µ

> **Author**: Database Platform Architect | **Version**: v1.0 | **Update Time**: 2026-02-07
> **Scenario**: Enterprise-grade MongoDB database operations and administration | **Complexity**: â­â­â­â­

## ğŸ¯ Abstract

This document provides comprehensive exploration of MongoDB enterprise deployment architecture, database administration practices, and performance optimization strategies. Based on large-scale production environment experience, it offers complete technical guidance from cluster setup to advanced sharding and replication management, helping enterprises build highly available, scalable NoSQL database platforms with integrated monitoring, backup, and disaster recovery capabilities.

## 1. MongoDB Enterprise Architecture

### 1.1 Core Component Architecture

```mermaid
graph TB
    subgraph "MongoDB Infrastructure"
        A[MongoDB Instances]
        B[Replica Sets]
        C[Sharded Clusters]
        D[Config Servers]
        E[Mongos Routers]
    end
    
    subgraph "Data Management"
        F[Data Sharding]
        G[Replication]
        H[Backup & Restore]
        I[Index Management]
        J[Performance Tuning]
    end
    
    subgraph "Security & Access Control"
        K[Authentication]
        L[Authorization]
        M[Encryption]
        N[Audit Logging]
        O[Network Security]
    end
    
    subgraph "Monitoring & Operations"
        P[Database Monitoring]
        Q[Performance Metrics]
        R[Alerting System]
        S[Log Management]
        T[Capacity Planning]
    end
    
    subgraph "High Availability"
        U[Automatic Failover]
        V[Load Balancing]
        W[Disaster Recovery]
        X[Backup Strategies]
        Y[Recovery Procedures]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    
    F --> G
    G --> H
    H --> I
    I --> J
    
    K --> L
    L --> M
    M --> N
    N --> O
    
    P --> Q
    Q --> R
    R --> S
    S --> T
    
    U --> V
    V --> W
    W --> X
    X --> Y
```

### 1.2 Enterprise Deployment Architecture

```yaml
mongodb_enterprise_deployment:
  replica_set_configuration:
    production_replica_set:
      name: "rs0"
      members:
        - host: "mongo-primary-0.mongo-svc.production.svc.cluster.local:27017"
          priority: 10
          votes: 1
          tags:
            dc: "primary-datacenter"
            rack: "rack-1"
        
        - host: "mongo-secondary-0.mongo-svc.production.svc.cluster.local:27017"
          priority: 8
          votes: 1
          tags:
            dc: "primary-datacenter"
            rack: "rack-2"
        
        - host: "mongo-secondary-1.mongo-svc.production.svc.cluster.local:27017"
          priority: 6
          votes: 1
          tags:
            dc: "secondary-datacenter"
            rack: "rack-3"
      
      settings:
        chainingAllowed: true
        heartbeatTimeoutSecs: 10
        electionTimeoutMillis: 10000
        catchUpTimeoutMillis: 30000
  
  sharded_cluster:
    config_servers:
      replica_set: "configRS"
      members:
        - "config-0.config-svc.production.svc.cluster.local:27017"
        - "config-1.config-svc.production.svc.cluster.local:27017"
        - "config-2.config-svc.production.svc.cluster.local:27017"
    
    shards:
      - name: "shard0"
        replica_set: "shard0RS"
        members:
          - "shard0-0.shard-svc.production.svc.cluster.local:27017"
          - "shard0-1.shard-svc.production.svc.cluster.local:27017"
          - "shard0-2.shard-svc.production.svc.cluster.local:27017"
      
      - name: "shard1"
        replica_set: "shard1RS"
        members:
          - "shard1-0.shard-svc.production.svc.cluster.local:27017"
          - "shard1-1.shard-svc.production.svc.cluster.local:27017"
          - "shard1-2.shard-svc.production.svc.cluster.local:27017"
    
    mongos:
      - "mongos-0.mongos-svc.production.svc.cluster.local:27017"
      - "mongos-1.mongos-svc.production.svc.cluster.local:27017"
      - "mongos-2.mongos-svc.production.svc.cluster.local:27017"
  
  security_configuration:
    authentication:
      mechanism: "SCRAM-SHA-256"
      users:
        - username: "admin"
          roles: ["root"]
          database: "admin"
        - username: "application"
          roles: ["readWrite"]
          database: "application_db"
        - username: "monitoring"
          roles: ["clusterMonitor"]
          database: "admin"
    
    tls_ssl:
      mode: "requireSSL"
      certificateKeyFile: "/etc/ssl/mongodb.pem"
      CAFile: "/etc/ssl/ca.pem"
      clusterFile: "/etc/ssl/cluster.pem"
    
    encryption_at_rest:
      enabled: true
      keyVaultNamespace: "admin.datakeys"
      kmsProviders:
        local:
          keyFile: "/data/configdb/mongod-local-kms.key"
```

## 2. Advanced Database Administration

### 2.1 Replica Set Management

```javascript
// MongoDB Shell Commands for Replica Set Management

// 1. åˆå§‹åŒ–å‰¯æœ¬é›†
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo-primary-0:27017", priority: 10 },
    { _id: 1, host: "mongo-secondary-0:27017", priority: 8 },
    { _id: 2, host: "mongo-secondary-1:27017", priority: 6 }
  ]
});

// 2. æ·»åŠ ä»²è£è€…
rs.addArb("arbiter-0:27017");

// 3. é…ç½®æ ‡ç­¾å’Œä¼˜å…ˆçº§
cfg = rs.conf();
cfg.members[0].tags = { "dc": "primary", "usage": "production" };
cfg.members[1].tags = { "dc": "secondary", "usage": "production" };
cfg.settings.getLastErrorModes = {
  "datacenter": { "dc": 2 },
  "production": { "usage": "production" }
};
rs.reconfig(cfg);

// 4. ç›‘æ§å‰¯æœ¬é›†çŠ¶æ€
rs.status();
rs.printSlaveReplicationInfo();

// 5. æ•…éšœè½¬ç§»æµ‹è¯•
rs.stepDown(60); // ä¸»èŠ‚ç‚¹é™çº§60ç§’
```

### 2.2 Sharding Configuration

```javascript
// Sharding Setup and Management

// 1. å¯ç”¨åˆ†ç‰‡
sh.enableSharding("application_db");

// 2. æ·»åŠ åˆ†ç‰‡
sh.addShard("shard0RS/shard0-0:27017,shard0-1:27017,shard0-2:27017");
sh.addShard("shard1RS/shard1-0:27017,shard1-1:27017,shard1-2:27017");

// 3. åˆ›å»ºåˆ†ç‰‡é”®
db.users.createIndex({ "region": 1, "_id": 1 });
sh.shardCollection("application_db.users", { "region": 1, "_id": 1 });

// 4. ç›‘æ§åˆ†ç‰‡çŠ¶æ€
sh.status();
db.chunks.find().pretty();
db.stats();

// 5. å¹³è¡¡å™¨ç®¡ç†
sh.getBalancerState();
sh.startBalancer();
sh.stopBalancer();
sh.setBalancerState(false);
```

## 3. Performance Optimization

### 3.1 Index Management and Optimization

```javascript
// Advanced Index Management

// 1. åˆ›å»ºå¤åˆç´¢å¼•
db.orders.createIndex({
  "customer_id": 1,
  "order_date": -1,
  "status": 1
}, {
  name: "customer_orders_idx",
  background: true,
  sparse: false
});

// 2. åˆ›å»ºæ–‡æœ¬ç´¢å¼•
db.products.createIndex({
  "name": "text",
  "description": "text",
  "category": "text"
}, {
  weights: {
    name: 10,
    description: 5,
    category: 2
  },
  name: "product_text_search"
});

// 3. åˆ›å»ºåœ°ç†ä½ç½®ç´¢å¼•
db.stores.createIndex({
  "location": "2dsphere"
});

// 4. ç´¢å¼•ä½¿ç”¨åˆ†æ
db.orders.aggregate([
  { $indexStats: {} },
  { $sort: { "accesses.ops": -1 } }
]);

// 5. æŸ¥è¯¢æ€§èƒ½åˆ†æ
db.orders.explain("executionStats").find({
  "customer_id": ObjectId("507f1f77bcf86cd799439011"),
  "order_date": { $gte: ISODate("2023-01-01") }
}).sort({ "order_date": -1 }).limit(10);
```

### 3.2 Query Optimization

```javascript
// Query Optimization Techniques

// 1. ä½¿ç”¨æŠ•å½±å‡å°‘æ•°æ®ä¼ è¾“
db.users.find(
  { "status": "active" },
  { "name": 1, "email": 1, "created_at": 1, "_id": 0 }
);

// 2. æ‰¹é‡æ“ä½œä¼˜åŒ–
db.orders.bulkWrite([
  {
    insertOne: {
      "document": {
        "customer_id": ObjectId("..."),
        "items": [...],
        "total": 100.00,
        "created_at": new Date()
      }
    }
  },
  {
    updateOne: {
      "filter": { "_id": ObjectId("...") },
      "update": { "$set": { "status": "processed" } }
    }
  }
], { ordered: false });

// 3. èšåˆç®¡é“ä¼˜åŒ–
db.sales.aggregate([
  { $match: { "date": { $gte: ISODate("2023-01-01") } } },
  { $group: {
      _id: { "year": { $year: "$date" }, "month": { $month: "$date" } },
      total_sales: { $sum: "$amount" },
      count: { $sum: 1 }
    }
  },
  { $sort: { "_id.year": 1, "_id.month": 1 } },
  { $limit: 12 }
]);
```

## 4. Security Implementation

### 4.1 Advanced Security Configuration

```yaml
# security_config.yaml
security:
  authorization: "enabled"
  javascriptEnabled: false
  redactClientLogData: true
  
  sasl:
    hostName: "mongodb.company.com"
    serviceName: "mongodb"
  
  ldap:
    servers: ["ldap.company.com:389"]
    bind:
      method: "sasl"
      saslMechanism: "PLAIN"
    transportSecurity: "tls"
    userToDNMapping: '[{"match": "(.+)@COMPANY.COM", "ldapQuery": "(&(objectClass=user)(sAMAccountName={0}))"}]'
  
  auditLog:
    destination: "file"
    format: "JSON"
    path: "/var/log/mongodb/audit.log"
    filter: '{"atype": {"$in": ["authenticate", "createCollection", "dropCollection", "createIndex", "dropIndex"]}}'
```

### 4.2 Role-Based Access Control

```javascript
// RBAC Implementation

// 1. åˆ›å»ºè‡ªå®šä¹‰è§’è‰²
db.createRole({
  role: "application_reader",
  privileges: [
    {
      resource: { db: "application_db", collection: "" },
      actions: ["find", "collStats", "dbStats"]
    }
  ],
  roles: []
}, { w: "majority", wtimeout: 5000 });

// 2. åˆ›å»ºç”¨æˆ·å¹¶åˆ†é…è§’è‰²
db.createUser({
  user: "app_user",
  pwd: passwordPrompt(),
  roles: [
    { role: "application_reader", db: "application_db" },
    { role: "readWrite", db: "analytics_db" }
  ],
  customData: {
    "department": "engineering",
    "team": "backend"
  }
});

// 3. è§’è‰²ç»§æ‰¿å’Œæƒé™ç®¡ç†
db.grantRolesToUser("app_user", [
  { role: "dbAdmin", db: "logging_db" }
]);

// 4. å®¡è®¡å’Œç›‘æ§
db.system.roles.find().pretty();
db.getUser("app_user");
```

## 5. Backup and Disaster Recovery

### 5.1 Automated Backup Strategy

```bash
#!/bin/bash
# mongodb_backup.sh

BACKUP_DIR="/backup/mongodb"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

# 1. é€»è¾‘å¤‡ä»½ (mongodump)
perform_logical_backup() {
    echo "Starting logical backup..."
    
    mongodump \
        --host="mongodb.production.svc.cluster.local:27017" \
        --username="backup_user" \
        --password="${BACKUP_PASSWORD}" \
        --authenticationDatabase="admin" \
        --out="${BACKUP_DIR}/logical_${DATE}" \
        --gzip \
        --oplog
    
    # éªŒè¯å¤‡ä»½å®Œæ•´æ€§
    if [ $? -eq 0 ]; then
        echo "Logical backup completed successfully"
        # åˆ›å»ºæ ¡éªŒå’Œ
        find "${BACKUP_DIR}/logical_${DATE}" -type f -exec md5sum {} \; > "${BACKUP_DIR}/logical_${DATE}.md5"
    else
        echo "Logical backup failed"
        exit 1
    fi
}

# 2. ç‰©ç†å¤‡ä»½ (æ–‡ä»¶ç³»ç»Ÿå¿«ç…§)
perform_physical_backup() {
    echo "Starting physical backup..."
    
    # ä½¿ç”¨LVMå¿«ç…§æˆ–äº‘æä¾›å•†å¿«ç…§
    VOLUME_ID="vol-1234567890abcdef"
    
    # AWS EBSå¿«ç…§ç¤ºä¾‹
    aws ec2 create-snapshot \
        --volume-id $VOLUME_ID \
        --description "MongoDB Physical Backup - $DATE" \
        --tag-specifications 'ResourceType=snapshot,Tags=[{Key=Name,Value=mongodb-backup-'$DATE'}]'
    
    if [ $? -eq 0 ]; then
        echo "Physical backup snapshot created"
    else
        echo "Physical backup failed"
        exit 1
    fi
}

# 3. å¢é‡å¤‡ä»½
perform_incremental_backup() {
    echo "Starting incremental backup..."
    
    # è·å–ä¸Šæ¬¡å¤‡ä»½çš„æ—¶é—´æˆ³
    LAST_BACKUP_TIME=$(cat "${BACKUP_DIR}/last_oplog_timestamp" 2>/dev/null || echo "0")
    
    mongodump \
        --host="mongodb.production.svc.cluster.local:27017" \
        --username="backup_user" \
        --password="${BACKUP_PASSWORD}" \
        --authenticationDatabase="admin" \
        --out="${BACKUP_DIR}/incremental_${DATE}" \
        --oplog \
        --oplogStart="${LAST_BACKUP_TIME}"
    
    # ä¿å­˜å½“å‰oplogæ—¶é—´æˆ³
    mongo --host="mongodb.production.svc.cluster.local:27017" \
          --username="backup_user" \
          --password="${BACKUP_PASSWORD}" \
          --authenticationDatabase="admin" \
          --eval "db.printSlaveReplicationInfo()" | \
          grep "timestamp:" | tail -1 | awk '{print $2}' > "${BACKUP_DIR}/last_oplog_timestamp"
}

# 4. å¤‡ä»½æ¸…ç†
cleanup_old_backups() {
    echo "Cleaning up old backups..."
    find "${BACKUP_DIR}" -name "logical_*" -mtime +${RETENTION_DAYS} -exec rm -rf {} \;
    find "${BACKUP_DIR}" -name "incremental_*" -mtime +7 -exec rm -rf {} \;
}

# 5. å¤‡ä»½éªŒè¯
verify_backup() {
    echo "Verifying backup integrity..."
    
    # éªŒè¯æ ¡éªŒå’Œ
    cd "${BACKUP_DIR}/logical_${DATE}"
    md5sum -c "../logical_${DATE}.md5"
    
    # æµ‹è¯•æ¢å¤åˆ°ä¸´æ—¶å®ä¾‹
    mongorestore \
        --host="temp-mongodb:27017" \
        --drop \
        --gzip \
        "${BACKUP_DIR}/logical_${DATE}"
}

# ä¸»å¤‡ä»½æµç¨‹
main() {
    echo "=== MongoDB Backup Process Started at $(date) ==="
    
    perform_logical_backup
    perform_physical_backup
    perform_incremental_backup
    cleanup_old_backups
    verify_backup
    
    echo "=== MongoDB Backup Process Completed at $(date) ==="
}

main
```

### 5.2 Point-in-Time Recovery

```bash
#!/bin/bash
# mongodb_point_in_time_recovery.sh

TARGET_TIME="2023-12-01T10:30:00"
BASE_BACKUP_DIR="/backup/mongodb"
RECOVERY_DIR="/recovery/mongodb"

# 1. æ‰¾åˆ°æœ€æ¥è¿‘çš„åŸºç¡€å¤‡ä»½
find_closest_backup() {
    local target_ts=$(date -d "$TARGET_TIME" +%s)
    local closest_backup=""
    local min_diff=999999999
    
    for backup in $(find "$BASE_BACKUP_DIR" -name "logical_*" -type d); do
        backup_time=$(basename "$backup" | cut -d'_' -f2-3)
        backup_ts=$(date -d "${backup_time:0:8} ${backup_time:9:2}:${backup_time:11:2}:${backup_time:13:2}" +%s)
        
        if [ $backup_ts -le $target_ts ]; then
            diff=$((target_ts - backup_ts))
            if [ $diff -lt $min_diff ]; then
                min_diff=$diff
                closest_backup=$backup
            fi
        fi
    done
    
    echo "$closest_backup"
}

# 2. æ¢å¤åŸºç¡€å¤‡ä»½
restore_base_backup() {
    local backup_dir=$1
    
    echo "Restoring base backup from: $backup_dir"
    
    mongorestore \
        --host="recovery-mongodb:27017" \
        --drop \
        --gzip \
        --oplogReplay \
        "$backup_dir"
}

# 3. åº”ç”¨å¢é‡å¤‡ä»½
apply_incremental_backups() {
    local base_time=$1
    local target_time=$2
    
    echo "Applying incremental backups from $base_time to $target_time"
    
    for increment in $(find "$BASE_BACKUP_DIR" -name "incremental_*" -type d | sort); do
        increment_time=$(basename "$increment" | cut -d'_' -f2-3)
        
        if [[ "$increment_time" > "$base_time" ]] && [[ "$increment_time" < "$target_time" ]]; then
            echo "Applying incremental backup: $increment"
            mongorestore \
                --host="recovery-mongodb:27017" \
                --gzip \
                --oplogReplay \
                "$increment"
        fi
    done
}

# 4. æ—¶é—´ç‚¹æ¢å¤
point_in_time_recovery() {
    local closest_backup=$(find_closest_backup)
    
    if [ -z "$closest_backup" ]; then
        echo "No suitable base backup found"
        exit 1
    fi
    
    echo "Using base backup: $closest_backup"
    
    # åœæ­¢MongoDBæœåŠ¡
    systemctl stop mongod
    
    # æ¸…ç†æ•°æ®ç›®å½•
    rm -rf /var/lib/mongo/*
    
    # æ¢å¤åŸºç¡€å¤‡ä»½
    restore_base_backup "$closest_backup"
    
    # åº”ç”¨å¢é‡å¤‡ä»½
    base_time=$(basename "$closest_backup" | cut -d'_' -f2-3)
    apply_incremental_backups "$base_time" "$TARGET_TIME"
    
    # å¯åŠ¨MongoDBæœåŠ¡
    systemctl start mongod
    
    echo "Point-in-time recovery completed"
}

point_in_time_recovery
```

## 6. Monitoring and Alerting

### 6.1 Comprehensive Monitoring Setup

```javascript
// MongoDB Monitoring Scripts

// 1. æ•°æ®åº“å¥åº·æ£€æŸ¥
db.adminCommand({ replSetGetStatus: 1 });
db.serverStatus();
db.currentOp();

// 2. æ€§èƒ½æŒ‡æ ‡æ”¶é›†
db.serverStatus().connections;
db.serverStatus().network;
db.serverStatus().opcounters;
db.serverStatus().mem;

// 3. å­˜å‚¨ä½¿ç”¨æƒ…å†µ
db.stats();
db.runCommand({ dbStats: 1, scale: 1024 * 1024 }); // MBå•ä½

// 4. æ…¢æŸ¥è¯¢åˆ†æ
db.system.profile.find({ millis: { $gt: 1000 } }).sort({ ts: -1 }).limit(10);

// 5. ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡
db.orders.aggregate([
  { $indexStats: {} },
  { $project: { 
      name: 1, 
      accesses: 1, 
      usage_ratio: { $divide: ["$accesses.ops", { $add: ["$accesses.since", 1] }] }
    }
  },
  { $sort: { "accesses.ops": -1 } }
]);
```

### 6.2 Prometheus Exporter Configuration

```yaml
# mongodb_exporter_config.yaml
scrape_configs:
  - job_name: 'mongodb'
    static_configs:
      - targets: ['mongodb-0:9216', 'mongodb-1:9216', 'mongodb-2:9216']
    scrape_interval: 30s
    scrape_timeout: 10s
    
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: '$1'
      
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
        replacement: '$1'

# Alerting Rules
groups:
  - name: mongodb.rules
    rules:
      - alert: MongoDBDown
        expr: mongodb_up == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "MongoDB instance is down"
          description: "MongoDB instance {{ $labels.instance }} is not responding"
      
      - alert: MongoDBReplicaSetMemberDown
        expr: mongodb_rs_members_state != 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "MongoDB replica set member issue"
          description: "MongoDB replica set member {{ $labels.rs_state }} is not in primary state"
      
      - alert: MongoDBHighConnectionUsage
        expr: mongodb_connections_used / mongodb_connections_available > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High MongoDB connection usage"
          description: "MongoDB connections usage is {{ $value | humanizePercentage }}"
```

## 7. Capacity Planning and Scaling

### 7.1 Resource Scaling Guidelines

```yaml
# scaling_guidelines.yaml
capacity_planning:
  sizing_recommendations:
    small_workload:
      data_size: "< 100GB"
      concurrent_users: "< 1000"
      recommended_config:
        cpu: "2-4 cores"
        memory: "8-16GB"
        storage: "SSD 200-500GB"
        replica_set_members: 3
    
    medium_workload:
      data_size: "100GB - 1TB"
      concurrent_users: "1000-10000"
      recommended_config:
        cpu: "4-8 cores"
        memory: "16-32GB"
        storage: "SSD 1-2TB"
        replica_set_members: 5
        sharded_clusters: 2-3 shards
    
    large_workload:
      data_size: "> 1TB"
      concurrent_users: "> 10000"
      recommended_config:
        cpu: "8-16 cores"
        memory: "32-64GB"
        storage: "SSD 2-5TB per shard"
        replica_set_members: 5-7
        sharded_clusters: 4-8 shards
        config_servers: 3
        mongos_routers: 3-5

scaling_strategies:
  vertical_scaling:
    when_to_use: "Single instance performance bottlenecks"
    limitations: "Hardware limits, maintenance windows required"
    procedure:
      - Schedule maintenance window
      - Stop MongoDB service
      - Upgrade hardware resources
      - Restart MongoDB service
      - Validate cluster health
  
  horizontal_scaling:
    when_to_use: "Data growth beyond single instance capacity"
    procedure:
      - Add new shard to cluster
      - Rebalance data across shards
      - Update application connection strings
      - Monitor performance metrics
```

### 7.2 Automated Scaling Scripts

```bash
#!/bin/bash
# mongodb_auto_scaling.sh

CLUSTER_NAME="production-cluster"
THRESHOLD_CPU=80
THRESHOLD_DISK=85

# 1. ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ
check_resource_usage() {
    # CPUä½¿ç”¨ç‡æ£€æŸ¥
    cpu_usage=$(mongostat --host localhost:27017 --rowcount 1 | awk 'NR==3 {print $2}' | tr -d '%')
    
    # ç£ç›˜ä½¿ç”¨ç‡æ£€æŸ¥
    disk_usage=$(df /var/lib/mongo | awk 'NR==2 {print $5}' | tr -d '%')
    
    echo "CPU Usage: ${cpu_usage}%"
    echo "Disk Usage: ${disk_usage}%"
    
    return $((${cpu_usage} > ${THRESHOLD_CPU} || ${disk_usage} > ${THRESHOLD_DISK}))
}

# 2. è‡ªåŠ¨æ‰©å®¹å†³ç­–
auto_scale_decision() {
    if check_resource_usage; then
        echo "Resource thresholds exceeded, initiating scaling..."
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥å‚ç›´æ‰©å®¹
        current_cpu=$(lscpu | grep "^CPU(s):" | awk '{print $2}')
        if [ $current_cpu -lt 16 ]; then
            echo "Initiating vertical scaling..."
            vertical_scale
        else
            echo "Initiating horizontal scaling..."
            horizontal_scale
        fi
    else
        echo "Resource usage within acceptable limits"
    fi
}

# 3. å‚ç›´æ‰©å®¹
vertical_scale() {
    # è¿™é‡Œåº”è¯¥é›†æˆäº‘æä¾›å•†çš„APIæ¥è°ƒæ•´å®ä¾‹å¤§å°
    echo "Vertical scaling not implemented in this example"
    # å®é™…å®ç°ä¼šè°ƒç”¨AWS/Azure/GCP APIæ¥è°ƒæ•´å®ä¾‹è§„æ ¼
}

# 4. æ°´å¹³æ‰©å®¹ï¼ˆæ·»åŠ åˆ†ç‰‡ï¼‰
horizontal_scale() {
    # è·å–æ–°çš„åˆ†ç‰‡åç§°
    new_shard_name="shard$(date +%s)"
    
    # æ·»åŠ æ–°åˆ†ç‰‡åˆ°é›†ç¾¤
    mongo --host "mongos-0:27017" << EOF
    sh.addShard("${new_shard_name}/${new_shard_name}-0:27017,${new_shard_name}-1:27017,${new_shard_name}-2:27017")
EOF
    
    # è§¦å‘æ•°æ®é‡æ–°å¹³è¡¡
    mongo --host "mongos-0:27017" << EOF
    sh.startBalancer()
EOF
    
    echo "Added new shard: ${new_shard_name}"
}

# 5. æ‰©å®¹åéªŒè¯
post_scale_validation() {
    echo "Validating post-scaling status..."
    
    # æ£€æŸ¥é›†ç¾¤çŠ¶æ€
    mongo --host "mongos-0:27017" --eval "sh.status()"
    
    # æ£€æŸ¥åˆ†ç‰‡åˆ†å¸ƒ
    mongo --host "mongos-0:27017" --eval "db.printShardingStatus()"
    
    # ç›‘æ§æ€§èƒ½æŒ‡æ ‡
    sleep 300  # ç­‰å¾…ç¨³å®š
    mongo --host "mongos-0:27017" --eval "db.serverStatus()"
}

# ä¸»å¾ªç¯
while true; do
    auto_scale_decision
    sleep 3600  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
done
```

---
*This document is based on enterprise-level MongoDB practice experience and continuously updated with the latest technologies and best practices.*
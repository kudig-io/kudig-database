# 32 - Lease / Event / Node YAML é…ç½®å‚è€ƒ

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-02

**æœ¬æ–‡æ¡£å…¨é¢è¦†ç›– Lease(ç§Ÿçº¦)ã€Event(äº‹ä»¶)ã€Node(èŠ‚ç‚¹)çš„ YAML é…ç½®**,åŒ…æ‹¬å®Œæ•´å­—æ®µè¯´æ˜ã€Lease åœ¨ Leader Election ä¸­çš„åº”ç”¨ã€Event äº‹ä»¶ç±»å‹ã€Node èŠ‚ç‚¹ç®¡ç†ã€ç”Ÿäº§å®è·µæ¡ˆä¾‹ç­‰ã€‚

---

## ğŸ“‹ ç›®å½•

1. [Lease ç§Ÿçº¦é…ç½®](#1-lease-ç§Ÿçº¦é…ç½®)
2. [Event äº‹ä»¶é…ç½®](#2-event-äº‹ä»¶é…ç½®)
3. [Node èŠ‚ç‚¹é…ç½®](#3-node-èŠ‚ç‚¹é…ç½®)
4. [ç”Ÿäº§æ¡ˆä¾‹](#4-ç”Ÿäº§æ¡ˆä¾‹)
5. [æ•…éšœæ’æŸ¥](#5-æ•…éšœæ’æŸ¥)

---

## 1. Lease ç§Ÿçº¦é…ç½®

### 1.1 Lease åŸºç¡€æ¦‚å¿µ

Lease(ç§Ÿçº¦)æ˜¯ Kubernetes ä¸­çš„**åˆ†å¸ƒå¼é”æœºåˆ¶**,ä¸»è¦ç”¨äº:

- **èŠ‚ç‚¹å¿ƒè·³**: Kubelet é€šè¿‡æ›´æ–° Lease å‘æ§åˆ¶å¹³é¢æŠ¥å‘ŠèŠ‚ç‚¹å­˜æ´»çŠ¶æ€(å–ä»£ Node Status æ›´æ–°,å‡å°‘ etcd å‹åŠ›)
- **Leader Election**: æ§åˆ¶å™¨é€šè¿‡ç«äº‰ Lease å®ç° Leader é€‰ä¸¾(ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªå®ä¾‹è¿è¡Œ)
- **API Server Identity**: API Server å®ä¾‹é€šè¿‡ Lease æ ‡è¯†èº«ä»½(v1.26+)

### 1.2 Lease å®Œæ•´å­—æ®µ

```yaml
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  # Lease åç§°
  name: my-controller-leader
  # å‘½åç©ºé—´(Lease æ˜¯å‘½åç©ºé—´çº§èµ„æº)
  namespace: kube-system
  # æ ‡ç­¾(å¯é€‰,ç”¨äºæ ‡è¯†ç”¨é€”)
  labels:
    app: my-controller
    component: leader-election
spec:
  # === æ ¸å¿ƒå­—æ®µ ===
  
  # æŒæœ‰è€…æ ‡è¯†(å½“å‰æŒæœ‰ Lease çš„å®ä¾‹ ID)
  holderIdentity: "my-controller-pod-abc123"
  
  # ç§Ÿçº¦æœ‰æ•ˆæœŸ(ç§’,æ¨è 10-15 ç§’)
  leaseDurationSeconds: 15
  
  # è·å–æ—¶é—´(é¦–æ¬¡è·å¾— Lease çš„æ—¶é—´æˆ³,RFC3339 æ ¼å¼)
  acquireTime: "2026-02-10T10:00:00.123456Z"
  
  # ç»­çº¦æ—¶é—´(æœ€åä¸€æ¬¡ç»­çº¦çš„æ—¶é—´æˆ³,RFC3339 æ ¼å¼)
  renewTime: "2026-02-10T10:00:05.654321Z"
  
  # ç§Ÿçº¦è½¬æ¢æ¬¡æ•°(Leader åˆ‡æ¢æ¬¡æ•°,ç”¨äºæ£€æµ‹é¢‘ç¹åˆ‡æ¢)
  leaseTransitions: 3
  
  # === é«˜çº§å­—æ®µ(v1.27+) ===
  
  # ä¼˜å…ˆä½¿ç”¨çš„ Leader(å¯é€‰,ç”¨äºæç¤ºä¸‹æ¬¡ Leader é€‰ä¸¾ä¼˜å…ˆé€‰æ‹©è¯¥å®ä¾‹)
  # æ³¨æ„: è¿™æ˜¯ä¸€ä¸ª"æç¤º"å­—æ®µ,ä¸æ˜¯å¼ºåˆ¶è¦æ±‚
  preferredHolder: "my-controller-pod-xyz789"
  
  # ç­–ç•¥(å¯é€‰,v1.27 Alpha,å®šä¹‰ Leader Election ç­–ç•¥)
  strategy: OldestEmulationVersion  # æˆ– nil
```

### 1.3 Lease ç”¨é€”è¯¦è§£

#### 1.3.1 èŠ‚ç‚¹å¿ƒè·³(kube-node-lease)

ä» Kubernetes v1.14 å¼€å§‹,Kubelet ä½¿ç”¨ Lease æ›¿ä»£ Node Status æ›´æ–°ä½œä¸ºå¿ƒè·³æœºåˆ¶:

```yaml
# èŠ‚ç‚¹å¿ƒè·³ Lease(ç”± Kubelet è‡ªåŠ¨åˆ›å»ºå’Œç»´æŠ¤)
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  # åç§°ä¸èŠ‚ç‚¹åä¸€è‡´
  name: node1
  # å›ºå®šå‘½åç©ºé—´
  namespace: kube-node-lease
  # Kubelet è‡ªåŠ¨æ·»åŠ çš„æ ‡ç­¾
  labels:
    kubernetes.io/hostname: node1
spec:
  # æŒæœ‰è€…ä¸ºèŠ‚ç‚¹å
  holderIdentity: "node1"
  # é»˜è®¤ 40 ç§’(ä¸ --node-status-update-frequency ç›¸å…³)
  leaseDurationSeconds: 40
  # Kubelet æ¯ 10 ç§’æ›´æ–°ä¸€æ¬¡(--node-lease-renew-interval-seconds)
  renewTime: "2026-02-10T10:00:30Z"
  acquireTime: "2026-02-10T09:00:00Z"
  leaseTransitions: 0  # èŠ‚ç‚¹å¿ƒè·³ä¸æ¶‰åŠåˆ‡æ¢
```

**ä¼˜åŠ¿**:

- **å‡å°‘ etcd å‹åŠ›**: Node Status å¯¹è±¡è¾ƒå¤§(åŒ…å« conditions, addresses, capacity ç­‰),è€Œ Lease ä»… ~200 å­—èŠ‚
- **é™ä½ç½‘ç»œå¼€é”€**: Lease æ›´æ–°é¢‘ç‡å¯ç‹¬ç«‹é…ç½®,ä¸å½±å“ Node Status æ›´æ–°
- **æé«˜æ£€æµ‹é€Ÿåº¦**: æ›´é¢‘ç¹çš„å¿ƒè·³(é»˜è®¤ 10 ç§’)ä½¿èŠ‚ç‚¹æ•…éšœæ£€æµ‹æ›´å¿«

**ç›¸å…³ Kubelet å‚æ•°**:

```bash
# kubelet å¯åŠ¨å‚æ•°
--node-lease-duration-seconds=40           # Lease æœ‰æ•ˆæœŸ
--node-lease-renew-interval-seconds=10     # ç»­çº¦é—´éš”
--node-status-update-frequency=10s         # Node Status æ›´æ–°é¢‘ç‡(ä¿ç•™ç”¨äºçŠ¶æ€å˜æ›´)
```

#### 1.3.2 Leader Election(æ§åˆ¶å™¨)

æ§åˆ¶å™¨é€šè¿‡ç«äº‰ Lease å®ç° Leader é€‰ä¸¾:

```yaml
# Leader Election Lease(ç”± client-go è‡ªåŠ¨ç®¡ç†)
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  name: my-controller
  namespace: default
spec:
  # å½“å‰ Leader çš„ Pod åç§°
  holderIdentity: "my-controller-7d8f9b5c6-abc123"
  # ç§Ÿçº¦æœ‰æ•ˆæœŸ(æ¨è 10-15 ç§’)
  leaseDurationSeconds: 15
  # é¦–æ¬¡è·å¾— Leader çš„æ—¶é—´
  acquireTime: "2026-02-10T10:00:00Z"
  # æœ€åä¸€æ¬¡ç»­çº¦æ—¶é—´
  renewTime: "2026-02-10T10:01:30Z"
  # Leader åˆ‡æ¢æ¬¡æ•°
  leaseTransitions: 2
```

**å·¥ä½œæµç¨‹**:

```
å¤šä¸ª Pod å‰¯æœ¬(Replica 1, 2, 3)
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. å°è¯•è·å– Lease(é€šè¿‡ CREATE è¯·æ±‚)                             â”‚
â”‚    - Replica 1: CREATE Lease/my-controller â†’ æˆåŠŸ(æˆä¸º Leader)  â”‚
â”‚    - Replica 2: CREATE Lease/my-controller â†’ å¤±è´¥(å·²å­˜åœ¨)       â”‚
â”‚    - Replica 3: CREATE Lease/my-controller â†’ å¤±è´¥(å·²å­˜åœ¨)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Leader å®šæœŸç»­çº¦(UPDATE è¯·æ±‚)                                 â”‚
â”‚    - Replica 1(Leader): æ¯ 10 ç§’æ›´æ–° renewTime                  â”‚
â”‚    - Replica 2/3(Follower): ç›‘å¬ Lease å˜åŒ–                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Leader æ•…éšœ â†’ Follower æ¥ç®¡                                  â”‚
â”‚    - Replica 1 Crash,åœæ­¢ç»­çº¦                                   â”‚
â”‚    - Replica 2 æ£€æµ‹åˆ° Lease è¿‡æœŸ(renewTime + 15s < now)         â”‚
â”‚    - Replica 2 å°è¯•æ›´æ–° Lease.holderIdentity = "replica-2"      â”‚
â”‚      * ä½¿ç”¨ Optimistic Concurrency(resourceVersion æ£€æŸ¥)        â”‚
â”‚      * å¦‚æœæˆåŠŸ â†’ æˆä¸ºæ–° Leader                                 â”‚
â”‚      * å¦‚æœå¤±è´¥(Replica 3 å…ˆæŠ¢åˆ°)â†’ ç»§ç»­ç­‰å¾…                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**client-go ä»£ç ç¤ºä¾‹**(Go):

```go
import (
    "context"
    "time"
    
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/tools/leaderelection"
    "k8s.io/client-go/tools/leaderelection/resourcelock"
)

func runLeaderElection(clientset *kubernetes.Clientset) {
    // åˆ›å»º Lease é”
    lock := &resourcelock.LeaseLock{
        LeaseMeta: metav1.ObjectMeta{
            Name:      "my-controller",
            Namespace: "default",
        },
        Client: clientset.CoordinationV1(),
        LockConfig: resourcelock.ResourceLockConfig{
            Identity: "my-controller-pod-abc123",  // å½“å‰ Pod åç§°
        },
    }
    
    // é…ç½® Leader Election
    leaderelection.RunOrDie(context.Background(), leaderelection.LeaderElectionConfig{
        Lock:            lock,
        LeaseDuration:   15 * time.Second,  // ç§Ÿçº¦æœ‰æ•ˆæœŸ
        RenewDeadline:   10 * time.Second,  // ç»­çº¦æˆªæ­¢æ—¶é—´
        RetryPeriod:     2 * time.Second,   // é‡è¯•é—´éš”
        Callbacks: leaderelection.LeaderCallbacks{
            // æˆä¸º Leader æ—¶è°ƒç”¨
            OnStartedLeading: func(ctx context.Context) {
                fmt.Println("I am the leader!")
                // å¯åŠ¨æ§åˆ¶å™¨é€»è¾‘...
            },
            // å¤±å» Leader æ—¶è°ƒç”¨
            OnStoppedLeading: func() {
                fmt.Println("I lost leadership!")
                // åœæ­¢æ§åˆ¶å™¨é€»è¾‘...
            },
            // æ–° Leader äº§ç”Ÿæ—¶è°ƒç”¨(æ‰€æœ‰å‰¯æœ¬éƒ½ä¼šæ”¶åˆ°)
            OnNewLeader: func(identity string) {
                fmt.Printf("New leader: %s\n", identity)
            },
        },
    })
}
```

#### 1.3.3 API Server Identity(v1.26+)

API Server ä½¿ç”¨ Lease æ ‡è¯†è‡ªå·±çš„èº«ä»½,ç”¨äºåè°ƒå’Œç›‘æ§:

```yaml
# API Server Identity Lease(ç”± kube-apiserver è‡ªåŠ¨åˆ›å»º)
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  name: kube-apiserver-xxx  # API Server å®ä¾‹ ID
  namespace: kube-system
  labels:
    apiserver.kubernetes.io/identity: kube-apiserver
spec:
  holderIdentity: "kube-apiserver-xxx_xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
  leaseDurationSeconds: 3600  # 1 å°æ—¶
  renewTime: "2026-02-10T10:00:00Z"
```

**ç”¨é€”**:

- **ç›‘æ§**: ç»Ÿè®¡å½“å‰è¿è¡Œçš„ API Server å®ä¾‹æ•°é‡
- **åè°ƒ**: ç”¨äº API Server ä¹‹é—´çš„åè°ƒæ“ä½œ(å¦‚ Storage Version è¿ç§»)

---

## 2. Event äº‹ä»¶é…ç½®

### 2.1 Event åŸºç¡€æ¦‚å¿µ

Event(äº‹ä»¶)æ˜¯ Kubernetes ä¸­çš„**å®¡è®¡å’Œè°ƒè¯•æœºåˆ¶**,è®°å½•é›†ç¾¤ä¸­å‘ç”Ÿçš„é‡è¦æ“ä½œ:

- **èµ„æºç”Ÿå‘½å‘¨æœŸ**: Pod åˆ›å»ºã€è°ƒåº¦ã€å¯åŠ¨ã€å¤±è´¥ç­‰
- **ç³»ç»Ÿå‘Šè­¦**: èŠ‚ç‚¹èµ„æºä¸è¶³ã€é•œåƒæ‹‰å–å¤±è´¥ã€å·æŒ‚è½½å¤±è´¥ç­‰
- **æ§åˆ¶å™¨è¡Œä¸º**: Deployment æ»šåŠ¨æ›´æ–°ã€HPA æ‰©ç¼©å®¹ã€PVC ç»‘å®šç­‰

### 2.2 Event å®Œæ•´å­—æ®µ

#### 2.2.1 events.k8s.io/v1 (æ¨è,v1.19+)

```yaml
apiVersion: events.k8s.io/v1
kind: Event
metadata:
  name: my-pod.17e2a1b2c3d4e5f6  # è‡ªåŠ¨ç”Ÿæˆ: <å¯¹è±¡å>.<hash>
  namespace: default
  # äº‹ä»¶çš„åˆ›å»ºæ—¶é—´
  creationTimestamp: "2026-02-10T10:00:00Z"
spec:
  # === å…³è”å¯¹è±¡ ===
  
  # äº‹ä»¶æ¶‰åŠçš„ä¸»è¦å¯¹è±¡
  regarding:
    apiVersion: v1
    kind: Pod
    name: my-pod
    namespace: default
    uid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
    resourceVersion: "12345"
  
  # ç›¸å…³å¯¹è±¡(å¯é€‰,å¦‚ PVC æŒ‚è½½äº‹ä»¶ä¸­çš„ PV)
  related:
    apiVersion: v1
    kind: PersistentVolume
    name: pv-abc123
  
  # === äº‹ä»¶å†…å®¹ ===
  
  # äº‹ä»¶åŸå› (ç®€çŸ­æ ‡è¯†,å¦‚ FailedScheduling, Pulled, Started)
  reason: FailedScheduling
  
  # äº‹ä»¶æ¶ˆæ¯(è¯¦ç»†æè¿°)
  message: "0/3 nodes are available: 3 Insufficient cpu."
  
  # äº‹ä»¶ç±»å‹: Normal(æ­£å¸¸) æˆ– Warning(å‘Šè­¦)
  type: Warning
  
  # === æ—¶é—´ä¿¡æ¯ ===
  
  # é¦–æ¬¡å‘ç”Ÿæ—¶é—´
  eventTime: "2026-02-10T10:00:00.123456Z"
  
  # === äº‹ä»¶æ¥æº ===
  
  # æŠ¥å‘Šè€…(ç”Ÿæˆäº‹ä»¶çš„ç»„ä»¶)
  reportingController: "default-scheduler"
  
  # æŠ¥å‘Šå®ä¾‹(ç»„ä»¶çš„å…·ä½“å®ä¾‹)
  reportingInstance: "default-scheduler-node1"
  
  # === é‡å¤äº‹ä»¶å¤„ç† ===
  
  # åŠ¨ä½œ(Binding, Started, Killing ç­‰)
  action: "Scheduling"
  
  # é‡å¤æ¬¡æ•°(ç›¸åŒäº‹ä»¶å‘ç”Ÿæ¬¡æ•°)
  series:
    count: 5              # é‡å¤æ¬¡æ•°
    lastObservedTime: "2026-02-10T10:00:30Z"  # æœ€åå‘ç”Ÿæ—¶é—´
  
  # === å¤‡æ³¨ ===
  
  # å¤‡æ³¨ä¿¡æ¯(å¯é€‰,æ‰©å±•ä¿¡æ¯)
  note: "Pod triggered scale-up"
```

#### 2.2.2 v1/Event (æ—§ç‰ˆæœ¬,å·²å¼ƒç”¨ä½†ä»å…¼å®¹)

```yaml
apiVersion: v1
kind: Event
metadata:
  name: my-pod.17e2a1b2c3d4e5f6
  namespace: default
# æ¶‰åŠçš„å¯¹è±¡
involvedObject:
  apiVersion: v1
  kind: Pod
  name: my-pod
  namespace: default
  uid: "xxx"
  resourceVersion: "12345"
# äº‹ä»¶åŸå› 
reason: FailedScheduling
# äº‹ä»¶æ¶ˆæ¯
message: "0/3 nodes are available: 3 Insufficient cpu."
# äº‹ä»¶ç±»å‹
type: Warning
# é¦–æ¬¡å‘ç”Ÿæ—¶é—´
firstTimestamp: "2026-02-10T10:00:00Z"
# æœ€åå‘ç”Ÿæ—¶é—´
lastTimestamp: "2026-02-10T10:00:30Z"
# é‡å¤æ¬¡æ•°
count: 5
# äº‹ä»¶æ¥æº
source:
  component: default-scheduler
  host: node1
```

### 2.3 å¸¸è§ Event Reason åˆ—è¡¨

#### Pod ç”Ÿå‘½å‘¨æœŸäº‹ä»¶

| Reason | Type | æè¿° |
|--------|------|------|
| **Scheduled** | Normal | Pod å·²æˆåŠŸè°ƒåº¦åˆ°èŠ‚ç‚¹ |
| **FailedScheduling** | Warning | è°ƒåº¦å¤±è´¥(èµ„æºä¸è¶³ã€æ±¡ç‚¹ä¸åŒ¹é…ç­‰) |
| **Pulling** | Normal | å¼€å§‹æ‹‰å–å®¹å™¨é•œåƒ |
| **Pulled** | Normal | é•œåƒæ‹‰å–æˆåŠŸ |
| **Failed** | Warning | é•œåƒæ‹‰å–å¤±è´¥(é•œåƒä¸å­˜åœ¨ã€è®¤è¯å¤±è´¥ç­‰) |
| **BackOff** | Warning | å®¹å™¨å¯åŠ¨å¤±è´¥,å¤„äº BackOff çŠ¶æ€ |
| **Created** | Normal | å®¹å™¨å·²åˆ›å»º |
| **Started** | Normal | å®¹å™¨å·²å¯åŠ¨ |
| **Killing** | Normal | æ­£åœ¨ç»ˆæ­¢å®¹å™¨ |
| **Preempting** | Normal | Pod è¢«æŠ¢å (ä¼˜å…ˆçº§æ›´é«˜çš„ Pod åˆ°è¾¾) |
| **Unhealthy** | Warning | å¥åº·æ£€æŸ¥å¤±è´¥ |

#### èŠ‚ç‚¹äº‹ä»¶

| Reason | Type | æè¿° |
|--------|------|------|
| **NodeReady** | Normal | èŠ‚ç‚¹å˜ä¸º Ready çŠ¶æ€ |
| **NodeNotReady** | Warning | èŠ‚ç‚¹å˜ä¸º NotReady çŠ¶æ€ |
| **NodeSchedulable** | Normal | èŠ‚ç‚¹å˜ä¸ºå¯è°ƒåº¦ |
| **NodeNotSchedulable** | Warning | èŠ‚ç‚¹è¢«æ ‡è®°ä¸ºä¸å¯è°ƒåº¦ |
| **RegisteredNode** | Normal | æ–°èŠ‚ç‚¹æ³¨å†Œåˆ°é›†ç¾¤ |
| **RemovingNode** | Warning | èŠ‚ç‚¹æ­£åœ¨è¢«ç§»é™¤ |

#### å­˜å‚¨äº‹ä»¶

| Reason | Type | æè¿° |
|--------|------|------|
| **SuccessfulAttachVolume** | Normal | å·æˆåŠŸæŒ‚è½½åˆ°èŠ‚ç‚¹ |
| **FailedAttachVolume** | Warning | å·æŒ‚è½½å¤±è´¥ |
| **SuccessfulMountVolume** | Normal | å·æˆåŠŸæŒ‚è½½åˆ° Pod |
| **FailedMount** | Warning | å·æŒ‚è½½åˆ° Pod å¤±è´¥ |
| **VolumeResizeFailed** | Warning | å·æ‰©å®¹å¤±è´¥ |
| **VolumeResizeSuccessful** | Normal | å·æ‰©å®¹æˆåŠŸ |

#### æ§åˆ¶å™¨äº‹ä»¶

| Reason | Type | æè¿° |
|--------|------|------|
| **SuccessfulCreate** | Normal | Deployment/ReplicaSet æˆåŠŸåˆ›å»º Pod |
| **FailedCreate** | Warning | åˆ›å»º Pod å¤±è´¥(é…é¢è¶…é™ç­‰) |
| **ScalingReplicaSet** | Normal | ReplicaSet æ‰©ç¼©å®¹ |
| **SuccessfulDelete** | Normal | æˆåŠŸåˆ é™¤ Pod |
| **FailedDelete** | Warning | åˆ é™¤ Pod å¤±è´¥ |

---

## 3. Node èŠ‚ç‚¹é…ç½®

### 3.1 Node åŸºç¡€æ¦‚å¿µ

Node(èŠ‚ç‚¹)æ˜¯ Kubernetes é›†ç¾¤çš„**å·¥ä½œè´Ÿè½½è½½ä½“**,è¿è¡Œ Kubelet å’Œå®¹å™¨:

- **è‡ªåŠ¨æ³¨å†Œ**: Kubelet å¯åŠ¨æ—¶è‡ªåŠ¨å‘ API Server æ³¨å†ŒèŠ‚ç‚¹
- **çŠ¶æ€åŒæ­¥**: Kubelet å®šæœŸä¸ŠæŠ¥èŠ‚ç‚¹çŠ¶æ€(Ready/NotReadyã€èµ„æºå®¹é‡ç­‰)
- **æ ‡ç­¾ç®¡ç†**: é€šè¿‡æ ‡ç­¾å®ç°èŠ‚ç‚¹é€‰æ‹©å™¨ã€äº²å’Œæ€§è°ƒåº¦
- **æ±¡ç‚¹ç®¡ç†**: é€šè¿‡æ±¡ç‚¹(Taint)é©±é€æˆ–é˜»æ­¢ Pod è°ƒåº¦

### 3.2 Node å®Œæ•´å­—æ®µ

```yaml
apiVersion: v1
kind: Node
metadata:
  name: node1
  # æ ‡ç­¾(ç”¨äº nodeSelector, affinity)
  labels:
    kubernetes.io/hostname: node1
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64
    node-role.kubernetes.io/control-plane: ""  # æ§åˆ¶å¹³é¢èŠ‚ç‚¹
    node-role.kubernetes.io/worker: ""         # å·¥ä½œèŠ‚ç‚¹
    # è‡ªå®šä¹‰æ ‡ç­¾
    environment: production
    zone: us-west-1a
    instance-type: c5.2xlarge
  # æ³¨è§£
  annotations:
    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
    node.alpha.kubernetes.io/ttl: "0"
    volumes.kubernetes.io/controller-managed-attach-detach: "true"
spec:
  # === åŸºç¡€é…ç½® ===
  
  # Pod CIDR(èŠ‚ç‚¹ä¸Š Pod çš„ IP åœ°å€èŒƒå›´,ç”± CNI åˆ†é…)
  podCIDR: 10.244.1.0/24
  
  # Pod CIDRs(å¤šåè®®æ ˆæ”¯æŒ,v1.16+)
  podCIDRs:
    - 10.244.1.0/24        # IPv4
    - fd00:10:244:1::/64   # IPv6
  
  # äº‘æä¾›å•† ID(å¦‚ AWS: aws:///us-west-1a/i-0abcd1234efgh5678)
  providerID: "aws:///us-west-1a/i-0abcd1234efgh5678"
  
  # === æ±¡ç‚¹(Taint)é…ç½® ===
  
  taints:
    # æ±¡ç‚¹ 1: ç¦æ­¢è°ƒåº¦(NoSchedule)
    - key: node-role.kubernetes.io/control-plane
      effect: NoSchedule
    
    # æ±¡ç‚¹ 2: ä¼˜å…ˆé©±é€(PreferNoSchedule)
    - key: example.com/maintenance
      value: "true"
      effect: PreferNoSchedule
    
    # æ±¡ç‚¹ 3: ç«‹å³é©±é€(NoExecute)
    - key: node.kubernetes.io/unreachable
      effect: NoExecute
      timeAdded: "2026-02-10T10:00:00Z"
  
  # === è°ƒåº¦æ§åˆ¶ ===
  
  # ä¸å¯è°ƒåº¦æ ‡è®°(true = ä¸è°ƒåº¦æ–° Pod,ä½†ä¸é©±é€ç°æœ‰ Pod)
  unschedulable: false
  
  # === é…ç½®æº(Kubelet é…ç½®æ¥æº) ===
  
  configSource:
    configMap:
      name: kubelet-config
      namespace: kube-system
      uid: "xxx"
      kubeletConfigKey: kubelet

status:
  # === èµ„æºå®¹é‡ ===
  
  # èŠ‚ç‚¹æ€»å®¹é‡
  capacity:
    cpu: "8"               # 8 æ ¸ CPU
    memory: "32Gi"         # 32 GB å†…å­˜
    ephemeral-storage: "100Gi"  # 100 GB ä¸´æ—¶å­˜å‚¨
    hugepages-1Gi: "0"
    hugepages-2Mi: "0"
    pods: "110"            # æœ€å¤š 110 ä¸ª Pod(--max-pods)
  
  # å¯åˆ†é…èµ„æº(æ‰£é™¤ç³»ç»Ÿé¢„ç•™)
  allocatable:
    cpu: "7500m"           # 7.5 æ ¸(é¢„ç•™ 0.5 æ ¸ç»™ç³»ç»Ÿ)
    memory: "30Gi"         # 30 GB(é¢„ç•™ 2 GB ç»™ç³»ç»Ÿ)
    ephemeral-storage: "90Gi"
    pods: "110"
  
  # === èŠ‚ç‚¹çŠ¶æ€æ¡ä»¶ ===
  
  conditions:
    # èŠ‚ç‚¹å°±ç»ªçŠ¶æ€
    - type: Ready
      status: "True"       # True, False, Unknown
      lastHeartbeatTime: "2026-02-10T10:05:00Z"
      lastTransitionTime: "2026-02-10T09:00:00Z"
      reason: KubeletReady
      message: "kubelet is posting ready status"
    
    # å†…å­˜å‹åŠ›
    - type: MemoryPressure
      status: "False"
      lastHeartbeatTime: "2026-02-10T10:05:00Z"
      lastTransitionTime: "2026-02-10T09:00:00Z"
      reason: KubeletHasSufficientMemory
      message: "kubelet has sufficient memory available"
    
    # ç£ç›˜å‹åŠ›
    - type: DiskPressure
      status: "False"
      lastHeartbeatTime: "2026-02-10T10:05:00Z"
      lastTransitionTime: "2026-02-10T09:00:00Z"
      reason: KubeletHasNoDiskPressure
      message: "kubelet has no disk pressure"
    
    # PID å‹åŠ›
    - type: PIDPressure
      status: "False"
      lastHeartbeatTime: "2026-02-10T10:05:00Z"
      lastTransitionTime: "2026-02-10T09:00:00Z"
      reason: KubeletHasSufficientPID
      message: "kubelet has sufficient PID available"
    
    # ç½‘ç»œä¸å¯ç”¨(ä»…åœ¨ CNI æœªå°±ç»ªæ—¶ä¸º True)
    - type: NetworkUnavailable
      status: "False"
      lastHeartbeatTime: "2026-02-10T10:05:00Z"
      lastTransitionTime: "2026-02-10T09:00:00Z"
      reason: RouteCreated
      message: "RouteController created a route"
  
  # === èŠ‚ç‚¹åœ°å€ ===
  
  addresses:
    - type: InternalIP       # å†…éƒ¨ IP(é›†ç¾¤å†…é€šä¿¡)
      address: 10.0.1.100
    - type: ExternalIP       # å¤–éƒ¨ IP(å¯é€‰,å…¬ç½‘ IP)
      address: 203.0.113.50
    - type: Hostname         # ä¸»æœºå
      address: node1
    - type: InternalDNS      # å†…éƒ¨ DNS(å¯é€‰)
      address: node1.internal.example.com
    - type: ExternalDNS      # å¤–éƒ¨ DNS(å¯é€‰)
      address: node1.example.com
  
  # === å®ˆæŠ¤è¿›ç¨‹ç«¯ç‚¹ ===
  
  daemonEndpoints:
    # Kubelet ç«¯ç‚¹
    kubeletEndpoint:
      Port: 10250  # Kubelet API ç«¯å£
  
  # === èŠ‚ç‚¹ä¿¡æ¯ ===
  
  nodeInfo:
    # æ“ä½œç³»ç»Ÿ
    operatingSystem: linux
    architecture: amd64
    
    # å†…æ ¸ç‰ˆæœ¬
    kernelVersion: "5.15.0-60-generic"
    
    # æ“ä½œç³»ç»Ÿé•œåƒ
    osImage: "Ubuntu 22.04.1 LTS"
    
    # å®¹å™¨è¿è¡Œæ—¶ç‰ˆæœ¬
    containerRuntimeVersion: "containerd://1.7.13"
    
    # Kubelet ç‰ˆæœ¬
    kubeletVersion: "v1.32.0"
    
    # Kube-Proxy ç‰ˆæœ¬
    kubeProxyVersion: "v1.32.0"
    
    # Machine ID
    machineID: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    
    # System UUID
    systemUUID: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
    
    # Boot ID
    bootID: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
  
  # === é•œåƒåˆ—è¡¨(èŠ‚ç‚¹ä¸Šå·²ç¼“å­˜çš„é•œåƒ) ===
  
  images:
    - names:
        - "registry.k8s.io/pause:3.9"
        - "registry.k8s.io/pause@sha256:xxx"
      sizeBytes: 514000
    - names:
        - "nginx:1.21"
      sizeBytes: 142000000
  
  # === å·ä¿¡æ¯(å·²æŒ‚è½½çš„å·) ===
  
  volumesInUse:
    - kubernetes.io/csi/ebs.csi.aws.com^vol-0abcd1234efgh5678
  
  volumesAttached:
    - name: kubernetes.io/csi/ebs.csi.aws.com^vol-0abcd1234efgh5678
      devicePath: /dev/xvda
```

### 3.3 Node Condition è¯¦è§£

| Condition Type | Status=True å«ä¹‰ | Status=False å«ä¹‰ |
|---------------|-----------------|------------------|
| **Ready** | èŠ‚ç‚¹å¥åº·,å¯æ¥æ”¶ Pod | èŠ‚ç‚¹å¼‚å¸¸,ä¸å¯æ¥æ”¶ Pod |
| **MemoryPressure** | å†…å­˜å‹åŠ›è¿‡å¤§,å¯èƒ½é©±é€ Pod | å†…å­˜å……è¶³ |
| **DiskPressure** | ç£ç›˜å‹åŠ›è¿‡å¤§,å¯èƒ½é©±é€ Pod | ç£ç›˜å……è¶³ |
| **PIDPressure** | è¿›ç¨‹æ•°è¿‡å¤š,å¯èƒ½é©±é€ Pod | è¿›ç¨‹æ•°æ­£å¸¸ |
| **NetworkUnavailable** | ç½‘ç»œæœªé…ç½®(CNI æœªå°±ç»ª) | ç½‘ç»œæ­£å¸¸ |

**è§¦å‘æ¡ä»¶**(Kubelet å‚æ•°):

```bash
# å†…å­˜å‹åŠ›é˜ˆå€¼
--eviction-hard=memory.available<100Mi
--eviction-soft=memory.available<300Mi
--eviction-soft-grace-period=memory.available=1m30s

# ç£ç›˜å‹åŠ›é˜ˆå€¼
--eviction-hard=nodefs.available<10%,nodefs.inodesFree<5%
--eviction-soft=nodefs.available<15%,nodefs.inodesFree<10%

# PID å‹åŠ›é˜ˆå€¼
--eviction-hard=pid.available<1000
```

---

## 4. ç”Ÿäº§æ¡ˆä¾‹

### 4.1 æ¡ˆä¾‹ 1: Leader Election é«˜å¯ç”¨æ§åˆ¶å™¨

**åœºæ™¯**: éƒ¨ç½²ä¸€ä¸ªè‡ªå®šä¹‰æ§åˆ¶å™¨,ä½¿ç”¨ Leader Election ç¡®ä¿å•å®ä¾‹è¿è¡Œ

```yaml
# controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-controller
  namespace: default
spec:
  replicas: 3  # éƒ¨ç½² 3 ä¸ªå‰¯æœ¬(é«˜å¯ç”¨)
  selector:
    matchLabels:
      app: my-controller
  template:
    metadata:
      labels:
        app: my-controller
    spec:
      serviceAccountName: my-controller
      containers:
        - name: controller
          image: myregistry.com/my-controller:v1.0.0
          args:
            # å¯ç”¨ Leader Election
            - --leader-elect=true
            - --leader-elect-lease-duration=15s
            - --leader-elect-renew-deadline=10s
            - --leader-elect-retry-period=2s
            - --leader-elect-resource-name=my-controller
            - --leader-elect-resource-namespace=default
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 1000m
              memory: 512Mi

---
# controller-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-controller
  namespace: default

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-controller-leader-election
  namespace: default
rules:
  # å…è®¸åˆ›å»ºå’Œæ›´æ–° Lease
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "create", "update", "patch"]
  # æ—§ç‰ˆæœ¬æ§åˆ¶å™¨å¯èƒ½ä½¿ç”¨ ConfigMap/Endpoints(å‘åå…¼å®¹)
  - apiGroups: [""]
    resources: ["configmaps", "endpoints"]
    verbs: ["get", "create", "update", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-controller-leader-election
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: my-controller-leader-election
subjects:
  - kind: ServiceAccount
    name: my-controller
    namespace: default

---
# æ§åˆ¶å™¨ä¸šåŠ¡é€»è¾‘çš„å…¶ä»– RBAC(ç¤ºä¾‹)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: my-controller
rules:
  - apiGroups: [""]
    resources: ["pods", "services"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "update", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: my-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: my-controller
subjects:
  - kind: ServiceAccount
    name: my-controller
    namespace: default
```

**éªŒè¯ Leader Election**:

```bash
# æŸ¥çœ‹ Lease çŠ¶æ€
kubectl get lease my-controller -n default -o yaml

# è¾“å‡ºç¤ºä¾‹:
# spec:
#   holderIdentity: "my-controller-7d8f9b5c6-abc123"  # å½“å‰ Leader
#   leaseDurationSeconds: 15
#   renewTime: "2026-02-10T10:05:30Z"
#   leaseTransitions: 1

# æŸ¥çœ‹æ§åˆ¶å™¨æ—¥å¿—
kubectl logs -n default -l app=my-controller --tail=20

# è¾“å‡ºç¤ºä¾‹:
# Pod my-controller-7d8f9b5c6-abc123: I am the leader!
# Pod my-controller-7d8f9b5c6-def456: Waiting for leader election...
# Pod my-controller-7d8f9b5c6-ghi789: Waiting for leader election...

# æ¨¡æ‹Ÿ Leader æ•…éšœ
kubectl delete pod my-controller-7d8f9b5c6-abc123 -n default

# è§‚å¯Ÿæ–° Leader äº§ç”Ÿ(é€šå¸¸ 10-15 ç§’å†…)
kubectl logs -n default -l app=my-controller --tail=20 -f
# è¾“å‡º:
# Pod my-controller-7d8f9b5c6-def456: Became the leader!
```

### 4.2 æ¡ˆä¾‹ 2: é€šè¿‡ Event è°ƒè¯• Pod å¯åŠ¨å¤±è´¥

**åœºæ™¯**: Pod ä¸€ç›´å¤„äº Pending çŠ¶æ€

```bash
# 1. æŸ¥çœ‹ Pod åŸºæœ¬ä¿¡æ¯
kubectl get pod my-app -n production
# è¾“å‡º:
# NAME     READY   STATUS    RESTARTS   AGE
# my-app   0/1     Pending   0          5m

# 2. æŸ¥çœ‹ Pod Events
kubectl describe pod my-app -n production | grep -A 10 "Events:"
# è¾“å‡º:
# Events:
#   Type     Reason            Age   From               Message
#   ----     ------            ----  ----               -------
#   Warning  FailedScheduling  3m    default-scheduler  0/5 nodes are available: 
#                                                         2 node(s) had taint {node-role.kubernetes.io/control-plane: }, that the pod didn't tolerate, 
#                                                         3 Insufficient cpu.

# 3. ä½¿ç”¨ kubectl get events æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
kubectl get events -n production --field-selector involvedObject.name=my-app --sort-by='.lastTimestamp'

# è¾“å‡º:
# LAST SEEN   TYPE      REASON             OBJECT      MESSAGE
# 5m          Warning   FailedScheduling   Pod/my-app  0/5 nodes are available: 3 Insufficient cpu.

# 4. åˆ†æé—®é¢˜
# é—®é¢˜: CPU èµ„æºä¸è¶³,3 ä¸ªå·¥ä½œèŠ‚ç‚¹çš„å¯ç”¨ CPU éƒ½ä¸æ»¡è¶³ Pod çš„è¯·æ±‚(spec.resources.requests.cpu)

# 5. è§£å†³æ–¹æ¡ˆ
# æ–¹æ¡ˆ 1: é™ä½ Pod çš„ CPU è¯·æ±‚
kubectl edit pod my-app -n production
# ä¿®æ”¹: resources.requests.cpu: 1000m â†’ 500m

# æ–¹æ¡ˆ 2: æ‰©å®¹é›†ç¾¤èŠ‚ç‚¹
# æˆ– æ–¹æ¡ˆ 3: é©±é€ä½ä¼˜å…ˆçº§ Pod
```

### 4.3 æ¡ˆä¾‹ 3: èŠ‚ç‚¹ç»´æŠ¤ - æ·»åŠ æ±¡ç‚¹é©±é€ Pod

**åœºæ™¯**: éœ€è¦ç»´æŠ¤ node2,å°†æ‰€æœ‰ Pod è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹

```bash
# 1. æ ‡è®°èŠ‚ç‚¹ä¸ºä¸å¯è°ƒåº¦(ç¦æ­¢æ–° Pod è°ƒåº¦)
kubectl cordon node2
# è¾“å‡º:
# node/node2 cordoned

# 2. æ·»åŠ  NoExecute æ±¡ç‚¹(é©±é€ç°æœ‰ Pod)
kubectl taint nodes node2 maintenance=true:NoExecute
# è¾“å‡º:
# node/node2 tainted

# 3. è§‚å¯Ÿ Pod è¿ç§»
kubectl get pods -A -o wide --field-selector spec.nodeName=node2 --watch
# è¾“å‡º:
# NAMESPACE   NAME      READY   STATUS        RESTARTS   NODE
# default     pod1      1/1     Terminating   0          node2
# default     pod2      1/1     Terminating   0          node2
# (ç­‰å¾… Pod ç»ˆæ­¢å¹¶åœ¨å…¶ä»–èŠ‚ç‚¹é‡å»º)

# 4. æ‰§è¡Œç»´æŠ¤æ“ä½œ
# (å¦‚å‡çº§å†…æ ¸ã€æ›´æ¢ç¡¬ä»¶ç­‰)

# 5. ç»´æŠ¤å®Œæˆå,ç§»é™¤æ±¡ç‚¹å¹¶æ¢å¤è°ƒåº¦
kubectl taint nodes node2 maintenance=true:NoExecute-  # ç§»é™¤æ±¡ç‚¹
kubectl uncordon node2                                  # æ¢å¤è°ƒåº¦
# è¾“å‡º:
# node/node2 untainted
# node/node2 uncordoned
```

**ä¼˜é›…é©±é€(Drain)**:

```bash
# ä½¿ç”¨ kubectl drain å‘½ä»¤(ç›¸å½“äº cordon + taint + ç­‰å¾… Pod ç»ˆæ­¢)
kubectl drain node2 --ignore-daemonsets --delete-emptydir-data
# å‚æ•°è¯´æ˜:
# --ignore-daemonsets: å¿½ç•¥ DaemonSet(æ— æ³•è¿ç§»)
# --delete-emptydir-data: åˆ é™¤ emptyDir å·æ•°æ®
# --grace-period=60: ä¼˜é›…ç»ˆæ­¢ç­‰å¾…æ—¶é—´(é»˜è®¤ 30 ç§’)
# --timeout=5m: æ€»è¶…æ—¶æ—¶é—´

# æ¢å¤è°ƒåº¦
kubectl uncordon node2
```

### 4.4 æ¡ˆä¾‹ 4: èŠ‚ç‚¹æ ‡ç­¾ç®¡ç† - æŒ‰ç¡¬ä»¶ç±»å‹è°ƒåº¦

**åœºæ™¯**: é›†ç¾¤æœ‰ GPU èŠ‚ç‚¹å’Œ CPU èŠ‚ç‚¹,AI è®­ç»ƒ Pod éœ€è¦è°ƒåº¦åˆ° GPU èŠ‚ç‚¹

```bash
# 1. ä¸º GPU èŠ‚ç‚¹æ·»åŠ æ ‡ç­¾
kubectl label nodes gpu-node1 accelerator=nvidia-tesla-v100
kubectl label nodes gpu-node2 accelerator=nvidia-tesla-v100

# 2. ä¸º CPU èŠ‚ç‚¹æ·»åŠ æ ‡ç­¾
kubectl label nodes cpu-node1 accelerator=none
kubectl label nodes cpu-node2 accelerator=none

# 3. æŸ¥çœ‹èŠ‚ç‚¹æ ‡ç­¾
kubectl get nodes --show-labels | grep accelerator

# 4. AI è®­ç»ƒ Pod ä½¿ç”¨ nodeSelector
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: ai-training
spec:
  nodeSelector:
    accelerator: nvidia-tesla-v100  # ä»…è°ƒåº¦åˆ° GPU èŠ‚ç‚¹
  containers:
    - name: training
      image: nvidia/cuda:12.0-runtime
      resources:
        limits:
          nvidia.com/gpu: 1  # è¯·æ±‚ 1 ä¸ª GPU
EOF

# 5. éªŒè¯è°ƒåº¦ç»“æœ
kubectl get pod ai-training -o wide
# è¾“å‡º:
# NAME          READY   STATUS    NODE
# ai-training   1/1     Running   gpu-node1
```

**ä½¿ç”¨ Node Affinity(æ›´çµæ´»)**:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ai-training-advanced
spec:
  affinity:
    nodeAffinity:
      # å¿…é¡»æ»¡è¶³çš„æ¡ä»¶(ç¡¬äº²å’Œæ€§)
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              # å¿…é¡»æ˜¯ GPU èŠ‚ç‚¹
              - key: accelerator
                operator: In
                values:
                  - nvidia-tesla-v100
                  - nvidia-tesla-a100
      # ä¼˜å…ˆæ»¡è¶³çš„æ¡ä»¶(è½¯äº²å’Œæ€§)
      preferredDuringSchedulingIgnoredDuringExecution:
        # ä¼˜å…ˆé€‰æ‹© A100(æƒé‡ 100)
        - weight: 100
          preference:
            matchExpressions:
              - key: accelerator
                operator: In
                values:
                  - nvidia-tesla-a100
        # å…¶æ¬¡é€‰æ‹© us-west åŒºåŸŸ(æƒé‡ 50)
        - weight: 50
          preference:
            matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                  - us-west-1a
  containers:
    - name: training
      image: nvidia/cuda:12.0-runtime
```

### 4.5 æ¡ˆä¾‹ 5: ç›‘æ§èŠ‚ç‚¹å¿ƒè·³ Lease

**åœºæ™¯**: ç›‘æ§èŠ‚ç‚¹æ˜¯å¦åœ¨çº¿(é€šè¿‡ Lease æ¯” Node Status æ›´å®æ—¶)

```bash
# æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹çš„ Lease
kubectl get leases -n kube-node-lease

# è¾“å‡º:
# NAME    HOLDER   AGE
# node1   node1    30d
# node2   node2    30d
# node3   node3    30d

# æŸ¥çœ‹ç‰¹å®šèŠ‚ç‚¹çš„ Lease è¯¦æƒ…
kubectl get lease node1 -n kube-node-lease -o yaml

# å…³é”®å­—æ®µ:
# spec:
#   renewTime: "2026-02-10T10:05:30Z"  # æœ€åå¿ƒè·³æ—¶é—´
#   leaseDurationSeconds: 40            # å¿ƒè·³æœ‰æ•ˆæœŸ

# è®¡ç®—èŠ‚ç‚¹æ˜¯å¦åœ¨çº¿:
# å½“å‰æ—¶é—´ - renewTime < leaseDurationSeconds â†’ åœ¨çº¿
# å½“å‰æ—¶é—´ - renewTime >= leaseDurationSeconds â†’ ç¦»çº¿(NotReady)
```

**Prometheus ç›‘æ§æŸ¥è¯¢(å‡è®¾æœ‰ kube-state-metrics)**:

```promql
# èŠ‚ç‚¹æœ€åå¿ƒè·³æ—¶é—´(Unix æ—¶é—´æˆ³)
kube_lease_renew_time{namespace="kube-node-lease"}

# èŠ‚ç‚¹ç¦»çº¿æ—¶é—´(ç§’)
time() - kube_lease_renew_time{namespace="kube-node-lease"}

# èŠ‚ç‚¹ç¦»çº¿å‘Šè­¦(è¶…è¿‡ 60 ç§’æœªå¿ƒè·³)
(time() - kube_lease_renew_time{namespace="kube-node-lease"}) > 60
```

---

## 5. æ•…éšœæ’æŸ¥

### 5.1 Leader Election é¢‘ç¹åˆ‡æ¢

**ç—‡çŠ¶**: Lease.spec.leaseTransitions ä¸æ–­å¢åŠ ,æ§åˆ¶å™¨æ—¥å¿—é¢‘ç¹å‡ºç° "lost leadership"

```bash
# æŸ¥çœ‹ Lease åˆ‡æ¢æ¬¡æ•°
kubectl get lease my-controller -n default -o jsonpath='{.spec.leaseTransitions}'
# è¾“å‡º: 50 (å¼‚å¸¸é«˜,æ­£å¸¸åº”è¯¥ < 5)

# å¯èƒ½åŸå› :
# 1. Leader Pod é¢‘ç¹é‡å¯
kubectl get pods -n default -l app=my-controller
# æ£€æŸ¥ RESTARTS åˆ—

# 2. ç½‘ç»œé—®é¢˜å¯¼è‡´ç»­çº¦å¤±è´¥
kubectl logs -n default -l app=my-controller | grep -i "lost leadership\|network"

# 3. Lease é…ç½®ä¸åˆç†(ç»­çº¦é—´éš”è¿‡çŸ­)
kubectl get lease my-controller -n default -o jsonpath='{.spec.leaseDurationSeconds}'
# æ¨èé…ç½®: leaseDuration=15s, renewDeadline=10s, retryPeriod=2s

# è§£å†³æ–¹æ¡ˆ:
# 1. å¢åŠ  Lease æœ‰æ•ˆæœŸ
# 2. æ£€æŸ¥ Pod å¥åº·æ£€æŸ¥é…ç½®
# 3. æ£€æŸ¥ API Server è´Ÿè½½(æ˜¯å¦é™æµ)
```

### 5.2 èŠ‚ç‚¹ NotReady ä½† Lease æ­£å¸¸

**ç—‡çŠ¶**: Node Condition ä¸º NotReady,ä½† Lease ä»åœ¨æ›´æ–°

```bash
# æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes
# è¾“å‡º:
# NAME    STATUS     ROLES    AGE   VERSION
# node1   NotReady   <none>   30d   v1.32.0

# æŸ¥çœ‹ Lease(å‘ç°ä»åœ¨æ›´æ–°)
kubectl get lease node1 -n kube-node-lease -o yaml
# spec:
#   renewTime: "2026-02-10T10:06:00Z"  # ä»åœ¨æ›´æ–°!

# å¯èƒ½åŸå› :
# Kubelet ä»åœ¨è¿è¡Œ(èƒ½ç»­çº¦ Lease),ä½†èŠ‚ç‚¹å…¶ä»–ç»„ä»¶å¼‚å¸¸(å¦‚ CNIã€å®¹å™¨è¿è¡Œæ—¶)

# æ’æŸ¥æ­¥éª¤:
# 1. æŸ¥çœ‹èŠ‚ç‚¹ Conditions
kubectl describe node node1 | grep -A 5 "Conditions:"
# è¾“å‡º:
#   Ready            False   ... KubeletNotReady  container runtime network not ready

# 2. æ£€æŸ¥å®¹å™¨è¿è¡Œæ—¶
ssh node1
systemctl status containerd  # æˆ– docker

# 3. æ£€æŸ¥ CNI
kubectl logs -n kube-system -l app=calico-node --field-selector spec.nodeName=node1

# 4. æ£€æŸ¥ Kubelet æ—¥å¿—
journalctl -u kubelet -n 100
```

### 5.3 Event è¿‡å¤šå¯¼è‡´ etcd å‹åŠ›

**ç—‡çŠ¶**: etcd å­˜å‚¨ç©ºé—´å¢é•¿è¿‡å¿«,å¤§é‡ Event å¯¹è±¡

```bash
# æŸ¥çœ‹ Event æ•°é‡
kubectl get events --all-namespaces | wc -l
# è¾“å‡º: 10000 (è¿‡å¤š!)

# æŸ¥çœ‹ etcd å¤§å°
kubectl exec -n kube-system etcd-xxx -- etcdctl endpoint status --write-out=table

# è§£å†³æ–¹æ¡ˆ:
# 1. å‡å°‘ Event TTL(é»˜è®¤ 1 å°æ—¶)
# åœ¨ kube-apiserver å¯åŠ¨å‚æ•°ä¸­æ·»åŠ :
--event-ttl=10m  # ç¼©çŸ­ä¸º 10 åˆ†é’Ÿ

# 2. é™åˆ¶ Event é€Ÿç‡(v1.27+)
# åˆ›å»º EventRateLimit Admission é…ç½®:
apiVersion: eventratelimit.admission.k8s.io/v1alpha1
kind: Configuration
limits:
  - type: Namespace
    qps: 50
    burst: 100
    cacheSize: 2000

# 3. æ¸…ç†å†å² Event(ä¸´æ—¶æ–¹æ¡ˆ)
kubectl delete events --all-namespaces --field-selector reason=FailedScheduling,type=Warning
```

### 5.4 Pod è°ƒåº¦å¤±è´¥ - æ— å¯ç”¨èŠ‚ç‚¹

**ç—‡çŠ¶**: Pod Pending,Event æ˜¾ç¤º "0/5 nodes are available"

```bash
# æŸ¥çœ‹ Event è¯¦ç»†ä¿¡æ¯
kubectl describe pod my-pod -n default | grep -A 10 "Events:"
# è¾“å‡º:
# Events:
#   Type     Reason            Message
#   ----     ------            -------
#   Warning  FailedScheduling  0/5 nodes are available:
#                                2 node(s) had taint {node-role.kubernetes.io/control-plane: }, that the pod didn't tolerate,
#                                3 Insufficient memory.

# åˆ†æ:
# 1. 2 ä¸ªèŠ‚ç‚¹æ˜¯æ§åˆ¶å¹³é¢(æœ‰æ±¡ç‚¹,Pod æ— å®¹å¿)
# 2. 3 ä¸ªå·¥ä½œèŠ‚ç‚¹å†…å­˜ä¸è¶³

# è§£å†³æ–¹æ¡ˆ:
# æ–¹æ¡ˆ 1: æ·»åŠ å®¹å¿(å…è®¸è°ƒåº¦åˆ°æ§åˆ¶å¹³é¢,ä¸æ¨è)
spec:
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      effect: NoSchedule

# æ–¹æ¡ˆ 2: é™ä½å†…å­˜è¯·æ±‚
spec:
  resources:
    requests:
      memory: 2Gi  # é™ä½åˆ°åˆç†å€¼

# æ–¹æ¡ˆ 3: æ‰©å®¹é›†ç¾¤èŠ‚ç‚¹
```

### 5.5 è°ƒè¯•æŠ€å·§

```bash
# 1. å®æ—¶ç›‘å¬ Event
kubectl get events --all-namespaces --watch

# 2. è¿‡æ»¤ç‰¹å®šç±»å‹ Event
kubectl get events --all-namespaces --field-selector type=Warning

# 3. æŒ‰æ—¶é—´æ’åº Event
kubectl get events --all-namespaces --sort-by='.lastTimestamp'

# 4. æŸ¥çœ‹ç‰¹å®šå¯¹è±¡çš„ Event
kubectl get events --field-selector involvedObject.name=my-pod,involvedObject.namespace=default

# 5. ç›‘æ§ Lease å˜åŒ–
kubectl get leases -n kube-node-lease --watch

# 6. æŸ¥çœ‹èŠ‚ç‚¹èµ„æºåˆ†é…æƒ…å†µ
kubectl describe nodes | grep -A 5 "Allocated resources:"

# 7. æ¨¡æ‹Ÿè°ƒåº¦(æ£€æŸ¥ Pod ä¸ºä½•æ— æ³•è°ƒåº¦)
kubectl get pod my-pod -o yaml | kubectl apply --dry-run=server -f -
```

---

## ğŸ“š å‚è€ƒèµ„æº

- **å®˜æ–¹æ–‡æ¡£**:
  - [Lease API Reference](https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/)
  - [Event API Reference](https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/)
  - [Node API Reference](https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/node-v1/)
  - [Leader Election](https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/)
- **client-go Leader Election**: https://pkg.go.dev/k8s.io/client-go/tools/leaderelection

---

**æœ€ä½³å®è·µæ€»ç»“**:

### Lease æœ€ä½³å®è·µ:

1. **Leader Election é…ç½®**: ä½¿ç”¨æ¨èå€¼ `leaseDuration=15s, renewDeadline=10s, retryPeriod=2s`
2. **ç›‘æ§åˆ‡æ¢æ¬¡æ•°**: ç›‘æ§ `leaseTransitions`,å¼‚å¸¸å¢é•¿è¯´æ˜ç¨³å®šæ€§é—®é¢˜
3. **é¿å…å¤šä¸ª Lease**: æ¯ä¸ªæ§åˆ¶å™¨ä½¿ç”¨å”¯ä¸€çš„ Lease åç§°,é¿å…å†²çª
4. **å‘½åè§„èŒƒ**: Lease åç§°ä¸æ§åˆ¶å™¨åç§°ä¸€è‡´,ä¾¿äºæ’æŸ¥

### Event æœ€ä½³å®è·µ:

1. **ä½¿ç”¨ events.k8s.io/v1**: ä¼˜å…ˆä½¿ç”¨æ–°ç‰ˆæœ¬ Event API(v1.19+)
2. **æ§åˆ¶ Event æ•°é‡**: é…ç½®åˆç†çš„ `--event-ttl` å’Œ EventRateLimit
3. **ç»“æ„åŒ–æ—¥å¿—**: å…³é”®æ“ä½œåŒæ—¶è®°å½•æ—¥å¿—å’Œ Event,ä¾¿äºå®¡è®¡
4. **é¿å…é«˜é¢‘ Event**: ä½¿ç”¨ Event Series æœºåˆ¶èšåˆé‡å¤äº‹ä»¶

### Node æœ€ä½³å®è·µ:

1. **æ ‡ç­¾è§„èŒƒåŒ–**: ä½¿ç”¨æ ‡å‡†æ ‡ç­¾ `topology.kubernetes.io/zone`, `node.kubernetes.io/instance-type` ç­‰
2. **æ±¡ç‚¹ç®¡ç†**: ç»´æŠ¤æ—¶ä½¿ç”¨ `kubectl drain`,é¿å…æ‰‹åŠ¨åˆ é™¤ Pod
3. **èµ„æºé¢„ç•™**: é…ç½® `--system-reserved` å’Œ `--kube-reserved`,é¿å…èŠ‚ç‚¹èµ„æºè€—å°½
4. **ç›‘æ§ Lease**: é€šè¿‡ Lease ç›‘æ§èŠ‚ç‚¹å¿ƒè·³,æ¯” Node Status æ›´å®æ—¶

---

ğŸš€ **Leaseã€Eventã€Node æ˜¯ Kubernetes é›†ç¾¤è¿ç»´çš„åŸºç¡€ç»„ä»¶,æŒæ¡å®ƒä»¬æ˜¯é›†ç¾¤ç¨³å®šè¿è¡Œçš„å…³é”®!**

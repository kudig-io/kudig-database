# AI/ML å·¥ä½œè´Ÿè½½æ•…éšœæ’æŸ¥æŒ‡å—

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **æ–‡æ¡£ç±»å‹**: AIåŸºç¡€è®¾æ–½è¿ç»´ä¿éšœ

## 0. 10 åˆ†é’Ÿå¿«é€Ÿè¯Šæ–­

1. **GPU å¯è§æ€§**ï¼š`kubectl get nodes -o jsonpath='{.items[*].status.allocatable.nvidia\.com/gpu}'`ï¼Œç¡®è®¤èµ„æºæš´éœ²ã€‚
2. **è®¾å¤‡æ’ä»¶**ï¼šæ£€æŸ¥ Device Plugin DaemonSet çŠ¶æ€ä¸æ—¥å¿—ã€‚
3. **è®­ç»ƒä½œä¸š**ï¼šæŸ¥çœ‹åˆ†å¸ƒå¼è®­ç»ƒ Pod äº‹ä»¶ï¼Œå…³æ³¨ NCCL/ç½‘ç»œæŠ¥é”™ã€‚
4. **æ•°æ®ä¸å­˜å‚¨**ï¼šç¡®è®¤æ•°æ®é›† PVC æŒ‚è½½ã€I/O ååä¸çƒ­ç‚¹ã€‚
5. **èµ„æºè¯·æ±‚**ï¼šæ ¸å¯¹ GPU/CPU/å†…å­˜ requests/limitsï¼Œé¿å…ç¢ç‰‡åŒ–ã€‚
6. **å¿«é€Ÿç¼“è§£**ï¼š
   - é™ä½ batch size æˆ–å¯ç”¨æ··åˆç²¾åº¦ã€‚
   - è°ƒæ•´äº²å’Œæ€§/æ‹“æ‰‘ï¼Œè®©è®­ç»ƒ Pod åŒæœºæˆ¿/åŒäº¤æ¢æœºã€‚
7. **è¯æ®ç•™å­˜**ï¼šä¿å­˜è®­ç»ƒæ—¥å¿—ã€GPU æŒ‡æ ‡ã€Pod äº‹ä»¶ä¸æ‹“æ‰‘ä¿¡æ¯ã€‚

## ğŸ¤– AI/ML å·¥ä½œè´Ÿè½½å¸¸è§é—®é¢˜ä¸å½±å“åˆ†æ

### AI/ML ç‰¹æœ‰æ•…éšœç°è±¡

| é—®é¢˜ç±»å‹ | å…¸å‹ç°è±¡ | å½±å“ç¨‹åº¦ | ç´§æ€¥çº§åˆ« |
|---------|---------|---------|---------|
| GPU èµ„æºè°ƒåº¦å¤±è´¥ | `0/5 nodes are available: 5 Insufficient nvidia.com/gpu` | â­â­â­ é«˜ | P0 |
| åˆ†å¸ƒå¼è®­ç»ƒé€šä¿¡å¤±è´¥ | `NCCL error: unhandled cuda error` | â­â­â­ é«˜ | P0 |
| æ¨¡å‹æœåŠ¡æ¨ç†è¶…æ—¶ | `model inference timeout after 30s` | â­â­ ä¸­ | P1 |
| æ•°æ®é›†åŠ è½½æ€§èƒ½é—®é¢˜ | `dataset loading took 30+ minutes` | â­â­ ä¸­ | P1 |
| GPU å†…å­˜ä¸è¶³å´©æºƒ | `CUDA out of memory` | â­â­â­ é«˜ | P0 |
| æ¨¡å‹ç‰ˆæœ¬ç®¡ç†æ··ä¹± | `serving model version mismatch` | â­â­ ä¸­ | P1 |
| è®­ç»ƒä»»åŠ¡èµ„æºæµªè´¹ | `GPU utilization < 20%` | â­â­ ä¸­ | P1 |
| æˆæœ¬æ§åˆ¶å¤±æ•ˆ | `unexpected GPU billing spike` | â­â­â­ é«˜ | P0 |

### AI/ML å·¥ä½œè´Ÿè½½çŠ¶æ€æ£€æŸ¥

```bash
# GPU èµ„æºçŠ¶æ€æ£€æŸ¥
echo "=== GPU èµ„æºçŠ¶æ€æ£€æŸ¥ ==="
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.capacity.nvidia\.com/gpu}{"\n"}{end}'

# NVIDIA Device Plugin çŠ¶æ€
echo "=== NVIDIA Device Plugin çŠ¶æ€ ==="
kubectl get pods -n kube-system -l app=nvidia-device-plugin-daemonset

# GPU åˆ©ç”¨ç‡ç›‘æ§
echo "=== GPU åˆ©ç”¨ç‡æ£€æŸ¥ ==="
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.allocatable.nvidia\.com/gpu}{" allocated\n"}{end}'

# åˆ†å¸ƒå¼è®­ç»ƒä½œä¸šçŠ¶æ€
echo "=== åˆ†å¸ƒå¼è®­ç»ƒä½œä¸šçŠ¶æ€ ==="
kubectl get jobs -l app=distributed-training --all-namespaces
kubectl get pods -l app=distributed-training --all-namespaces -o wide

# æ¨¡å‹æœåŠ¡çŠ¶æ€
echo "=== æ¨¡å‹æœåŠ¡çŠ¶æ€ ==="
kubectl get services -l app=model-serving --all-namespaces
kubectl get deployments -l app=model-serving --all-namespaces
```

## ğŸ” AI/ML å·¥ä½œè´Ÿè½½é—®é¢˜è¯Šæ–­æ–¹æ³•

### è¯Šæ–­åŸç†è¯´æ˜

AI/ML å·¥ä½œè´Ÿè½½æ•…éšœè¯Šæ–­éœ€è¦è€ƒè™‘ä»¥ä¸‹ç‰¹æ®Šå› ç´ ï¼š

1. **ç¡¬ä»¶åŠ é€Ÿå±‚é¢**ï¼šGPU/TPU èµ„æºç®¡ç†ã€é©±åŠ¨ç¨‹åºå…¼å®¹æ€§
2. **åˆ†å¸ƒå¼è®¡ç®—å±‚é¢**ï¼šèŠ‚ç‚¹é—´é€šä¿¡ã€æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œ
3. **æ¡†æ¶ç‰¹å¼‚æ€§**ï¼šTensorFlowã€PyTorchã€MXNet ç­‰æ¡†æ¶å·®å¼‚
4. **æ•°æ®ç®¡é“å±‚é¢**ï¼šæ•°æ®åŠ è½½ã€é¢„å¤„ç†ã€ç¼“å­˜æœºåˆ¶
5. **æ€§èƒ½ä¼˜åŒ–å±‚é¢**ï¼šæ‰¹å¤„ç†å¤§å°ã€æ··åˆç²¾åº¦ã€å†…å­˜ä¼˜åŒ–

### AI/ML é—®é¢˜è¯Šæ–­å†³ç­–æ ‘

```
AI/ML å·¥ä½œè´Ÿè½½æ•…éšœ
    â”œâ”€â”€ GPU èµ„æºé—®é¢˜
    â”‚   â”œâ”€â”€ è®¾å¤‡æ’ä»¶çŠ¶æ€
    â”‚   â”œâ”€â”€ GPU é©±åŠ¨å…¼å®¹æ€§
    â”‚   â”œâ”€â”€ èµ„æºè¯·æ±‚é…ç½®
    â”‚   â””â”€â”€ GPU å†…å­˜åˆ†é…
    â”œâ”€â”€ åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜
    â”‚   â”œâ”€â”€ NCCL/RDMA é…ç½®
    â”‚   â”œâ”€â”€ ç½‘ç»œç­–ç•¥é™åˆ¶
    â”‚   â”œâ”€â”€ èŠ‚ç‚¹äº²å’Œæ€§é…ç½®
    â”‚   â””â”€â”€ é€šä¿¡è¶…æ—¶è®¾ç½®
    â”œâ”€â”€ æ¨¡å‹æœåŠ¡é—®é¢˜
    â”‚   â”œâ”€â”€ æ¨ç†æ€§èƒ½ç“¶é¢ˆ
    â”‚   â”œâ”€â”€ æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
    â”‚   â”œâ”€â”€ æ‰¹å¤„ç†é…ç½®
    â”‚   â””â”€â”€ è‡ªåŠ¨æ‰©ç¼©å®¹è®¾ç½®
    â””â”€â”€ æ•°æ®å¤„ç†é—®é¢˜
        â”œâ”€â”€ æ•°æ®åŠ è½½æ€§èƒ½
        â”œâ”€â”€ å­˜å‚¨ I/O ç“¶é¢ˆ
        â”œâ”€â”€ ç¼“å­˜ç­–ç•¥é…ç½®
        â””â”€â”€ æ•°æ®é¢„å¤„ç†æ•ˆç‡
```

### è¯¦ç»†è¯Šæ–­å‘½ä»¤

#### 1. GPU èµ„æºè¯Šæ–­

```bash
#!/bin/bash
# GPU èµ„æºè¯Šæ–­è„šæœ¬

echo "=== GPU èµ„æºè¯Šæ–­ ==="

# 1. æ£€æŸ¥ GPU ç¡¬ä»¶çŠ¶æ€
echo "1. GPU ç¡¬ä»¶çŠ¶æ€æ£€æŸ¥:"
nvidia-smi --query-gpu=name,memory.total,memory.used,utilization.gpu --format=csv

# 2. æ£€æŸ¥ NVIDIA Device Plugin
echo "2. NVIDIA Device Plugin çŠ¶æ€:"
kubectl get pods -n kube-system -l app=nvidia-device-plugin-daemonset -o wide

# æ£€æŸ¥ Device Plugin æ—¥å¿—
echo "Device Plugin æ—¥å¿—æ‘˜è¦:"
kubectl logs -n kube-system -l app=nvidia-device-plugin-daemonset --tail=50 | grep -i error

# 3. æ£€æŸ¥ GPU èµ„æºæš´éœ²æƒ…å†µ
echo "3. GPU èµ„æºåœ¨èŠ‚ç‚¹ä¸Šçš„æš´éœ²æƒ…å†µ:"
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): \(.status.capacity["nvidia.com/gpu"] // "0") GPUs"'

# 4. æ£€æŸ¥ GPU èµ„æºåˆ†é…æƒ…å†µ
echo "4. GPU èµ„æºåˆ†é…æƒ…å†µ:"
kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.containers[].resources.requests["nvidia.com/gpu"] != null) | "\(.metadata.namespace)/\(.metadata.name): \(.spec.containers[].resources.requests["nvidia.com/gpu"]) GPUs"'

# 5. GPU å†…å­˜ä½¿ç”¨æ£€æŸ¥
echo "5. GPU å†…å­˜ä½¿ç”¨æ£€æŸ¥:"
for node in $(kubectl get nodes -o name | cut -d/ -f2); do
  echo "èŠ‚ç‚¹ $node:"
  kubectl debug node/$node -it --image=ubuntu:20.04 -- chroot /host nvidia-smi --query-gpu=memory.used,memory.free --format=csv,noheader,nounits 2>/dev/null || echo "  æ— æ³•è®¿é—®èŠ‚ç‚¹ GPU ä¿¡æ¯"
done
```

#### 2. åˆ†å¸ƒå¼è®­ç»ƒè¯Šæ–­

```bash
#!/bin/bash
# åˆ†å¸ƒå¼è®­ç»ƒè¯Šæ–­è„šæœ¬

echo "=== åˆ†å¸ƒå¼è®­ç»ƒè¯Šæ–­ ==="

# 1. æ£€æŸ¥è®­ç»ƒä½œä¸šçŠ¶æ€
echo "1. åˆ†å¸ƒå¼è®­ç»ƒä½œä¸šçŠ¶æ€:"
kubectl get jobs -l training=distributed --all-namespaces
kubectl get pods -l training=distributed --all-namespaces --field-selector=status.phase!=Running

# 2. æ£€æŸ¥ NCCL é…ç½®
echo "2. NCCL ç¯å¢ƒå˜é‡æ£€æŸ¥:"
kubectl get pods -l training=distributed --all-namespaces -o json | jq -r '.items[].spec.containers[].env[] | select(.name | startswith("NCCL")) | "\(.name)=\(.value)"' | sort | uniq

# 3. æ£€æŸ¥ç½‘ç»œç­–ç•¥
echo "3. ç½‘ç»œç­–ç•¥å¯¹åˆ†å¸ƒå¼è®­ç»ƒçš„å½±å“:"
kubectl get networkpolicies --all-namespaces | grep -E "(distributed|training|nccl)" || echo "æœªæ‰¾åˆ°ç›¸å…³çš„ç½‘ç»œç­–ç•¥"

# 4. æ£€æŸ¥èŠ‚ç‚¹äº²å’Œæ€§å’Œæ‹“æ‰‘
echo "4. èŠ‚ç‚¹äº²å’Œæ€§é…ç½®:"
kubectl get pods -l training=distributed --all-namespaces -o json | jq -r '.items[] | "\(.metadata.name): \(.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[].matchExpressions[].values[])"'

# 5. æ£€æŸ¥ RDMA/é«˜æ€§èƒ½ç½‘ç»œé…ç½®
echo "5. é«˜æ€§èƒ½ç½‘ç»œé…ç½®æ£€æŸ¥:"
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): \(.metadata.labels["feature.node.kubernetes.io/network-sriov.capable"] // "unknown") SR-IOV, \(.metadata.labels["feature.node.kubernetes.io/network-rdma.available"] // "unknown") RDMA"'

# 6. åˆ†å¸ƒå¼è®­ç»ƒæ—¥å¿—åˆ†æ
echo "6. åˆ†å¸ƒå¼è®­ç»ƒé”™è¯¯æ—¥å¿—åˆ†æ:"
for pod in $(kubectl get pods -l training=distributed --all-namespaces -o name); do
  echo "æ£€æŸ¥ $pod:"
  kubectl logs $pod --tail=100 2>/dev/null | grep -i -E "(error|exception|nccl|timeout|connection)" | head -5
done
```

#### 3. æ¨¡å‹æœåŠ¡è¯Šæ–­

```bash
#!/bin/bash
# æ¨¡å‹æœåŠ¡è¯Šæ–­è„šæœ¬

echo "=== æ¨¡å‹æœåŠ¡è¯Šæ–­ ==="

# 1. æ¨¡å‹æœåŠ¡éƒ¨ç½²çŠ¶æ€
echo "1. æ¨¡å‹æœåŠ¡éƒ¨ç½²çŠ¶æ€:"
kubectl get deployments -l app=model-serving --all-namespaces
kubectl get services -l app=model-serving --all-namespaces

# 2. æ¨¡å‹æ¨ç†æ€§èƒ½æ£€æŸ¥
echo "2. æ¨¡å‹æ¨ç†æ€§èƒ½æ£€æŸ¥:"
for svc in $(kubectl get services -l app=model-serving --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}/{.metadata.name}{"\n"}{end}'); do
  namespace=$(echo $svc | cut -d/ -f1)
  service=$(echo $svc | cut -d/ -f2)
  echo "æµ‹è¯•æœåŠ¡ $namespace/$service:"
  
  # ç®€å•çš„å¥åº·æ£€æŸ¥
  kubectl get service $service -n $namespace -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}' | xargs -I {} timeout 10 curl -s http://{}/health 2>/dev/null && echo "  âœ“ å¥åº·æ£€æŸ¥é€šè¿‡" || echo "  âœ— å¥åº·æ£€æŸ¥å¤±è´¥"
done

# 3. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†æ£€æŸ¥
echo "3. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†æ£€æŸ¥:"
kubectl get configmaps -l model-version --all-namespaces -o json | jq -r '.items[] | "\(.metadata.namespace)/\(.metadata.name): \(.data.version // "unspecified")"'

# 4. æ¨ç†èµ„æºä½¿ç”¨æƒ…å†µ
echo "4. æ¨ç†èµ„æºä½¿ç”¨æƒ…å†µ:"
kubectl top pods -l app=model-serving --all-namespaces

# 5. æ¨¡å‹åŠ è½½æ—¶é—´æ£€æŸ¥
echo "5. æ¨¡å‹åŠ è½½æ—¶é—´æ£€æŸ¥:"
for pod in $(kubectl get pods -l app=model-serving --all-namespaces -o name); do
  echo "æ£€æŸ¥ $pod æ¨¡å‹åŠ è½½æ—¶é—´:"
  kubectl logs $pod --tail=200 2>/dev/null | grep -i "model loaded\|loading model\|loaded in" | tail -3
done

# 6. æ‰¹å¤„ç†é…ç½®æ£€æŸ¥
echo "6. æ‰¹å¤„ç†é…ç½®æ£€æŸ¥:"
kubectl get deployments -l app=model-serving --all-namespaces -o json | jq -r '.items[] | "\(.metadata.namespace)/\(.metadata.name): batch_size=\(.spec.template.spec.containers[0].env[] | select(.name=="BATCH_SIZE") .value // "default")"'
```

#### 4. æ•°æ®å¤„ç†è¯Šæ–­

```bash
#!/bin/bash
# æ•°æ®å¤„ç†è¯Šæ–­è„šæœ¬

echo "=== æ•°æ®å¤„ç†è¯Šæ–­ ==="

# 1. å­˜å‚¨æ€§èƒ½æ£€æŸ¥
echo "1. å­˜å‚¨æ€§èƒ½æ£€æŸ¥:"
kubectl get pv -o json | jq -r '.items[] | select(.spec.csi.driver=="pd.csi.storage.gke.io" or .spec.csi.driver=="diskplugin.csi.alibabacloud.com") | "\(.metadata.name): \(.spec.csi.volumeHandle)"'

# æ£€æŸ¥å­˜å‚¨ç±»æ€§èƒ½
kubectl get storageclasses -o json | jq -r '.items[] | "\(.metadata.name): \(.parameters.type // .parameters.diskType // "standard")"'

# 2. æ•°æ®åŠ è½½æ€§èƒ½æ£€æŸ¥
echo "2. æ•°æ®åŠ è½½æ€§èƒ½æ£€æŸ¥:"
for pod in $(kubectl get pods -l data-processing=active --all-namespaces -o name); do
  echo "æ£€æŸ¥ $pod æ•°æ®åŠ è½½æ€§èƒ½:"
  kubectl logs $pod --tail=100 2>/dev/null | grep -i -E "(data loading|dataset|prefetch|cache)" | tail -5
done

# 3. ç¼“å­˜é…ç½®æ£€æŸ¥
echo "3. ç¼“å­˜é…ç½®æ£€æŸ¥:"
kubectl get pods -l data-processing=active --all-namespaces -o json | jq -r '.items[].spec.containers[].env[] | select(.name | contains("CACHE") or contains("BUFFER")) | "\(.name)=\(.value)"' | sort | uniq

# 4. å­˜å‚¨ I/O ç›‘æ§
echo "4. å­˜å‚¨ I/O ç›‘æ§:"
kubectl get pods -l data-processing=active --all-namespaces -o json | jq -r '.items[] | "\(.metadata.namespace)/\(.metadata.name): \(.spec.volumes[]?.persistentVolumeClaim.claimName // "no PVC")"'

# 5. æ•°æ®é¢„å¤„ç†æ•ˆç‡æ£€æŸ¥
echo "5. æ•°æ®é¢„å¤„ç†æ•ˆç‡æ£€æŸ¥:"
for job in $(kubectl get jobs -l data-preprocessing=active --all-namespaces -o name); do
  echo "æ£€æŸ¥ä½œä¸š $job:"
  kubectl describe $job | grep -E "(Active|Succeeded|Failed)"
done
```

## ğŸ”§ AI/ML å·¥ä½œè´Ÿè½½é—®é¢˜è§£å†³æ–¹æ¡ˆ

### GPU èµ„æºé—®é¢˜è§£å†³

#### æ–¹æ¡ˆä¸€ï¼šNVIDIA Device Plugin é…ç½®ä¼˜åŒ–

```yaml
# ä¼˜åŒ–çš„ NVIDIA Device Plugin é…ç½®
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  template:
    metadata:
      labels:
        name: nvidia-device-plugin-ds
    spec:
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      containers:
      - image: nvcr.io/nvidia/k8s-device-plugin:v0.14.0
        name: nvidia-device-plugin-ctr
        args: 
        - "--fail-on-init-error=false"
        - "--device-discovery-strategy=auto"
        - "--device-list-strategy=envvar"
        - "--pass-device-specs=true"
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
        - name: device-plugin
          mountPath: /var/lib/kubelet/device-plugins
      volumes:
      - name: device-plugin
        hostPath:
          path: /var/lib/kubelet/device-plugins
```

#### æ–¹æ¡ˆäºŒï¼šGPU èµ„æºè¯·æ±‚ä¼˜åŒ–

```yaml
# ä¼˜åŒ–çš„ GPU å·¥ä½œè´Ÿè½½é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-training-job
spec:
  template:
    spec:
      containers:
      - name: training-container
        image: nvidia/cuda:12.0-devel-ubuntu20.04
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: "16Gi"
            cpu: "8"
          requests:
            nvidia.com/gpu: "1"
            memory: "8Gi"
            cpu: "4"
        env:
        # GPU å†…å­˜ä¼˜åŒ–
        - name: TF_FORCE_GPU_ALLOW_GROWTH
          value: "true"
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:128"
        # æ··åˆç²¾åº¦è®­ç»ƒ
        - name: NVIDIA_TF32_OVERRIDE
          value: "0"
        - name: TORCH_CUDNN_V8_API_ENABLED
          value: "1"
        volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
          readOnly: true
      volumes:
      - name: nvidia-install-dir-host
        hostPath:
          path: /home/kubernetes/bin/nvidia
          type: Directory
```

### åˆ†å¸ƒå¼è®­ç»ƒé—®é¢˜è§£å†³

#### æ–¹æ¡ˆä¸€ï¼šNCCL ä¼˜åŒ–é…ç½®

```yaml
# åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–é…ç½®
apiVersion: batch/v1
kind: Job
metadata:
  name: distributed-training
spec:
  parallelism: 4
  completions: 4
  template:
    spec:
      containers:
      - name: trainer
        image: pytorch/pytorch:2.0-cuda11.7-cudnn8-runtime
        env:
        # NCCL ä¼˜åŒ–é…ç½®
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_DISABLE
          value: "0"
        - name: NCCL_IB_CUDA_SUPPORT
          value: "1"
        - name: NCCL_NET_GDR_LEVEL
          value: "2"
        - name: NCCL_BUFFSIZE
          value: "8388608"
        - name: NCCL_NSOCKS_PERTHREAD
          value: "4"
        - name: NCCL_SOCKET_NTHREADS
          value: "2"
        # åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
        - name: WORLD_SIZE
          value: "4"
        - name: MASTER_ADDR
          value: "distributed-training-0.distributed-training-headless"
        - name: MASTER_PORT
          value: "12345"
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
```

#### æ–¹æ¡ˆäºŒï¼šèŠ‚ç‚¹äº²å’Œæ€§é…ç½®

```yaml
# åˆ†å¸ƒå¼è®­ç»ƒèŠ‚ç‚¹äº²å’Œæ€§é…ç½®
apiVersion: batch/v1
kind: Job
metadata:
  name: distributed-training
spec:
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: distributed-training
            topologyKey: "kubernetes.io/hostname"
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - "A100-SXM4-80GB"
                - "H100-PCIE-80GB"
              - key: kubernetes.io/arch
                operator: In
                values:
                - "amd64"
      containers:
      - name: trainer
        # ... å…¶ä»–é…ç½®
```

### æ¨¡å‹æœåŠ¡é—®é¢˜è§£å†³

#### æ–¹æ¡ˆä¸€ï¼šæ¨¡å‹æœåŠ¡ä¼˜åŒ–é…ç½®

```yaml
# ä¼˜åŒ–çš„æ¨¡å‹æœåŠ¡é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-serving
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: model-server
        image: tensorflow/serving:2.13.0-gpu
        ports:
        - containerPort: 8501
        env:
        # æ¨¡å‹æœåŠ¡å™¨ä¼˜åŒ–
        - name: MODEL_NAME
          value: "resnet50"
        - name: TENSORFLOW_INTER_OP_PARALLELISM
          value: "0"
        - name: TENSORFLOW_INTRA_OP_PARALLELISM
          value: "0"
        - name: OMP_NUM_THREADS
          value: "4"
        - name: BATCH_SIZE
          value: "32"
        - name: MAX_BATCH_SIZE
          value: "64"
        - name: BATCH_TIMEOUT_MICROS
          value: "10000"
        # GPU ä¼˜åŒ–
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: TF_FORCE_GPU_ALLOW_GROWTH
          value: "true"
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: "8Gi"
            cpu: "4"
          requests:
            nvidia.com/gpu: "1"
            memory: "4Gi"
            cpu: "2"
        readinessProbe:
          httpGet:
            path: /v1/models/resnet50
            port: 8501
          initialDelaySeconds: 30
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /v1/models/resnet50
            port: 8501
          initialDelaySeconds: 60
          periodSeconds: 10
```

#### æ–¹æ¡ˆäºŒï¼šè‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®

```yaml
# æ¨¡å‹æœåŠ¡è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: model-serving-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-serving
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: model_inference_latency_seconds
      target:
        type: AverageValue
        averageValue: "0.1"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
```

### æ•°æ®å¤„ç†é—®é¢˜è§£å†³

#### æ–¹æ¡ˆä¸€ï¼šé«˜æ€§èƒ½å­˜å‚¨é…ç½®

```yaml
# é«˜æ€§èƒ½å­˜å‚¨é…ç½®
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ml-dataset-pvc
spec:
  accessModes:
  - ReadWriteMany
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 1Ti

---
# é«˜æ€§èƒ½å­˜å‚¨ç±»é…ç½®
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: pd.csi.storage.gke.io  # GKE ç¤ºä¾‹
parameters:
  type: pd-ssd
  replication-type: none
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
```

#### æ–¹æ¡ˆäºŒï¼šæ•°æ®åŠ è½½ä¼˜åŒ–é…ç½®

```yaml
# æ•°æ®åŠ è½½ä¼˜åŒ–çš„è®­ç»ƒä½œä¸šé…ç½®
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training-with-optimized-data-loading
spec:
  template:
    spec:
      initContainers:
      - name: dataset-preparation
        image: busybox
        command: ['sh', '-c']
        args:
        - |
          # é¢„åŠ è½½æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜
          mkdir -p /shared/dataset-cache
          # è¿™é‡Œæ·»åŠ å…·ä½“çš„æ•°æ®é¢„åŠ è½½é€»è¾‘
        volumeMounts:
        - name: shared-data
          mountPath: /shared
      containers:
      - name: trainer
        image: pytorch/pytorch:2.0-cuda11.7-cudnn8-runtime
        env:
        # æ•°æ®åŠ è½½ä¼˜åŒ–
        - name: DATALOADER_NUM_WORKERS
          value: "4"
        - name: DATALOADER_PREFETCH_FACTOR
          value: "2"
        - name: DATALOADER_PIN_MEMORY
          value: "true"
        - name: TORCH_DISTRIBUTED_DEBUG
          value: "DETAIL"
        # ç¼“å­˜é…ç½®
        - name: DATASET_CACHE_DIR
          value: "/shared/dataset-cache"
        volumeMounts:
        - name: shared-data
          mountPath: /shared
        - name: local-ssd
          mountPath: /local-ssd
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
      volumes:
      - name: shared-data
        persistentVolumeClaim:
          claimName: ml-dataset-pvc
      - name: local-ssd
        hostPath:
          path: /mnt/disks/ssd0
          type: Directory
```

## âš ï¸ æ‰§è¡Œé£é™©è¯„ä¼°

| æ“ä½œ | é£é™©ç­‰çº§ | å½±å“è¯„ä¼° | å›æ»šæ–¹æ¡ˆ |
|------|---------|---------|---------|
| GPU é©±åŠ¨æ›´æ–° | â­â­â­ é«˜ | å¯èƒ½å¯¼è‡´èŠ‚ç‚¹ä¸å¯ç”¨ | ä½¿ç”¨ DaemonSet æ»šåŠ¨æ›´æ–° |
| åˆ†å¸ƒå¼è®­ç»ƒé…ç½®è°ƒæ•´ | â­â­ ä¸­ | å¯èƒ½å½±å“è®­ç»ƒæ”¶æ•›æ€§ | ä¿ç•™åŸé…ç½®ä½œä¸ºå¤‡ä»½ |
| æ¨¡å‹æœåŠ¡æ‰©ç¼©å®¹ç­–ç•¥è°ƒæ•´ | â­â­ ä¸­ | å¯èƒ½å½±å“æœåŠ¡è´¨é‡ | ç›‘æ§æŒ‡æ ‡å¹¶åŠæ—¶è°ƒæ•´ |
| å­˜å‚¨æ€§èƒ½ä¼˜åŒ– | â­â­ ä¸­ | å¯èƒ½å¢åŠ å­˜å‚¨æˆæœ¬ | é€æ­¥æµ‹è¯•å¹¶ç›‘æ§æˆæœ¬ |

## ğŸ“Š AI/ML å·¥ä½œè´Ÿè½½éªŒè¯ä¸ç›‘æ§

### AI/ML å·¥ä½œè´Ÿè½½éªŒè¯è„šæœ¬

```bash
#!/bin/bash
# AI/ML å·¥ä½œè´Ÿè½½éªŒè¯è„šæœ¬

echo "=== AI/ML å·¥ä½œè´Ÿè½½éªŒè¯ ==="

# 1. GPU èµ„æºéªŒè¯
echo "1. GPU èµ„æºéªŒè¯:"
GPU_NODES=$(kubectl get nodes -o jsonpath='{.items[?(@.status.capacity.nvidia\.com/gpu)].metadata.name}')
if [ -n "$GPU_NODES" ]; then
  echo "âœ“ å‘ç° GPU èŠ‚ç‚¹: $GPU_NODES"
  
  # éªŒè¯ GPU å¯ç”¨æ€§
  for node in $GPU_NODES; do
    GPU_COUNT=$(kubectl get node $node -o jsonpath='{.status.capacity.nvidia\.com/gpu}')
    echo "  èŠ‚ç‚¹ $node: $GPU_COUNT ä¸ª GPU"
  done
else
  echo "âŒ æœªå‘ç° GPU èŠ‚ç‚¹"
fi

# 2. åˆ†å¸ƒå¼è®­ç»ƒéªŒè¯
echo "2. åˆ†å¸ƒå¼è®­ç»ƒéªŒè¯:"
TEST_JOB_NAME="test-distributed-training-$(date +%s)"
cat << EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: $TEST_JOB_NAME
spec:
  parallelism: 2
  completions: 2
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: nccl-test
        image: nvcr.io/nvidia/nccl-test:2.14.3-cuda11.7
        command: ["all_reduce_perf"]
        args: ["-b", "8", "-e", "128M", "-f", "2", "-g", "1"]
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
        env:
        - name: NCCL_DEBUG
          value: "INFO"
EOF

echo "å·²åˆ›å»ºæµ‹è¯•ä½œä¸š: $TEST_JOB_NAME"
echo "ç­‰å¾…æµ‹è¯•å®Œæˆ..."

# ç­‰å¾…æµ‹è¯•å®Œæˆ
sleep 60

JOB_STATUS=$(kubectl get job $TEST_JOB_NAME -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null)
if [ "$JOB_STATUS" = "True" ]; then
  echo "âœ“ åˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•é€šè¿‡"
else
  echo "âš  åˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•å¯èƒ½å­˜åœ¨é—®é¢˜"
  kubectl logs job/$TEST_JOB_NAME --tail=20
fi

# æ¸…ç†æµ‹è¯•èµ„æº
kubectl delete job $TEST_JOB_NAME

# 3. æ¨¡å‹æœåŠ¡éªŒè¯
echo "3. æ¨¡å‹æœåŠ¡éªŒè¯:"
# è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æ¨¡å‹æœåŠ¡éªŒè¯é€»è¾‘

# 4. æ•°æ®å¤„ç†éªŒè¯
echo "4. æ•°æ®å¤„ç†éªŒè¯:"
# è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æ•°æ®å¤„ç†éªŒè¯é€»è¾‘

echo "AI/ML å·¥ä½œè´Ÿè½½éªŒè¯å®Œæˆï¼"
```

### AI/ML ç›‘æ§å‘Šè­¦é…ç½®

```yaml
# Prometheus AI/ML ç›‘æ§å‘Šè­¦
groups:
- name: ai-ml-workloads
  rules:
  - alert: GPUNotAvailable
    expr: kube_node_status_capacity{resource="nvidia_com_gpu"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "GPU èµ„æºä¸å¯ç”¨"
      description: "èŠ‚ç‚¹ {{ $labels.node }} ä¸Šæ²¡æœ‰å¯ç”¨çš„ GPU èµ„æº"

  - alert: LowGPUUtilization
    expr: avg(rate(DCGM_FI_DEV_GPU_UTIL[5m])) < 20
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "GPU åˆ©ç”¨ç‡è¿‡ä½"
      description: "GPU å¹³å‡åˆ©ç”¨ç‡ä½äº 20%ï¼Œå¯èƒ½å­˜åœ¨èµ„æºæµªè´¹"

  - alert: DistributedTrainingFailure
    expr: kube_job_status_failed{job_name=~"distributed-training.*"} > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "åˆ†å¸ƒå¼è®­ç»ƒå¤±è´¥"
      description: "åˆ†å¸ƒå¼è®­ç»ƒä½œä¸š {{ $labels.job_name }} å¤±è´¥"

  - alert: ModelInferenceTimeout
    expr: histogram_quantile(0.99, rate(model_inference_duration_seconds_bucket[5m])) > 30
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "æ¨¡å‹æ¨ç†è¶…æ—¶"
      description: "99% çš„æ¨¡å‹æ¨ç†è¯·æ±‚è€—æ—¶è¶…è¿‡ 30 ç§’"

  - alert: DatasetLoadingSlow
    expr: rate(dataset_loading_duration_seconds_sum[5m]) / rate(dataset_loading_duration_seconds_count[5m]) > 1800
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "æ•°æ®é›†åŠ è½½ç¼“æ…¢"
      description: "å¹³å‡æ•°æ®é›†åŠ è½½æ—¶é—´è¶…è¿‡ 30 åˆ†é’Ÿ"

  - alert: HighGPUMemoryUsage
    expr: DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL * 100 > 90
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "GPU å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "GPU å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡ 90%"

  - alert: UnexpectedGPUCost
    expr: rate(container_accelerator_allocation_cost_usd_per_hour[1h]) > 10
    for: 1h
    labels:
      severity: warning
    annotations:
      summary: "å¼‚å¸¸ GPU æˆæœ¬"
      description: "GPU ä½¿ç”¨æˆæœ¬å¼‚å¸¸å‡é«˜ï¼Œå½“å‰å°æ—¶è´¹ç”¨è¶…è¿‡ $10"
```

## ğŸ“š AI/ML å·¥ä½œè´Ÿè½½æœ€ä½³å®è·µ

### AI/ML èµ„æºç®¡ç†é…ç½®

```yaml
# AI/ML èµ„æºç®¡ç†æœ€ä½³å®è·µé…ç½®
aiMlBestPractices:
  gpuManagement:
    devicePlugin:
      image: nvcr.io/nvidia/k8s-device-plugin:v0.14.0
      resources:
        limits:
          memory: "128Mi"
          cpu: "100m"
        requests:
          memory: "64Mi"
          cpu: "50m"
    
    resourceQuotas:
      - name: ai-team-gpu-quota
        namespace: ai-team
        limits:
          "requests.nvidia.com/gpu": "32"
          "limits.nvidia.com/gpu": "32"
    
    priorityClasses:
      - name: high-priority-ml
        value: 1000000
        globalDefault: false
        description: "é«˜ä¼˜å…ˆçº§ ML å·¥ä½œè´Ÿè½½"
  
  distributedTraining:
    frameworks:
      pytorch:
        image: pytorch/pytorch:2.0-cuda11.7-cudnn8-runtime
        ncclSettings:
          socketInterface: eth0
          ibSupport: true
          bufferSize: "8M"
      
      tensorflow:
        image: tensorflow/tensorflow:2.13.0-gpu
        collectiveSettings:
          implementation: nccl
          timeout: "1800"
    
    networkRequirements:
      bandwidth: "10Gbps"
      latency: "<1ms"
      rdma: enabled
  
  modelServing:
    optimization:
      batching:
        enabled: true
        maxSize: 64
        timeout: "10ms"
      
      autoscaling:
        minReplicas: 2
        maxReplicas: 20
        metrics:
          - type: cpu
            target: 70%
          - type: memory
            target: 80%
          - type: custom
            metricName: inference_latency
            target: "100ms"
    
    monitoring:
      metrics:
        - inference_requests_total
        - inference_duration_seconds
        - model_loading_duration_seconds
        - gpu_utilization
```

### AI/ML æˆæœ¬ä¼˜åŒ–ç­–ç•¥

```bash
#!/bin/bash
# AI/ML æˆæœ¬ä¼˜åŒ–è„šæœ¬

COST_REPORT="/var/log/kubernetes/ml-cost-optimization-$(date +%Y%m%d).log"

{
  echo "=== AI/ML æˆæœ¬ä¼˜åŒ–æŠ¥å‘Š $(date) ==="
  
  # 1. GPU èµ„æºåˆ©ç”¨ç‡åˆ†æ
  echo "1. GPU èµ„æºåˆ©ç”¨ç‡åˆ†æ:"
  kubectl get nodes -o json | jq -r '
    .items[] | 
    select(.status.capacity."nvidia.com/gpu") |
    "\(.metadata.name): \(.status.capacity."nvidia.com/gpu") GPUs, allocatable: \(.status.allocatable."nvidia.com/gpu")"
  '
  
  # 2. å·¥ä½œè´Ÿè½½èµ„æºè¯·æ±‚åˆ†æ
  echo "2. å·¥ä½œè´Ÿè½½èµ„æºè¯·æ±‚åˆ†æ:"
  kubectl get pods --all-namespaces -o json | jq -r '
    .items[] |
    select(.spec.containers[].resources.requests."nvidia.com/gpu") |
    "\(.metadata.namespace)/\(.metadata.name): requested \(.spec.containers[].resources.requests."nvidia.com/gpu") GPUs"
  ' | head -10
  
  # 3. Spot å®ä¾‹ä½¿ç”¨å»ºè®®
  echo "3. Spot å®ä¾‹ä½¿ç”¨å»ºè®®:"
  # åˆ†æå¯è¿ç§»åˆ° Spot å®ä¾‹çš„å·¥ä½œè´Ÿè½½
  kubectl get pods --all-namespaces -o json | jq -r '
    .items[] |
    select(
      .spec.containers[].resources.requests."nvidia.com/gpu" and
      (.metadata.labels.training != "production" or .metadata.labels.tier != "production")
    ) |
    "\(.metadata.namespace)/\(.metadata.name)"
  ' | head -5
  
  # 4. è‡ªåŠ¨æ‰©ç¼©å®¹ä¼˜åŒ–å»ºè®®
  echo "4. è‡ªåŠ¨æ‰©ç¼©å®¹ä¼˜åŒ–å»ºè®®:"
  kubectl get hpa --all-namespaces -o json | jq -r '
    .items[] |
    "\(.metadata.namespace)/\(.metadata.name): min=\(.spec.minReplicas), max=\(.spec.maxReplicas)"
  '
  
} >> "$COST_REPORT"

echo "æˆæœ¬ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: $COST_REPORT"
```

## ğŸ”„ å…¸å‹ AI/ML æ•…éšœæ¡ˆä¾‹

### æ¡ˆä¾‹ä¸€ï¼šåˆ†å¸ƒå¼è®­ç»ƒ NCCL é”™è¯¯

**é—®é¢˜æè¿°**ï¼šPyTorch åˆ†å¸ƒå¼è®­ç»ƒä½œä¸šé¢‘ç¹å‡ºç° NCCL é€šä¿¡é”™è¯¯ï¼Œè®­ç»ƒé€Ÿåº¦ææ…¢ã€‚

**æ ¹æœ¬åŸå› **ï¼šç½‘ç»œç­–ç•¥é˜»æ­¢äº†èŠ‚ç‚¹é—´çš„ RDMA é€šä¿¡ï¼ŒNCCL å›é€€åˆ° TCP æ¨¡å¼ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ›´æ–°ç½‘ç»œç­–ç•¥å…è®¸ RDMA æµé‡
2. é…ç½® NCCL ä½¿ç”¨æ­£ç¡®çš„ç½‘ç»œæ¥å£
3. è°ƒæ•´ NCCL ç¼“å†²åŒºå¤§å°å’Œçº¿ç¨‹æ•°

### æ¡ˆä¾‹äºŒï¼šGPU å†…å­˜ç¢ç‰‡åŒ–å¯¼è‡´ OOM

**é—®é¢˜æè¿°**ï¼šé•¿æ—¶é—´è¿è¡Œçš„è®­ç»ƒä»»åŠ¡å‡ºç°å‘¨æœŸæ€§çš„ CUDA Out of Memory é”™è¯¯ã€‚

**æ ¹æœ¬åŸå› **ï¼šPyTorch å†…å­˜åˆ†é…å™¨äº§ç”Ÿç¢ç‰‡ï¼Œå³ä½¿æ€»å†…å­˜è¶³å¤Ÿä¹Ÿä¼šå‡ºç° OOMã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å¯ç”¨å†…å­˜æ± å’Œç¢ç‰‡æ•´ç†
2. è°ƒæ•´æ‰¹å¤„ç†å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯
3. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒå‡å°‘å†…å­˜ä½¿ç”¨

## ğŸ“ AI/ML æ”¯æŒèµ„æº

**æ¡†æ¶å®˜æ–¹æ–‡æ¡£**ï¼š
- PyTorch: https://pytorch.org/tutorials/beginner/dist_overview.html
- TensorFlow: https://www.tensorflow.org/guide/distributed_training
- NVIDIA: https://docs.nvidia.com/datacenter/cloud-native/kubernetes/install-k8s.html

**ç¤¾åŒºæ”¯æŒ**ï¼š
- Kubernetes AI/ML SIG: https://github.com/kubernetes/community/tree/master/sig-ai
- Kubeflow ç¤¾åŒº: https://www.kubeflow.org/
- NVIDIA å¼€å‘è€…è®ºå›: https://forums.developer.nvidia.com/
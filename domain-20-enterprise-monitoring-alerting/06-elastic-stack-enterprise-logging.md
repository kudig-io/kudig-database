# Elastic Stackä¼ä¸šçº§æ—¥å¿—åˆ†ææ·±åº¦å®è·µ

> **ä½œè€…**: ä¼ä¸šçº§æ—¥å¿—åˆ†ææ¶æ„ä¸“å®¶ | **ç‰ˆæœ¬**: v1.0 | **æ›´æ–°æ—¶é—´**: 2026-02-07
> **é€‚ç”¨åœºæ™¯**: ä¼ä¸šçº§æ—¥å¿—æ²»ç†ä¸å®æ—¶åˆ†æ | **å¤æ‚åº¦**: â­â­â­â­â­

## ğŸ¯ æ‘˜è¦

æœ¬æ–‡æ¡£æ·±å…¥æ¢è®¨Elastic Stackä¼ä¸šçº§æ—¥å¿—åˆ†æç³»ç»Ÿçš„æ¶æ„è®¾è®¡ã€éƒ¨ç½²å®è·µå’Œè¿ç»´ç®¡ç†ï¼ŒåŸºäºå¤§è§„æ¨¡ä¼ä¸šç¯å¢ƒçš„å®è·µç»éªŒï¼Œæä¾›ä»æ—¥å¿—é‡‡é›†åˆ°æ™ºèƒ½åˆ†æçš„å®Œæ•´æŠ€æœ¯æŒ‡å—ï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºé«˜æ•ˆã€å¯é çš„æ—¥å¿—æ²»ç†ä½“ç³»ã€‚

## 1. Elastic Stackæ¶æ„æ·±åº¦è§£æ

### 1.1 æ ¸å¿ƒç»„ä»¶æ¶æ„

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†å±‚"
        A[Filebeat] --> B[Log Files]
        C[Metricbeat] --> D[System Metrics]
        E[Winlogbeat] --> F[Windows Events]
        G[Packetbeat] --> H[Network Traffic]
        I[Auditbeat] --> J[Security Events]
        K[Functionbeat] --> L[Serverless Logs]
    end
    
    subgraph "æ•°æ®å¤„ç†å±‚"
        M[Logstash] --> N[Data Parsing]
        O[Elasticsearch Ingest Node] --> P[Pipeline Processing]
        Q[Kafka] --> R[Buffer Queue]
        S[Redis] --> T[Cache Layer]
    end
    
    subgraph "å­˜å‚¨æ£€ç´¢å±‚"
        U[Elasticsearch Cluster] --> V[Master Nodes]
        U --> W[Data Nodes]
        U --> X[Ingest Nodes]
        V --> Y[Cluster State]
        W --> Z[Shard Distribution]
        X --> AA[Document Indexing]
    end
    
    subgraph "åˆ†æå±•ç¤ºå±‚"
        AB[Kibana] --> AC[Dashboards]
        AD[Elasticsearch SQL] --> AE[Ad-hoc Queries]
        AF[Machine Learning] --> AG[Anomaly Detection]
        AH[Alerting] --> AI[Notification System]
    end
    
    subgraph "æ²»ç†ç®¡æ§å±‚"
        AJ[Security] --> AK[RBAC/RBAC]
        AL[Index Lifecycle] --> AM[ILM Policies]
        AN[Monitoring] --> AO[Elastic Stack Monitoring]
        AP[Backup] --> AQ[Snapshot Repository]
    end
```

### 1.2 ä¼ä¸šçº§æ¶æ„ä¼˜åŠ¿

#### 1.2.1 é«˜å¯ç”¨æ€§ä¿éšœ
- **é›†ç¾¤é«˜å¯ç”¨**: å¤šèŠ‚ç‚¹é›†ç¾¤éƒ¨ç½²ç¡®ä¿æœåŠ¡è¿ç»­æ€§
- **æ•°æ®å†—ä½™**: å‰¯æœ¬åˆ†ç‰‡æœºåˆ¶ä¿è¯æ•°æ®å®‰å…¨æ€§
- **æ•…éšœè‡ªåŠ¨æ¢å¤**: è‡ªåŠ¨æ•…éšœæ£€æµ‹å’ŒèŠ‚ç‚¹æ›¿æ¢æœºåˆ¶
- **è·¨åŒºåŸŸå¤åˆ¶**: åœ°ç†ä½ç½®åˆ†æ•£çš„æ•°æ®å¤‡ä»½ç­–ç•¥

#### 1.2.2 æ€§èƒ½ä¼˜åŒ–èƒ½åŠ›
- **æ°´å¹³æ‰©å±•**: æ”¯æŒåŠ¨æ€å¢åŠ èŠ‚ç‚¹å¤„ç†æ›´å¤§æ•°æ®é‡
- **æ™ºèƒ½åˆ†ç‰‡**: è‡ªåŠ¨ä¼˜åŒ–åˆ†ç‰‡åˆ†å¸ƒå’Œå¤§å°
- **ç¼“å­˜æœºåˆ¶**: å¤šå±‚æ¬¡ç¼“å­˜æå‡æŸ¥è¯¢æ€§èƒ½
- **å‹ç¼©å­˜å‚¨**: é«˜æ•ˆçš„æ•°æ®å‹ç¼©ç®—æ³•èŠ‚çœå­˜å‚¨ç©ºé—´

## 2. ä¼ä¸šçº§éƒ¨ç½²æ¶æ„

### 2.1 å¤šå±‚é«˜å¯ç”¨éƒ¨ç½²

#### 2.1.1 Elasticsearché›†ç¾¤éƒ¨ç½²

```yaml
# elasticsearch-cluster.yaml
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: enterprise-logs
  namespace: logging
spec:
  version: 8.11.3
  nodeSets:
  # ä¸»èŠ‚ç‚¹é›† - ä¸“ç”¨é›†ç¾¤ç®¡ç†
  - name: master-nodes
    count: 3
    config:
      node.roles: ["master"]
      cluster.routing.allocation.disk.threshold_enabled: true
      cluster.routing.allocation.disk.watermark.low: "85%"
      cluster.routing.allocation.disk.watermark.high: "90%"
      cluster.routing.allocation.disk.watermark.flood_stage: "95%"
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 4Gi
              cpu: 2
            limits:
              memory: 8Gi
              cpu: 4
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms4g -Xmx4g"
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  elasticsearch.k8s.elastic.co/cluster-name: enterprise-logs
                  elasticsearch.k8s.elastic.co/node-set-name: master-nodes
              topologyKey: kubernetes.io/hostname
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100Gi
        storageClassName: fast-ssd

  # æ•°æ®èŠ‚ç‚¹é›† - ä¸“ç”¨æ•°æ®å­˜å‚¨å’Œæœç´¢
  - name: data-nodes
    count: 6
    config:
      node.roles: ["data", "ingest"]
      indices.breaker.total.use_real_memory: true
      indices.breaker.fielddata.limit: "40%"
      indices.breaker.request.limit: "20%"
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 16Gi
              cpu: 4
            limits:
              memory: 32Gi
              cpu: 8
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms16g -Xmx16g"
        initContainers:
        - name: sysctl
          securityContext:
            privileged: true
          command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Ti
        storageClassName: standard

  # åè°ƒèŠ‚ç‚¹é›† - ä¸“ç”¨è¯·æ±‚åè°ƒ
  - name: coordinating-nodes
    count: 3
    config:
      node.roles: ["ingest"]
      search.remote.connect: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 8Gi
              cpu: 2
            limits:
              memory: 16Gi
              cpu: 4
          env:
          - name: ES_JAVA_OPTS
            value: "-Xms8g -Xmx8g"
```

#### 2.1.2 Filebeatåˆ†å¸ƒå¼é‡‡é›†

```yaml
# filebeat-deployment.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: logging
  labels:
    app: filebeat
spec:
  selector:
    matchLabels:
      app: filebeat
  template:
    metadata:
      labels:
        app: filebeat
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: filebeat
        image: docker.elastic.co/beats/filebeat:8.11.3
        args: [
          "-c", "/etc/filebeat.yml",
          "-e",
        ]
        env:
        - name: ELASTICSEARCH_HOST
          value: "enterprise-logs-es-http.logging.svc.cluster.local"
        - name: ELASTICSEARCH_PORT
          value: "9200"
        - name: ELASTICSEARCH_USERNAME
          valueFrom:
            secretKeyRef:
              name: elasticsearch-credentials
              key: username
        - name: ELASTICSEARCH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: elasticsearch-credentials
              key: password
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          runAsUser: 0
        resources:
          limits:
            memory: 1Gi
            cpu: 1000m
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: timezone
          mountPath: /etc/localtime
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0600
          name: filebeat-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
      - name: timezone
        hostPath:
          path: /etc/localtime
      - name: data
        hostPath:
          path: /var/lib/filebeat-data
          type: DirectoryOrCreate
```

#### 2.1.3 Filebeaté…ç½®ä¼˜åŒ–

```yaml
# filebeat-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: logging
  labels:
    app: filebeat
data:
  filebeat.yml: |-
    filebeat.inputs:
    # åº”ç”¨æ—¥å¿—é‡‡é›†
    - type: log
      enabled: true
      paths:
        - /var/log/containers/*_application_*.log
      processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
        - decode_json_fields:
            fields: ["message"]
            process_array: false
            max_depth: 10
            target: ""
            overwrite_keys: true
        - drop_fields:
            fields: ["input", "agent", "ecs", "log", "stream"]
      fields:
        log_type: application
        environment: production
      
    # ç³»ç»Ÿæ—¥å¿—é‡‡é›†
    - type: log
      enabled: true
      paths:
        - /var/log/*.log
        - /var/log/*/*.log
      exclude_files: ['\.gz$']
      multiline.pattern: '^\d{4}-\d{2}-\d{2}'
      multiline.negate: true
      multiline.match: after
      processors:
        - add_locale: ~
        - add_fields:
            target: ''
            fields:
              log_type: system
              environment: production
              
    # å®‰å…¨æ—¥å¿—é‡‡é›†
    - type: log
      enabled: true
      paths:
        - /var/log/auth.log
        - /var/log/secure
      processors:
        - add_fields:
            target: ''
            fields:
              log_type: security
              environment: production

    # è¾“å‡ºé…ç½®
    output.elasticsearch:
      hosts: ["${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}"]
      username: "${ELASTICSEARCH_USERNAME}"
      password: "${ELASTICSEARCH_PASSWORD}"
      index: "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"
      bulk_max_size: 2048
      worker: 2
      compression_level: 9
      timeout: 90
      max_retries: 3
      
    # è´Ÿè½½å‡è¡¡
    output.elasticsearch.loadbalance: true
    
    # SSLé…ç½®
    output.elasticsearch.ssl:
      enabled: true
      certificate_authorities: ["/etc/pki/root/ca.pem"]
      certificate: "/etc/pki/client/cert.pem"
      key: "/etc/pki/client/key.pem"
      verification_mode: certificate
      
    # é˜Ÿåˆ—é…ç½®
    queue.mem:
      events: 4096
      flush.min_events: 512
      flush.timeout: 5s
      
    # æ—¥å¿—é…ç½®
    logging.level: info
    logging.to_files: true
    logging.files:
      path: /var/log/filebeat
      name: filebeat
      keepfiles: 7
      permissions: 0644
      
    # ç›‘æ§é…ç½®
    http.enabled: true
    http.host: localhost
    http.port: 5066
```

### 2.2 å®‰å…¨åŠ å›ºé…ç½®

#### 2.2.1 ç½‘ç»œå®‰å…¨ç­–ç•¥

```yaml
# elastic-network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: elasticsearch-network-policy
  namespace: logging
spec:
  podSelector:
    matchLabels:
      elasticsearch.k8s.elastic.co/cluster-name: enterprise-logs
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # å…è®¸Kibanaè®¿é—®
  - from:
    - podSelector:
        matchLabels:
          app: kibana
    ports:
    - protocol: TCP
      port: 9200
  # å…è®¸Beatså®¢æˆ·ç«¯è®¿é—®
  - from:
    - namespaceSelector:
        matchLabels:
          name: application
    ports:
    - protocol: TCP
      port: 9200
  # å…è®¸å†…éƒ¨èŠ‚ç‚¹é€šä¿¡
  - from:
    - podSelector:
        matchLabels:
          elasticsearch.k8s.elastic.co/cluster-name: enterprise-logs
    ports:
    - protocol: TCP
      port: 9300
  egress:
  # å…è®¸è®¿é—®å¤–éƒ¨ä¾èµ–
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
    ports:
    - protocol: TCP
      port: 53  # DNS
    - protocol: UDP
      port: 53  # DNS
  # å…è®¸èŠ‚ç‚¹é—´é€šä¿¡
  - to:
    - podSelector:
        matchLabels:
          elasticsearch.k8s.elastic.co/cluster-name: enterprise-logs
    ports:
    - protocol: TCP
      port: 9300
```

#### 2.2.2 è®¤è¯æˆæƒé…ç½®

```yaml
# elastic-security-config.yaml
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: enterprise-logs
  namespace: logging
spec:
  auth:
    fileRealm:
    - username: admin
      password: ${ADMIN_PASSWORD}
      roles: ["superuser"]
    - username: kibana_system
      password: ${KIBANA_PASSWORD}
      roles: ["kibana_system"]
    - username: beats_system
      password: ${BEATS_PASSWORD}
      roles: ["beats_system"]
    - username: logstash_system
      password: ${LOGSTASH_PASSWORD}
      roles: ["logstash_system"]
      
  secureSettings:
  - secretName: elasticsearch-secure-settings
  
  http:
    tls:
      certificate:
        secretName: elasticsearch-http-certs
      selfSignedCertificate:
        disabled: true
        
  transport:
    tls:
      certificate:
        secretName: elasticsearch-transport-certs
      selfSignedCertificate:
        disabled: true
```

## 3. ä¼ä¸šçº§æ—¥å¿—æ²»ç†ç­–ç•¥

### 3.1 ç»Ÿä¸€æ—¥å¿—æ ¼å¼æ ‡å‡†åŒ–

#### 3.1.1 ECS(Elastic Common Schema)å®æ–½

```yaml
# ecs-mapping-template.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ecs-mapping-template
  namespace: logging
data:
  ecs-template.json: |
    {
      "index_patterns": ["filebeat-*", "metricbeat-*"],
      "settings": {
        "number_of_shards": 3,
        "number_of_replicas": 1,
        "refresh_interval": "30s",
        "blocks": {
          "read_only_allow_delete": "false"
        }
      },
      "mappings": {
        "_source": {
          "enabled": true
        },
        "properties": {
          "@timestamp": {
            "type": "date"
          },
          "labels": {
            "type": "object",
            "dynamic": true
          },
          "message": {
            "type": "text",
            "norms": false
          },
          "tags": {
            "type": "keyword"
          },
          "agent": {
            "properties": {
              "ephemeral_id": {
                "type": "keyword"
              },
              "id": {
                "type": "keyword"
              },
              "name": {
                "type": "keyword"
              },
              "type": {
                "type": "keyword"
              },
              "version": {
                "type": "keyword"
              }
            }
          },
          "container": {
            "properties": {
              "id": {
                "type": "keyword"
              },
              "image": {
                "properties": {
                  "name": {
                    "type": "keyword"
                  }
                }
              },
              "name": {
                "type": "keyword"
              },
              "runtime": {
                "type": "keyword"
              }
            }
          },
          "host": {
            "properties": {
              "architecture": {
                "type": "keyword"
              },
              "hostname": {
                "type": "keyword"
              },
              "name": {
                "type": "keyword"
              },
              "os": {
                "properties": {
                  "family": {
                    "type": "keyword"
                  },
                  "platform": {
                    "type": "keyword"
                  },
                  "version": {
                    "type": "keyword"
                  }
                }
              }
            }
          },
          "kubernetes": {
            "properties": {
              "container": {
                "properties": {
                  "name": {
                    "type": "keyword"
                  }
                }
              },
              "namespace": {
                "type": "keyword"
              },
              "node": {
                "properties": {
                  "name": {
                    "type": "keyword"
                  }
                }
              },
              "pod": {
                "properties": {
                  "name": {
                    "type": "keyword"
                  },
                  "uid": {
                    "type": "keyword"
                  }
                }
              }
            }
          },
          "log": {
            "properties": {
              "file": {
                "properties": {
                  "path": {
                    "type": "keyword"
                  }
                }
              },
              "level": {
                "type": "keyword"
              },
              "offset": {
                "type": "long"
              }
            }
          }
        }
      }
    }
```

#### 3.1.2 æ—¥å¿—å­—æ®µæ ‡å‡†åŒ–å¤„ç†å™¨

```yaml
# log-processing-pipeline.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-processing-pipeline
  namespace: logging
data:
  pipeline.conf: |
    input {
      beats {
        port => 5044
        ssl => true
        ssl_certificate => "/etc/pki/tls/certs/logstash.crt"
        ssl_key => "/etc/pki/tls/private/logstash.key"
      }
    }
    
    filter {
      # æ ‡å‡†åŒ–æ—¶é—´æˆ³
      date {
        match => [ "timestamp", "ISO8601" ]
        target => "@timestamp"
        remove_field => [ "timestamp" ]
      }
      
      # è§£æJSONæ¶ˆæ¯ä½“
      json {
        source => "message"
        skip_on_invalid_json => true
      }
      
      # æ ‡å‡†åŒ–æ—¥å¿—çº§åˆ«
      mutate {
        lowercase => [ "log.level" ]
        convert => {
          "http.response.status_code" => "integer"
          "http.request.duration" => "float"
        }
      }
      
      # æ·»åŠ ç¯å¢ƒæ ‡è¯†
      mutate {
        add_field => {
          "[fields][environment]" => "%{[@metadata][beat]}"
          "[fields][service]" => "%{[kubernetes][container][name]}"
        }
      }
      
      # IPåœ°ç†ä½ç½®è§£æ
      geoip {
        source => "[client][ip]"
        target => "client_geo"
        database => "/usr/share/GeoIP/GeoLite2-City.mmdb"
      }
      
      # ç”¨æˆ·ä»£ç†è§£æ
      useragent {
        source => "[http][request][headers][user-agent]"
        target => "user_agent"
        regexes => "/etc/logstash/regexes.yaml"
      }
      
      # æ•°æ®è„±æ•å¤„ç†
      if [message] =~ /password|secret|token/i {
        mutate {
          gsub => [
            "message", "(password|secret|token)[=:][^&\s]+", "\\1=***MASKED***"
          ]
        }
      }
    }
    
    output {
      elasticsearch {
        hosts => ["https://elasticsearch:9200"]
        index => "logs-%{[@metadata][beat]}-%{+YYYY.MM.dd}"
        user => "${ELASTICSEARCH_USERNAME}"
        password => "${ELASTICSEARCH_PASSWORD}"
        ssl_certificate_verification => true
        ilm_enabled => true
        ilm_rollover_alias => "logs-%{[@metadata][beat]}"
        ilm_pattern => "{now/d}-000001"
        ilm_policy => "logs-policy"
      }
    }
```

### 3.2 ç”Ÿå‘½å‘¨æœŸç®¡ç†ç­–ç•¥

#### 3.2.1 ILM(Index Lifecycle Management)ç­–ç•¥

```yaml
# ilm-policies.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ilm-policies
  namespace: logging
data:
  application-logs-policy.json: |
    {
      "policy": {
        "phases": {
          "hot": {
            "actions": {
              "rollover": {
                "max_age": "7d",
                "max_size": "50gb",
                "max_docs": 10000000
              },
              "set_priority": {
                "priority": 100
              }
            }
          },
          "warm": {
            "min_age": "7d",
            "actions": {
              "allocate": {
                "number_of_replicas": 1,
                "include": {},
                "exclude": {
                  "box_type": "hot"
                }
              },
              "forcemerge": {
                "max_num_segments": 1
              },
              "set_priority": {
                "priority": 50
              }
            }
          },
          "cold": {
            "min_age": "30d",
            "actions": {
              "allocate": {
                "number_of_replicas": 0,
                "require": {
                  "box_type": "cold"
                },
                "include": {},
                "exclude": {}
              },
              "freeze": {},
              "set_priority": {
                "priority": 0
              }
            }
          },
          "delete": {
            "min_age": "365d",
            "actions": {
              "delete": {}
            }
          }
        }
      }
    }

  system-logs-policy.json: |
    {
      "policy": {
        "phases": {
          "hot": {
            "actions": {
              "rollover": {
                "max_age": "3d",
                "max_size": "20gb"
              },
              "set_priority": {
                "priority": 150
              }
            }
          },
          "warm": {
            "min_age": "3d",
            "actions": {
              "allocate": {
                "number_of_replicas": 1
              },
              "forcemerge": {
                "max_num_segments": 1
              }
            }
          },
          "delete": {
            "min_age": "90d",
            "actions": {
              "delete": {}
            }
          }
        }
      }
    }

  security-logs-policy.json: |
    {
      "policy": {
        "phases": {
          "hot": {
            "actions": {
              "rollover": {
                "max_age": "1d",
                "max_size": "10gb"
              }
            }
          },
          "warm": {
            "min_age": "1d",
            "actions": {
              "readonly": {}
            }
          },
          "cold": {
            "min_age": "7d",
            "actions": {
              "freeze": {}
            }
          },
          "delete": {
            "min_age": "180d",
            "actions": {
              "delete": {}
            }
          }
        }
      }
    }
```

## 4. æ™ºèƒ½åˆ†æä¸å‘Šè­¦

### 4.1 æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹

#### 4.1.1 å¼‚å¸¸æ£€æµ‹ä½œä¸šé…ç½®

```yaml
# ml-anomaly-detection.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-anomaly-jobs
  namespace: logging
data:
  http-response-codes.json: |
    {
      "job_id": "http_response_codes_analysis",
      "job_type": "anomaly_detector",
      "description": "HTTPå“åº”ç å¼‚å¸¸æ£€æµ‹",
      "analysis_config": {
        "bucket_span": "15m",
        "detectors": [
          {
            "detector_description": "å¼‚å¸¸HTTPé”™è¯¯ç‡",
            "function": "high_count",
            "by_field_name": "http.response.status_code",
            "partition_field_name": "kubernetes.namespace"
          }
        ],
        "influencers": [
          "kubernetes.namespace",
          "kubernetes.pod.name",
          "host.name"
        ]
      },
      "data_description": {
        "time_field": "@timestamp",
        "time_format": "epoch_ms"
      },
      "model_plot_config": {
        "enabled": true,
        "annotations_enabled": true
      },
      "analysis_limits": {
        "model_memory_limit": "1GB",
        "categorization_examples_limit": 4
      },
      "datafeed_config": {
        "datafeed_id": "datafeed-http_response_codes_analysis",
        "indices": ["filebeat-*"],
        "scroll_size": 1000,
        "delayed_data_check_config": {
          "enabled": true
        },
        "query": {
          "bool": {
            "filter": [
              {
                "exists": {
                  "field": "http.response.status_code"
                }
              },
              {
                "range": {
                  "http.response.status_code": {
                    "gte": 400
                  }
                }
              }
            ]
          }
        }
      }
    }

  log-volume-anomaly.json: |
    {
      "job_id": "log_volume_anomaly_detection",
      "job_type": "anomaly_detector",
      "description": "æ—¥å¿—é‡å¼‚å¸¸æ£€æµ‹",
      "analysis_config": {
        "bucket_span": "1h",
        "detectors": [
          {
            "detector_description": "å¼‚å¸¸æ—¥å¿—é‡å¢é•¿",
            "function": "high_count",
            "partition_field_name": "kubernetes.container.name"
          }
        ],
        "influencers": [
          "kubernetes.container.name",
          "kubernetes.namespace"
        ]
      },
      "data_description": {
        "time_field": "@timestamp"
      },
      "model_plot_config": {
        "enabled": true
      },
      "analysis_limits": {
        "model_memory_limit": "512MB"
      },
      "datafeed_config": {
        "datafeed_id": "datafeed-log_volume_anomaly_detection",
        "indices": ["filebeat-*"],
        "query_delay": "60s",
        "frequency": "150s",
        "query": {
          "match_all": {}
        }
      }
    }
```

#### 4.1.2 å®æ—¶å‘Šè­¦é…ç½®

```yaml
# alerting-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-alerts
  namespace: logging
data:
  alert-rules.json: |
    {
      "name": "ä¼ä¸šçº§æ—¥å¿—å‘Šè­¦è§„åˆ™",
      "schedule": "0 */5 * * * ?",
      "inputs": [
        {
          "search": {
            "request": {
              "search_type": "query_then_fetch",
              "indices": ["filebeat-*"],
              "rest_total_hits_as_int": true,
              "body": {
                "size": 0,
                "query": {
                  "bool": {
                    "must": [
                      {
                        "range": {
                          "@timestamp": {
                            "gte": "now-5m",
                            "lt": "now"
                          }
                        }
                      }
                    ],
                    "should": [
                      {
                        "match": {
                          "log.level": "ERROR"
                        }
                      },
                      {
                        "match": {
                          "log.level": "FATAL"
                        }
                      }
                    ],
                    "minimum_should_match": 1
                  }
                },
                "aggs": {
                  "error_counts": {
                    "terms": {
                      "field": "kubernetes.container.name.keyword",
                      "size": 10
                    }
                  }
                }
              }
            }
          }
        }
      ],
      "triggers": [
        {
          "name": "é«˜é¢‘é”™è¯¯æ—¥å¿—å‘Šè­¦",
          "severity": "high",
          "condition": {
            "script": {
              "source": "ctx.results[0].hits.total.value > params.threshold",
              "lang": "painless",
              "params": {
                "threshold": 100
              }
            }
          },
          "actions": [
            {
              "name": "å‘é€å‘Šè­¦é€šçŸ¥",
              "destination_id": "slack_notifications",
              "message_template": {
                "source": "å‘ç°é«˜é¢‘é”™è¯¯æ—¥å¿—:\n- æ—¶é—´èŒƒå›´: {{ctx.periodStart}} - {{ctx.periodEnd}}\n- é”™è¯¯æ€»æ•°: {{ctx.results[0].hits.total.value}}\n- è¯¦æƒ…è¯·æŸ¥çœ‹Kibanaä»ªè¡¨æ¿"
              },
              "throttle_enabled": true,
              "throttle": {
                "value": 10,
                "unit": "MINUTES"
              }
            }
          ]
        }
      ]
    }
```

### 4.2 è‡ªå®šä¹‰åˆ†æä»ªè¡¨æ¿

#### 4.2.1 æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡çœ‹æ¿

```json
{
  "dashboard": {
    "title": "ä¼ä¸šçº§æ—¥å¿—åˆ†æä»ªè¡¨æ¿",
    "description": "ç»¼åˆå±•ç¤ºåº”ç”¨å¥åº·çŠ¶å†µå’Œç³»ç»Ÿæ€§èƒ½",
    "panels": [
      {
        "id": "error_rate_panel",
        "type": "visualization",
        "title": "é”™è¯¯ç‡è¶‹åŠ¿",
        "visState": {
          "title": "é”™è¯¯ç‡è¶‹åŠ¿",
          "type": "line",
          "params": {
            "addTooltip": true,
            "addLegend": true,
            "legendPosition": "right",
            "times": [],
            "addTimeMarker": false,
            "dimensions": {
              "x": {
                "accessor": 0,
                "format": {
                  "id": "date",
                  "params": {
                    "pattern": "YYYY-MM-DD HH:mm"
                  }
                },
                "params": {
                  "date": true,
                  "interval": "auto",
                  "time_zone": "browser"
                },
                "aggType": "date_histogram"
              },
              "y": [
                {
                  "accessor": 2,
                  "format": {
                    "id": "percent"
                  },
                  "params": {},
                  "aggType": "avg"
                }
              ]
            }
          },
          "aggs": [
            {
              "id": "1",
              "enabled": true,
              "type": "count",
              "schema": "metric",
              "params": {}
            },
            {
              "id": "2",
              "enabled": true,
              "type": "date_histogram",
              "schema": "segment",
              "params": {
                "field": "@timestamp",
                "interval": "auto",
                "customInterval": "2h",
                "min_doc_count": 1,
                "extended_bounds": {}
              }
            },
            {
              "id": "3",
              "enabled": true,
              "type": "filters",
              "schema": "group",
              "params": {
                "filters": [
                  {
                    "input": {
                      "query": {
                        "match": {
                          "log.level": "ERROR"
                        }
                      }
                    },
                    "label": "é”™è¯¯æ—¥å¿—"
                  },
                  {
                    "input": {
                      "query": {
                        "match_all": {}
                      }
                    },
                    "label": "æ€»æ—¥å¿—"
                  }
                ]
              }
            }
          ]
        }
      }
    ]
  }
}
```

## 5. ä¼ä¸šçº§æœ€ä½³å®è·µ

### 5.1 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 5.1.1 ç´¢å¼•ä¼˜åŒ–é…ç½®

```yaml
# index-optimization.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: index-optimization-settings
  namespace: logging
data:
  optimization-settings.json: |
    {
      "index": {
        "number_of_shards": 3,
        "number_of_replicas": 1,
        "refresh_interval": "30s",
        "translog.durability": "async",
        "translog.sync_interval": "30s",
        "blocks": {
          "read_only_allow_delete": "false"
        },
        "codec": "best_compression",
        "routing": {
          "allocation": {
            "enable": "all",
            "total_shards_per_node": 5
          }
        },
        "unassigned": {
          "node_left": {
            "delayed_timeout": "5m"
          }
        }
      },
      "index.lifecycle": {
        "name": "logs-policy",
        "rollover_alias": "logs-current"
      }
    }
```

#### 5.1.2 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

```yaml
# query-optimization.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: query-optimization-settings
  namespace: logging
data:
  query-settings.json: |
    {
      "indices.queries.cache.size": "10%",
      "indices.fielddata.cache.size": "20%",
      "indices.requests.cache.enable": true,
      "search.default_search_timeout": "30s",
      "thread_pool.search.size": 20,
      "thread_pool.search.queue_size": 1000,
      "thread_pool.index.size": 10,
      "thread_pool.index.queue_size": 200
    }
```

### 5.2 æˆæœ¬æ§åˆ¶ç­–ç•¥

#### 5.2.1 å­˜å‚¨æˆæœ¬ä¼˜åŒ–

```bash
#!/bin/bash
# storage-cost-optimization.sh

echo "=== Elasticsearchå­˜å‚¨æˆæœ¬ä¼˜åŒ–åˆ†æ ==="

# 1. åˆ†æç´¢å¼•å¤§å°
echo "1. å½“å‰ç´¢å¼•å¤§å°åˆ†æ:"
curl -s -u ${ES_USER}:${ES_PASS} \
  "${ES_HOST}:9200/_cat/indices?v&s=store.size:desc&bytes=gb" | head -20

# 2. è¯†åˆ«å¤§ç´¢å¼•
echo "2. è¯†åˆ«å¤§äº10GBçš„ç´¢å¼•:"
curl -s -u ${ES_USER}:${ES_PASS} \
  "${ES_HOST}:9200/_cat/indices?h=index,store.size&s=store.size:desc" | \
  awk '$2+0 > 10 {print $1, $2"GB"}'

# 3. åˆ†æå­—æ®µå ç”¨
echo "3. å­—æ®µå­˜å‚¨å ç”¨åˆ†æ:"
for index in $(curl -s -u ${ES_USER}:${ES_PASS} "${ES_HOST}:9200/_cat/indices?h=index" | grep filebeat | head -5); do
  echo "ç´¢å¼•: $index"
  curl -s -u ${ES_USER}:${ES_PASS} "${ES_HOST}:9200/$index/_field_caps" | \
    jq -r '.fields | to_entries[] | "\(.key): \(.value.text?.metadata?.size // 0) bytes"' | \
    sort -k2 -nr | head -10
  echo "---"
done

# 4. å»ºè®®çš„ä¼˜åŒ–æªæ–½
echo "4. å­˜å‚¨ä¼˜åŒ–å»ºè®®:"
echo "   - å¯ç”¨æ›´å¥½çš„å‹ç¼©ç®—æ³•"
echo "   - åˆ é™¤ä¸å¿…è¦çš„å­—æ®µ"
echo "   - è°ƒæ•´åˆ†ç‰‡æ•°é‡"
echo "   - å®æ–½æ›´ç§¯æçš„ILMç­–ç•¥"
echo "   - è€ƒè™‘å†·çƒ­æ•°æ®åˆ†ç¦»"
```

#### 5.2.2 èµ„æºä½¿ç”¨ç›‘æ§

```yaml
# resource-monitoring.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: elasticsearch-resource-alerts
  namespace: monitoring
spec:
  groups:
  - name: elasticsearch.resource.monitoring
    rules:
    # JVMå †å†…å­˜ä½¿ç”¨ç‡
    - alert: ElasticsearchHeapUsageHigh
      expr: |
        elasticsearch_jvm_memory_used_bytes{area="heap"} / 
        elasticsearch_jvm_memory_max_bytes{area="heap"} > 0.85
      for: 5m
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "Elasticsearchå †å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
        description: "{{ $labels.node }}èŠ‚ç‚¹å †å†…å­˜ä½¿ç”¨ç‡: {{ $value }}%"
    
    # ç£ç›˜ä½¿ç”¨ç‡
    - alert: ElasticsearchDiskUsageHigh
      expr: |
        elasticsearch_filesystem_data_available_bytes / 
        elasticsearch_filesystem_data_size_bytes < 0.15
      for: 10m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "Elasticsearchç£ç›˜ç©ºé—´ä¸è¶³"
        description: "{{ $labels.node }}èŠ‚ç‚¹ç£ç›˜å‰©ä½™ç©ºé—´: {{ $value }}%"
    
    # CPUä½¿ç”¨ç‡
    - alert: ElasticsearchCPUUsageHigh
      expr: |
        rate(elasticsearch_process_cpu_percent[5m]) > 80
      for: 5m
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "Elasticsearch CPUä½¿ç”¨ç‡è¿‡é«˜"
        description: "{{ $labels.node }}èŠ‚ç‚¹CPUä½¿ç”¨ç‡: {{ $value }}%"
```

## 6. æ•…éšœæ’æŸ¥ä¸ç»´æŠ¤

### 6.1 å¸¸è§é—®é¢˜è¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# elasticsearch-troubleshooting.sh

echo "=== Elasticsearchæ•…éšœè¯Šæ–­å·¥å…· ==="

CLUSTER_URL="https://elasticsearch:9200"
AUTH="-u admin:${ES_PASSWORD}"

# 1. é›†ç¾¤å¥åº·çŠ¶æ€
echo "1. é›†ç¾¤å¥åº·çŠ¶æ€:"
curl -s ${AUTH} "${CLUSTER_URL}/_cluster/health?pretty"

# 2. èŠ‚ç‚¹çŠ¶æ€
echo "2. èŠ‚ç‚¹çŠ¶æ€:"
curl -s ${AUTH} "${CLUSTER_URL}/_nodes/stats?pretty" | jq '.nodes[].name'

# 3. æœªåˆ†é…åˆ†ç‰‡
echo "3. æœªåˆ†é…åˆ†ç‰‡æ£€æŸ¥:"
curl -s ${AUTH} "${CLUSTER_URL}/_cat/shards?h=index,shard,prirep,state,unassigned.reason" | \
  grep UNASSIGNED

# 4. ç´¢å¼•ç»Ÿè®¡
echo "4. ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯:"
curl -s ${AUTH} "${CLUSTER_URL}/_stats" | jq '{
  "æ€»æ–‡æ¡£æ•°": .indices._all.primaries.docs.count,
  "æ€»å­˜å‚¨å¤§å°": .indices._all.primaries.store.size_in_bytes,
  "ç´¢å¼•æ•°é‡": .indices | keys | length
}'

# 5. æ…¢æŸ¥è¯¢æ—¥å¿—
echo "5. æ…¢æŸ¥è¯¢åˆ†æ:"
curl -s ${AUTH} "${CLUSTER_URL}/_search?pretty" -H 'Content-Type: application/json' -d'
{
  "query": {
    "range": {
      "@timestamp": {
        "gte": "now-1h"
      }
    }
  },
  "aggs": {
    "slow_queries": {
      "terms": {
        "field": "kubernetes.container.name.keyword",
        "size": 10,
        "order": {
          "_count": "desc"
        }
      }
    }
  },
  "size": 0
}'

# 6. GCæ´»åŠ¨ç›‘æ§
echo "6. åƒåœ¾å›æ”¶æ´»åŠ¨:"
curl -s ${AUTH} "${CLUSTER_URL}/_nodes/stats/jvm?pretty" | \
  jq '.nodes[].jvm.gc.collectors.old.collection_count'
```

### 6.2 ç»´æŠ¤æ“ä½œè‡ªåŠ¨åŒ–

```python
# maintenance-automation.py
import requests
import json
import time
from datetime import datetime, timedelta

class ElasticsearchMaintenance:
    def __init__(self, es_url, username, password):
        self.es_url = es_url
        self.auth = (username, password)
        self.session = requests.Session()
        self.session.auth = self.auth
        self.session.verify = False
        
    def get_cluster_health(self):
        """è·å–é›†ç¾¤å¥åº·çŠ¶æ€"""
        response = self.session.get(f"{self.es_url}/_cluster/health")
        return response.json()
    
    def optimize_indices(self):
        """ä¼˜åŒ–ç´¢å¼•æ€§èƒ½"""
        # è·å–æ‰€æœ‰ç´¢å¼•
        indices_response = self.session.get(f"{self.es_url}/_cat/indices?format=json")
        indices = [idx['index'] for idx in indices_response.json() 
                  if not idx['index'].startswith('.')]
        
        for index in indices:
            print(f"ä¼˜åŒ–ç´¢å¼•: {index}")
            
            # å¼ºåˆ¶åˆå¹¶æ®µ
            merge_response = self.session.post(
                f"{self.es_url}/{index}/_forcemerge?max_num_segments=1"
            )
            print(f"å¼ºåˆ¶åˆå¹¶ç»“æœ: {merge_response.status_code}")
            
            # åˆ·æ–°ç´¢å¼•
            refresh_response = self.session.post(f"{self.es_url}/{index}/_refresh")
            print(f"åˆ·æ–°ç´¢å¼•ç»“æœ: {refresh_response.status_code}")
            
            time.sleep(1)
    
    def cleanup_old_indices(self, days_to_keep=30):
        """æ¸…ç†æ—§ç´¢å¼•"""
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        cutoff_str = cutoff_date.strftime("%Y.%m.%d")
        
        # è·å–æ‰€æœ‰ç´¢å¼•
        indices_response = self.session.get(f"{self.es_url}/_cat/indices?format=json")
        indices = [idx['index'] for idx in indices_response.json()]
        
        # è¯†åˆ«å¯åˆ é™¤çš„ç´¢å¼•
        indices_to_delete = []
        for index in indices:
            if index.startswith('filebeat-') or index.startswith('metricbeat-'):
                # æå–æ—¥æœŸéƒ¨åˆ†
                try:
                    date_part = index.split('-')[-1]
                    index_date = datetime.strptime(date_part, "%Y.%m.%d")
                    if index_date < cutoff_date:
                        indices_to_delete.append(index)
                except ValueError:
                    continue
        
        # åˆ é™¤æ—§ç´¢å¼•
        for index in indices_to_delete:
            print(f"åˆ é™¤æ—§ç´¢å¼•: {index}")
            delete_response = self.session.delete(f"{self.es_url}/{index}")
            print(f"åˆ é™¤ç»“æœ: {delete_response.status_code}")
    
    def rebalance_cluster(self):
        """é‡æ–°å¹³è¡¡é›†ç¾¤"""
        # å¯ç”¨åˆ†ç‰‡åˆ†é…
        allocation_response = self.session.put(
            f"{self.es_url}/_cluster/settings",
            json={
                "persistent": {
                    "cluster.routing.allocation.enable": "all"
                }
            }
        )
        print(f"å¯ç”¨åˆ†ç‰‡åˆ†é…: {allocation_response.status_code}")
        
        # ç­‰å¾…é‡å¹³è¡¡å®Œæˆ
        print("ç­‰å¾…é›†ç¾¤é‡å¹³è¡¡...")
        while True:
            health = self.get_cluster_health()
            if health['status'] == 'green' and health['relocating_shards'] == 0:
                print("é›†ç¾¤é‡å¹³è¡¡å®Œæˆ")
                break
            time.sleep(30)
    
    def run_daily_maintenance(self):
        """æ‰§è¡Œæ—¥å¸¸ç»´æŠ¤ä»»åŠ¡"""
        print(f"å¼€å§‹æ‰§è¡Œæ—¥å¸¸ç»´æŠ¤ - {datetime.now()}")
        
        # 1. æ£€æŸ¥é›†ç¾¤å¥åº·
        health = self.get_cluster_health()
        print(f"é›†ç¾¤çŠ¶æ€: {health['status']}")
        
        if health['status'] != 'green':
            print("è­¦å‘Š: é›†ç¾¤ä¸æ˜¯ç»¿è‰²çŠ¶æ€ï¼Œè·³è¿‡ç»´æŠ¤æ“ä½œ")
            return
            
        # 2. ä¼˜åŒ–ç´¢å¼•
        self.optimize_indices()
        
        # 3. æ¸…ç†æ—§ç´¢å¼•
        self.cleanup_old_indices(days_to_keep=45)
        
        # 4. é‡æ–°å¹³è¡¡é›†ç¾¤
        self.rebalance_cluster()
        
        print(f"æ—¥å¸¸ç»´æŠ¤å®Œæˆ - {datetime.now()}")

# ä½¿ç”¨ç¤ºä¾‹
maintenance = ElasticsearchMaintenance(
    "https://elasticsearch:9200",
    "admin",
    "your_password"
)
maintenance.run_daily_maintenance()
```

é€šè¿‡ä»¥ä¸Šä¼ä¸šçº§Elastic Stackæ·±åº¦å®è·µï¼Œä¼ä¸šå¯ä»¥æ„å»ºå®Œæ•´çš„æ—¥å¿—æ²»ç†ä½“ç³»ï¼Œå®ç°ä»æ—¥å¿—é‡‡é›†ã€å­˜å‚¨ã€åˆ†æåˆ°å‘Šè­¦çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ç®¡ç†ï¼Œæ˜¾è‘—æå‡è¿ç»´æ•ˆç‡å’Œç³»ç»Ÿå¯é æ€§ã€‚
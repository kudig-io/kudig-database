# 02-å¤šäº‘æ··åˆéƒ¨ç½²ç­–ç•¥

> **é€‚ç”¨èŒƒå›´**: Kubernetes v1.25-v1.32 | **ç»´æŠ¤çŠ¶æ€**: ğŸ”§ æŒç»­æ›´æ–°ä¸­ | **ä¸“å®¶çº§åˆ«**: â­â­â­â­â­

## ğŸ“‹ æ¦‚è¿°

å¤šäº‘æ··åˆéƒ¨ç½²æ˜¯ç°ä»£ä¼ä¸šITæˆ˜ç•¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨å¤šä¸ªäº‘å¹³å°å’Œæœ¬åœ°ç¯å¢ƒä¸­å®ç°Kubernetesé›†ç¾¤çš„ç»Ÿä¸€ç®¡ç†å’Œå®¹ç¾éƒ¨ç½²ã€‚

## â˜ï¸ å¤šäº‘æ¶æ„æ¨¡å¼

### å¤šäº‘éƒ¨ç½²æ‹“æ‰‘

#### 1. ä¸»å¤‡æ¨¡å¼ (Active-Passive)
```yaml
# ä¸»é›†ç¾¤é…ç½®
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: primary-cluster
  labels:
    cluster-type: primary
spec:
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: primary-control-plane
---
# å¤‡ç”¨é›†ç¾¤é…ç½®
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: standby-cluster
  labels:
    cluster-type: standby
spec:
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: standby-control-plane
```

#### 2. ä¸»ä¸»æ¨¡å¼ (Active-Active)
```yaml
# å¤šåŒºåŸŸè´Ÿè½½å‡è¡¡é…ç½®
apiVersion: v1
kind: Service
metadata:
  name: global-lb
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: global-app
```

### è·¨äº‘ç½‘ç»œäº’è”

#### 1. VPNç½‘å…³é…ç½®
```bash
# AWS VPNè¿æ¥é…ç½®
aws ec2 create-vpn-connection \
  --type ipsec.1 \
  --customer-gateway-id cgw-12345678 \
  --vpn-gateway-id vgw-87654321 \
  --options TunnelInsideCidrList=["169.254.10.0/30","169.254.11.0/30"]
```

#### 2. ä¸“çº¿è¿æ¥è®¾ç½®
```yaml
# Google Cloud Interconnecté…ç½®
apiVersion: compute.cnrm.cloud.google.com/v1beta1
kind: ComputeInterconnectAttachment
metadata:
  name: interconnect-attachment
spec:
  routerRef:
    name: cloud-router
  region: us-central1
  type: DEDICATED
  bandwidth: BPS_10G
  candidateSubnets:
  - 169.254.100.0/29
```

## ğŸ”„ æ•°æ®åŒæ­¥ç­–ç•¥

### å®¹å™¨é•œåƒåŒæ­¥

#### 1. Harborå¤šå®ä¾‹åŒæ­¥
```yaml
# Harborå¤åˆ¶è§„åˆ™é…ç½®
apiVersion: goharbor.io/v1alpha1
kind: ReplicationPolicy
metadata:
  name: cross-cloud-sync
spec:
  srcRegistry:
    name: harbor-primary
  destRegistry:
    name: harbor-secondary
  trigger:
    type: scheduled
    cron: "0 2 * * *"
  filters:
  - name: "prod-*"
    type: name
  destNamespace: production
  override: true
  speed: 0
  copyByChunk: false
```

#### 2. é•œåƒä»“åº“ç¼“å­˜ç­–ç•¥
```yaml
# Registryç¼“å­˜é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: registry-cache
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: registry
        image: registry:2.8
        env:
        - name: REGISTRY_PROXY_REMOTEURL
          value: https://registry-1.docker.io
        - name: REGISTRY_STORAGE_DELETE_ENABLED
          value: "true"
        volumeMounts:
        - name: cache-volume
          mountPath: /var/lib/registry
```

### é…ç½®æ•°æ®åŒæ­¥

#### 1. GitOpså¤šé›†ç¾¤åŒæ­¥
```yaml
# ArgoCD ApplicationSeté…ç½®
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: multi-cluster-apps
spec:
  generators:
  - clusters:
      selector:
        matchLabels:
          environment: production
  template:
    metadata:
      name: '{{name}}-app'
    spec:
      project: default
      source:
        repoURL: https://github.com/org/apps.git
        targetRevision: HEAD
        path: charts/app
      destination:
        server: '{{server}}'
        namespace: app-namespace
```

#### 2. Secretè·¨é›†ç¾¤åŒæ­¥
```yaml
# Sealed Secretsé…ç½®
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: database-credentials
  namespace: production
spec:
  encryptedData:
    username: AgB2...
    password: AgC3...
  template:
    metadata:
      name: database-credentials
      namespace: production
    type: Opaque
```

## ğŸ›¡ï¸ å®‰å…¨ç®¡æ§ç­–ç•¥

### ç»Ÿä¸€èº«ä»½è®¤è¯

#### 1. OIDCå¤šäº‘é›†æˆ
```yaml
# Dexé…ç½®æ”¯æŒå¤šäº‘æä¾›å•†
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dex
spec:
  template:
    spec:
      containers:
      - name: dex
        image: dexidp/dex:v2.35.0
        args:
        - dex
        - serve
        - --web-http-addr=:5556
        - --telemetry-addr=:5558
        - --config=/etc/dex/config.yaml
        volumeMounts:
        - name: config
          mountPath: /etc/dex
      volumes:
      - name: config
        configMap:
          name: dex-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: dex-config
data:
  config.yaml: |
    issuer: https://dex.example.com
    storage:
      type: kubernetes
      config:
        inCluster: true
    web:
      http: 0.0.0.0:5556
    connectors:
    - type: oidc
      id: aws
      name: AWS SSO
      config:
        issuer: https://oidc.eks.us-west-2.amazonaws.com/id/ABC123
        clientID: $AWS_CLIENT_ID
        clientSecret: $AWS_CLIENT_SECRET
        redirectURI: https://dex.example.com/callback
    - type: oidc
      id: gcp
      name: Google Cloud
      config:
        issuer: https://accounts.google.com
        clientID: $GCP_CLIENT_ID
        clientSecret: $GCP_CLIENT_SECRET
        redirectURI: https://dex.example.com/callback
```

#### 2. ç»Ÿä¸€RBACç­–ç•¥
```yaml
# è·¨é›†ç¾¤RBACåŒæ­¥
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cross-cluster-admin
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cross-cluster-admin-binding
subjects:
- kind: Group
  name: sso:admins
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cross-cluster-admin
  apiGroup: rbac.authorization.k8s.io
```

### ç½‘ç»œå®‰å…¨ç­–ç•¥

#### 1. ç»Ÿä¸€ç½‘ç»œç­–ç•¥
```yaml
# å…¨å±€ç½‘ç»œç­–ç•¥æ¨¡æ¿
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: global-default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
spec:
  podSelector:
    matchLabels:
      app: monitoring
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9090
```

#### 2. æœåŠ¡ç½‘æ ¼å®‰å…¨
```yaml
# Istioå¤šé›†ç¾¤å®‰å…¨é…ç½®
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: cross-cluster-policy
  namespace: istio-system
spec:
  action: ALLOW
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/istio-system/sa/istio-reader-service-account"]
    to:
    - operation:
        ports: ["15012", "15017"]
```

## ğŸ’° æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### èµ„æºè°ƒåº¦ä¼˜åŒ–

#### 1. è·¨äº‘Spotå®ä¾‹åˆ©ç”¨
```yaml
# Spotå®ä¾‹èŠ‚ç‚¹ç»„é…ç½®
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: spot-cluster
  region: us-west-2
managedNodeGroups:
- name: spot-ng
  instanceTypes: ["m5.large", "m5.xlarge", "m5.2xlarge"]
  spot: true
  desiredCapacity: 10
  minSize: 5
  maxSize: 20
  labels:
    lifecycle: spot
  taints:
  - key: spot
    value: "true"
    effect: NoSchedule
```

#### 2. æ··åˆå®ä¾‹ç±»å‹è°ƒåº¦
```yaml
# æ··åˆå®ä¾‹è°ƒåº¦å™¨é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cost-optimized-app
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: node.kubernetes.io/instance-type
                operator: In
                values: ["t3.medium", "t3.large"]
          - weight: 50
            preference:
              matchExpressions:
              - key: lifecycle
                operator: In
                values: ["spot"]
```

### æˆæœ¬ç›‘æ§å‘Šè­¦

#### 1. å¤šäº‘æˆæœ¬èšåˆ
```yaml
# Prometheuså¤šäº‘æˆæœ¬æŒ‡æ ‡
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cross-cloud-cost-alerts
spec:
  groups:
  - name: cost.rules
    rules:
    - alert: HighCloudCost
      expr: sum by(cloud_provider) (rate(kube_node_status_capacity_cpu_cores[1h]) * 0.1) > 1000
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "High cloud computing costs detected"
        description: "{{ $labels.cloud_provider }} costs exceeded threshold"
```

#### 2. é¢„ç®—ç®¡ç†ç­–ç•¥
```yaml
# Kubernetesèµ„æºé…é¢ä¸é¢„ç®—è”åŠ¨
apiVersion: v1
kind: ResourceQuota
metadata:
  name: monthly-budget-quota
spec:
  hard:
    requests.cpu: "100"
    requests.memory: 200Gi
    limits.cpu: "200"
    limits.memory: 400Gi
    persistentvolumeclaims: "50"
  scopeSelector:
    matchExpressions:
    - scopeName: PriorityClass
      operator: In
      values: ["high-cost", "medium-cost"]
```

## ğŸ¯ æ•…éšœåˆ‡æ¢ç­–ç•¥

### è‡ªåŠ¨æ•…éšœæ£€æµ‹

#### 1. é›†ç¾¤å¥åº·æ£€æŸ¥
```yaml
# é›†ç¾¤å¥åº·æ¢é’ˆé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-health-check
data:
  health-check.sh: |
    #!/bin/bash
    set -e
    
    # æ£€æŸ¥API Serverå¯ç”¨æ€§
    kubectl get nodes --request-timeout=5s >/dev/null 2>&1 || exit 1
    
    # æ£€æŸ¥æ ¸å¿ƒç»„ä»¶çŠ¶æ€
    kubectl get pods -n kube-system -l tier=control-plane --no-headers | \
      grep -v Running && exit 1
    
    # æ£€æŸ¥èŠ‚ç‚¹å°±ç»ªçŠ¶æ€
    kubectl get nodes --no-headers | grep -v Ready && exit 1
    
    echo "Cluster is healthy"
```

#### 2. åº”ç”¨å¥åº·ç›‘æ§
```yaml
# åº”ç”¨çº§å¥åº·æ£€æŸ¥é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: health-monitored-app
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: myapp:latest
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
```

### æ‰‹åŠ¨/è‡ªåŠ¨åˆ‡æ¢

#### 1. DNSæ•…éšœåˆ‡æ¢
```yaml
# ExternalDNSé…ç½®æ”¯æŒå¤šé›†ç¾¤
apiVersion: externaldns.k8s.io/v1alpha1
kind: DNSEndpoint
metadata:
  name: multi-cluster-dns
spec:
  endpoints:
  - dnsName: app.example.com
    recordTTL: 300
    recordType: A
    targets:
    - 203.0.113.1  # Primary cluster IP
    - 203.0.113.2  # Secondary cluster IP
    providerSpecific:
    - name: weight
      value: "100"   # Primary weight
    - name: weight
      value: "0"     # Secondary weight (standby)
```

#### 2. æµé‡åˆ‡æ¢è„šæœ¬
```bash
#!/bin/bash
# å¤šé›†ç¾¤æµé‡åˆ‡æ¢è„šæœ¬

PRIMARY_CLUSTER="https://primary-api.example.com"
STANDBY_CLUSTER="https://standby-api.example.com"
SERVICE_NAME="my-service"

switch_to_standby() {
    echo "Switching traffic to standby cluster..."
    
    # æ›´æ–°DNSæƒé‡
    kubectl patch dnsendpoint multi-cluster-dns \
      -p '{"spec":{"endpoints":[{"dnsName":"app.example.com","recordTTL":300,"recordType":"A","targets":["203.0.113.1","203.0.113.2"],"providerSpecific":[{"name":"weight","value":"0"},{"name":"weight","value":"100"}]}]}}' \
      --type=merge
    
    # éªŒè¯åˆ‡æ¢
    sleep 30
    curl -f https://app.example.com/health || {
        echo "Health check failed after switch!"
        exit 1
    }
    
    echo "Traffic switched successfully to standby cluster"
}

# ä½¿ç”¨ç¤ºä¾‹
# switch_to_standby
```

## ğŸ“Š ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### ç»Ÿä¸€ç›‘æ§é¢æ¿

#### 1. å¤šé›†ç¾¤Prometheusè”é‚¦
```yaml
# Prometheusè”é‚¦é…ç½®
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

scrape_configs:
  - job_name: 'federate'
    scrape_interval: 15s
    honor_labels: true
    metrics_path: '/federate'
    params:
      'match[]':
        - '{job=~"kubernetes-.*"}'
        - '{__name__=~"job:.*"}'
    static_configs:
      - targets:
        - 'prometheus-primary:9090'
        - 'prometheus-secondary:9090'
        - 'prometheus-tertiary:9090'
```

#### 2. è·¨é›†ç¾¤Grafanaä»ªè¡¨æ¿
```json
{
  "dashboard": {
    "title": "Multi-Cluster Overview",
    "panels": [
      {
        "title": "Cluster Status Summary",
        "type": "stat",
        "targets": [
          {
            "expr": "count by(cluster) (up{job=\"kubernetes-nodes\"})",
            "legendFormat": "{{cluster}}"
          }
        ]
      },
      {
        "title": "Cross-Cluster Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "{{cluster}} - p95"
          }
        ]
      }
    ]
  }
}
```

### æ—¥å¿—é›†ä¸­æ”¶é›†

#### 1. å¤šäº‘æ—¥å¿—æ¶æ„
```yaml
# Fluentdå¤šé›†ç¾¤æ—¥å¿—æ”¶é›†
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluent.conf: |
    <source>
      @type forward
      port 24224
      bind 0.0.0.0
    </source>
    
    <filter kubernetes.**>
      @type kubernetes_metadata
      tag_to_kubernetes_name_regexp (?<pod_name>[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})
    </filter>
    
    <match **>
      @type elasticsearch
      host elasticsearch.logging.svc
      port 9200
      logstash_format true
      logstash_prefix ${record['kubernetes']['namespace']}-${ENV['CLUSTER_NAME']}
      include_tag_key true
      tag_key cluster
      flush_interval 10s
    </match>
```

## ğŸ”§ å®æ–½æœ€ä½³å®è·µ

### éƒ¨ç½²å‰æ£€æŸ¥æ¸…å•
- [ ] ç¡®å®šä¸šåŠ¡è¿ç»­æ€§è¦æ±‚(RTO/RPO)
- [ ] é€‰æ‹©åˆé€‚çš„å¤šäº‘æ¶æ„æ¨¡å¼
- [ ] è®¾è®¡ç½‘ç»œè¿é€šæ€§å’Œå®‰å…¨ç­–ç•¥
- [ ] åˆ¶å®šæ•°æ®åŒæ­¥å’Œå¤‡ä»½ç­–ç•¥
- [ ] å»ºç«‹ç»Ÿä¸€çš„èº«ä»½è®¤è¯ä½“ç³»
- [ ] é…ç½®ç›‘æ§å‘Šè­¦å’Œæ—¥å¿—æ”¶é›†
- [ ] åˆ¶å®šæ•…éšœåˆ‡æ¢å’Œæ¢å¤æµç¨‹

### è¿è¥ç»´æŠ¤è¦ç‚¹
- [ ] å®šæœŸè¿›è¡Œæ•…éšœåˆ‡æ¢æ¼”ç»ƒ
- [ ] ç›‘æ§å„äº‘å¹³å°çš„æˆæœ¬å˜åŒ–
- [ ] ä¿æŒå„é›†ç¾¤ç‰ˆæœ¬åŒæ­¥
- [ ] æ›´æ–°å®‰å…¨ç­–ç•¥å’Œè®¿é—®æ§åˆ¶
- [ ] ä¼˜åŒ–èµ„æºé…ç½®å’Œè°ƒåº¦ç­–ç•¥
- [ ] ç»´æŠ¤æ¶æ„æ–‡æ¡£å’Œæ“ä½œæ‰‹å†Œ

---

*æœ¬æ–‡æ¡£æä¾›å¤šäº‘æ··åˆéƒ¨ç½²çš„å…¨é¢æŒ‡å¯¼ï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºé«˜å¯ç”¨ã€å®‰å…¨çš„åˆ†å¸ƒå¼Kubernetesæ¶æ„*
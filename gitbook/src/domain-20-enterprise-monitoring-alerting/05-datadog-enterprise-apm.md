# Datadogä¼ä¸šçº§APMæ·±åº¦å®è·µ

> **ä½œè€…**: ä¼ä¸šçº§APMæ¶æ„ä¸“å®¶ | **ç‰ˆæœ¬**: v1.0 | **æ›´æ–°æ—¶é—´**: 2026-02-07
> **é€‚ç”¨åœºæ™¯**: ä¼ä¸šçº§åº”ç”¨æ€§èƒ½ç›‘æ§ | **å¤æ‚åº¦**: â­â­â­â­â­

## ğŸ¯ æ‘˜è¦

æœ¬æ–‡æ¡£æ·±å…¥æ¢è®¨Datadogä¼ä¸šçº§APM(Application Performance Monitoring)ç³»ç»Ÿçš„æ¶æ„è®¾è®¡ã€éƒ¨ç½²å®è·µå’Œè¿ç»´ç®¡ç†ï¼ŒåŸºäºå¤§è§„æ¨¡ä¼ä¸šç¯å¢ƒçš„å®è·µç»éªŒï¼Œæä¾›ä»åº”ç”¨æ€§èƒ½ç›‘æ§åˆ°ç”¨æˆ·ä½“éªŒä¼˜åŒ–çš„å®Œæ•´æŠ€æœ¯æŒ‡å—ï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºå…¨é¢çš„åº”ç”¨å¯è§‚æµ‹æ€§ä½“ç³»ã€‚

## 1. Datadog APMæ¶æ„æ·±åº¦è§£æ

### 1.1 æ ¸å¿ƒç»„ä»¶æ¶æ„

```mermaid
graph TB
    subgraph "æ•°æ®é‡‡é›†å±‚"
        A[Datadog Agent] --> B[Apm Agent]
        B --> C[Tracer Libraries]
        C --> D[Java/.NET/Python/Go]
        C --> E[JavaScript/Node.js]
        C --> F[Ruby/PHP]
        
        A --> G[Integrations]
        G --> H[Database Clients]
        G --> I[HTTP Clients]
        G --> J[Message Queues]
    end
    
    subgraph "å¤„ç†åˆ†æå±‚"
        K[Trace Processor] --> L[Ingest Pipeline]
        L --> M[Normalization]
        M --> N[Sampling Engine]
        N --> O[Aggregation]
        
        P[Analytics Processor] --> Q[Metrics Generator]
        Q --> R[Service Map Builder]
        R --> S[Dependency Graph]
    end
    
    subgraph "å­˜å‚¨æ£€ç´¢å±‚"
        T[Trace Storage] --> U[Hot Storage]
        T --> V[Cold Storage]
        U --> W[Elasticsearch]
        V --> X[S3/Object Store]
        
        Y[Index Service] --> Z[Query Engine]
        Z --> AA[APM UI/API]
    end
    
    subgraph "å¯è§†åŒ–åˆ†æå±‚"
        AB[Service Map] --> AC[Performance Dashboard]
        AD[Flame Graphs] --> AE[Waterfall View]
        AF[Anomaly Detection] --> AG[Root Cause Analysis]
        AH[User Experience] --> AI[Real User Monitoring]
    end
```

### 1.2 æŠ€æœ¯æ¶æ„ä¼˜åŠ¿

#### 1.2.1 åˆ†å¸ƒå¼è¿½è¸ªèƒ½åŠ›
- **è‡ªåŠ¨instrumentation**: æ”¯æŒä¸»æµè¯­è¨€æ¡†æ¶çš„è‡ªåŠ¨åŸ‹ç‚¹
- **æ‰‹åŠ¨instrumentation**: æä¾›çµæ´»çš„æ‰‹åŠ¨åŸ‹ç‚¹API
- **ä¸Šä¸‹æ–‡ä¼ æ’­**: æ”¯æŒè·¨è¿›ç¨‹ã€è·¨æœåŠ¡çš„trace contextä¼ é€’
- **é‡‡æ ·ç­–ç•¥**: æ™ºèƒ½é‡‡æ ·ç®—æ³•å¹³è¡¡æ€§èƒ½å’Œæ•°æ®å®Œæ•´æ€§

#### 1.2.2 å®æ—¶åˆ†æå¤„ç†
- **æµå¼å¤„ç†**: å®æ—¶å¤„ç†å’Œåˆ†ætraceæ•°æ®
- **å¼‚å¸¸æ£€æµ‹**: åŸºäºæœºå™¨å­¦ä¹ çš„æ€§èƒ½å¼‚å¸¸è‡ªåŠ¨è¯†åˆ«
- **æ ¹å› åˆ†æ**: è‡ªåŠ¨å…³è”åˆ†ææ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆæ ¹æº
- **è¶‹åŠ¿é¢„æµ‹**: åŸºäºå†å²æ•°æ®çš„æ€§èƒ½è¶‹åŠ¿é¢„æµ‹

## 2. ä¼ä¸šçº§éƒ¨ç½²æ¶æ„

### 2.1 é«˜å¯ç”¨éƒ¨ç½²æ–¹æ¡ˆ

#### 2.1.1 å¤šåŒºåŸŸéƒ¨ç½²æ¶æ„

```yaml
# datadog-apm-multiregion.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: datadog-apm

---
# åŒºåŸŸ1: ä¸»è¦æ•°æ®ä¸­å¿ƒ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: datadog-agent-primary
  namespace: datadog-apm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: datadog-agent
      region: primary
  template:
    metadata:
      labels:
        app: datadog-agent
        region: primary
    spec:
      containers:
      - name: datadog-agent
        image: datadog/agent:7
        env:
        - name: DD_API_KEY
          valueFrom:
            secretKeyRef:
              name: datadog-secrets
              key: api-key
        - name: DD_SITE
          value: "datadoghq.com"
        - name: DD_APM_ENABLED
          value: "true"
        - name: DD_APM_NON_LOCAL_TRAFFIC
          value: "true"
        ports:
        - containerPort: 8126
          name: traceport
        resources:
          requests:
            cpu: "500m"
            memory: "512Mi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
        volumeMounts:
        - name: dockersocket
          mountPath: /var/run/docker.sock
        - name: procdir
          mountPath: /host/proc
          readOnly: true
        - name: cgroups
          mountPath: /host/sys/fs/cgroup
          readOnly: true
      volumes:
      - hostPath:
          path: /var/run/docker.sock
        name: dockersocket
      - hostPath:
          path: /proc
        name: procdir
      - hostPath:
          path: /sys/fs/cgroup
        name: cgroups

---
# åŒºåŸŸ2: å¤‡ä»½æ•°æ®ä¸­å¿ƒ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: datadog-agent-secondary
  namespace: datadog-apm
spec:
  replicas: 2
  selector:
    matchLabels:
      app: datadog-agent
      region: secondary
  template:
    metadata:
      labels:
        app: datadog-agent
        region: secondary
    spec:
      containers:
      - name: datadog-agent
        image: datadog/agent:7
        env:
        - name: DD_API_KEY
          valueFrom:
            secretKeyRef:
              name: datadog-secrets
              key: api-key
        - name: DD_SITE
          value: "datadoghq.com"
        - name: DD_APM_ENABLED
          value: "true"
        - name: DD_APM_NON_LOCAL_TRAFFIC
          value: "true"
        ports:
        - containerPort: 8126
          name: traceport
        resources:
          requests:
            cpu: "300m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
```

#### 2.1.2 è´Ÿè½½å‡è¡¡é…ç½®

```yaml
# apm-loadbalancer.yaml
apiVersion: v1
kind: Service
metadata:
  name: datadog-apm-lb
  namespace: datadog-apm
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  type: LoadBalancer
  ports:
  - name: apm-trace
    port: 8126
    targetPort: 8126
    protocol: TCP
  selector:
    app: datadog-agent
  loadBalancerSourceRanges:
  - "10.0.0.0/8"
  - "172.16.0.0/12"
  - "192.168.0.0/16"
```

### 2.2 å®‰å…¨åŠ å›ºé…ç½®

#### 2.2.1 ç½‘ç»œå®‰å…¨ç­–ç•¥

```yaml
# apm-network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: datadog-apm-policy
  namespace: datadog-apm
spec:
  podSelector:
    matchLabels:
      app: datadog-agent
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # å…è®¸åº”ç”¨Podå‘é€traceæ•°æ®
  - from:
    - namespaceSelector:
        matchLabels:
          purpose: application
    ports:
    - protocol: TCP
      port: 8126
  # å…è®¸å†…éƒ¨é€šä¿¡
  - from:
    - podSelector:
        matchLabels:
          app: datadog-agent
    ports:
    - protocol: TCP
      port: 5000
      port: 5001
  egress:
  # å…è®¸è®¿é—®Datadogåç«¯
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
    ports:
    - protocol: TCP
      port: 443
  # å…è®¸DNSæŸ¥è¯¢
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
```

#### 2.2.2 è®¤è¯æˆæƒé…ç½®

```yaml
# apm-security-config.yaml
apiVersion: v1
kind: Secret
metadata:
  name: datadog-apm-secrets
  namespace: datadog-apm
type: Opaque
data:
  api-key: <base64-encoded-api-key>
  app-key: <base64-encoded-app-key>

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: datadog-apm-config
  namespace: datadog-apm
data:
  datadog.yaml: |
    # APMé…ç½®
    apm_config:
      enabled: true
      receiver_port: 8126
      # å®‰å…¨é…ç½®
      receiver_timeout: 30
      max_connections: 1000
      max_payload_size: 50MB
      
      # é‡‡æ ·é…ç½®
      max_traces_per_second: 10
      ignore_resources: 
        - "GET /health"
        - "POST /metrics"
      
      # æ•°æ®å¤„ç†
      bucket_size_seconds: 10
      extra_sample_rate: 1.0
      max_events_per_second: 200
      
    # å®‰å…¨æ—¥å¿—
    logs:
      enabled: true
      log_level: INFO
      logs_config:
        use_http: true
        send_logs: true
        
    # å®‰å…¨è®¾ç½®
    security_agent:
      enabled: true
      runtime_security_config:
        enabled: true
        fim_enabled: true
```

## 3. ä¼ä¸šçº§ç›‘æ§ç­–ç•¥

### 3.1 æœåŠ¡çº§åˆ«æŒ‡æ ‡(SLI/SLO)

#### 3.1.1 æ ¸å¿ƒSLIå®šä¹‰

```yaml
# sli-slo-definition.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: apm-sli-slo-rules
  namespace: monitoring
spec:
  groups:
  - name: apm.sli.rules
    rules:
    # å“åº”æ—¶é—´SLI
    - record: apm_service_response_time_sli
      expr: |
        histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)) <= 0.5
    
    # é”™è¯¯ç‡SLI
    - record: apm_service_error_rate_sli
      expr: |
        sum(rate(http_requests_total{status=~"5.."}[5m])) by (service) / 
        sum(rate(http_requests_total[5m])) by (service) <= 0.01
    
    # å¯ç”¨æ€§SLI
    - record: apm_service_availability_sli
      expr: |
        (sum(rate(http_requests_total[5m])) by (service) - 
         sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)) / 
        sum(rate(http_requests_total[5m])) by (service) >= 0.999
    
    # ååé‡SLI
    - record: apm_service_throughput_sli
      expr: |
        sum(rate(http_requests_total[5m])) by (service) >= 100
```

#### 3.1.2 SLOå‘Šè­¦é…ç½®

```yaml
# slo-alerting.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: apm-slo-alerts
  namespace: monitoring
spec:
  groups:
  - name: apm.slo.alerts
    rules:
    # å“åº”æ—¶é—´SLOè¿è§„
    - alert: APMSlowResponseTime
      expr: |
        apm_service_response_time_sli < 0.95
      for: 5m
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "æœåŠ¡ {{ $labels.service }} å“åº”æ—¶é—´SLOè¿è§„"
        description: "95thç™¾åˆ†ä½å“åº”æ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼Œå½“å‰å€¼: {{ $value }}"
    
    # é”™è¯¯ç‡SLOè¿è§„
    - alert: APMHighErrorRate
      expr: |
        apm_service_error_rate_sli > 0.01
      for: 2m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "æœåŠ¡ {{ $labels.service }} é”™è¯¯ç‡SLOè¿è§„"
        description: "é”™è¯¯ç‡è¶…è¿‡é˜ˆå€¼ï¼Œå½“å‰å€¼: {{ $value }}"
    
    # å¯ç”¨æ€§SLOè¿è§„
    - alert: APMLowAvailability
      expr: |
        apm_service_availability_sli < 0.999
      for: 10m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "æœåŠ¡ {{ $labels.service }} å¯ç”¨æ€§SLOè¿è§„"
        description: "æœåŠ¡å¯ç”¨æ€§ä½äºç›®æ ‡å€¼ï¼Œå½“å‰å€¼: {{ $value }}"
```

### 3.2 æ™ºèƒ½å‘Šè­¦ç­–ç•¥

#### 3.2.1 å¼‚å¸¸æ£€æµ‹å‘Šè­¦

```python
# anomaly_detection.py
import numpy as np
from sklearn.ensemble import IsolationForest
from prometheus_api_client import PrometheusConnect
import json
import time

class APMAnomalyDetector:
    def __init__(self, prometheus_url):
        self.prom = PrometheusConnect(url=prometheus_url, disable_ssl=True)
        self.model = IsolationForest(contamination=0.1, random_state=42)
        self.baseline_data = {}
        
    def collect_baseline_metrics(self, service_name, duration="24h"):
        """æ”¶é›†åŸºçº¿æŒ‡æ ‡æ•°æ®"""
        queries = {
            'response_time': f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{service="{service_name}"}}[5m]))',
            'error_rate': f'sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[5m])) / sum(rate(http_requests_total{{service="{service_name}"}}[5m]))',
            'throughput': f'sum(rate(http_requests_total{{service="{service_name}"}}[5m]))'
        }
        
        baseline_data = {}
        for metric_name, query in queries.items():
            result = self.prom.custom_query(query=query)
            if result:
                values = [float(item[1]) for item in result[0]['values']]
                baseline_data[metric_name] = np.array(values)
                
        self.baseline_data[service_name] = baseline_data
        return baseline_data
    
    def detect_anomalies(self, service_name, current_metrics):
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        if service_name not in self.baseline_data:
            self.collect_baseline_metrics(service_name)
            
        anomalies = {}
        baseline = self.baseline_data[service_name]
        
        for metric_name, current_value in current_metrics.items():
            if metric_name in baseline:
                # ä½¿ç”¨å­¤ç«‹æ£®æ—ç®—æ³•æ£€æµ‹å¼‚å¸¸
                combined_data = np.concatenate([baseline[metric_name], [current_value]])
                predictions = self.model.fit_predict(combined_data.reshape(-1, 1))
                
                # æœ€æ–°æ•°æ®ç‚¹æ˜¯å¦ä¸ºå¼‚å¸¸
                is_anomaly = predictions[-1] == -1
                anomaly_score = self.model.decision_function(combined_data.reshape(-1, 1))[-1]
                
                anomalies[metric_name] = {
                    'is_anomaly': is_anomaly,
                    'score': anomaly_score,
                    'current_value': current_value,
                    'baseline_mean': np.mean(baseline[metric_name]),
                    'baseline_std': np.std(baseline[metric_name])
                }
                
        return anomalies
    
    def generate_alerts(self, service_name, anomalies):
        """ç”Ÿæˆå‘Šè­¦"""
        alerts = []
        for metric_name, anomaly_info in anomalies.items():
            if anomaly_info['is_anomaly']:
                alert = {
                    'alertname': f'APMAnomaly_{metric_name.title()}',
                    'service': service_name,
                    'severity': 'warning' if abs(anomaly_info['score']) < 0.5 else 'critical',
                    'summary': f'{metric_name}å‡ºç°å¼‚å¸¸è¡Œä¸º',
                    'description': f'å½“å‰å€¼: {anomaly_info["current_value"]:.4f}, '
                                 f'åŸºçº¿å‡å€¼: {anomaly_info["baseline_mean"]:.4f}, '
                                 f'å¼‚å¸¸åˆ†æ•°: {anomaly_info["score"]:.4f}'
                }
                alerts.append(alert)
                
        return alerts

# ä½¿ç”¨ç¤ºä¾‹
detector = APMAnomalyDetector("http://prometheus:9090")
anomalies = detector.detect_anomalies("user-service", {
    'response_time': 2.5,
    'error_rate': 0.05,
    'throughput': 1500
})
alerts = detector.generate_alerts("user-service", anomalies)
```

## 4. æ€§èƒ½ä¼˜åŒ–å®è·µ

### 4.1 é‡‡æ ·ç­–ç•¥ä¼˜åŒ–

#### 4.1.1 æ™ºèƒ½é‡‡æ ·é…ç½®

```yaml
# intelligent-sampling.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: datadog-apm-sampling
  namespace: datadog-apm
data:
  sampling-rules.json: |
    {
      "rules": [
        {
          "name": "high_priority_services",
          "service": "(payment|auth|user)-service",
          "sample_rate": 1.0,
          "priority": "high"
        },
        {
          "name": "error_sampling",
          "sample_rate": 1.0,
          "priority": "high",
          "conditions": [
            {
              "metric": "error.rate",
              "operator": ">",
              "value": 0.01
            }
          ]
        },
        {
          "name": "slow_request_sampling",
          "sample_rate": 1.0,
          "priority": "medium",
          "conditions": [
            {
              "metric": "duration",
              "operator": ">",
              "value": 1000
            }
          ]
        },
        {
          "name": "default_sampling",
          "sample_rate": 0.1,
          "priority": "low"
        }
      ],
      "default_sample_rate": 0.1
    }
```

#### 4.1.2 åŠ¨æ€é‡‡æ ·è°ƒæ•´

```python
# dynamic_sampling.py
import time
import threading
from typing import Dict, List
import requests

class DynamicSampler:
    def __init__(self, config_endpoint: str):
        self.config_endpoint = config_endpoint
        self.current_rates: Dict[str, float] = {}
        self.metrics_cache: Dict[str, List[float]] = {}
        self.update_interval = 300  # 5åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡
        
    def start_auto_adjustment(self):
        """å¯åŠ¨è‡ªåŠ¨é‡‡æ ·ç‡è°ƒæ•´"""
        def adjustment_loop():
            while True:
                try:
                    self._adjust_sampling_rates()
                    time.sleep(self.update_interval)
                except Exception as e:
                    print(f"é‡‡æ ·ç‡è°ƒæ•´å¤±è´¥: {e}")
                    
        thread = threading.Thread(target=adjustment_loop, daemon=True)
        thread.start()
        
    def _adjust_sampling_rates(self):
        """æ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è°ƒæ•´é‡‡æ ·ç‡"""
        # è·å–å½“å‰ç³»ç»ŸæŒ‡æ ‡
        metrics = self._collect_system_metrics()
        
        # è®¡ç®—æ–°çš„é‡‡æ ·ç‡
        new_rates = {}
        
        # CPUä½¿ç”¨ç‡è¿‡é«˜æ—¶é™ä½é‡‡æ ·ç‡
        if metrics['cpu_usage'] > 80:
            new_rates['default'] = max(0.05, self.current_rates.get('default', 0.1) * 0.5)
        elif metrics['cpu_usage'] < 30:
            new_rates['default'] = min(0.2, self.current_rates.get('default', 0.1) * 1.5)
            
        # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜æ—¶é™ä½é‡‡æ ·ç‡
        if metrics['memory_usage'] > 85:
            for service in ['default', 'high_priority']:
                current_rate = self.current_rates.get(service, 0.1)
                new_rates[service] = max(0.02, current_rate * 0.3)
                
        # æ›´æ–°é‡‡æ ·ç‡é…ç½®
        self._update_sampling_config(new_rates)
        
    def _collect_system_metrics(self) -> Dict[str, float]:
        """æ”¶é›†ç³»ç»Ÿèµ„æºä½¿ç”¨æŒ‡æ ‡"""
        # æ¨¡æ‹Ÿè·å–æŒ‡æ ‡æ•°æ®
        return {
            'cpu_usage': 65.2,
            'memory_usage': 72.8,
            'disk_usage': 45.1,
            'network_io': 1250.5
        }
        
    def _update_sampling_config(self, new_rates: Dict[str, float]):
        """æ›´æ–°é‡‡æ ·ç‡é…ç½®"""
        self.current_rates.update(new_rates)
        
        # å‘é€åˆ°é…ç½®ä¸­å¿ƒ
        config_data = {
            'sampling_rates': self.current_rates,
            'timestamp': time.time(),
            'version': 'dynamic_' + str(int(time.time()))
        }
        
        try:
            response = requests.post(
                f"{self.config_endpoint}/api/v1/sampling/config",
                json=config_data,
                timeout=10
            )
            if response.status_code == 200:
                print(f"é‡‡æ ·ç‡é…ç½®æ›´æ–°æˆåŠŸ: {new_rates}")
        except Exception as e:
            print(f"é…ç½®æ›´æ–°å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
sampler = DynamicSampler("http://config-server:8080")
sampler.start_auto_adjustment()
```

### 4.2 æ•°æ®å­˜å‚¨ä¼˜åŒ–

#### 4.2.1 åˆ†å±‚å­˜å‚¨ç­–ç•¥

```yaml
# tiered-storage.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: datadog-apm-storage
  namespace: datadog-apm
data:
  storage-config.yaml: |
    # åˆ†å±‚å­˜å‚¨é…ç½®
    storage:
      # çƒ­æ•°æ®å­˜å‚¨ (æœ€è¿‘7å¤©)
      hot_storage:
        type: elasticsearch
        retention_days: 7
        index_pattern: "apm-hot-*"
        replicas: 2
        shards: 10
        
      # æ¸©æ•°æ®å­˜å‚¨ (8-30å¤©)
      warm_storage:
        type: opensearch
        retention_days: 30
        index_pattern: "apm-warm-*"
        replicas: 1
        shards: 5
        
      # å†·æ•°æ®å­˜å‚¨ (31å¤©ä»¥ä¸Š)
      cold_storage:
        type: s3
        retention_days: 365
        bucket: "company-apm-archive"
        compression: gzip
        
      # å½’æ¡£å­˜å‚¨ (é•¿æœŸä¿å­˜)
      archive_storage:
        type: glacier
        retention_days: 3650
        vault: "apm-long-term-archive"
        
    # æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
    lifecycle:
      hot_to_warm_days: 7
      warm_to_cold_days: 30
      cold_to_archive_days: 365
      
    # å­˜å‚¨ä¼˜åŒ–
    optimization:
      indexing_strategy: "time-series"
      compression_level: "high"
      data_rollup:
        enabled: true
        intervals: ["1h", "1d", "7d"]
```

## 5. ä¼ä¸šçº§æœ€ä½³å®è·µ

### 5.1 æ ‡ç­¾å’Œå…ƒæ•°æ®ç®¡ç†

#### 5.1.1 ç»Ÿä¸€æ ‡ç­¾ç­–ç•¥

```yaml
# tagging-strategy.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-tagging-strategy
  namespace: datadog-apm
data:
  tags.yaml: |
    # ç»Ÿä¸€æ ‡ç­¾å‘½åè§„èŒƒ
    tagging_standards:
      # ä¸šåŠ¡æ ‡ç­¾
      business:
        - "team:{team_name}"
        - "product:{product_name}"
        - "environment:{env}"
        - "version:{app_version}"
        - "region:{region}"
        
      # æŠ€æœ¯æ ‡ç­¾
      technical:
        - "service:{service_name}"
        - "namespace:{k8s_namespace}"
        - "pod:{pod_name}"
        - "node:{node_name}"
        - "container:{container_name}"
        
      # è¿ç»´æ ‡ç­¾
      operational:
        - "owner:{owner_email}"
        - "tier:{tier_level}"
        - "criticality:{criticality}"
        - "sla:{sla_level}"
        
      # å®‰å…¨æ ‡ç­¾
      security:
        - "data_classification:{classification}"
        - "pii_data:{has_pii}"
        - "pci_compliant:{pci_status}"
        
    # æ ‡ç­¾ç»§æ‰¿è§„åˆ™
    inheritance_rules:
      service:
        inherits_from: ["namespace", "team"]
        required: true
        
      environment:
        inherits_from: ["cluster", "region"]
        required: true
        
      owner:
        inherits_from: ["team"]
        required: true
```

### 5.2 æˆæœ¬ä¼˜åŒ–ç­–ç•¥

#### 5.2.1 èµ„æºé…é¢ç®¡ç†

```yaml
# resource-quota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: datadog-apm-quota
  namespace: datadog-apm
spec:
  hard:
    requests.cpu: "4"
    requests.memory: "8Gi"
    limits.cpu: "8"
    limits.memory: "16Gi"
    persistentvolumeclaims: "10"
    requests.storage: "100Gi"

---
apiVersion: v1
kind: LimitRange
metadata:
  name: datadog-apm-limits
  namespace: datadog-apm
spec:
  limits:
  - default:
      cpu: "1"
      memory: "2Gi"
    defaultRequest:
      cpu: "500m"
      memory: "1Gi"
    type: Container
    max:
      cpu: "2"
      memory: "4Gi"
    min:
      cpu: "100m"
      memory: "256Mi"
```

#### 5.2.2 æˆæœ¬ç›‘æ§å‘Šè­¦

```yaml
# cost-monitoring.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: apm-cost-alerts
  namespace: monitoring
spec:
  groups:
  - name: apm.cost.monitoring
    rules:
    # Datadogè´¹ç”¨ç›‘æ§
    - record: datadog_monthly_cost
      expr: |
        sum(increase(datadog_host_hours_total[30d])) * 15 +
        sum(increase(datadog_apm_trace_bytes_total[30d]) / 1073741824) * 0.10 +
        sum(increase(datadog_log_ingested_bytes_total[30d]) / 1073741824) * 0.10
    
    # è´¹ç”¨è¶…æ ‡å‘Šè­¦
    - alert: DatadogCostOverBudget
      expr: |
        datadog_monthly_cost > 10000
      for: 1h
      labels:
        severity: warning
        team: finance
      annotations:
        summary: "Datadogæœˆåº¦è´¹ç”¨è¶…å‡ºé¢„ç®—"
        description: "å½“å‰æœˆåº¦è´¹ç”¨: {{ $value }} USDï¼Œé¢„ç®—ä¸Šé™: 10000 USD"
    
    # èµ„æºåˆ©ç”¨ç‡ä½å‘Šè­¦
    - alert: DatadogLowResourceUtilization
      expr: |
        avg(kube_pod_container_resource_requests{namespace="datadog-apm"} / 
            kube_pod_container_resource_limits{namespace="datadog-apm"}) < 0.3
      for: 24h
      labels:
        severity: info
        team: sre
      annotations:
        summary: "Datadog APMèµ„æºåˆ©ç”¨ç‡åä½"
        description: "å¹³å‡èµ„æºåˆ©ç”¨ç‡: {{ $value }}, å»ºè®®ä¼˜åŒ–èµ„æºé…ç½®"
```

## 6. æ•…éšœæ’æŸ¥ä¸è¯Šæ–­

### 6.1 å¸¸è§é—®é¢˜è¯Šæ–­

#### 6.1.1 Traceæ•°æ®ä¸¢å¤±æ’æŸ¥

```bash
#!/bin/bash
# trace_loss_diagnosis.sh

echo "=== Datadog APM Traceæ•°æ®ä¸¢å¤±è¯Šæ–­ ==="

# 1. æ£€æŸ¥AgentçŠ¶æ€
echo "1. æ£€æŸ¥Datadog AgentçŠ¶æ€:"
kubectl get pods -n datadog-apm -l app=datadog-agent

# 2. æ£€æŸ¥Agentæ—¥å¿—
echo "2. æ£€æŸ¥Agenté”™è¯¯æ—¥å¿—:"
kubectl logs -n datadog-apm -l app=datadog-agent --tail=100 | grep -i error

# 3. æ£€æŸ¥ç½‘ç»œè¿æ¥
echo "3. æ£€æŸ¥ç½‘ç»œè¿æ¥åˆ°Datadog:"
kubectl exec -n datadog-apm -l app=datadog-agent -- \
  curl -sv https://trace.agent.datadoghq.com/api/v0.2/traces 2>&1 | head -20

# 4. æ£€æŸ¥é‡‡æ ·é…ç½®
echo "4. æ£€æŸ¥é‡‡æ ·é…ç½®:"
kubectl exec -n datadog-apm -l app=datadog-agent -- \
  cat /etc/datadog-agent/datadog.yaml | grep -A 10 "apm_config"

# 5. æ£€æŸ¥åº”ç”¨åŸ‹ç‚¹çŠ¶æ€
echo "5. æ£€æŸ¥åº”ç”¨TracerçŠ¶æ€:"
for pod in $(kubectl get pods -n application -l app=myapp -o name); do
  echo "æ£€æŸ¥Pod: $pod"
  kubectl exec -n application $pod -- ps aux | grep dd-trace
done

# 6. æ£€æŸ¥æŒ‡æ ‡æ•°æ®
echo "6. æ£€æŸ¥APMæŒ‡æ ‡æ•°æ®:"
curl -s "http://prometheus:9090/api/v1/query?query=rate(datadog_trace_processed_total[5m])" | jq '.'
```

#### 6.1.2 æ€§èƒ½ç“¶é¢ˆåˆ†æ

```python
# performance_bottleneck_analyzer.py
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import seaborn as sns

class PerformanceBottleneckAnalyzer:
    def __init__(self, prometheus_url):
        self.prom_url = prometheus_url
        self.metrics_data = {}
        
    def collect_performance_data(self, service_name, duration_hours=24):
        """æ”¶é›†æ€§èƒ½æ•°æ®"""
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=duration_hours)
        
        queries = {
            'response_time': f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{service="{service_name}"}}[5m]))',
            'throughput': f'sum(rate(http_requests_total{{service="{service_name}"}}[5m]))',
            'error_rate': f'sum(rate(http_requests_total{{service="{service_name}",status=~"5.."}}[5m])) / sum(rate(http_requests_total{{service="{service_name}"}}[5m]))',
            'cpu_usage': f'rate(container_cpu_usage_seconds_total{{container!="POD",container!="",namespace="application",pod=~".*{service_name}.*"}}[5m])',
            'memory_usage': f'container_memory_working_set_bytes{{container!="POD",container!="",namespace="application",pod=~".*{service_name}.*"}}'
        }
        
        for metric_name, query in queries.items():
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„Prometheus API
            # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            timestamps = pd.date_range(start_time, end_time, freq='5min')
            if metric_name == 'response_time':
                values = [0.1 + 0.3 * (1 + np.sin(i/10)) for i in range(len(timestamps))]
            elif metric_name == 'throughput':
                values = [100 + 50 * np.random.random() for _ in range(len(timestamps))]
            elif metric_name == 'error_rate':
                values = [0.001 + 0.02 * np.random.random() for _ in range(len(timestamps))]
            else:
                values = [50 + 30 * np.random.random() for _ in range(len(timestamps))]
                
            self.metrics_data[metric_name] = pd.DataFrame({
                'timestamp': timestamps,
                'value': values
            })
    
    def identify_bottlenecks(self):
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # åˆ†æå“åº”æ—¶é—´å¼‚å¸¸
        rt_data = self.metrics_data['response_time']
        rt_threshold = rt_data['value'].quantile(0.95) * 1.5
        slow_periods = rt_data[rt_data['value'] > rt_threshold]
        
        if not slow_periods.empty:
            bottlenecks.append({
                'type': 'å“åº”æ—¶é—´ç“¶é¢ˆ',
                'severity': 'high',
                'periods': len(slow_periods),
                'avg_slow_time': slow_periods['value'].mean(),
                'recommendation': 'æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½å’Œå¤–éƒ¨æœåŠ¡è°ƒç”¨'
            })
        
        # åˆ†æCPUä½¿ç”¨ç‡å¼‚å¸¸
        cpu_data = self.metrics_data['cpu_usage']
        cpu_high = cpu_data[cpu_data['value'] > 80]
        
        if not cpu_high.empty:
            bottlenecks.append({
                'type': 'CPUèµ„æºç“¶é¢ˆ',
                'severity': 'medium',
                'periods': len(cpu_high),
                'avg_usage': cpu_high['value'].mean(),
                'recommendation': 'è€ƒè™‘æ°´å¹³æ‰©å±•æˆ–ä¼˜åŒ–ä»£ç é€»è¾‘'
            })
            
        # åˆ†æå†…å­˜ä½¿ç”¨å¼‚å¸¸
        mem_data = self.metrics_data['memory_usage']
        mem_gb = mem_data['value'] / (1024**3)
        mem_high = mem_gb[mem_gb > 1.5]
        
        if not mem_high.empty:
            bottlenecks.append({
                'type': 'å†…å­˜èµ„æºç“¶é¢ˆ',
                'severity': 'medium',
                'periods': len(mem_high),
                'avg_usage_gb': mem_high.mean(),
                'recommendation': 'æ£€æŸ¥å†…å­˜æ³„æ¼å’Œå¯¹è±¡æ± é…ç½®'
            })
            
        return bottlenecks
    
    def generate_report(self, service_name):
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        bottlenecks = self.identify_bottlenecks()
        
        report = f"""
# {service_name} æ€§èƒ½ç“¶é¢ˆåˆ†ææŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## å‘ç°çš„ç“¶é¢ˆé—®é¢˜:

"""
        
        for i, bottleneck in enumerate(bottlenecks, 1):
            severity_icon = "ğŸ”´" if bottleneck['severity'] == 'high' else "ğŸŸ¡"
            report += f"""{i}. {severity_icon} {bottleneck['type']}
   - ä¸¥é‡ç¨‹åº¦: {bottleneck['severity']}
   - å½±å“æ—¶æ®µ: {bottleneck['periods']}ä¸ªæ—¶é—´æ®µ
   - å¹³å‡å€¼: {bottleneck['avg_slow_time']:.3f}s
   - å»ºè®®æªæ–½: {bottleneck['recommendation']}

"""
        
        if not bottlenecks:
            report += "âœ… æœªå‘ç°æ˜æ˜¾æ€§èƒ½ç“¶é¢ˆ\n"
            
        report += """
## ä¼˜åŒ–å»ºè®®:

1. å®æ–½ç¼“å­˜ç­–ç•¥å‡å°‘é‡å¤è®¡ç®—
2. ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢è¯­å¥å’Œç´¢å¼•
3. è€ƒè™‘å¼‚æ­¥å¤„ç†éå…³é”®ä¸šåŠ¡é€»è¾‘
4. å®æ–½è¿æ¥æ± å’Œèµ„æºå¤ç”¨
5. å®šæœŸè¿›è¡Œæ€§èƒ½å‹æµ‹å’Œè°ƒä¼˜
"""
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
analyzer = PerformanceBottleneckAnalyzer("http://prometheus:9090")
analyzer.collect_performance_data("user-service", 24)
report = analyzer.generate_report("user-service")
print(report)
```

## 7. æœªæ¥å‘å±•ä¸æ¼”è¿›

### 7.1 æŠ€æœ¯å‘å±•è¶‹åŠ¿

#### 7.1.1 AIé©±åŠ¨çš„æ™ºèƒ½ç›‘æ§

- **è‡ªé€‚åº”é˜ˆå€¼**: åŸºäºæœºå™¨å­¦ä¹ çš„åŠ¨æ€é˜ˆå€¼è®¾ç½®
- **é¢„æµ‹æ€§ç»´æŠ¤**: åŸºäºå†å²æ•°æ®çš„æ•…éšœé¢„æµ‹
- **è‡ªåŠ¨åŒ–æ ¹å› åˆ†æ**: AIè¾…åŠ©çš„æ•…éšœæ ¹å› å¿«é€Ÿå®šä½
- **æ™ºèƒ½å‘Šè­¦æŠ‘åˆ¶**: å‡å°‘å‘Šè­¦å™ªéŸ³çš„æ™ºèƒ½ç®—æ³•

#### 7.1.2 äº‘åŸç”Ÿæ·±åº¦é›†æˆ

- **Service Meshé›†æˆ**: ä¸Istioã€Linkerdç­‰æœåŠ¡ç½‘æ ¼æ·±åº¦é›†æˆ
- **Serverlessç›‘æ§**: æ”¯æŒå‡½æ•°å³æœåŠ¡çš„ç»†ç²’åº¦ç›‘æ§
- **è¾¹ç¼˜è®¡ç®—ç›‘æ§**: åˆ†å¸ƒå¼è¾¹ç¼˜èŠ‚ç‚¹çš„ç»Ÿä¸€ç›‘æ§
- **å¤šäº‘ç¯å¢ƒæ”¯æŒ**: è·¨äº‘å¹³å°çš„ä¸€è‡´ç›‘æ§ä½“éªŒ

é€šè¿‡ä»¥ä¸Šä¼ä¸šçº§APMæ·±åº¦å®è·µï¼Œä¼ä¸šå¯ä»¥æ„å»ºå…¨é¢çš„åº”ç”¨æ€§èƒ½ç›‘æ§ä½“ç³»ï¼Œå®ç°ä»è¢«åŠ¨å“åº”åˆ°ä¸»åŠ¨é¢„é˜²çš„è¿ç»´æ¨¡å¼è½¬å˜ï¼Œæ˜¾è‘—æå‡åº”ç”¨è´¨é‡å’Œç”¨æˆ·ä½“éªŒã€‚
# Redis Enterprise Cache Operations æ·±åº¦å®žè·µ

> **Author**: Cache Platform Architect | **Version**: v1.0 | **Update Time**: 2026-02-07
> **Scenario**: Enterprise-grade Redis cache operations and high availability management | **Complexity**: â­â­â­â­

## ðŸŽ¯ Abstract

This document provides comprehensive exploration of Redis enterprise deployment architecture, cache management strategies, and high availability implementation. Based on large-scale production environment experience, it offers complete technical guidance from cluster setup to advanced persistence and performance tuning, helping enterprises build ultra-fast, reliable caching platforms with integrated monitoring, automatic failover, and disaster recovery capabilities for mission-critical applications.

## 1. Redis Enterprise Architecture

### 1.1 Core Component Architecture

```mermaid
graph TB
    subgraph "Redis Infrastructure"
        A[Redis Instances]
        B[Master-Slave Replication]
        C[Redis Cluster]
        D[Sentinel Monitoring]
        E[Proxy Layer]
    end
    
    subgraph "Data Management"
        F[Data Persistence]
        G[Memory Management]
        H[Eviction Policies]
        I[Data Partitioning]
        J[Cache Warming]
    end
    
    subgraph "High Availability"
        K[Automatic Failover]
        L[Load Balancing]
        M[Health Monitoring]
        N[Backup & Restore]
        O[Disaster Recovery]
    end
    
    subgraph "Security & Access Control"
        P[Authentication]
        Q[Authorization]
        R[Encryption]
        S[Network Security]
        T[Audit Logging]
    end
    
    subgraph "Performance Optimization"
        U[Memory Optimization]
        V[Connection Pooling]
        W[Pipeline Operations]
        X[Batch Processing]
        Y[Compression]
    end
    
    subgraph "Monitoring & Operations"
        Z[Performance Metrics]
        AA[Alerting System]
        AB[Log Management]
        AC[Capacity Planning]
        AD[Troubleshooting]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    
    F --> G
    G --> H
    H --> I
    I --> J
    
    K --> L
    L --> M
    M --> N
    N --> O
    
    P --> Q
    Q --> R
    R --> S
    S --> T
    
    U --> V
    V --> W
    W --> X
    X --> Y
    
    Z --> AA
    AA --> AB
    AB --> AC
    AC --> AD
```

### 1.2 Enterprise Deployment Architecture

```yaml
redis_enterprise_deployment:
  cluster_configuration:
    production_cluster:
      name: "redis-prod-cluster"
      port: 6379
      nodes:
        - host: "redis-node-0.redis-svc.production.svc.cluster.local"
          port: 6379
          role: "master"
          memory: "8GB"
          maxmemory: "6GB"
          maxmemory_policy: "allkeys-lru"
        
        - host: "redis-node-1.redis-svc.production.svc.cluster.local"
          port: 6379
          role: "slave"
          memory: "8GB"
          replication:
            master_host: "redis-node-0"
            master_port: 6379
        
        - host: "redis-node-2.redis-svc.production.svc.cluster.local"
          port: 6379
          role: "master"
          memory: "8GB"
          maxmemory: "6GB"
          maxmemory_policy: "allkeys-lfu"
      
      cluster_slots: 16384
      hash_tags: "{}"
      cluster_require_full_coverage: false
  
  sentinel_configuration:
    sentinels:
      - host: "redis-sentinel-0.redis-sentinel-svc.production.svc.cluster.local"
        port: 26379
        quorum: 2
        down_after_milliseconds: 5000
        failover_timeout: 10000
      
      - host: "redis-sentinel-1.redis-sentinel-svc.production.svc.cluster.local"
        port: 26379
        quorum: 2
        down_after_milliseconds: 5000
        failover_timeout: 10000
      
      - host: "redis-sentinel-2.redis-sentinel-svc.production.svc.cluster.local"
        port: 26379
        quorum: 2
        down_after_milliseconds: 5000
        failover_timeout: 10000
  
  persistence_configuration:
    rdb:
      save_intervals:
        - "900 1"    # 15åˆ†é’Ÿå†…è‡³å°‘1ä¸ªkeyå˜åŒ–
        - "300 10"   # 5åˆ†é’Ÿå†…è‡³å°‘10ä¸ªkeyå˜åŒ–
        - "60 10000" # 1åˆ†é’Ÿå†…è‡³å°‘10000ä¸ªkeyå˜åŒ–
      compression: "yes"
      checksum: "yes"
    
    aof:
      enabled: "yes"
      filename: "appendonly.aof"
      fsync: "everysec"
      auto_aof_rewrite_percentage: 100
      auto_aof_rewrite_min_size: "64mb"
  
  security_configuration:
    authentication:
      requirepass: "super_secure_redis_password_2023"
      masterauth: "super_secure_redis_password_2023"
    
    tls_ssl:
      tls_port: 6380
      tls_cert_file: "/etc/redis/tls/redis.crt"
      tls_key_file: "/etc/redis/tls/redis.key"
      tls_ca_cert_file: "/etc/redis/tls/ca.crt"
      tls_auth_clients: "yes"
    
    acl:
      users:
        - username: "default"
          passwords: ["super_secure_redis_password_2023"]
          commands: ["+@all"]
          keys: ["*"]
        
        - username: "application"
          passwords: ["app_password_2023"]
          commands: ["+get", "+set", "+exists", "+expire"]
          keys: ["app:*"]
        
        - username: "monitoring"
          passwords: ["monitor_password_2023"]
          commands: ["+info", "+client", "+ping"]
          keys: [""]
```

## 2. Advanced Cache Management

### 2.1 Redis Cluster Management

```bash
#!/bin/bash
# redis_cluster_management.sh

REDIS_NODES=(
    "redis-node-0:6379"
    "redis-node-1:6379" 
    "redis-node-2:6379"
    "redis-node-3:6379"
    "redis-node-4:6379"
    "redis-node-5:6379"
)

# 1. åˆ›å»ºRedisé›†ç¾¤
create_redis_cluster() {
    echo "Creating Redis cluster..."
    
    redis-cli --cluster create \
        ${REDIS_NODES[0]} ${REDIS_NODES[1]} ${REDIS_NODES[2]} \
        ${REDIS_NODES[3]} ${REDIS_NODES[4]} ${REDIS_NODES[5]} \
        --cluster-replicas 1 \
        --cluster-yes
    
    echo "Redis cluster created successfully"
}

# 2. æ£€æŸ¥é›†ç¾¤çŠ¶æ€
check_cluster_status() {
    echo "Checking cluster status..."
    
    for node in "${REDIS_NODES[@]}"; do
        echo "Node: $node"
        redis-cli -h ${node%:*} -p ${node#*:} cluster info
        echo "---"
    done
}

# 3. æ·»åŠ æ–°èŠ‚ç‚¹åˆ°é›†ç¾¤
add_cluster_node() {
    local new_node=$1
    local master_node=$2
    
    echo "Adding node $new_node to cluster..."
    
    # æ·»åŠ ä¸»èŠ‚ç‚¹
    redis-cli --cluster add-node $new_node $master_node --cluster-slave
    
    # é‡æ–°åˆ†ç‰‡æ•°æ®
    redis-cli --cluster reshard $master_node \
        --cluster-from all \
        --cluster-to $new_node \
        --cluster-slots 1000 \
        --cluster-yes
}

# 4. åˆ é™¤é›†ç¾¤èŠ‚ç‚¹
remove_cluster_node() {
    local node_to_remove=$1
    
    echo "Removing node $node_to_remove from cluster..."
    
    # èŽ·å–èŠ‚ç‚¹ID
    node_id=$(redis-cli -h ${node_to_remove%:*} -p ${node_to_remove#*:} cluster myid)
    
    # åˆ é™¤èŠ‚ç‚¹
    redis-cli --cluster del-node ${REDIS_NODES[0]} $node_id
}

# 5. é›†ç¾¤æ•…éšœè½¬ç§»æµ‹è¯•
test_failover() {
    local master_node=${REDIS_NODES[0]}
    
    echo "Testing failover for $master_node..."
    
    # å¼ºåˆ¶ä¸»èŠ‚ç‚¹ä¸‹çº¿
    redis-cli -h ${master_node%:*} -p ${master_node#*:} debug segfault
    
    # ç­‰å¾…æ•…éšœè½¬ç§»å®Œæˆ
    sleep 30
    
    # éªŒè¯æ–°ä¸»èŠ‚ç‚¹
    check_cluster_status
}
```

### 2.2 Advanced Data Structures and Patterns

```python
# Python Redisé«˜çº§æ•°æ®ç»“æž„ä½¿ç”¨ç¤ºä¾‹
import redis
import json
import time
from datetime import datetime, timedelta

class RedisAdvancedPatterns:
    def __init__(self, host='localhost', port=6379, db=0):
        self.redis_client = redis.Redis(
            host=host, 
            port=port, 
            db=db,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5
        )
    
    # 1. ä½¿ç”¨Sorted Setå®žçŽ°æŽ’è¡Œæ¦œ
    def update_leaderboard(self, user_id, score):
        """æ›´æ–°ç”¨æˆ·æŽ’è¡Œæ¦œåˆ†æ•°"""
        key = "leaderboard:weekly"
        pipeline = self.redis_client.pipeline()
        
        # æ›´æ–°åˆ†æ•°
        pipeline.zadd(key, {user_id: score})
        
        # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆä¸€å‘¨ï¼‰
        pipeline.expire(key, 7 * 24 * 3600)
        
        # èŽ·å–æŽ’å
        pipeline.zrevrank(key, user_id)
        pipeline.zscore(key, user_id)
        
        results = pipeline.execute()
        rank = results[2]
        current_score = results[3]
        
        return {
            'user_id': user_id,
            'score': current_score,
            'rank': rank + 1 if rank is not None else None
        }
    
    def get_top_players(self, limit=10):
        """èŽ·å–æŽ’è¡Œæ¦œå‰Nå"""
        key = "leaderboard:weekly"
        top_players = self.redis_client.zrevrange(
            key, 0, limit-1, withscores=True
        )
        return [(player[0], player[1]) for player in top_players]
    
    # 2. ä½¿ç”¨Hashå®žçŽ°ç”¨æˆ·èµ„æ–™ç¼“å­˜
    def cache_user_profile(self, user_id, profile_data, ttl=3600):
        """ç¼“å­˜ç”¨æˆ·èµ„æ–™"""
        key = f"user:profile:{user_id}"
        self.redis_client.hset(key, mapping=profile_data)
        self.redis_client.expire(key, ttl)
        return True
    
    def get_user_profile(self, user_id):
        """èŽ·å–ç”¨æˆ·èµ„æ–™"""
        key = f"user:profile:{user_id}"
        profile = self.redis_client.hgetall(key)
        return profile if profile else None
    
    # 3. ä½¿ç”¨Bitmapå®žçŽ°ç­¾åˆ°åŠŸèƒ½
    def user_checkin(self, user_id, date=None):
        """ç”¨æˆ·ç­¾åˆ°"""
        if date is None:
            date = datetime.now()
        
        key = f"checkin:{date.strftime('%Y-%m')}"
        offset = int(date.strftime('%d')) - 1
        
        # è®¾ç½®ä½å›¾
        result = self.redis_client.setbit(key, offset, 1)
        self.redis_client.expire(key, 31 * 24 * 3600)  # ä¿ç•™ä¸€ä¸ªæœˆ
        
        return result == 0  # è¿”å›žTrueè¡¨ç¤ºé¦–æ¬¡ç­¾åˆ°
    
    def get_checkin_stats(self, user_id, year_month):
        """èŽ·å–ç”¨æˆ·ç­¾åˆ°ç»Ÿè®¡"""
        key = f"checkin:{year_month}"
        
        # èŽ·å–æ•´ä¸ªæœˆçš„ç­¾åˆ°æƒ…å†µ
        bitmap = self.redis_client.get(key)
        if not bitmap:
            return {'total_days': 0, 'checkin_days': 0, 'consecutive_days': 0}
        
        # ç»Ÿè®¡ç­¾åˆ°å¤©æ•°
        checkin_days = bin(int.from_bytes(bitmap, 'big')).count('1')
        
        # è®¡ç®—è¿žç»­ç­¾åˆ°å¤©æ•°
        consecutive_days = 0
        current_consecutive = 0
        
        for bit in bin(int.from_bytes(bitmap, 'big'))[2:].zfill(31):
            if bit == '1':
                current_consecutive += 1
                consecutive_days = max(consecutive_days, current_consecutive)
            else:
                current_consecutive = 0
        
        return {
            'total_days': 31,
            'checkin_days': checkin_days,
            'consecutive_days': consecutive_days
        }
    
    # 4. ä½¿ç”¨HyperLogLogå®žçŽ°åŸºæ•°ç»Ÿè®¡
    def track_unique_visitors(self, page_id, user_id):
        """è·Ÿè¸ªé¡µé¢å”¯ä¸€è®¿å®¢"""
        key = f"visitors:daily:{page_id}:{datetime.now().strftime('%Y-%m-%d')}"
        self.redis_client.pfadd(key, user_id)
        self.redis_client.expire(key, 25 * 3600)  # ä¿ç•™25å°æ—¶
        return True
    
    def get_unique_visitor_count(self, page_id, days=7):
        """èŽ·å–é¡µé¢è¿‘Nå¤©å”¯ä¸€è®¿å®¢æ•°"""
        keys = []
        for i in range(days):
            date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
            keys.append(f"visitors:daily:{page_id}:{date}")
        
        # åˆå¹¶å¤šä¸ªHyperLogLog
        merged_key = f"visitors:merged:{page_id}:{int(time.time())}"
        self.redis_client.pfmerge(merged_key, *keys)
        
        count = self.redis_client.pfcount(merged_key)
        
        # æ¸…ç†ä¸´æ—¶é”®
        self.redis_client.delete(merged_key)
        
        return count

# ä½¿ç”¨ç¤ºä¾‹
redis_patterns = RedisAdvancedPatterns()

# æŽ’è¡Œæ¦œæ“ä½œ
redis_patterns.update_leaderboard("user_123", 1500)
top_players = redis_patterns.get_top_players(5)

# ç”¨æˆ·èµ„æ–™ç¼“å­˜
profile_data = {
    "name": "John Doe",
    "email": "john@example.com",
    "avatar": "avatar_url",
    "last_login": str(datetime.now())
}
redis_patterns.cache_user_profile("user_123", profile_data)
user_profile = redis_patterns.get_user_profile("user_123")

# ç­¾åˆ°åŠŸèƒ½
redis_patterns.user_checkin("user_123")
stats = redis_patterns.get_checkin_stats("user_123", "2023-12")

# å”¯ä¸€è®¿å®¢ç»Ÿè®¡
redis_patterns.track_unique_visitors("homepage", "visitor_456")
unique_count = redis_patterns.get_unique_visitor_count("homepage", 7)
```

## 3. Performance Optimization

### 3.1 Memory Optimization Strategies

```bash
#!/bin/bash
# redis_memory_optimization.sh

REDIS_HOST="localhost"
REDIS_PORT=6379

# 1. å†…å­˜ä½¿ç”¨åˆ†æž
analyze_memory_usage() {
    echo "=== Redis Memory Analysis ==="
    
    # åŸºæœ¬å†…å­˜ä¿¡æ¯
    redis-cli -h $REDIS_HOST -p $REDIS_PORT info memory
    
    # å†…å­˜ç¢Žç‰‡çŽ‡
    fragmentation=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT info memory | grep mem_fragmentation_ratio | cut -d: -f2)
    echo "Memory Fragmentation Ratio: $fragmentation"
    
    # æœ€å¤§å†…å­˜é…ç½®
    maxmemory=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT config get maxmemory | tail -1)
    echo "Max Memory: $maxmemory bytes"
    
    # å½“å‰ä½¿ç”¨å†…å­˜
    used_memory=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT info memory | grep used_memory: | cut -d: -f2)
    echo "Used Memory: $used_memory bytes"
}

# 2. å¤§Keyæ£€æµ‹
find_large_keys() {
    echo "Finding large keys..."
    
    redis-cli -h $REDIS_HOST -p $REDIS_PORT --bigkeys
    
    # è¯¦ç»†çš„å¤§Keyåˆ†æž
    echo "Detailed large key analysis:"
    redis-cli -h $REDIS_HOST -p $REDIS_PORT scan 0 | while read cursor keys; do
        for key in $keys; do
            if [ "$key" != "0" ]; then
                type=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT type $key)
                mem_usage=$(redis-cli -h $REDIS_HOST -p $REDIS_PORT memory usage $key)
                if [ "$mem_usage" -gt 1048576 ]; then  # å¤§äºŽ1MB
                    echo "Large Key: $key ($type) - Size: $mem_usage bytes"
                fi
            fi
        done
    done
}

# 3. å†…å­˜ä¼˜åŒ–å»ºè®®
optimize_memory() {
    echo "Applying memory optimizations..."
    
    # è®¾ç½®åˆç†çš„æœ€å¤§å†…å­˜
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set maxmemory 2gb
    
    # è®¾ç½®å†…å­˜æ·˜æ±°ç­–ç•¥
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set maxmemory-policy allkeys-lru
    
    # å¯ç”¨å†…å­˜åŽ‹ç¼©ï¼ˆRedis 7.0+ï¼‰
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set activedefrag yes
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set active-defrag-ignore-bytes 100mb
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set active-defrag-threshold-lower 10
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set active-defrag-threshold-upper 100
    
    # ä¼˜åŒ–å“ˆå¸Œç»“æž„
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set hash-max-ziplist-entries 512
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set hash-max-ziplist-value 64
    
    # ä¼˜åŒ–é›†åˆç»“æž„
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set set-max-intset-entries 512
    
    # ä¼˜åŒ–æœ‰åºé›†åˆ
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set zset-max-ziplist-entries 128
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set zset-max-ziplist-value 64
}

# 4. è¿žæŽ¥ä¼˜åŒ–
optimize_connections() {
    echo "Optimizing connections..."
    
    # å¢žåŠ æœ€å¤§è¿žæŽ¥æ•°
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set maxclients 10000
    
    # å¯ç”¨TCP keepalive
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set tcp-keepalive 300
    
    # ä¼˜åŒ–TCP backlog
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set tcp-backlog 511
    
    # å¯ç”¨pipelineæ‰¹å¤„ç†
    echo "Consider using pipeline operations for better performance"
}

# 5. æŒä¹…åŒ–ä¼˜åŒ–
optimize_persistence() {
    echo "Optimizing persistence..."
    
    # RDBä¼˜åŒ–
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set save "900 1 300 10 60 10000"
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set rdbcompression yes
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set rdbchecksum yes
    
    # AOFä¼˜åŒ–
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set appendonly yes
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set appendfsync everysec
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set auto-aof-rewrite-percentage 100
    redis-cli -h $REDIS_HOST -p $REDIS_PORT config set auto-aof-rewrite-min-size 64mb
}

# æ‰§è¡Œæ‰€æœ‰ä¼˜åŒ–
main() {
    analyze_memory_usage
    echo ""
    find_large_keys
    echo ""
    optimize_memory
    echo ""
    optimize_connections
    echo ""
    optimize_persistence
    echo ""
    echo "Memory optimization completed!"
}

main
```

### 3.2 Pipeline and Batch Operations

```python
# Redis Pipeline and Batch Operations
import redis
import time
import json
from typing import List, Dict, Any

class RedisPipelineOperations:
    def __init__(self, host='localhost', port=6379, db=0):
        self.redis_client = redis.Redis(
            host=host,
            port=port,
            db=db,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5
        )
    
    def batch_set_with_pipeline(self, key_value_pairs: Dict[str, Any], ttl: int = None) -> bool:
        """ä½¿ç”¨Pipelineæ‰¹é‡è®¾ç½®é”®å€¼å¯¹"""
        pipeline = self.redis_client.pipeline(transaction=False)
        
        for key, value in key_value_pairs.items():
            if isinstance(value, (dict, list)):
                value = json.dumps(value)
            pipeline.set(key, value)
            if ttl:
                pipeline.expire(key, ttl)
        
        results = pipeline.execute()
        return all(results)
    
    def batch_get_with_pipeline(self, keys: List[str]) -> Dict[str, Any]:
        """ä½¿ç”¨Pipelineæ‰¹é‡èŽ·å–é”®å€¼"""
        pipeline = self.redis_client.pipeline(transaction=False)
        
        for key in keys:
            pipeline.get(key)
        
        results = pipeline.execute()
        
        return_dict = {}
        for i, key in enumerate(keys):
            value = results[i]
            if value:
                try:
                    # å°è¯•è§£æžJSON
                    return_dict[key] = json.loads(value)
                except json.JSONDecodeError:
                    return_dict[key] = value
            else:
                return_dict[key] = None
        
        return return_dict
    
    def atomic_counter_operations(self, counter_keys: List[str], increment_values: List[int]) -> List[int]:
        """åŽŸå­è®¡æ•°å™¨æ“ä½œ"""
        pipeline = self.redis_client.pipeline(transaction=True)
        
        for key, incr_value in zip(counter_keys, increment_values):
            pipeline.incrby(key, incr_value)
        
        results = pipeline.execute()
        return results
    
    def cache_warming_pipeline(self, warmup_data: List[Dict]) -> int:
        """ç¼“å­˜é¢„çƒ­Pipeline"""
        pipeline = self.redis_client.pipeline(transaction=False)
        success_count = 0
        
        for item in warmup_data:
            key = item.get('key')
            value = item.get('value')
            ttl = item.get('ttl', 3600)
            
            if key and value:
                if isinstance(value, (dict, list)):
                    value = json.dumps(value)
                
                pipeline.setex(key, ttl, value)
                success_count += 1
        
        pipeline.execute()
        return success_count
    
    def multi_operation_transaction(self, operations: List[Dict]) -> List[Any]:
        """å¤šæ“ä½œäº‹åŠ¡å¤„ç†"""
        pipeline = self.redis_client.pipeline(transaction=True)
        
        for op in operations:
            op_type = op.get('type')
            key = op.get('key')
            value = op.get('value')
            args = op.get('args', [])
            
            if op_type == 'set':
                pipeline.set(key, value, *args)
            elif op_type == 'get':
                pipeline.get(key)
            elif op_type == 'incr':
                pipeline.incr(key)
            elif op_type == 'hset':
                pipeline.hset(key, *args)
            elif op_type == 'zadd':
                pipeline.zadd(key, *args)
            # å¯ä»¥æ·»åŠ æ›´å¤šæ“ä½œç±»åž‹
        
        try:
            results = pipeline.execute()
            return results
        except redis.WatchError:
            # å¤„ç†ä¹è§‚é”å†²çª
            return None

# ä½¿ç”¨ç¤ºä¾‹
pipeline_ops = RedisPipelineOperations()

# æ‰¹é‡è®¾ç½®æ“ä½œ
batch_data = {
    'user:1001:name': 'Alice',
    'user:1001:email': 'alice@example.com',
    'user:1002:name': 'Bob',
    'user:1002:email': 'bob@example.com'
}
pipeline_ops.batch_set_with_pipeline(batch_data, ttl=7200)

# æ‰¹é‡èŽ·å–æ“ä½œ
keys_to_fetch = ['user:1001:name', 'user:1001:email', 'user:1002:name']
results = pipeline_ops.batch_get_with_pipeline(keys_to_fetch)

# åŽŸå­è®¡æ•°å™¨æ“ä½œ
counter_keys = ['page_views:home', 'page_views:products', 'page_views:about']
increments = [1, 3, 1]
new_counts = pipeline_ops.atomic_counter_operations(counter_keys, increments)

# ç¼“å­˜é¢„çƒ­
warmup_items = [
    {'key': 'popular_products', 'value': ['prod_1', 'prod_2', 'prod_3'], 'ttl': 1800},
    {'key': 'featured_articles', 'value': ['art_1', 'art_2'], 'ttl': 3600},
    {'key': 'site_config', 'value': {'theme': 'dark', 'language': 'en'}, 'ttl': 86400}
]
warmed_items = pipeline_ops.cache_warming_pipeline(warmup_items)
```

## 4. High Availability and Disaster Recovery

### 4.1 Sentinel-based Failover Setup

```bash
#!/bin/bash
# redis_sentinel_setup.sh

SENTINEL_CONFIG_DIR="/etc/redis/sentinel"
MASTER_NAME="mymaster"
MASTER_IP="192.168.1.100"
MASTER_PORT=6379
QUORUM=2

# 1. åˆ›å»ºSentinelé…ç½®æ–‡ä»¶
create_sentinel_config() {
    local sentinel_port=$1
    local config_file="$SENTINEL_CONFIG_DIR/sentinel_$sentinel_port.conf"
    
    cat > $config_file << EOF
port $sentinel_port
bind 0.0.0.0
daemonize yes
pidfile /var/run/redis-sentinel-$sentinel_port.pid
logfile /var/log/redis/sentinel_$sentinel_port.log
dir /tmp

sentinel monitor $MASTER_NAME $MASTER_IP $MASTER_PORT $QUORUM
sentinel down-after-milliseconds $MASTER_NAME 5000
sentinel failover-timeout $MASTER_NAME 10000
sentinel parallel-syncs $MASTER_NAME 1

sentinel auth-pass $MASTER_NAME "super_secure_redis_password_2023"
sentinel auth-user $MASTER_NAME "sentinel"

# é€šçŸ¥è„šæœ¬
sentinel notification-script $MASTER_NAME /etc/redis/scripts/sentinel_notify.sh
sentinel client-reconfig-script $MASTER_NAME /etc/redis/scripts/sentinel_reconfig.sh
EOF

    echo "Created sentinel config: $config_file"
}

# 2. å¯åŠ¨Sentinelå®žä¾‹
start_sentinel() {
    local sentinel_port=$1
    local config_file="$SENTINEL_CONFIG_DIR/sentinel_$sentinel_port.conf"
    
    redis-sentinel $config_file
    echo "Started Sentinel on port $sentinel_port"
}

# 3. éªŒè¯SentinelçŠ¶æ€
verify_sentinel() {
    local sentinel_port=$1
    
    echo "Verifying Sentinel on port $sentinel_port:"
    redis-cli -p $sentinel_port sentinel masters
    redis-cli -p $sentinel_port sentinel slaves $MASTER_NAME
}

# 4. æ•…éšœè½¬ç§»æµ‹è¯•è„šæœ¬
failover_test() {
    echo "Testing failover..."
    
    # èŽ·å–å½“å‰ä¸»èŠ‚ç‚¹
    current_master=$(redis-cli -p 26379 sentinel get-master-addr-by-name $MASTER_NAME)
    echo "Current master: $current_master"
    
    # å¼ºåˆ¶ä¸»èŠ‚ç‚¹ä¸‹çº¿
    redis-cli -h $MASTER_IP -p $MASTER_PORT debug segfault
    
    # ç­‰å¾…æ•…éšœè½¬ç§»
    sleep 30
    
    # éªŒè¯æ–°ä¸»èŠ‚ç‚¹
    new_master=$(redis-cli -p 26379 sentinel get-master-addr-by-name $MASTER_NAME)
    echo "New master: $new_master"
    
    if [ "$current_master" != "$new_master" ]; then
        echo "Failover successful!"
    else
        echo "Failover failed!"
    fi
}

# 5. ä¸»ä»Žåˆ‡æ¢é€šçŸ¥è„šæœ¬
cat > /etc/redis/scripts/sentinel_notify.sh << 'EOF'
#!/bin/bash

EVENT_TYPE=$1
MASTER_NAME=$2
MASTER_IP=$3
MASTER_PORT=$4

LOG_FILE="/var/log/redis/sentinel_notifications.log"

echo "$(date): Event $EVENT_TYPE for master $MASTER_NAME at $MASTER_IP:$MASTER_PORT" >> $LOG_FILE

case $EVENT_TYPE in
    +reset-master)
        echo "Master has been reset" >> $LOG_FILE
        ;;
    +slave)
        SLAVE_IP=$5
        SLAVE_PORT=$6
        echo "New slave added: $SLAVE_IP:$SLAVE_PORT" >> $LOG_FILE
        ;;
    +failover-state-reconf-slaves)
        echo "Failover in progress - reconfiguring slaves" >> $LOG_FILE
        ;;
    +failover-end)
        echo "Failover completed successfully" >> $LOG_FILE
        # è¿™é‡Œå¯ä»¥æ·»åŠ åº”ç”¨å±‚çš„é€šçŸ¥é€»è¾‘
        ;;
esac
EOF

chmod +x /etc/redis/scripts/sentinel_notify.sh

# ä¸»é…ç½®
create_sentinel_config 26379
create_sentinel_config 26380
create_sentinel_config 26381

# å¯åŠ¨å®žä¾‹
start_sentinel 26379
start_sentinel 26380
start_sentinel 26381

# éªŒè¯é…ç½®
sleep 5
verify_sentinel 26379
```

### 4.2 Backup and Recovery Procedures

```bash
#!/bin/bash
# redis_backup_restore.sh

BACKUP_DIR="/backup/redis"
RETENTION_DAYS=30
REDIS_HOST="localhost"
REDIS_PORT=6379

# 1. RDBå¤‡ä»½
backup_rdb() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="$BACKUP_DIR/rdb_backup_$timestamp.rdb"
    
    echo "Starting RDB backup..."
    
    # è§¦å‘BGSAVE
    redis-cli -h $REDIS_HOST -p $REDIS_PORT bgsave
    
    # ç­‰å¾…å¤‡ä»½å®Œæˆ
    while [ "$(redis-cli -h $REDIS_HOST -p $REDIS_PORT lastsave)" = "0" ]; do
        sleep 1
    done
    
    # å¤åˆ¶RDBæ–‡ä»¶
    cp /var/lib/redis/dump.rdb $backup_file
    
    # éªŒè¯å¤‡ä»½æ–‡ä»¶
    if [ -f "$backup_file" ] && [ -s "$backup_file" ]; then
        # åˆ›å»ºæ ¡éªŒå’Œ
        md5sum $backup_file > "$backup_file.md5"
        echo "RDB backup completed: $backup_file"
        return 0
    else
        echo "RDB backup failed"
        return 1
    fi
}

# 2. AOFå¤‡ä»½
backup_aof() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="$BACKUP_DIR/aof_backup_$timestamp.aof"
    
    echo "Starting AOF backup..."
    
    # BGREWRITEAOFé‡å†™AOFæ–‡ä»¶
    redis-cli -h $REDIS_HOST -p $REDIS_PORT bgrewriteaof
    
    # ç­‰å¾…é‡å†™å®Œæˆ
    sleep 10
    
    # å¤åˆ¶AOFæ–‡ä»¶
    cp /var/lib/redis/appendonly.aof $backup_file
    
    # éªŒè¯å¤‡ä»½æ–‡ä»¶
    if [ -f "$backup_file" ] && [ -s "$backup_file" ]; then
        md5sum $backup_file > "$backup_file.md5"
        echo "AOF backup completed: $backup_file"
        return 0
    else
        echo "AOF backup failed"
        return 1
    fi
}

# 3. å¢žé‡å¤‡ä»½
incremental_backup() {
    local timestamp=$(date +%Y%m%d_%H%M%S)
    local backup_file="$BACKUP_DIR/incr_backup_$timestamp.aof"
    
    echo "Starting incremental backup..."
    
    # èŽ·å–å½“å‰AOFæ–‡ä»¶
    cp /var/lib/redis/appendonly.aof $backup_file
    
    # è®°å½•å¤‡ä»½æ—¶é—´ç‚¹
    redis-cli -h $REDIS_HOST -p $REDIS_PORT time | head -1 > "$BACKUP_DIR/last_backup_time"
    
    if [ -f "$backup_file" ]; then
        echo "Incremental backup completed: $backup_file"
        return 0
    else
        echo "Incremental backup failed"
        return 1
    fi
}

# 4. å¤‡ä»½éªŒè¯
verify_backup() {
    local backup_file=$1
    
    echo "Verifying backup: $backup_file"
    
    # éªŒè¯æ ¡éªŒå’Œ
    if [ -f "$backup_file.md5" ]; then
        md5sum -c "$backup_file.md5"
        if [ $? -ne 0 ]; then
            echo "Backup verification failed - checksum mismatch"
            return 1
        fi
    fi
    
    # æµ‹è¯•æ¢å¤åˆ°ä¸´æ—¶å®žä¾‹
    local temp_port=6380
    redis-server --port $temp_port --dir /tmp --dbfilename temp_dump.rdb &
    temp_pid=$!
    
    sleep 2
    
    # åŠ è½½å¤‡ä»½æ•°æ®
    redis-cli -p $temp_port shutdown nosave
    cp $backup_file /tmp/temp_dump.rdb
    redis-server --port $temp_port --dir /tmp --dbfilename temp_dump.rdb &
    
    sleep 2
    
    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    local key_count=$(redis-cli -p $temp_port dbsize)
    echo "Recovered database contains $key_count keys"
    
    # æ¸…ç†ä¸´æ—¶å®žä¾‹
    redis-cli -p $temp_port shutdown nosave
    kill $temp_pid 2>/dev/null
    
    echo "Backup verification completed"
    return 0
}

# 5. æ¢å¤æ“ä½œ
restore_backup() {
    local backup_file=$1
    local target_host=${2:-localhost}
    local target_port=${3:-6379}
    
    echo "Restoring backup: $backup_file to $target_host:$target_port"
    
    # åœæ­¢ç›®æ ‡Rediså®žä¾‹
    redis-cli -h $target_host -p $target_port shutdown nosave
    
    # å¤‡ä»½çŽ°æœ‰æ•°æ®
    local existing_backup="/tmp/existing_data_$(date +%Y%m%d_%H%M%S).rdb"
    cp /var/lib/redis/dump.rdb $existing_backup
    
    # æ¢å¤å¤‡ä»½æ–‡ä»¶
    cp $backup_file /var/lib/redis/dump.rdb
    
    # å¯åŠ¨Rediså®žä¾‹
    systemctl start redis
    
    # éªŒè¯æ¢å¤
    sleep 5
    local recovered_keys=$(redis-cli -h $target_host -p $target_port dbsize)
    echo "Recovery completed. Database contains $recovered_keys keys"
}

# 6. å¤‡ä»½æ¸…ç†
cleanup_old_backups() {
    echo "Cleaning up old backups..."
    find "$BACKUP_DIR" -name "rdb_backup_*.rdb" -mtime +$RETENTION_DAYS -delete
    find "$BACKUP_DIR" -name "aof_backup_*.aof" -mtime +$RETENTION_DAYS -delete
    find "$BACKUP_DIR" -name "incr_backup_*.aof" -mtime +7 -delete
}

# 7. è‡ªåŠ¨å¤‡ä»½è°ƒåº¦
setup_cron_jobs() {
    echo "Setting up automatic backup schedule..."
    
    # æ¯å¤©å‡Œæ™¨2ç‚¹è¿›è¡ŒRDBå¤‡ä»½
    echo "0 2 * * * $0 backup_rdb" | crontab -
    
    # æ¯å°æ—¶è¿›è¡Œå¢žé‡å¤‡ä»½
    echo "0 * * * * $0 incremental_backup" | crontab -
    
    # æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹è¿›è¡ŒAOFå¤‡ä»½
    echo "0 3 * * 0 $0 backup_aof" | crontab -
    
    # æ¯å¤©å‡Œæ™¨4ç‚¹æ¸…ç†æ—§å¤‡ä»½
    echo "0 4 * * * $0 cleanup_old_backups" | crontab -
}

# ä¸»å‡½æ•°
main() {
    case "$1" in
        backup_rdb)
            backup_rdb
            ;;
        backup_aof)
            backup_aof
            ;;
        incremental_backup)
            incremental_backup
            ;;
        restore)
            restore_backup "$2" "$3" "$4"
            ;;
        verify)
            verify_backup "$2"
            ;;
        cleanup)
            cleanup_old_backups
            ;;
        schedule)
            setup_cron_jobs
            ;;
        *)
            echo "Usage: $0 {backup_rdb|backup_aof|incremental_backup|restore|verify|cleanup|schedule}"
            echo "Examples:"
            echo "  $0 backup_rdb"
            echo "  $0 restore /backup/redis/rdb_backup_20231201_020000.rdb"
            echo "  $0 verify /backup/redis/rdb_backup_20231201_020000.rdb"
            echo "  $0 schedule"
            ;;
    esac
}

main "$@"
```

## 5. Monitoring and Alerting

### 5.1 Comprehensive Monitoring Setup

```python
# Redisç›‘æŽ§å’Œå‘Šè­¦ç³»ç»Ÿ
import redis
import time
import json
import smtplib
from email.mime.text import MIMEText
from datetime import datetime, timedelta
from typing import Dict, List, Optional

class RedisMonitor:
    def __init__(self, host='localhost', port=6379, password=None):
        self.redis_client = redis.Redis(
            host=host,
            port=port,
            password=password,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5
        )
        self.alert_thresholds = {
            'memory_usage_percent': 85,
            'connected_clients': 1000,
            'blocked_clients': 10,
            'rejected_connections': 5,
            'evicted_keys': 100,
            'keyspace_hits_ratio': 0.8,
            'latency_ms': 10
        }
        self.alert_recipients = ['admin@company.com', 'ops@company.com']
    
    def get_server_info(self) -> Dict:
        """èŽ·å–æœåŠ¡å™¨åŸºæœ¬ä¿¡æ¯"""
        try:
            info = self.redis_client.info()
            return {
                'version': info.get('redis_version'),
                'mode': info.get('redis_mode'),
                'uptime': info.get('uptime_in_seconds'),
                'connected_clients': info.get('connected_clients'),
                'blocked_clients': info.get('blocked_clients'),
                'used_memory': info.get('used_memory'),
                'used_memory_human': info.get('used_memory_human'),
                'used_memory_peak': info.get('used_memory_peak'),
                'mem_fragmentation_ratio': info.get('mem_fragmentation_ratio'),
                'total_commands_processed': info.get('total_commands_processed'),
                'instantaneous_ops_per_sec': info.get('instantaneous_ops_per_sec'),
                'keyspace_hits': info.get('keyspace_hits'),
                'keyspace_misses': info.get('keyspace_misses')
            }
        except Exception as e:
            return {'error': str(e)}
    
    def get_memory_stats(self) -> Dict:
        """èŽ·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            info = self.redis_client.info('memory')
            memory_stats = self.redis_client.memory_stats()
            
            maxmemory = info.get('maxmemory', 0)
            used_memory = info.get('used_memory', 0)
            memory_usage_percent = (used_memory / maxmemory * 100) if maxmemory > 0 else 0
            
            return {
                'used_memory': used_memory,
                'used_memory_human': info.get('used_memory_human'),
                'used_memory_rss': info.get('used_memory_rss'),
                'used_memory_peak': info.get('used_memory_peak'),
                'maxmemory': maxmemory,
                'maxmemory_human': info.get('maxmemory_human'),
                'memory_usage_percent': round(memory_usage_percent, 2),
                'mem_fragmentation_ratio': info.get('mem_fragmentation_ratio'),
                'allocator_active': memory_stats.get('allocator_active', 0),
                'allocator_resident': memory_stats.get('allocator_resident', 0)
            }
        except Exception as e:
            return {'error': str(e)}
    
    def get_performance_metrics(self) -> Dict:
        """èŽ·å–æ€§èƒ½æŒ‡æ ‡"""
        try:
            # èŽ·å–å»¶è¿Ÿä¿¡æ¯
            latency_info = self.redis_client.latency_latest()
            
            # èŽ·å–æ…¢æŸ¥è¯¢æ—¥å¿—
            slowlog = self.redis_client.slowlog_get(10)
            
            # è®¡ç®—å‘½ä¸­çŽ‡
            info = self.redis_client.info()
            keyspace_hits = info.get('keyspace_hits', 0)
            keyspace_misses = info.get('keyspace_misses', 0)
            total_lookups = keyspace_hits + keyspace_misses
            hit_ratio = (keyspace_hits / total_lookups) if total_lookups > 0 else 0
            
            return {
                'instantaneous_ops_per_sec': info.get('instantaneous_ops_per_sec'),
                'total_commands_processed': info.get('total_commands_processed'),
                'keyspace_hits': keyspace_hits,
                'keyspace_misses': keyspace_misses,
                'hit_ratio': round(hit_ratio, 4),
                'rejected_connections': info.get('rejected_connections'),
                'sync_full': info.get('sync_full'),
                'sync_partial_ok': info.get('sync_partial_ok'),
                'expired_keys': info.get('expired_keys'),
                'evicted_keys': info.get('evicted_keys'),
                'latency_latest': dict(latency_info) if latency_info else {}
            }
        except Exception as e:
            return {'error': str(e)}
    
    def check_alerts(self) -> List[Dict]:
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        alerts = []
        
        try:
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨çŽ‡
            memory_stats = self.get_memory_stats()
            if 'memory_usage_percent' in memory_stats:
                usage = memory_stats['memory_usage_percent']
                if usage > self.alert_thresholds['memory_usage_percent']:
                    alerts.append({
                        'type': 'HIGH_MEMORY_USAGE',
                        'severity': 'WARNING',
                        'message': f'Memory usage is {usage}% (threshold: {self.alert_thresholds["memory_usage_percent"]}%)',
                        'current_value': usage,
                        'threshold': self.alert_thresholds['memory_usage_percent']
                    })
            
            # æ£€æŸ¥è¿žæŽ¥æ•°
            server_info = self.get_server_info()
            if 'connected_clients' in server_info:
                clients = server_info['connected_clients']
                if clients > self.alert_thresholds['connected_clients']:
                    alerts.append({
                        'type': 'HIGH_CLIENT_CONNECTIONS',
                        'severity': 'WARNING',
                        'message': f'Connected clients: {clients} (threshold: {self.alert_thresholds["connected_clients"]})',
                        'current_value': clients,
                        'threshold': self.alert_thresholds['connected_clients']
                    })
            
            # æ£€æŸ¥è¢«é˜»å¡žå®¢æˆ·ç«¯
            if 'blocked_clients' in server_info:
                blocked = server_info['blocked_clients']
                if blocked > self.alert_thresholds['blocked_clients']:
                    alerts.append({
                        'type': 'BLOCKED_CLIENTS',
                        'severity': 'CRITICAL',
                        'message': f'Blocked clients: {blocked} (threshold: {self.alert_thresholds["blocked_clients"]})',
                        'current_value': blocked,
                        'threshold': self.alert_thresholds['blocked_clients']
                    })
            
            # æ£€æŸ¥é©±é€é”®æ•°é‡
            perf_metrics = self.get_performance_metrics()
            if 'evicted_keys' in perf_metrics:
                evicted = perf_metrics['evicted_keys']
                if evicted > self.alert_thresholds['evicted_keys']:
                    alerts.append({
                        'type': 'KEYS_EVICTED',
                        'severity': 'WARNING',
                        'message': f'Evicted keys: {evicted} (threshold: {self.alert_thresholds["evicted_keys"]})',
                        'current_value': evicted,
                        'threshold': self.alert_thresholds['evicted_keys']
                    })
            
            # æ£€æŸ¥å‘½ä¸­çŽ‡
            if 'hit_ratio' in perf_metrics:
                hit_ratio = perf_metrics['hit_ratio']
                if hit_ratio < self.alert_thresholds['keyspace_hits_ratio']:
                    alerts.append({
                        'type': 'LOW_HIT_RATIO',
                        'severity': 'WARNING',
                        'message': f'Keyspace hit ratio: {hit_ratio} (threshold: {self.alert_thresholds["keyspace_hits_ratio"]})',
                        'current_value': hit_ratio,
                        'threshold': self.alert_thresholds['keyspace_hits_ratio']
                    })
        
        except Exception as e:
            alerts.append({
                'type': 'MONITORING_ERROR',
                'severity': 'CRITICAL',
                'message': f'Monitoring error: {str(e)}'
            })
        
        return alerts
    
    def send_alert(self, alert: Dict):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        subject = f"Redis Alert - {alert['type']} ({alert['severity']})"
        body = f"""
Redis Alert Notification
========================
Time: {datetime.now().isoformat()}
Type: {alert['type']}
Severity: {alert['severity']}
Message: {alert['message']}

Details:
- Current Value: {alert.get('current_value', 'N/A')}
- Threshold: {alert.get('threshold', 'N/A')}
        """
        
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶ã€Slackã€é’‰é’‰ç­‰é€šçŸ¥æ–¹å¼
        print(f"ALERT: {subject}")
        print(body)
        print("-" * 50)
    
    def monitor_loop(self, interval: int = 60):
        """æŒç»­ç›‘æŽ§å¾ªçŽ¯"""
        print(f"Starting Redis monitoring (interval: {interval}s)")
        
        while True:
            try:
                alerts = self.check_alerts()
                for alert in alerts:
                    self.send_alert(alert)
                
                # è®°å½•ç›‘æŽ§æ•°æ®
                timestamp = datetime.now().isoformat()
                metrics = {
                    'timestamp': timestamp,
                    'server_info': self.get_server_info(),
                    'memory_stats': self.get_memory_stats(),
                    'performance_metrics': self.get_performance_metrics(),
                    'alerts': len(alerts)
                }
                
                # å¯ä»¥å°†æŒ‡æ ‡å­˜å‚¨åˆ°æ—¶åºæ•°æ®åº“
                print(f"[{timestamp}] Monitoring cycle completed - {len(alerts)} alerts")
                
            except Exception as e:
                print(f"Monitoring error: {e}")
            
            time.sleep(interval)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = RedisMonitor(password="your_redis_password")
    
    # å•æ¬¡æ£€æŸ¥
    alerts = monitor.check_alerts()
    for alert in alerts:
        monitor.send_alert(alert)
    
    # æˆ–å¯åŠ¨æŒç»­ç›‘æŽ§
    # monitor.monitor_loop(interval=30)
```

### 5.2 Prometheus Exporter Configuration

```yaml
# redis_exporter_config.yaml
redis_exporter:
  redis_addr: "localhost:6379"
  redis_user: ""
  redis_password: "your_redis_password"
  namespace: "redis"
  check_keys: "db0=user:*,db0=session:*"
  check_single_keys: "db0:stats:total_users,db0:stats:active_sessions"
  script_path: "/etc/redis/scripts/metrics.lua"

scrape_configs:
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 15s
    scrape_timeout: 10s

# Alerting Rules
groups:
  - name: redis.rules
    rules:
      - alert: RedisDown
        expr: redis_up == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Redis instance is down"
          description: "Redis instance {{ $labels.instance }} is not responding"
      
      - alert: RedisMemoryHigh
        expr: (redis_memory_used_bytes / redis_memory_max_bytes * 100) > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage high"
          description: "Redis memory usage is {{ $value | humanizePercentage }}"
      
      - alert: RedisRejectedConnections
        expr: irate(redis_rejected_connections_total[5m]) > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Redis rejecting connections"
          description: "Redis is rejecting connections at a rate of {{ $value }} per second"
      
      - alert: RedisEvictionsHigh
        expr: irate(redis_evicted_keys_total[5m]) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Redis key evictions"
          description: "Redis is evicting keys at a rate of {{ $value }} per second"
      
      - alert: RedisHitRateLow
        expr: (irate(redis_keyspace_hits_total[5m]) / (irate(redis_keyspace_hits_total[5m]) + irate(redis_keyspace_misses_total[5m]))) < 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low Redis hit rate"
          description: "Redis keyspace hit rate is below 80%: {{ $value | humanizePercentage }}"
```

---
*This document is based on enterprise-level Redis practice experience and continuously updated with the latest technologies and best practices.*
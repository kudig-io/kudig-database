# Scheduler æ•…éšœæ’æŸ¥æŒ‡å—

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-01 | **éš¾åº¦**: é«˜çº§

## ğŸ¯ æœ¬æ–‡æ¡£ä»·å€¼

Scheduler æ˜¯é›†ç¾¤çš„â€œå¤§è„‘â€ï¼Œå†³å®šäº†èµ„æºçš„ä½¿ç”¨æ•ˆç‡å’Œåº”ç”¨çš„ç¨³å®šæ€§ã€‚æœ¬æ–‡æ¡£ä¸ä»…å…³æ³¨â€œä¸ºä»€ä¹ˆè°ƒåº¦ä¸äº†â€ï¼Œæ›´å…³æ³¨â€œå¦‚ä½•è°ƒåº¦å¾—æ›´å¥½â€ã€‚

### ğŸ“ åˆå­¦è€…è§†è§’
- **æ ¸å¿ƒæ¦‚å¿µ**ï¼šScheduler ç›‘å¬ API Server ä¸­æ–°åˆ›å»ºä¸”æœªåˆ†é…èŠ‚ç‚¹çš„ Podï¼Œæ ¹æ®ä¸€å¥—ç®—æ³•ä¸ºå®ƒé€‰å‡ºä¸€ä¸ªâ€œæœ€åˆé€‚â€çš„å®¶ï¼ˆNodeï¼‰ã€‚
- **ç®€å•ç±»æ¯”**ï¼šScheduler å°±åƒä¸€ä¸ªæˆ¿äº§ä¸­ä»‹ï¼Œæ‰‹é‡Œæœ‰ä¸€å †å®¢æˆ·ï¼ˆPodï¼‰å’Œä¸€å †æˆ¿æºï¼ˆNodeï¼‰ã€‚å®ƒä¼šæ ¹æ®å®¢æˆ·çš„è¦æ±‚ï¼ˆèµ„æºè¯·æ±‚ã€äº²å’Œæ€§ï¼‰å’Œæˆ¿æºçš„æƒ…å†µï¼ˆå‰©ä½™ CPUã€å†…å­˜ï¼‰æ¥æ’®åˆäº¤æ˜“ã€‚

### ğŸ‘¨â€ğŸ’» èµ„æ·±ä¸“å®¶è§†è§’
- **è°ƒåº¦æ¡†æ¶ï¼ˆFrameworkï¼‰**ï¼šç†è§£æ’ä»¶åŒ–çš„è°ƒåº¦æµç¨‹ï¼ˆFilter, Score, Bind ç­‰æ‰©å±•ç‚¹ï¼‰å¦‚ä½•ååŒå·¥ä½œã€‚
- **å¹¶å‘ä¸æ€§èƒ½**ï¼šåˆ†æ `parallelism` å‚æ•°å¯¹å¤§è§„æ¨¡é›†ç¾¤è°ƒåº¦çš„å½±å“ï¼Œä»¥åŠ `Score` æ’ä»¶çš„è®¡ç®—æƒé‡å¦‚ä½•å¾®è°ƒã€‚
- **é«˜çº§è°ƒåº¦ç‰¹æ€§**ï¼šæ·±å…¥æ’æŸ¥ Pod Topology Spread Constraints (æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ) ä¸ Pod Disruption Budgets (PDB) çš„å†²çªåœºæ™¯ã€‚

---

## ç›®å½•

1. [é—®é¢˜ç°è±¡ä¸å½±å“åˆ†æ](#1-é—®é¢˜ç°è±¡ä¸å½±å“åˆ†æ)
2. [æ’æŸ¥æ–¹æ³•ä¸æ­¥éª¤](#2-æ’æŸ¥æ–¹æ³•ä¸æ­¥éª¤)
3. [è§£å†³æ–¹æ¡ˆä¸é£é™©æ§åˆ¶](#3-è§£å†³æ–¹æ¡ˆä¸é£é™©æ§åˆ¶)

---

## 0. 10 åˆ†é’Ÿå¿«é€Ÿè¯Šæ–­

1. **ç¡®è®¤ Scheduler å­˜æ´»ä¸é€‰ä¸¾**ï¼š`curl -k https://127.0.0.1:10259/healthz`ï¼Œ`kubectl get lease -n kube-system kube-scheduler -o wide`ï¼Œè‹¥æ—  Leader/é¢‘ç¹åˆ‡æ¢å…ˆæŸ¥è¯ä¹¦/ç½‘ç»œã€‚
2. **è§‚å¯Ÿ Pending åŸå› **ï¼š`kubectl get pods -A --field-selector=status.phase=Pending -o wide | head` + `kubectl describe pod <name> | grep -A30 Events`ï¼Œé”å®šä¸»å› ï¼ˆèµ„æºä¸è¶³/æ±¡ç‚¹/äº²å’Œ/æ‹“æ‰‘çº¦æŸ/PVCï¼‰ã€‚
3. **çœ‹è°ƒåº¦æ€§èƒ½**ï¼šç›‘æ§æŒ‡æ ‡ `scheduler_scheduling_attempts_total`ã€`scheduler_pod_scheduling_duration_seconds`ã€`workqueue_depth`ï¼Œæˆ– `kubectl get --raw "/metrics" | grep scheduler_scheduling_duration_seconds_bucket | head`ã€‚
4. **çƒ­ç‚¹æ’ä»¶/æ‰©å±•ç‚¹**ï¼š`kubectl logs -n kube-system kube-scheduler-<node> | grep "took too long" | head`ï¼Œå®šä½ Filter/Score/Bind æ’ä»¶è€—æ—¶ï¼›æ£€æŸ¥ `--parallelism`ã€`--bind-timeout-seconds` é…ç½®ã€‚
5. **æ‹“æ‰‘/åˆ†å¸ƒ/PDB å†²çª**ï¼šå¯¹ Pending Pod æŸ¥çœ‹ `topologySpreadConstraints`ã€`pdb`ï¼Œå¿…è¦æ—¶ç”¨ `kubectl describe pdb` ä¸èŠ‚ç‚¹æ ‡ç­¾äº¤å‰éªŒè¯ã€‚
6. **å¿«é€Ÿç¼“è§£**ï¼š
   - èµ„æºä¾§ï¼šä¸´æ—¶ç»™èŠ‚ç‚¹åŠ èµ„æºæˆ–ç§»é™¤å½±å“è°ƒåº¦çš„æ±¡ç‚¹/äº²å’Œçº¦æŸã€‚
   - æµé‡ä¾§ï¼šæš‚ç¼“å¤§è§„æ¨¡åˆ›å»º/æ‰¹é‡ Jobï¼›å¯¹æ§åˆ¶é¢ä½ä¼˜å…ˆçº§ Flow ä½¿ç”¨ APF è°ƒæ•´ã€‚
   - é…ç½®ä¾§ï¼šé€‚åº¦æé«˜ `--parallelism`ï¼Œå¯¹æ’ä»¶è€—æ—¶é«˜çš„åœºæ™¯å…³é—­ä¸å¿…è¦æ‰©å±•æˆ–è°ƒæ•´æƒé‡ã€‚
7. **è¯æ®ç•™å­˜**ï¼šä¿å­˜ Pending Pod äº‹ä»¶ã€Scheduler æ—¥å¿—ã€å…³é”®æŒ‡æ ‡ (è°ƒåº¦æ—¶å»¶ç›´æ–¹å›¾ã€æŠ¢å æ¬¡æ•°) ä»¥ä¾¿å¤ç›˜ã€‚

---

## 1. é—®é¢˜ç°è±¡ä¸å½±å“åˆ†æ

### 1.1 å¸¸è§é—®é¢˜ç°è±¡

#### 1.1.1 Scheduler æœåŠ¡ä¸å¯ç”¨

| ç°è±¡ | æŠ¥é”™ä¿¡æ¯ | æŠ¥é”™æ¥æº | æŸ¥çœ‹æ–¹å¼ |
|------|----------|----------|----------|
| è¿›ç¨‹æœªè¿è¡Œ | `kube-scheduler not running` | systemd/å®¹å™¨ | `systemctl status kube-scheduler` |
| è¿æ¥ API Server å¤±è´¥ | `error retrieving resource lock` | Scheduler æ—¥å¿— | Scheduler æ—¥å¿— |
| è¯ä¹¦é”™è¯¯ | `x509: certificate signed by unknown authority` | Scheduler æ—¥å¿— | Scheduler æ—¥å¿— |
| Leader é€‰ä¸¾å¤±è´¥ | `failed to acquire lease` | Scheduler æ—¥å¿— | Scheduler æ—¥å¿— |
| é…ç½®é”™è¯¯ | `unable to load scheduler config` | Scheduler æ—¥å¿— | Scheduler å¯åŠ¨æ—¥å¿— |

#### 1.1.2 Pod è°ƒåº¦å¤±è´¥

| ç°è±¡ | æŠ¥é”™ä¿¡æ¯ | æŠ¥é”™æ¥æº | æŸ¥çœ‹æ–¹å¼ |
|------|----------|----------|----------|
| èµ„æºä¸è¶³ | `Insufficient cpu/memory` | Pod Events | `kubectl describe pod` |
| èŠ‚ç‚¹ä¸æ»¡è¶³æ¡ä»¶ | `0/N nodes are available` | Pod Events | `kubectl describe pod` |
| äº²å’Œæ€§ä¸æ»¡è¶³ | `node(s) didn't match pod affinity/anti-affinity rules` | Pod Events | `kubectl describe pod` |
| æ±¡ç‚¹ä¸å®¹å¿ | `node(s) had taints that the pod didn't tolerate` | Pod Events | `kubectl describe pod` |
| PVC æœªç»‘å®š | `persistentvolumeclaim not found` | Pod Events | `kubectl describe pod` |
| ç«¯å£å†²çª | `node(s) didn't have free ports for the requested pod ports` | Pod Events | `kubectl describe pod` |
| æ‹“æ‰‘çº¦æŸä¸æ»¡è¶³ | `node(s) didn't match pod topology spread constraints` | Pod Events | `kubectl describe pod` |

#### 1.1.3 è°ƒåº¦æ€§èƒ½é—®é¢˜

| ç°è±¡ | æŠ¥é”™ä¿¡æ¯ | æŠ¥é”™æ¥æº | æŸ¥çœ‹æ–¹å¼ |
|------|----------|----------|----------|
| è°ƒåº¦å»¶è¿Ÿé«˜ | `scheduling_duration_seconds increased` | Prometheus | ç›‘æ§ç³»ç»Ÿ |
| è°ƒåº¦é˜Ÿåˆ—å †ç§¯ | å¤§é‡ Pod å¤„äº Pending | kubectl | `kubectl get pods --field-selector=status.phase=Pending` |
| æ’ä»¶æ‰§è¡Œæ…¢ | `plugin <name> took too long` | Scheduler æ—¥å¿— | Scheduler æ—¥å¿— |
| æŠ¢å é¢‘ç¹ | `preemption attempts increased` | Scheduler æ—¥å¿— | Scheduler æ—¥å¿— |

#### 1.1.4 è°ƒåº¦ç­–ç•¥é—®é¢˜

| ç°è±¡ | æŠ¥é”™ä¿¡æ¯ | æŠ¥é”™æ¥æº | æŸ¥çœ‹æ–¹å¼ |
|------|----------|----------|----------|
| è‡ªå®šä¹‰è°ƒåº¦å™¨æœªç”Ÿæ•ˆ | Pod æœªè¢«é¢„æœŸè°ƒåº¦å™¨è°ƒåº¦ | Pod Spec | `kubectl get pod -o yaml` |
| ä¼˜å…ˆçº§è°ƒåº¦å¼‚å¸¸ | é«˜ä¼˜å…ˆçº§ Pod æœªæŠ¢å  | Pod Events | `kubectl describe pod` |
| è°ƒåº¦é—¨æ§é˜»å¡ | `schedulingGates not cleared` | Pod Events | `kubectl describe pod` (v1.28+) |
| æ‰©å±•ç‚¹é”™è¯¯ | `extension point <name> failed` | Scheduler æ—¥å¿— | Scheduler æ—¥å¿— |

### 1.2 æŠ¥é”™æŸ¥çœ‹æ–¹å¼æ±‡æ€»

```bash
# æŸ¥çœ‹ Scheduler è¿›ç¨‹çŠ¶æ€ï¼ˆsystemd ç®¡ç†ï¼‰
systemctl status kube-scheduler

# æŸ¥çœ‹ Scheduler æ—¥å¿—ï¼ˆsystemd ç®¡ç†ï¼‰
journalctl -u kube-scheduler -f --no-pager -l

# æŸ¥çœ‹ Scheduler æ—¥å¿—ï¼ˆé™æ€ Pod æ–¹å¼ï¼‰
kubectl logs -n kube-system kube-scheduler-<node-name> --tail=500

# æŸ¥çœ‹ Scheduler å®¹å™¨æ—¥å¿—
crictl logs $(crictl ps -q --name kube-scheduler)

# æ£€æŸ¥ Scheduler å¥åº·çŠ¶æ€
curl -k https://127.0.0.1:10259/healthz

# æŸ¥çœ‹ Scheduler Leader ä¿¡æ¯
kubectl get leases -n kube-system kube-scheduler -o yaml

# æŸ¥çœ‹è°ƒåº¦å¤±è´¥çš„ Pod
kubectl get pods --all-namespaces --field-selector=status.phase=Pending

# æŸ¥çœ‹ Pod è°ƒåº¦äº‹ä»¶
kubectl describe pod <pod-name> | grep -A20 Events

# æŸ¥çœ‹ Scheduler æŒ‡æ ‡
curl -k https://127.0.0.1:10259/metrics | grep scheduler
```

### 1.3 å½±å“é¢åˆ†æ

#### 1.3.1 ç›´æ¥å½±å“

| å½±å“èŒƒå›´ | å½±å“ç¨‹åº¦ | å½±å“æè¿° |
|----------|----------|----------|
| **æ–° Pod è°ƒåº¦** | å®Œå…¨ä¸å¯ç”¨ | æ–°åˆ›å»ºçš„ Pod æ— æ³•è¢«è°ƒåº¦åˆ°èŠ‚ç‚¹ |
| **Pod é‡è°ƒåº¦** | ä¸å¯ç”¨ | éœ€è¦é‡è°ƒåº¦çš„ Podï¼ˆå¦‚èŠ‚ç‚¹é©±é€ï¼‰æ— æ³•è°ƒåº¦ |
| **æŠ¢å æœºåˆ¶** | å¤±æ•ˆ | é«˜ä¼˜å…ˆçº§ Pod æ— æ³•æŠ¢å ä½ä¼˜å…ˆçº§ Pod |
| **èµ„æºåˆ†é…** | åœæ» | é›†ç¾¤èµ„æºæ— æ³•è¢«åˆç†åˆ†é… |

#### 1.3.2 é—´æ¥å½±å“

| å½±å“èŒƒå›´ | å½±å“ç¨‹åº¦ | å½±å“æè¿° |
|----------|----------|----------|
| **ç°æœ‰å·¥ä½œè´Ÿè½½** | æ— ç›´æ¥å½±å“ | å·²è¿è¡Œçš„ Pod ç»§ç»­è¿è¡Œ |
| **Deployment æ‰©å®¹** | å¤±è´¥ | æ–°å‰¯æœ¬æ— æ³•è°ƒåº¦ |
| **DaemonSet éƒ¨ç½²** | éƒ¨åˆ†å½±å“ | æ–°èŠ‚ç‚¹ä¸Šçš„ DaemonSet Pod æ— æ³•è°ƒåº¦ |
| **Job/CronJob** | å¤±è´¥ | æ–°çš„ Job Pod æ— æ³•è°ƒåº¦ |
| **æ•…éšœæ¢å¤** | å»¶è¿Ÿ | èŠ‚ç‚¹æ•…éšœå Pod æ— æ³•é‡æ–°è°ƒåº¦ |
| **è‡ªåŠ¨æ‰©ç¼©å®¹** | å¤±æ•ˆ | HPA æ‰©å®¹çš„ Pod æ— æ³•è°ƒåº¦ |
| **æ»šåŠ¨æ›´æ–°** | é˜»å¡ | æ–°ç‰ˆæœ¬ Pod æ— æ³•è°ƒåº¦ï¼Œæ›´æ–°æ— æ³•å®Œæˆ |

#### 1.3.3 å½±å“è¯„ä¼°çŸ©é˜µ

| æ•…éšœæŒç»­æ—¶é—´ | å½±å“ç¨‹åº¦ | ä¸šåŠ¡å½±å“ | å“åº”ä¼˜å…ˆçº§ |
|--------------|----------|----------|------------|
| < 5 åˆ†é’Ÿ | ä½ | å°‘é‡ Pod è°ƒåº¦å»¶è¿Ÿ | P2 |
| 5-30 åˆ†é’Ÿ | ä¸­ | æ–°éƒ¨ç½²å’Œæ‰©å®¹å—é˜» | P1 |
| 30-60 åˆ†é’Ÿ | é«˜ | æ•…éšœæ¢å¤å—å½±å“ | P0 |
| > 60 åˆ†é’Ÿ | ä¸¥é‡ | ä¸šåŠ¡è¿ç»­æ€§é£é™© | P0 ç´§æ€¥ |

---

## 2. æ’æŸ¥æ–¹æ³•ä¸æ­¥éª¤

### 2.1 æ’æŸ¥åŸç†

Scheduler è´Ÿè´£å°† Pod åˆ†é…åˆ°åˆé€‚çš„èŠ‚ç‚¹ã€‚æ’æŸ¥éœ€è¦ä»ä»¥ä¸‹å±‚é¢ï¼š

#### 2.1.1 æœåŠ¡å±‚é¢
- **è°ƒåº¦å¾ªç¯(Scheduling Cycle)**ï¼šScheduler æŒç»­ç›‘å¬ API Server ä¸­ `spec.nodeName` ä¸ºç©ºçš„ Podï¼Œå°†å…¶åŠ å…¥è°ƒåº¦é˜Ÿåˆ—
- **ç»‘å®šå¾ªç¯(Binding Cycle)**ï¼šå¼‚æ­¥æ‰§è¡Œæœ€ç»ˆçš„ Pod ä¸ Node ç»‘å®šæ“ä½œï¼Œé¿å…é˜»å¡è°ƒåº¦å†³ç­–
- **å¥åº·æ£€æŸ¥**ï¼š`/healthz` ç«¯ç‚¹æ£€æŸ¥ Leader çŠ¶æ€ã€Informer ç¼“å­˜åŒæ­¥çŠ¶æ€
- **ä¼˜å…ˆçº§é˜Ÿåˆ—**ï¼šPod æŒ‰ `priorityClass` æ’åºï¼Œé«˜ä¼˜å…ˆçº§ Pod ä¼˜å…ˆè°ƒåº¦ï¼ŒåŒä¼˜å…ˆçº§æŒ‰åˆ›å»ºæ—¶é—´ FIFO

#### 2.1.2 è¿æ¥å±‚é¢
- **Informer æœºåˆ¶**ï¼šScheduler é€šè¿‡ Informer ç¼“å­˜èŠ‚ç‚¹/Pod/PV/PVC ç­‰èµ„æºï¼Œå‡å°‘ API è°ƒç”¨
- **List-Watch**ï¼šåˆå§‹ LIST å…¨é‡åŠ è½½ï¼Œåç»­ WATCH å¢é‡æ›´æ–°ï¼Œç½‘ç»œä¸­æ–­ä¼šå¯¼è‡´ç¼“å­˜å¤±æ•ˆé‡å»º
- **å®¢æˆ·ç«¯é™æµ**ï¼šScheduler å¯¹ API Server çš„ QPS/Burst é™åˆ¶ï¼Œé¿å…è¿‡è½½
- **è¯ä¹¦è®¤è¯**ï¼š`--kubeconfig` æˆ– `--authentication-kubeconfig` é…ç½®å®¢æˆ·ç«¯è¯ä¹¦

#### 2.1.3 é€‰ä¸¾å±‚é¢
- **Lease æœºåˆ¶**ï¼šå¤šä¸ª Scheduler å®ä¾‹é€šè¿‡ Lease èµ„æºç«äº‰æˆä¸º Leaderï¼Œé Leader å¾…å‘½
- **Leader æ ‡è¯†**ï¼š`kube-system/kube-scheduler` Lease çš„ `spec.holderIdentity` æ ‡è¯†å½“å‰ Leader
- **ç§Ÿçº¦ç»­æœŸ**ï¼šLeader æ¯éš” `--leader-elect-renew-deadline`(é»˜è®¤ 10s) ç»­æœŸï¼Œå¤±è´¥åˆ™è§¦å‘é‡æ–°é€‰ä¸¾
- **è„‘è£‚ä¿æŠ¤**ï¼šLease æœ‰å…¨å±€å”¯ä¸€æ€§ï¼Œä¿è¯åªæœ‰ä¸€ä¸ª Leader æ‰§è¡Œè°ƒåº¦

#### 2.1.4 ç®—æ³•å±‚é¢ - è°ƒåº¦æ¡†æ¶(Scheduling Framework)
Scheduler v1.19+ é‡‡ç”¨æ’ä»¶åŒ–è°ƒåº¦æ¡†æ¶ï¼Œæ ¸å¿ƒæ‰©å±•ç‚¹ï¼š

1. **é˜Ÿåˆ—æ’åº(QueueSort)**ï¼šå†³å®š Pod ä»é˜Ÿåˆ—ä¸­è¢«å–å‡ºçš„é¡ºåºï¼ˆé»˜è®¤æŒ‰ä¼˜å…ˆçº§ï¼‰
2. **PreFilter**ï¼šé¢„å¤„ç†ï¼Œå¦‚æ£€æŸ¥ PVC æ˜¯å¦å­˜åœ¨ï¼Œå¤±è´¥åˆ™è·³è¿‡æœ¬è½®è°ƒåº¦
3. **Filter(Predicate)**ï¼š**è¿‡æ»¤é˜¶æ®µ**ï¼Œå¹¶è¡Œæ£€æŸ¥æ¯ä¸ªèŠ‚ç‚¹æ˜¯å¦æ»¡è¶³ Pod è¦æ±‚
   - `NodeResourcesFit`ï¼šæ£€æŸ¥ CPU/å†…å­˜/ä¸´æ—¶å­˜å‚¨æ˜¯å¦å……è¶³
   - `NodeName`ï¼šæ£€æŸ¥ `spec.nodeName` æ˜¯å¦åŒ¹é…
   - `PodToleratesNodeTaints`ï¼šæ£€æŸ¥ Pod æ˜¯å¦å®¹å¿èŠ‚ç‚¹æ±¡ç‚¹
   - `NodeAffinity`ï¼šæ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ»¡è¶³äº²å’Œæ€§
   - `PodTopologySpread`ï¼šæ£€æŸ¥æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
   - `VolumeBinding`ï¼šæ£€æŸ¥ PVC ç»‘å®šä¸èŠ‚ç‚¹å­˜å‚¨èƒ½åŠ›
4. **PostFilter**ï¼šæ‰€æœ‰èŠ‚ç‚¹éƒ½è¢«è¿‡æ»¤åè§¦å‘ï¼Œæ‰§è¡Œ**æŠ¢å (Preemption)**å°è¯•é©±é€ä½ä¼˜å…ˆçº§ Pod
5. **PreScore**ï¼šè¯„åˆ†å‰çš„é¢„å¤„ç†ï¼Œå¦‚è®¡ç®—èŠ‚ç‚¹é—´å¹³è¡¡åº¦
6. **Score(Priority)**ï¼š**æ‰“åˆ†é˜¶æ®µ**ï¼Œä¸ºé€šè¿‡è¿‡æ»¤çš„èŠ‚ç‚¹æ‰“åˆ†ï¼ˆ0-100ï¼‰
   - `NodeResourcesBalancedAllocation`ï¼šCPU/å†…å­˜ä½¿ç”¨å‡è¡¡åº¦
   - `ImageLocality`ï¼šé•œåƒæ˜¯å¦å·²åœ¨èŠ‚ç‚¹ä¸Šï¼ˆå‡å°‘æ‹‰å–æ—¶é—´ï¼‰
   - `InterPodAffinity`ï¼šPod é—´äº²å’Œæ€§/åäº²å’Œæ€§
   - `NodeAffinity`ï¼šèŠ‚ç‚¹äº²å’Œæ€§æƒé‡
   - `TaintToleration`ï¼šå®¹å¿åº¦åŒ¹é…åº¦
7. **NormalizeScore**ï¼šå½’ä¸€åŒ–åˆ†æ•°åˆ° 0-100
8. **Reserve**ï¼šä¸º Pod é¢„ç•™èŠ‚ç‚¹èµ„æºï¼ˆå†…å­˜ä¸­æ ‡è®°ï¼Œæœªå®é™…ç»‘å®šï¼‰
9. **Permit**ï¼šå‡†å…¥æ£€æŸ¥ï¼Œå¯æš‚åœç»‘å®šç­‰å¾…å¤–éƒ¨æ¡ä»¶ï¼ˆå¦‚æ‰¹é‡è°ƒåº¦ï¼‰
10. **PreBind**ï¼šç»‘å®šå‰æ“ä½œï¼Œå¦‚ Volume Attach
11. **Bind**ï¼šæ‰§è¡Œå®é™…ç»‘å®šï¼ˆæ›´æ–° Pod çš„ `spec.nodeName`ï¼‰
12. **PostBind**ï¼šç»‘å®šåæ“ä½œï¼ˆé€šå¸¸ä¸ºç©ºï¼‰

**å¹¶è¡ŒåŒ–**ï¼š
- **Filter å¹¶è¡Œ**ï¼š`--parallelism`ï¼ˆé»˜è®¤ 16ï¼‰æ§åˆ¶å¹¶å‘æ£€æŸ¥çš„èŠ‚ç‚¹æ•°
- **Score ä¸²è¡Œ**ï¼šå„æ’ä»¶é¡ºåºæ‰§è¡Œï¼Œæƒé‡ç´¯åŠ 

#### 2.1.5 é…ç½®å±‚é¢
- **è°ƒåº¦ç­–ç•¥(Policy)**ï¼šv1.23 å‰é€šè¿‡ `--policy-config-file` é…ç½®ï¼Œå·²å¼ƒç”¨
- **è°ƒåº¦é…ç½®(KubeSchedulerConfiguration)**ï¼šv1.23+ æ¨èï¼Œé€šè¿‡ `--config` æŒ‡å®š YAML é…ç½®æ–‡ä»¶
- **æ’ä»¶å¯ç”¨/ç¦ç”¨**ï¼šå¯é€‰æ‹©æ€§å¯ç”¨/ç¦ç”¨å†…ç½®æ’ä»¶æˆ–æ³¨å†Œè‡ªå®šä¹‰æ’ä»¶
- **æ’ä»¶æƒé‡è°ƒæ•´**ï¼šScore æ’ä»¶æƒé‡é»˜è®¤ 1ï¼Œå¯è°ƒæ•´å½±å“æœ€ç»ˆåˆ†æ•°
- **è°ƒåº¦é—¨æ§(SchedulingGates)**ï¼šv1.27+ ç‰¹æ€§ï¼Œå¯æš‚åœè°ƒåº¦ç›´åˆ°å¤–éƒ¨æ¡ä»¶æ»¡è¶³ï¼ˆå¦‚é…é¢æ‰¹å‡†ï¼‰
- **å¤šè°ƒåº¦å™¨**ï¼šåŒä¸€é›†ç¾¤å¯è¿è¡Œå¤šä¸ªè°ƒåº¦å™¨ï¼ŒPod é€šè¿‡ `spec.schedulerName` æŒ‡å®š

### 2.2 æ’æŸ¥é€»è¾‘å†³ç­–æ ‘

```
å¼€å§‹æ’æŸ¥
    â”‚
    â”œâ”€â–º æ£€æŸ¥ Scheduler çŠ¶æ€
    â”‚       â”‚
    â”‚       â”œâ”€â–º è¿›ç¨‹ä¸å­˜åœ¨ â”€â”€â–º æ£€æŸ¥å¯åŠ¨å¤±è´¥åŸå› 
    â”‚       â”‚
    â”‚       â””â”€â–º è¿›ç¨‹å­˜åœ¨ â”€â”€â–º ç»§ç»­ä¸‹ä¸€æ­¥
    â”‚
    â”œâ”€â–º æ£€æŸ¥ API Server è¿æ¥
    â”‚       â”‚
    â”‚       â”œâ”€â–º è¿æ¥å¤±è´¥ â”€â”€â–º æ£€æŸ¥ç½‘ç»œå’Œè¯ä¹¦
    â”‚       â”‚
    â”‚       â””â”€â–º è¿æ¥æ­£å¸¸ â”€â”€â–º ç»§ç»­ä¸‹ä¸€æ­¥
    â”‚
    â”œâ”€â–º æ£€æŸ¥ Leader é€‰ä¸¾ï¼ˆHA åœºæ™¯ï¼‰
    â”‚       â”‚
    â”‚       â”œâ”€â–º é Leader â”€â”€â–º æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»– Leader
    â”‚       â”‚
    â”‚       â””â”€â–º æ˜¯ Leader â”€â”€â–º ç»§ç»­ä¸‹ä¸€æ­¥
    â”‚
    â”œâ”€â–º æ£€æŸ¥è°ƒåº¦å¤±è´¥åŸå› 
    â”‚       â”‚
    â”‚       â”œâ”€â–º èµ„æºä¸è¶³ â”€â”€â–º æ£€æŸ¥èŠ‚ç‚¹èµ„æº
    â”‚       â”‚
    â”‚       â”œâ”€â–º çº¦æŸä¸æ»¡è¶³ â”€â”€â–º æ£€æŸ¥äº²å’Œæ€§/æ±¡ç‚¹é…ç½®
    â”‚       â”‚
    â”‚       â””â”€â–º å…¶ä»–åŸå›  â”€â”€â–º æ ¹æ®äº‹ä»¶åˆ†æ
    â”‚
    â””â”€â–º æ£€æŸ¥è°ƒåº¦æ€§èƒ½
            â”‚
            â”œâ”€â–º å»¶è¿Ÿé«˜ â”€â”€â–º åˆ†ææ’ä»¶æ‰§è¡Œæ—¶é—´
            â”‚
            â””â”€â–º æ€§èƒ½æ­£å¸¸ â”€â”€â–º å®Œæˆæ’æŸ¥
```

### 2.3 æ’æŸ¥æ­¥éª¤å’Œå…·ä½“å‘½ä»¤

#### 2.3.1 ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ Scheduler è¿›ç¨‹çŠ¶æ€

```bash
# æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
ps aux | grep kube-scheduler | grep -v grep

# systemd ç®¡ç†çš„æœåŠ¡çŠ¶æ€
systemctl status kube-scheduler

# é™æ€ Pod æ–¹å¼æ£€æŸ¥
crictl ps -a | grep kube-scheduler

# æŸ¥çœ‹è¿›ç¨‹å¯åŠ¨å‚æ•°
cat /proc/$(pgrep kube-scheduler)/cmdline | tr '\0' '\n'

# æ£€æŸ¥å¥åº·ç«¯ç‚¹
curl -k https://127.0.0.1:10259/healthz

# æŸ¥çœ‹è¯¦ç»†å¥åº·çŠ¶æ€
curl -k 'https://127.0.0.1:10259/healthz?verbose'
```

#### 2.3.2 ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ API Server è¿æ¥

```bash
# æŸ¥çœ‹ Scheduler æ—¥å¿—ä¸­çš„è¿æ¥é”™è¯¯
journalctl -u kube-scheduler | grep -iE "(unable to connect|connection refused|error)"

# æµ‹è¯• kubeconfig æ˜¯å¦æœ‰æ•ˆ
kubectl --kubeconfig=/etc/kubernetes/scheduler.conf get nodes

# æ£€æŸ¥è¯ä¹¦æœ‰æ•ˆæœŸ
openssl x509 -in /etc/kubernetes/pki/scheduler.crt -noout -dates 2>/dev/null || \
openssl x509 -in /etc/kubernetes/scheduler.conf -noout -dates 2>/dev/null

# æ£€æŸ¥ API Server å¯è¾¾æ€§
curl -k https://<api-server-ip>:6443/healthz
```

#### 2.3.3 ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥ Leader é€‰ä¸¾

```bash
# æŸ¥çœ‹ Scheduler Lease
kubectl get leases -n kube-system kube-scheduler -o yaml

# è¾“å‡ºç¤ºä¾‹ï¼š
# spec:
#   holderIdentity: master-1_<uuid>
#   leaseDurationSeconds: 15
#   renewTime: "2024-01-15T10:30:00Z"

# æ£€æŸ¥å½“å‰å“ªä¸ª Scheduler æ˜¯ Leader
kubectl get leases -n kube-system kube-scheduler -o jsonpath='{.spec.holderIdentity}'

# æŸ¥çœ‹ Scheduler æ—¥å¿—ä¸­çš„é€‰ä¸¾ä¿¡æ¯
journalctl -u kube-scheduler | grep -iE "(became leader|acquired lease|lost lease)"

# é«˜å¯ç”¨åœºæ™¯ï¼šæ£€æŸ¥æ‰€æœ‰ Scheduler å®ä¾‹
for node in master-1 master-2 master-3; do
  echo "=== $node ==="
  ssh $node "crictl ps | grep kube-scheduler"
done
```

#### 2.3.4 ç¬¬å››æ­¥ï¼šæ£€æŸ¥è°ƒåº¦å¤±è´¥åŸå› 

```bash
# æŸ¥çœ‹æ‰€æœ‰ Pending Pod
kubectl get pods --all-namespaces --field-selector=status.phase=Pending

# æŸ¥çœ‹ Pod è°ƒåº¦äº‹ä»¶
kubectl describe pod <pod-name> -n <namespace> | grep -A30 Events

# æŸ¥çœ‹ Pod çš„è°ƒåº¦æ¡ä»¶
kubectl get pod <pod-name> -n <namespace> -o yaml | grep -A20 conditions

# æ£€æŸ¥èŠ‚ç‚¹èµ„æº
kubectl describe nodes | grep -A10 "Allocated resources"

# æŸ¥çœ‹èŠ‚ç‚¹å¯ç”¨èµ„æº
kubectl top nodes

# æ£€æŸ¥èŠ‚ç‚¹æ±¡ç‚¹
kubectl get nodes -o custom-columns='NAME:.metadata.name,TAINTS:.spec.taints'

# æ£€æŸ¥ç‰¹å®š Pod çš„äº²å’Œæ€§é…ç½®
kubectl get pod <pod-name> -n <namespace> -o yaml | grep -A50 affinity

# æ£€æŸ¥ PVC çŠ¶æ€
kubectl get pvc -n <namespace>

# æŸ¥çœ‹è°ƒåº¦å™¨è®°å½•çš„å¤±è´¥åŸå› 
kubectl get events --field-selector=reason=FailedScheduling --sort-by='.metadata.creationTimestamp'
```

#### 2.3.5 ç¬¬äº”æ­¥ï¼šæ£€æŸ¥è°ƒåº¦é…ç½®

```bash
# æŸ¥çœ‹ Scheduler é…ç½®æ–‡ä»¶
cat /etc/kubernetes/scheduler-config.yaml

# æ£€æŸ¥ Scheduler å¯åŠ¨å‚æ•°
crictl inspect $(crictl ps -q --name kube-scheduler) | jq '.info.config.process.args'

# æŸ¥çœ‹é»˜è®¤è°ƒåº¦å™¨é…ç½®ï¼ˆv1.25+ï¼‰
kubectl get configmap -n kube-system kube-scheduler -o yaml

# æ£€æŸ¥è°ƒåº¦å™¨ Profile
cat /etc/kubernetes/scheduler-config.yaml | grep -A50 profiles

# éªŒè¯é…ç½®è¯­æ³•
kube-scheduler --config=/etc/kubernetes/scheduler-config.yaml --dry-run
```

#### 2.3.6 ç¬¬å…­æ­¥ï¼šæ£€æŸ¥è°ƒåº¦æ€§èƒ½

```bash
# è·å–è°ƒåº¦å™¨æŒ‡æ ‡
curl -k https://127.0.0.1:10259/metrics | grep -E "scheduler_"

# å…³é”®æŒ‡æ ‡è¯´æ˜ï¼š
# scheduler_scheduling_duration_seconds - è°ƒåº¦å»¶è¿Ÿ
# scheduler_pending_pods - ç­‰å¾…è°ƒåº¦çš„ Pod æ•°
# scheduler_preemption_attempts_total - æŠ¢å å°è¯•æ¬¡æ•°
# scheduler_pod_scheduling_attempts - Pod è°ƒåº¦å°è¯•æ¬¡æ•°

# æ£€æŸ¥è°ƒåº¦å»¶è¿Ÿåˆ†å¸ƒ
curl -k https://127.0.0.1:10259/metrics | grep scheduler_scheduling_duration_seconds

# æ£€æŸ¥ Pending Pod æ•°é‡
curl -k https://127.0.0.1:10259/metrics | grep scheduler_pending_pods

# æ£€æŸ¥è°ƒåº¦é˜Ÿåˆ—çŠ¶æ€
curl -k https://127.0.0.1:10259/metrics | grep scheduler_queue_incoming_pods_total

# æ£€æŸ¥æ’ä»¶æ‰§è¡Œæ—¶é—´
curl -k https://127.0.0.1:10259/metrics | grep scheduler_plugin_execution_duration_seconds
```

#### 2.3.7 ç¬¬ä¸ƒæ­¥ï¼šæ£€æŸ¥æ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
journalctl -u kube-scheduler -f --no-pager

# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯æ—¥å¿—
journalctl -u kube-scheduler -p err --since "1 hour ago"

# é™æ€ Pod æ–¹å¼æŸ¥çœ‹æ—¥å¿—
crictl logs $(crictl ps -q --name kube-scheduler) 2>&1 | tail -500

# æŸ¥æ‰¾è°ƒåº¦å¤±è´¥ç›¸å…³æ—¥å¿—
journalctl -u kube-scheduler | grep -iE "(failed|unable|error|cannot)" | tail -50

# æé«˜æ—¥å¿—çº§åˆ«è¿›è¡Œè°ƒè¯•ï¼ˆä¸´æ—¶ï¼‰
# ä¿®æ”¹å¯åŠ¨å‚æ•°æ·»åŠ  --v=4 æˆ–æ›´é«˜

# æŸ¥çœ‹ç‰¹å®š Pod çš„è°ƒåº¦æ—¥å¿—
journalctl -u kube-scheduler | grep "<pod-name>" | tail -20
```

### 2.4 æ’æŸ¥æ³¨æ„äº‹é¡¹

#### 2.4.1 å®‰å…¨æ³¨æ„äº‹é¡¹

| æ³¨æ„é¡¹ | è¯´æ˜ | å»ºè®® |
|--------|------|------|
| **kubeconfig å®‰å…¨** | Scheduler çš„ kubeconfig æœ‰é›†ç¾¤æƒé™ | ä¸è¦æ³„éœ² |
| **è¯ä¹¦å®‰å…¨** | è¯ä¹¦ç”¨äº API Server è®¤è¯ | å¦¥å–„ä¿ç®¡ |
| **é…ç½®æ•æ„Ÿæ€§** | è°ƒåº¦é…ç½®å½±å“èµ„æºåˆ†é… | å˜æ›´éœ€å®¡æ‰¹ |

#### 2.4.2 æ“ä½œæ³¨æ„äº‹é¡¹

| æ³¨æ„é¡¹ | è¯´æ˜ | å»ºè®® |
|--------|------|------|
| **é«˜å¯ç”¨åœºæ™¯** | å¤š Scheduler å®ä¾‹éœ€è¦ Leader é€‰ä¸¾ | ç¡®ä¿åªæœ‰ä¸€ä¸ª Leader |
| **é…ç½®å˜æ›´** | é…ç½®å˜æ›´éœ€è¦é‡å¯ Scheduler | åœ¨ç»´æŠ¤çª—å£æ“ä½œ |
| **æ—¥å¿—çº§åˆ«** | é«˜æ—¥å¿—çº§åˆ«ä¼šå½±å“æ€§èƒ½ | è°ƒè¯•å®Œæˆåæ¢å¤ |
| **è‡ªå®šä¹‰è°ƒåº¦å™¨** | æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†è‡ªå®šä¹‰è°ƒåº¦å™¨ | ç¡®è®¤ schedulerName |

### ğŸš€ 2.5 æ·±åº¦è§£æï¼ˆä¸“å®¶ä¸“åŒºï¼‰

#### 2.5.1 è°ƒåº¦å™¨çš„ä¹è§‚å¹¶å‘ä¸é‡è¯•æœºåˆ¶
Scheduler åœ¨åšå†³å®šæ—¶å¹¶ä¸ä¼šé”å®šèŠ‚ç‚¹ï¼Œè€Œæ˜¯é‡‡ç”¨ä¹è§‚é”ï¼ˆOptimistic Concurrencyï¼‰ã€‚
- **ä¸“å®¶æç¤º**ï¼šå¦‚æœåœ¨ `Bind` é˜¶æ®µå¤±è´¥ï¼ˆé€šå¸¸æ˜¯ etcd å“åº”æ…¢æˆ–èµ„æºå·²è¢«æŠ¢å…ˆå ç”¨ï¼‰ï¼ŒPod ä¼šé‡æ–°è¿›å…¥è°ƒåº¦é˜Ÿåˆ—ã€‚é€šè¿‡è§‚å¯Ÿ `scheduler_pod_scheduling_attempts` æŒ‡æ ‡å¯ä»¥åˆ¤æ–­é›†ç¾¤æ˜¯å¦å­˜åœ¨æ¿€çƒˆçš„è°ƒåº¦ç«äº‰ã€‚

#### 2.5.2 èµ„æºé¢„ç•™ï¼ˆRequestsï¼‰ä¸è¶…å”®ï¼ˆOvercommitï¼‰
- **æ ¸å¿ƒé€»è¾‘**ï¼šScheduler åªçœ‹ `requests` è€Œé `limits` æˆ–èŠ‚ç‚¹å®é™… CPU/å†…å­˜è´Ÿè½½ã€‚
- **ä¸“å®¶æç¤º**ï¼šå¦‚æœèŠ‚ç‚¹å·²ç»å¾ˆå¡ä½† Scheduler è¿˜åœ¨å¾€ä¸Šé¢è°ƒåº¦ Podï¼Œè¯´æ˜ `requests` è®¾ç½®å¾—å¤ªå°ã€‚å»ºè®®ä½¿ç”¨ `VerticalPodAutoscaler` (VPA) æˆ–æ‰‹åŠ¨è°ƒæ•´ `requests` ä»¥é€¼è¿‘çœŸå®æ¶ˆè€—ã€‚

#### 2.5.3 äº²å’Œæ€§å†²çªçš„â€œæ­»ç»“â€
ç°è±¡ï¼šå¤šä¸ª Pod è®¾ç½®äº†å¼ºäº²å’Œæ€§ï¼ˆRequiredï¼‰ï¼Œä½†é›†ç¾¤ä¸­æ²¡æœ‰è¶³å¤Ÿçš„èŠ‚ç‚¹èƒ½åŒæ—¶æ»¡è¶³æ‰€æœ‰ Pod çš„äº’æ–¥æ¡ä»¶ã€‚
- **æ’æŸ¥æ€è·¯**ï¼šæ£€æŸ¥ `podAntiAffinity` çš„ `topologyKey`ã€‚å¦‚æœæ‰€æœ‰ Pod éƒ½è¦æ±‚åœ¨ä¸åŒèŠ‚ç‚¹ä¸” `topologyKey: kubernetes.io/hostname`ï¼Œè€ŒèŠ‚ç‚¹æ•°å°‘äºå‰¯æœ¬æ•°ï¼Œåˆ™å¿…å®šæœ‰ Pod Pendingã€‚
- **è§£å†³æ–¹æ¡ˆ**ï¼šè¯„ä¼°æ˜¯å¦å¯ä»¥ä½¿ç”¨ `preferredDuringSchedulingIgnoredDuringExecution` (è½¯äº²å’Œæ€§) æ¥å¢åŠ çµæ´»æ€§ã€‚

---

## 3. è§£å†³æ–¹æ¡ˆä¸é£é™©æ§åˆ¶

### 3.1 Scheduler è¿›ç¨‹æœªè¿è¡Œ

#### 3.1.1 è§£å†³æ­¥éª¤

```bash
# æ­¥éª¤ 1ï¼šæ£€æŸ¥å¯åŠ¨å¤±è´¥åŸå› 
journalctl -u kube-scheduler -b --no-pager | tail -100

# æ­¥éª¤ 2ï¼šæ£€æŸ¥é…ç½®æ–‡ä»¶è¯­æ³•
# éªŒè¯ YAML è¯­æ³•
python3 -c "import yaml; yaml.safe_load(open('/etc/kubernetes/manifests/kube-scheduler.yaml'))"

# æ­¥éª¤ 3ï¼šæ£€æŸ¥è¯ä¹¦æ–‡ä»¶
ls -la /etc/kubernetes/pki/
ls -la /etc/kubernetes/scheduler.conf

# æ­¥éª¤ 4ï¼šéªŒè¯ kubeconfig
kubectl --kubeconfig=/etc/kubernetes/scheduler.conf cluster-info

# æ­¥éª¤ 5ï¼šä¿®å¤é—®é¢˜åé‡å¯
# systemd æ–¹å¼
systemctl restart kube-scheduler

# é™æ€ Pod æ–¹å¼
mv /etc/kubernetes/manifests/kube-scheduler.yaml /tmp/
sleep 5
mv /tmp/kube-scheduler.yaml /etc/kubernetes/manifests/

# æ­¥éª¤ 6ï¼šéªŒè¯æ¢å¤
kubectl get pods -n kube-system | grep scheduler
curl -k https://127.0.0.1:10259/healthz
```

#### 3.1.2 æ‰§è¡Œé£é™©

| é£é™©ç­‰çº§ | é£é™©æè¿° | ç¼“è§£æªæ–½ |
|----------|----------|----------|
| **ä¸­** | é‡å¯æœŸé—´æ–° Pod æ— æ³•è°ƒåº¦ | åœ¨ç»´æŠ¤çª—å£æ“ä½œ |
| **ä½** | é…ç½®æ£€æŸ¥ä¸€èˆ¬æ— é£é™© | - |
| **ä¸­** | é…ç½®ä¿®æ”¹å¯èƒ½å¼•å…¥æ–°é—®é¢˜ | ä¿®æ”¹å‰å¤‡ä»½ |

#### 3.1.3 å®‰å…¨ç”Ÿäº§é£é™©æç¤º

```
âš ï¸  å®‰å…¨ç”Ÿäº§é£é™©æç¤ºï¼š
1. Scheduler ä¸å¯ç”¨æœŸé—´æ–° Pod å°†å¤„äº Pending çŠ¶æ€
2. å·²è¿è¡Œçš„ Pod ä¸å—å½±å“
3. é«˜å¯ç”¨é›†ç¾¤ç¡®ä¿å…¶ä»– Scheduler å®ä¾‹æ­£å¸¸
4. ä¿®æ”¹é…ç½®å‰å¤‡ä»½åŸå§‹æ–‡ä»¶
5. éªŒè¯æ¢å¤åæ£€æŸ¥ Pending Pod æ˜¯å¦è¢«è°ƒåº¦
```

### 3.2 Pod å› èµ„æºä¸è¶³æ— æ³•è°ƒåº¦

#### 3.2.1 è§£å†³æ­¥éª¤

```bash
# æ­¥éª¤ 1ï¼šç¡®è®¤èµ„æºä¸è¶³æƒ…å†µ
kubectl describe pod <pod-name> | grep -A10 Events

# æ­¥éª¤ 2ï¼šæ£€æŸ¥èŠ‚ç‚¹èµ„æºä½¿ç”¨
kubectl describe nodes | grep -A15 "Allocated resources"
kubectl top nodes

# æ­¥éª¤ 3ï¼šæ£€æŸ¥ Pod èµ„æºè¯·æ±‚
kubectl get pod <pod-name> -o yaml | grep -A10 resources

# æ­¥éª¤ 4ï¼šè§£å†³æ–¹æ¡ˆé€‰æ‹©
# æ–¹æ¡ˆ Aï¼šå‡å°‘ Pod èµ„æºè¯·æ±‚ï¼ˆå¦‚æœè¯·æ±‚è¿‡å¤§ï¼‰
kubectl patch deployment <name> -p '{"spec":{"template":{"spec":{"containers":[{"name":"<container>","resources":{"requests":{"cpu":"100m","memory":"128Mi"}}}]}}}}'

# æ–¹æ¡ˆ Bï¼šæ‰©å®¹èŠ‚ç‚¹èµ„æºï¼ˆæ·»åŠ æ–°èŠ‚ç‚¹ï¼‰
# è”ç³»è¿ç»´æˆ–äº‘å¹³å°æ·»åŠ èŠ‚ç‚¹

# æ–¹æ¡ˆ Cï¼šæ¸…ç†æ— ç”¨èµ„æº
kubectl get pods --all-namespaces | grep -E "(Evicted|Error|Completed)" | awk '{print $1,$2}' | xargs -L1 kubectl delete pod -n

# æ–¹æ¡ˆ Dï¼šä½¿ç”¨é›†ç¾¤è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ˆCAï¼‰
# ç¡®ä¿ Cluster Autoscaler å·²é…ç½®å¹¶æ­£å¸¸å·¥ä½œ
kubectl get pods -n kube-system | grep cluster-autoscaler

# æ­¥éª¤ 5ï¼šéªŒè¯è°ƒåº¦æˆåŠŸ
kubectl get pod <pod-name> -w
```

#### 3.2.2 æ‰§è¡Œé£é™©

| é£é™©ç­‰çº§ | é£é™©æè¿° | ç¼“è§£æªæ–½ |
|----------|----------|----------|
| **ä½** | å‡å°‘èµ„æºè¯·æ±‚å¯èƒ½å½±å“æ€§èƒ½ | æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ |
| **ä¸­** | æ·»åŠ èŠ‚ç‚¹éœ€è¦æ—¶é—´ | è¯„ä¼°ä¸šåŠ¡ç´§æ€¥ç¨‹åº¦ |
| **ä½** | æ¸…ç†èµ„æºä¸€èˆ¬æ— é£é™© | ç¡®è®¤æ˜¯æ— ç”¨èµ„æº |

#### 3.2.3 å®‰å…¨ç”Ÿäº§é£é™©æç¤º

```
âš ï¸  å®‰å…¨ç”Ÿäº§é£é™©æç¤ºï¼š
1. å‡å°‘èµ„æºè¯·æ±‚å‰ç¡®è®¤åº”ç”¨å®é™…éœ€æ±‚
2. ä¸è¦è¿‡åº¦å‡å°‘ request å¯¼è‡´èµ„æºäº‰æŠ¢
3. æ¸…ç†èµ„æºå‰ç¡®è®¤ä¸ä¼šå½±å“ä¸šåŠ¡
4. æ·»åŠ èŠ‚ç‚¹åéªŒè¯èŠ‚ç‚¹çŠ¶æ€æ­£å¸¸
5. è€ƒè™‘è®¾ç½® ResourceQuota é˜²æ­¢èµ„æºè¿‡åº¦ä½¿ç”¨
```

### 3.3 Pod å› äº²å’Œæ€§/æ±¡ç‚¹æ— æ³•è°ƒåº¦

#### 3.3.1 è§£å†³æ­¥éª¤

```bash
# æ­¥éª¤ 1ï¼šæ£€æŸ¥ Pod äº²å’Œæ€§é…ç½®
kubectl get pod <pod-name> -o yaml | grep -A30 affinity

# æ­¥éª¤ 2ï¼šæ£€æŸ¥èŠ‚ç‚¹æ ‡ç­¾
kubectl get nodes --show-labels

# æ­¥éª¤ 3ï¼šæ£€æŸ¥èŠ‚ç‚¹æ±¡ç‚¹
kubectl get nodes -o custom-columns='NAME:.metadata.name,TAINTS:.spec.taints'

# æ­¥éª¤ 4ï¼šæ£€æŸ¥ Pod å®¹å¿åº¦
kubectl get pod <pod-name> -o yaml | grep -A10 tolerations

# æ­¥éª¤ 5ï¼šè§£å†³æ–¹æ¡ˆé€‰æ‹©
# æ–¹æ¡ˆ Aï¼šä¿®æ”¹ Pod äº²å’Œæ€§é…ç½®
kubectl patch deployment <name> -p '{"spec":{"template":{"spec":{"affinity":null}}}}'

# æ–¹æ¡ˆ Bï¼šæ·»åŠ èŠ‚ç‚¹æ ‡ç­¾
kubectl label nodes <node-name> <key>=<value>

# æ–¹æ¡ˆ Cï¼šç§»é™¤èŠ‚ç‚¹æ±¡ç‚¹
kubectl taint nodes <node-name> <key>-

# æ–¹æ¡ˆ Dï¼šæ·»åŠ  Pod å®¹å¿åº¦
kubectl patch deployment <name> -p '{"spec":{"template":{"spec":{"tolerations":[{"key":"<key>","operator":"Exists","effect":"NoSchedule"}]}}}}'

# æ­¥éª¤ 6ï¼šéªŒè¯è°ƒåº¦
kubectl get pod <pod-name> -w
```

#### 3.3.2 æ‰§è¡Œé£é™©

| é£é™©ç­‰çº§ | é£é™©æè¿° | ç¼“è§£æªæ–½ |
|----------|----------|----------|
| **ä¸­** | ä¿®æ”¹äº²å’Œæ€§å¯èƒ½å½±å“é«˜å¯ç”¨ | è¯„ä¼°è°ƒåº¦ç­–ç•¥å˜æ›´å½±å“ |
| **ä½** | æ·»åŠ æ ‡ç­¾ä¸€èˆ¬æ— é£é™© | ç¡®è®¤æ ‡ç­¾ç”¨é€” |
| **ä¸­** | ç§»é™¤æ±¡ç‚¹å¯èƒ½å¯¼è‡´ä¸åˆé€‚çš„è°ƒåº¦ | è¯„ä¼°æ±¡ç‚¹çš„ä½œç”¨ |

#### 3.3.3 å®‰å…¨ç”Ÿäº§é£é™©æç¤º

```
âš ï¸  å®‰å…¨ç”Ÿäº§é£é™©æç¤ºï¼š
1. ä¿®æ”¹äº²å’Œæ€§å‰ç†è§£åŸæœ‰é…ç½®çš„ç›®çš„
2. èŠ‚ç‚¹æ±¡ç‚¹é€šå¸¸æœ‰ç‰¹å®šç”¨é€”ï¼Œç§»é™¤å‰éœ€è¯„ä¼°
3. æ‰¹é‡ä¿®æ”¹äº²å’Œæ€§å¯èƒ½å¯¼è‡´å¤§é‡ Pod é‡è°ƒåº¦
4. å»ºè®®ä½¿ç”¨è½¯äº²å’Œæ€§ï¼ˆpreferredï¼‰è€Œéç¡¬äº²å’Œæ€§ï¼ˆrequiredï¼‰
5. å˜æ›´åç›‘æ§ Pod åˆ†å¸ƒæƒ…å†µ
```

### 3.4 Scheduler æ€§èƒ½é—®é¢˜

#### 3.4.1 è§£å†³æ­¥éª¤

```bash
# æ­¥éª¤ 1ï¼šç¡®è®¤æ€§èƒ½ç“¶é¢ˆ
curl -k https://127.0.0.1:10259/metrics | grep scheduler_scheduling_duration_seconds

# æ­¥éª¤ 2ï¼šæ£€æŸ¥è°ƒåº¦é˜Ÿåˆ—
curl -k https://127.0.0.1:10259/metrics | grep scheduler_pending_pods

# æ­¥éª¤ 3ï¼šåˆ†ææ’ä»¶æ‰§è¡Œæ—¶é—´
curl -k https://127.0.0.1:10259/metrics | grep scheduler_plugin_execution_duration_seconds

# æ­¥éª¤ 4ï¼šä¼˜åŒ–è°ƒåº¦å™¨é…ç½®
# è°ƒæ•´å¹¶è¡Œåº¦ï¼ˆv1.25+ï¼‰
cat > /etc/kubernetes/scheduler-config.yaml << EOF
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
parallelism: 32  # é»˜è®¤ 16
profiles:
  - schedulerName: default-scheduler
    plugins:
      preScore:
        disabled:
          - name: InterPodAffinity  # ç¦ç”¨é«˜å¼€é”€æ’ä»¶ï¼ˆå¦‚ä¸éœ€è¦ï¼‰
EOF

# æ­¥éª¤ 5ï¼šé‡å¯ Scheduler åº”ç”¨é…ç½®
mv /etc/kubernetes/manifests/kube-scheduler.yaml /tmp/
sleep 5
mv /tmp/kube-scheduler.yaml /etc/kubernetes/manifests/

# æ­¥éª¤ 6ï¼šéªŒè¯æ€§èƒ½æ”¹å–„
curl -k https://127.0.0.1:10259/metrics | grep scheduler_scheduling_duration_seconds
```

#### 3.4.2 æ‰§è¡Œé£é™©

| é£é™©ç­‰çº§ | é£é™©æè¿° | ç¼“è§£æªæ–½ |
|----------|----------|----------|
| **ä¸­** | ç¦ç”¨æ’ä»¶å¯èƒ½å½±å“è°ƒåº¦ç­–ç•¥ | ç¡®è®¤æ’ä»¶ä½œç”¨åå†ç¦ç”¨ |
| **ä¸­** | é…ç½®å˜æ›´éœ€è¦é‡å¯ | åœ¨ç»´æŠ¤çª—å£æ“ä½œ |
| **ä½** | è°ƒæ•´å¹¶è¡Œåº¦ä¸€èˆ¬æ— é£é™© | æ ¹æ® CPU èµ„æºè°ƒæ•´ |

#### 3.4.3 å®‰å…¨ç”Ÿäº§é£é™©æç¤º

```
âš ï¸  å®‰å…¨ç”Ÿäº§é£é™©æç¤ºï¼š
1. ç¦ç”¨æ’ä»¶å‰ç†è§£å…¶åŠŸèƒ½
2. InterPodAffinity æ’ä»¶å¯¹æ€§èƒ½å½±å“å¤§ï¼Œä½†æŸäº›åœºæ™¯å¿…éœ€
3. å¢åŠ å¹¶è¡Œåº¦ä¼šå¢åŠ  CPU ä½¿ç”¨
4. é…ç½®å˜æ›´åç›‘æ§è°ƒåº¦å»¶è¿Ÿ
5. å¤§è§„æ¨¡é›†ç¾¤å»ºè®®ä½¿ç”¨è°ƒåº¦æ¡†æ¶æ‰©å±•
```

### 3.5 è‡ªå®šä¹‰è°ƒåº¦å™¨é—®é¢˜

#### 3.5.1 è§£å†³æ­¥éª¤

```bash
# æ­¥éª¤ 1ï¼šç¡®è®¤ Pod ä½¿ç”¨çš„è°ƒåº¦å™¨
kubectl get pod <pod-name> -o yaml | grep schedulerName

# æ­¥éª¤ 2ï¼šæ£€æŸ¥è‡ªå®šä¹‰è°ƒåº¦å™¨çŠ¶æ€
kubectl get pods -n kube-system | grep <scheduler-name>

# æ­¥éª¤ 3ï¼šæŸ¥çœ‹è‡ªå®šä¹‰è°ƒåº¦å™¨æ—¥å¿—
kubectl logs -n kube-system <custom-scheduler-pod>

# æ­¥éª¤ 4ï¼šå¦‚æœè‡ªå®šä¹‰è°ƒåº¦å™¨æ•…éšœï¼Œä¸´æ—¶ä½¿ç”¨é»˜è®¤è°ƒåº¦å™¨
kubectl patch deployment <name> -p '{"spec":{"template":{"spec":{"schedulerName":"default-scheduler"}}}}'

# æ­¥éª¤ 5ï¼šä¿®å¤è‡ªå®šä¹‰è°ƒåº¦å™¨
# æ£€æŸ¥è°ƒåº¦å™¨ Deployment
kubectl describe deployment -n kube-system <custom-scheduler>

# æ£€æŸ¥è°ƒåº¦å™¨ RBAC
kubectl get clusterrolebinding | grep <custom-scheduler>

# æ­¥éª¤ 6ï¼šæ¢å¤ä½¿ç”¨è‡ªå®šä¹‰è°ƒåº¦å™¨
kubectl patch deployment <name> -p '{"spec":{"template":{"spec":{"schedulerName":"<custom-scheduler>"}}}}'
```

#### 3.5.2 æ‰§è¡Œé£é™©

| é£é™©ç­‰çº§ | é£é™©æè¿° | ç¼“è§£æªæ–½ |
|----------|----------|----------|
| **ä¸­** | åˆ‡æ¢è°ƒåº¦å™¨å¯èƒ½å½±å“è°ƒåº¦ç­–ç•¥ | ä¸´æ—¶æªæ–½ï¼Œå°½å¿«ä¿®å¤åŸè°ƒåº¦å™¨ |
| **ä½** | æ—¥å¿—æŸ¥çœ‹æ— é£é™© | - |
| **ä¸­** | RBAC å˜æ›´å¯èƒ½å½±å“æƒé™ | è°¨æ…ä¿®æ”¹ |

#### 3.5.3 å®‰å…¨ç”Ÿäº§é£é™©æç¤º

```
âš ï¸  å®‰å…¨ç”Ÿäº§é£é™©æç¤ºï¼š
1. è‡ªå®šä¹‰è°ƒåº¦å™¨å¯èƒ½æœ‰ç‰¹å®šçš„è°ƒåº¦ç­–ç•¥
2. åˆ‡æ¢åˆ°é»˜è®¤è°ƒåº¦å™¨æ˜¯ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
3. ç¡®ä¿è‡ªå®šä¹‰è°ƒåº¦å™¨æœ‰æ­£ç¡®çš„ RBAC æƒé™
4. è‡ªå®šä¹‰è°ƒåº¦å™¨éœ€è¦æ­£ç¡®å¤„ç† Leader é€‰ä¸¾
5. ç›‘æ§è‡ªå®šä¹‰è°ƒåº¦å™¨çš„å¥åº·çŠ¶æ€
```

---

## é™„å½•

### A. Scheduler å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡åç§° | è¯´æ˜ | å‘Šè­¦é˜ˆå€¼å»ºè®® |
|----------|------|--------------|
| `scheduler_scheduling_duration_seconds` | è°ƒåº¦å»¶è¿Ÿ | P99 > 1s |
| `scheduler_pending_pods` | Pending Pod æ•° | > 100 |
| `scheduler_preemption_attempts_total` | æŠ¢å å°è¯•æ•° | å¼‚å¸¸å¢é•¿ |
| `scheduler_pod_scheduling_attempts` | è°ƒåº¦å°è¯•æ¬¡æ•° | æ¯ Pod > 10 |
| `scheduler_queue_incoming_pods_total` | å…¥é˜Ÿ Pod æ•° | ç›‘æ§è¶‹åŠ¿ |

### B. å¸¸è§å¯åŠ¨å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--config` | - | è°ƒåº¦å™¨é…ç½®æ–‡ä»¶è·¯å¾„ |
| `--leader-elect` | true | æ˜¯å¦å¯ç”¨ Leader é€‰ä¸¾ |
| `--bind-address` | 0.0.0.0 | ç›‘å¬åœ°å€ |
| `--secure-port` | 10259 | HTTPS ç«¯å£ |
| `--v` | 0 | æ—¥å¿—çº§åˆ« |

### C. è°ƒåº¦å¤±è´¥å¸¸è§åŸå› é€ŸæŸ¥

| é”™è¯¯ä¿¡æ¯ | åŸå›  | è§£å†³æ–¹å‘ |
|----------|------|----------|
| `Insufficient cpu` | CPU èµ„æºä¸è¶³ | æ·»åŠ èŠ‚ç‚¹æˆ–å‡å°‘è¯·æ±‚ |
| `Insufficient memory` | å†…å­˜èµ„æºä¸è¶³ | æ·»åŠ èŠ‚ç‚¹æˆ–å‡å°‘è¯·æ±‚ |
| `node(s) had taints` | èŠ‚ç‚¹æœ‰æ±¡ç‚¹ | æ·»åŠ å®¹å¿åº¦æˆ–ç§»é™¤æ±¡ç‚¹ |
| `didn't match node selector` | èŠ‚ç‚¹é€‰æ‹©å™¨ä¸åŒ¹é… | ä¿®æ”¹é€‰æ‹©å™¨æˆ–æ·»åŠ æ ‡ç­¾ |
| `didn't match pod affinity` | äº²å’Œæ€§ä¸æ»¡è¶³ | ä¿®æ”¹äº²å’Œæ€§é…ç½® |
| `PersistentVolumeClaim not found` | PVC ä¸å­˜åœ¨ | åˆ›å»º PVC |

---

## ğŸ“š D. ç”Ÿäº§ç¯å¢ƒå®æˆ˜æ¡ˆä¾‹ç²¾é€‰

### æ¡ˆä¾‹ 1ï¼šæ‹“æ‰‘åˆ†å¸ƒçº¦æŸé…ç½®é”™è¯¯å¯¼è‡´å¤§è§„æ¨¡ Pod Pending

#### ğŸ¯ æ•…éšœåœºæ™¯
æŸç”µå•†å…¬å¸åœ¨é»‘äº”ä¿ƒé”€å‰å¯¹æ ¸å¿ƒæœåŠ¡è¿›è¡Œæ‰©å®¹ï¼Œå°†å‰¯æœ¬æ•°ä» 50 æå‡è‡³ 200ï¼Œç»“æœ 150 ä¸ªæ–° Pod å…¨éƒ¨ Pendingï¼Œæ‰©å®¹å¤±è´¥ï¼Œå·®ç‚¹å½±å“å¤§ä¿ƒã€‚

#### ğŸ” æ’æŸ¥è¿‡ç¨‹
1. **ç°è±¡ç¡®è®¤**ï¼š
   ```bash
   kubectl get pods -n production | grep Pending | wc -l
   # 150
   
   # æŸ¥çœ‹è°ƒåº¦å¤±è´¥åŸå› 
   kubectl describe pod my-service-7d8f9c-xxxxx -n production
   # Events:
   # Warning  FailedScheduling  0/100 nodes are available: 150 pod didn't match pod topology spread constraints.
   ```

2. **æ‹“æ‰‘çº¦æŸæ£€æŸ¥**ï¼š
   ```bash
   kubectl get pod my-service-7d8f9c-xxxxx -n production -o yaml | grep -A20 topologySpreadConstraints
   # topologySpreadConstraints:
   # - maxSkew: 1
   #   topologyKey: kubernetes.io/hostname
   #   whenUnsatisfiable: DoNotSchedule  # âŒ ç¡¬çº¦æŸ
   #   labelSelector:
   #     matchLabels:
   #       app: my-service
   ```

3. **èŠ‚ç‚¹åˆ†å¸ƒåˆ†æ**ï¼š
   ```bash
   # ç»Ÿè®¡å„èŠ‚ç‚¹ç°æœ‰ Pod æ•°
   kubectl get pods -n production -l app=my-service -o json | \
     jq -r '.items[] | select(.spec.nodeName!=null) | .spec.nodeName' | \
     sort | uniq -c | sort -rn
   # 50 node-01  # âŒ ç¬¬ä¸€æ‰¹ 50 ä¸ª Pod å…¨åœ¨åŒä¸€èŠ‚ç‚¹
   ```

4. **æ ¹å› åˆ†æ**ï¼š
   - é…ç½®äº† `maxSkew: 1` + `whenUnsatisfiable: DoNotSchedule`ï¼ˆç¡¬çº¦æŸï¼‰
   - æ„å›¾ï¼šæ¯ä¸ªèŠ‚ç‚¹æœ€å¤šæ¯”å…¶ä»–èŠ‚ç‚¹å¤š 1 ä¸ª Pod
   - å®é™…ï¼šç¬¬ä¸€æ‰¹ 50 ä¸ª Pod å› è°ƒåº¦é¡ºåºéƒ½è½åœ¨ node-01ï¼ˆè¯¥èŠ‚ç‚¹èµ„æºå……è¶³ï¼‰
   - æ‰©å®¹æ—¶ï¼šæ–° Pod æ— æ³•è°ƒåº¦åˆ° node-01ï¼ˆå·²æœ‰ 50 ä¸ªï¼‰ï¼Œå…¶ä»–èŠ‚ç‚¹æœ€å¤šåªèƒ½æ”¾ 51 ä¸ªï¼Œæ— æ³•æ»¡è¶³ `maxSkew: 1` çº¦æŸ
   - **æ­»é”**ï¼š150 ä¸ªæ–° Pod æ°¸è¿œæ— æ³•è°ƒåº¦ï¼

#### âš¡ åº”æ€¥æªæ–½
1. **ä¸´æ—¶æ”¾å®½çº¦æŸ**ï¼ˆä¿®æ”¹ä¸ºè½¯çº¦æŸï¼‰ï¼š
   ```bash
   # ä¿®æ”¹ Deployment
   kubectl edit deployment my-service -n production
   
   # ä¿®æ”¹çº¦æŸ
   topologySpreadConstraints:
   - maxSkew: 1
     topologyKey: kubernetes.io/hostname
     whenUnsatisfiable: ScheduleAnyway  # âœ… æ”¹ä¸ºè½¯çº¦æŸ
     labelSelector:
       matchLabels:
         app: my-service
   ```

2. **è§¦å‘é‡æ–°è°ƒåº¦**ï¼š
   ```bash
   # Deployment ä¼šè‡ªåŠ¨è§¦å‘æ»šåŠ¨æ›´æ–°
   kubectl rollout status deployment my-service -n production
   
   # éªŒè¯ Pod è°ƒåº¦æˆåŠŸ
   kubectl get pods -n production -l app=my-service --field-selector=status.phase=Running | wc -l
   # 200  âœ… å…¨éƒ¨è°ƒåº¦æˆåŠŸ
   ```

3. **éªŒè¯åˆ†å¸ƒæƒ…å†µ**ï¼š
   ```bash
   kubectl get pods -n production -l app=my-service -o json | \
     jq -r '.items[] | select(.spec.nodeName!=null) | .spec.nodeName' | \
     sort | uniq -c | sort -rn | head -10
   # 52 node-01  # ç•¥æœ‰ä¸å‡è¡¡ï¼Œä½†å¯æ¥å—
   # 51 node-02
   # 50 node-03
   # ...
   ```

#### ğŸ›¡ï¸ é•¿æœŸä¼˜åŒ–
1. **ä¼˜åŒ–æ‹“æ‰‘çº¦æŸç­–ç•¥**ï¼š
   ```yaml
   # æ¨èé…ç½®
   topologySpreadConstraints:
   - maxSkew: 2  # âœ… æ”¾å®½è‡³ 2ï¼Œå¢åŠ è°ƒåº¦çµæ´»æ€§
     topologyKey: topology.kubernetes.io/zone  # âœ… å…ˆä¿è¯è·¨å¯ç”¨åŒºå‡è¡¡
     whenUnsatisfiable: DoNotSchedule  # è·¨ AZ ç¡¬çº¦æŸ
     labelSelector:
       matchLabels:
         app: my-service
   - maxSkew: 5  # âœ… èŠ‚ç‚¹çº§å®¹å¿æ›´å¤§åå·®
     topologyKey: kubernetes.io/hostname
     whenUnsatisfiable: ScheduleAnyway  # èŠ‚ç‚¹çº§è½¯çº¦æŸ
     labelSelector:
       matchLabels:
         app: my-service
   ```

2. **ä½¿ç”¨ PodDisruptionBudget ä¿è¯å¯ç”¨æ€§**ï¼š
   ```yaml
   apiVersion: policy/v1
   kind: PodDisruptionBudget
   metadata:
     name: my-service-pdb
     namespace: production
   spec:
     minAvailable: 80%  # ä¿è¯è‡³å°‘ 80% å‰¯æœ¬å¯ç”¨
     selector:
       matchLabels:
         app: my-service
   ```

3. **é¢„æ‰©å®¹æ¼”ç»ƒ**ï¼š
   ```bash
   # å¤§ä¿ƒå‰æ¨¡æ‹Ÿæ‰©å®¹
   kubectl scale deployment my-service --replicas=200 -n production
   
   # è§‚å¯Ÿ 5 åˆ†é’Ÿå†…æ˜¯å¦å…¨éƒ¨è°ƒåº¦æˆåŠŸ
   watch -n 5 'kubectl get pods -n production -l app=my-service | grep -E "(Pending|ContainerCreating)" | wc -l'
   ```

4. **ç›‘æ§å‘Šè­¦**ï¼š
   ```yaml
   # Prometheus å‘Šè­¦è§„åˆ™
   - alert: PodSchedulingFailed
     expr: kube_pod_status_phase{phase="Pending"} > 10
     for: 5m
     labels:
       severity: warning
     annotations:
       summary: "å¤§é‡ Pod è°ƒåº¦å¤±è´¥"
       description: "å‘½åç©ºé—´ {{ $labels.namespace }} æœ‰ {{ $value }} ä¸ª Pod Pending è¶…è¿‡ 5 åˆ†é’Ÿ"
   ```

#### ğŸ’¡ ç»éªŒæ€»ç»“
- **ç¡¬çº¦æŸé£é™©**ï¼š`DoNotSchedule` å¯èƒ½å¯¼è‡´è°ƒåº¦æ­»é”ï¼Œç”Ÿäº§ç¯å¢ƒæ…ç”¨
- **æµ‹è¯•ä¸è¶³**ï¼šæœªåœ¨é¢„ç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿå¤§è§„æ¨¡æ‰©å®¹
- **é…ç½®å¤æ‚æ€§**ï¼šæ‹“æ‰‘çº¦æŸè¯­ä¹‰ä¸ç›´è§‚ï¼Œéœ€æ·±å…¥ç†è§£
- **æ”¹è¿›æ–¹å‘**ï¼šä¼˜å…ˆä½¿ç”¨è½¯çº¦æŸã€å¤šå±‚æ¬¡æ‹“æ‰‘ç­–ç•¥ã€å……åˆ†æµ‹è¯•ã€ç›‘æ§å‘Šè­¦

---

### æ¡ˆä¾‹ 2ï¼šInter-Pod Affinity å¯¼è‡´è°ƒåº¦æ€§èƒ½æš´è·Œ

#### ğŸ¯ æ•…éšœåœºæ™¯
æŸ SaaS å…¬å¸é›†ç¾¤è§„æ¨¡ 500 èŠ‚ç‚¹ã€5000 Podï¼Œéƒ¨ç½²äº†ä¸€ä¸ªæ–°æœåŠ¡é…ç½®äº† Pod åäº²å’Œæ€§ï¼Œç»“æœ Scheduler è°ƒåº¦å»¶è¿Ÿä» 100ms æš´æ¶¨è‡³ 30sï¼Œå¯¼è‡´æ‰€æœ‰æ–° Pod è°ƒåº¦ç¼“æ…¢ï¼Œå½±å“å…¨å±€ã€‚

#### ğŸ” æ’æŸ¥è¿‡ç¨‹
1. **æ€§èƒ½æŒ‡æ ‡å¼‚å¸¸**ï¼š
   ```bash
   # Scheduler æŒ‡æ ‡
   curl -k https://127.0.0.1:10259/metrics | grep scheduler_scheduling_duration_seconds_bucket
   # scheduler_scheduling_duration_seconds_bucket{le="30"} 1523  # P99 > 30s âŒ
   
   # è°ƒåº¦é˜Ÿåˆ—å †ç§¯
   curl -k https://127.0.0.1:10259/metrics | grep scheduler_pending_pods
   # scheduler_pending_pods 350  # å¤§é‡ Pod ç­‰å¾…è°ƒåº¦
   ```

2. **æ’ä»¶æ€§èƒ½åˆ†æ**ï¼š
   ```bash
   # æŸ¥çœ‹æ’ä»¶æ‰§è¡Œè€—æ—¶
   curl -k https://127.0.0.1:10259/metrics | grep scheduler_plugin_execution_duration_seconds | grep InterPodAffinity
   # scheduler_plugin_execution_duration_seconds{plugin="InterPodAffinity",extension_point="Filter",...} 28.5  # âŒ å•æ¬¡ 28.5sï¼
   ```

3. **æ—¥å¿—åˆ†æ**ï¼š
   ```bash
   kubectl logs -n kube-system kube-scheduler-master1 | grep "took too long"
   # I1210 08:23:15.123456 1 scheduler.go:123] Plugin InterPodAffinity.Filter took too long to execute: 28.5s
   ```

4. **é—®é¢˜é…ç½®å®šä½**ï¼š
   ```bash
   # æŸ¥æ‰¾é…ç½®äº†å¤æ‚äº²å’Œæ€§çš„ Pod
   kubectl get pods -A -o json | jq -r '.items[] | select(.spec.affinity.podAntiAffinity!=null) | "\(.metadata.namespace)/\(.metadata.name)"' | head -10
   # production/new-service-abc123  # æ–°éƒ¨ç½²çš„æœåŠ¡
   
   kubectl get pod new-service-abc123 -n production -o yaml | grep -A50 podAntiAffinity
   # podAntiAffinity:
   #   requiredDuringSchedulingIgnoredDuringExecution:  # âŒ ç¡¬åäº²å’Œ
   #   - labelSelector:
   #       matchExpressions:
   #       - key: app
   #         operator: Exists  # âŒ åŒ¹é…æ‰€æœ‰ Podï¼
   #     topologyKey: kubernetes.io/hostname
   ```

5. **æ ¹å› åˆ†æ**ï¼š
   - é…ç½®äº† `operator: Exists`ï¼ŒåŒ¹é…é›†ç¾¤å†…æ‰€æœ‰ 5000 ä¸ª Pod
   - Scheduler éœ€è¦éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ£€æŸ¥æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰ Pod æ˜¯å¦åŒ¹é…äº²å’Œæ€§è§„åˆ™
   - **æ—¶é—´å¤æ‚åº¦**ï¼šO(èŠ‚ç‚¹æ•° Ã— æ¯èŠ‚ç‚¹ Pod æ•° Ã— è§„åˆ™å¤æ‚åº¦) = O(500 Ã— 10 Ã— N) = **æ•°ä¸‡æ¬¡åŒ¹é…è¿ç®—**
   - å½±å“å…¨å±€ï¼šScheduler å•çº¿ç¨‹ Score é˜¶æ®µè¢«é˜»å¡ï¼Œæ‰€æœ‰ Pod è°ƒåº¦å˜æ…¢

#### âš¡ åº”æ€¥æªæ–½
1. **ç«‹å³åˆ é™¤é—®é¢˜æœåŠ¡**ï¼š
   ```bash
   # åˆ é™¤æ–°éƒ¨ç½²çš„æœåŠ¡ï¼ˆä¸´æ—¶æ­¢è¡€ï¼‰
   kubectl delete deployment new-service -n production
   
   # éªŒè¯è°ƒåº¦æ€§èƒ½æ¢å¤
   curl -k https://127.0.0.1:10259/metrics | grep scheduler_scheduling_duration_seconds_bucket
   # P99 < 1s  âœ… æ¢å¤æ­£å¸¸
   ```

2. **ä¿®å¤é…ç½®åé‡æ–°éƒ¨ç½²**ï¼š
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: new-service
     namespace: production
   spec:
     template:
       spec:
         affinity:
           podAntiAffinity:
             preferredDuringSchedulingIgnoredDuringExecution:  # âœ… æ”¹ä¸ºè½¯åäº²å’Œ
             - weight: 100
               podAffinityTerm:
                 labelSelector:
                   matchLabels:  # âœ… ç²¾ç¡®åŒ¹é…è‡ªèº«
                     app: new-service
                 topologyKey: kubernetes.io/hostname
   ```

3. **éƒ¨ç½²å¹¶éªŒè¯**ï¼š
   ```bash
   kubectl apply -f new-service.yaml
   
   # ç›‘æ§è°ƒåº¦æ€§èƒ½
   kubectl get pods -n production -l app=new-service --watch
   # æ–° Pod åœ¨ 5 ç§’å†…è°ƒåº¦æˆåŠŸ âœ…
   ```

#### ğŸ›¡ï¸ é•¿æœŸä¼˜åŒ–
1. **å‡†å…¥æ§åˆ¶éªŒè¯äº²å’Œæ€§é…ç½®**ï¼š
   ```yaml
   # ä½¿ç”¨ OPA Gatekeeper ç­–ç•¥
   apiVersion: templates.gatekeeper.sh/v1
   kind: ConstraintTemplate
   metadata:
     name: podaffinityrestriction
   spec:
     crd:
       spec:
         names:
           kind: PodAffinityRestriction
     targets:
     - target: admission.k8s.gatekeeper.sh
       rego: |
         package podaffinity
         
         violation[{"msg": msg}] {
           affinity := input.review.object.spec.affinity.podAntiAffinity
           # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Exists æ“ä½œç¬¦åŒ¹é…æ‰€æœ‰ Pod
           affinity.requiredDuringSchedulingIgnoredDuringExecution[_].labelSelector.matchExpressions[_].operator == "Exists"
           not affinity.requiredDuringSchedulingIgnoredDuringExecution[_].labelSelector.matchExpressions[_].key == input.review.object.metadata.labels.app
           msg := "ç¦æ­¢ä½¿ç”¨ Exists æ“ä½œç¬¦åŒ¹é…æ‰€æœ‰ Pod çš„ç¡¬åäº²å’Œæ€§"
         }
   ```

2. **Scheduler é…ç½®ä¼˜åŒ–**ï¼š
   ```yaml
   apiVersion: kubescheduler.config.k8s.io/v1
   kind: KubeSchedulerConfiguration
   parallelism: 32  # âœ… æé«˜å¹¶è¡Œåº¦ï¼ˆé»˜è®¤ 16ï¼‰
   profiles:
   - schedulerName: default-scheduler
     plugins:
       score:
         disabled:
         - name: InterPodAffinity  # âœ… å¦‚ä¸éœ€è¦ï¼Œå¯ç¦ç”¨è¯„åˆ†é˜¶æ®µäº²å’Œæ€§æ’ä»¶
         enabled:
         - name: InterPodAffinity
           weight: 1  # âœ… é™ä½æƒé‡ï¼ˆé»˜è®¤ 1ï¼Œå¯è°ƒè‡³æ›´ä½ï¼‰
     pluginConfig:
     - name: InterPodAffinity
       args:
         hardPodAffinityWeight: 1  # âœ… é™ä½ç¡¬äº²å’Œæ€§æƒé‡
   ```

3. **æ€§èƒ½åŸºå‡†æµ‹è¯•**ï¼š
   ```bash
   # ä½¿ç”¨ kube-scheduler-simulator æ¨¡æ‹Ÿè°ƒåº¦
   git clone https://github.com/kubernetes-sigs/kube-scheduler-simulator
   cd kube-scheduler-simulator
   
   # å¯¼å…¥å½“å‰é›†ç¾¤çŠ¶æ€
   kubectl get nodes -o yaml > nodes.yaml
   kubectl get pods -A -o yaml > pods.yaml
   
   # æ¨¡æ‹Ÿæ–° Pod è°ƒåº¦
   # æµ‹è¯•ä¸åŒäº²å’Œæ€§é…ç½®çš„è°ƒåº¦è€—æ—¶
   ```

4. **ç›‘æ§å‘Šè­¦**ï¼š
   ```yaml
   # è°ƒåº¦å»¶è¿Ÿå‘Šè­¦
   - alert: SchedulerHighLatency
     expr: histogram_quantile(0.99, rate(scheduler_scheduling_duration_seconds_bucket[5m])) > 1
     for: 5m
     labels:
       severity: warning
     annotations:
       summary: "è°ƒåº¦å»¶è¿Ÿè¿‡é«˜"
       description: "Scheduler P99 è°ƒåº¦å»¶è¿Ÿ {{ $value }}sï¼Œè¶…è¿‡ 1s é˜ˆå€¼"
   
   # æ’ä»¶æ‰§è¡Œè€—æ—¶å‘Šè­¦
   - alert: SchedulerPluginSlow
     expr: histogram_quantile(0.99, rate(scheduler_plugin_execution_duration_seconds_bucket[5m])) > 5
     for: 5m
     labels:
       severity: warning
     annotations:
       summary: "è°ƒåº¦æ’ä»¶æ‰§è¡Œç¼“æ…¢"
       description: "æ’ä»¶ {{ $labels.plugin }} åœ¨æ‰©å±•ç‚¹ {{ $labels.extension_point }} æ‰§è¡Œè€—æ—¶ {{ $value }}s"
   ```

#### ğŸ’¡ ç»éªŒæ€»ç»“
- **é…ç½®é”™è¯¯**ï¼š`operator: Exists` åŒ¹é…èŒƒå›´è¿‡å¤§ï¼Œå¯¼è‡´æ€§èƒ½ç¾éš¾
- **æµ‹è¯•ä¸è¶³**ï¼šæœªåœ¨é¢„ç”Ÿäº§ç¯å¢ƒæµ‹è¯•å¤§è§„æ¨¡äº²å’Œæ€§é…ç½®
- **ç›‘æ§ç›²åŒº**ï¼šæœªç›‘æ§ Scheduler æ’ä»¶çº§æ€§èƒ½æŒ‡æ ‡
- **æ”¹è¿›æ–¹å‘**ï¼šå‡†å…¥æ§åˆ¶éªŒè¯ã€ç²¾ç¡®åŒ¹é…ã€è½¯äº²å’Œæ€§ä¼˜å…ˆã€æ€§èƒ½ç›‘æ§ã€å®šæœŸåŸºå‡†æµ‹è¯•

---

### æ¡ˆä¾‹ 3ï¼šèŠ‚ç‚¹èµ„æºç¢ç‰‡åŒ–å¯¼è‡´å¤§ Pod æ— æ³•è°ƒåº¦

#### ğŸ¯ æ•…éšœåœºæ™¯
æŸ AI å…¬å¸éœ€è¦éƒ¨ç½² GPU è®­ç»ƒä»»åŠ¡ï¼ŒPod è¯·æ±‚ 8 æ ¸ 32GB å†…å­˜ + 1 GPUï¼Œä½†é›†ç¾¤æœ‰ 100 ä¸ªèŠ‚ç‚¹ï¼Œæ€»èµ„æºå……è¶³ï¼ŒPod å´ä¸€ç›´ Pendingã€‚

#### ğŸ” æ’æŸ¥è¿‡ç¨‹
1. **ç°è±¡ç¡®è®¤**ï¼š
   ```bash
   kubectl describe pod gpu-training-job-xxxxx
   # Events:
   # Warning  FailedScheduling  0/100 nodes are available: 
   #   50 Insufficient cpu, 
   #   30 Insufficient memory, 
   #   20 Insufficient nvidia.com/gpu.
   ```

2. **é›†ç¾¤èµ„æºæ€»è§ˆ**ï¼š
   ```bash
   # æ€»èµ„æºç»Ÿè®¡
   kubectl describe nodes | grep -A5 "Allocated resources" | grep -E "(cpu|memory)" | \
     awk '{sum+=$2} END {print sum}'
   # æ€» CPU: 800 æ ¸ï¼ˆ100 èŠ‚ç‚¹ Ã— 8 æ ¸ï¼‰
   # æ€»å†…å­˜: 3200 GBï¼ˆ100 èŠ‚ç‚¹ Ã— 32GBï¼‰
   # å·²åˆ†é…: CPU 400 æ ¸, å†…å­˜ 1600 GB  # âœ… ä»… 50% åˆ©ç”¨ç‡
   ```

3. **å•èŠ‚ç‚¹èµ„æºæ£€æŸ¥**ï¼š
   ```bash
   # æŸ¥çœ‹æ¯ä¸ªèŠ‚ç‚¹å‰©ä½™èµ„æº
   kubectl get nodes -o json | jq -r '.items[] | 
     "\(.metadata.name) CPU:\(.status.allocatable.cpu) Mem:\(.status.allocatable.memory)"' | \
     head -10
   # node-01 CPU:8000m Mem:32Gi
   # node-02 CPU:8000m Mem:32Gi
   # ...
   
   # æ£€æŸ¥å®é™…å¯ç”¨èµ„æºï¼ˆå‡å»å·²åˆ†é…ï¼‰
   kubectl describe nodes | grep -A10 "Allocated resources:" | grep -E "cpu|memory" | head -20
   # node-01:
   #   cpu: 7500m (93%)  # âŒ å‰©ä½™ä»… 500m
   #   memory: 28Gi (87%)
   # node-02:
   #   cpu: 7200m (90%)  # âŒ å‰©ä½™ä»… 800m
   #   memory: 30Gi (93%)
   ```

4. **æ ¹å› åˆ†æ**ï¼š
   - **èµ„æºç¢ç‰‡åŒ–**ï¼šæ¯ä¸ªèŠ‚ç‚¹éƒ¨ç½²äº†å¤§é‡å° Podï¼ˆæ¯ä¸ª 100m CPUã€256Mi å†…å­˜ï¼‰
   - å•èŠ‚ç‚¹å‰©ä½™èµ„æºå‡ä¸è¶³ 8 æ ¸ 32GB
   - **ç±»æ¯”**ï¼šåœè½¦åœºæ€»è½¦ä½å……è¶³ï¼Œä½†éƒ½æ˜¯å°å‹è½¦ä½ï¼Œæ— æ³•åœä¸‹å¤§å‹å¡è½¦

#### âš¡ åº”æ€¥æªæ–½
1. **é©±é€ä½ä¼˜å…ˆçº§ Pod é‡Šæ”¾èµ„æº**ï¼š
   ```bash
   # æŸ¥æ‰¾å ç”¨èµ„æºå¤šçš„èŠ‚ç‚¹
   kubectl top nodes --sort-by=cpu | head -5
   
   # é€‰æ‹©ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé©±é€ä½ä¼˜å…ˆçº§ Pod
   kubectl get pods -A --field-selector spec.nodeName=node-01 -o json | \
     jq -r '.items[] | select(.spec.priorityClassName!="high-priority") | "\(.metadata.namespace)/\(.metadata.name)"' | \
     head -10 | xargs -n1 kubectl delete pod
   
   # ç­‰å¾… Pod è¢«é©±é€å’Œé‡æ–°è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹
   sleep 60
   
   # éªŒè¯èŠ‚ç‚¹èµ„æºé‡Šæ”¾
   kubectl describe node node-01 | grep -A5 "Allocated resources"
   # cpu: 2000m (25%)  # âœ… å¤§å¹…é‡Šæ”¾
   # memory: 8Gi (25%)
   ```

2. **è°ƒåº¦ GPU ä»»åŠ¡**ï¼š
   ```bash
   # ä¸º GPU Pod æŒ‡å®šèŠ‚ç‚¹ï¼ˆä¸´æ—¶ï¼‰
   kubectl patch pod gpu-training-job-xxxxx -p '{"spec":{"nodeName":"node-01"}}'
   
   # æˆ–é‡æ–°åˆ›å»º Podï¼ˆè°ƒåº¦å™¨ä¼šè‡ªåŠ¨é€‰æ‹©ï¼‰
   kubectl delete pod gpu-training-job-xxxxx
   kubectl apply -f gpu-job.yaml
   
   # éªŒè¯è°ƒåº¦æˆåŠŸ
   kubectl get pod gpu-training-job-xxxxx -o wide
   # NAME                      READY   STATUS    RESTARTS   AGE   NODE
   # gpu-training-job-xxxxx    1/1     Running   0          30s   node-01  âœ…
   ```

#### ğŸ›¡ï¸ é•¿æœŸä¼˜åŒ–
1. **èŠ‚ç‚¹æ± åˆ†å±‚ç­–ç•¥**ï¼š
   ```yaml
   # åˆ›å»ºä¸“ç”¨èŠ‚ç‚¹æ± 
   # èŠ‚ç‚¹æ±  1: é€šç”¨å° Podï¼ˆ100 èŠ‚ç‚¹ï¼‰
   kubectl label nodes node-{01..100} workload-type=general
   kubectl taint nodes node-{01..100} workload-type=general:NoSchedule
   
   # èŠ‚ç‚¹æ±  2: å¤§å†…å­˜/GPU ä»»åŠ¡ï¼ˆ20 èŠ‚ç‚¹ï¼Œ16 æ ¸ 64GB + GPUï¼‰
   kubectl label nodes gpu-node-{01..20} workload-type=gpu
   kubectl taint nodes gpu-node-{01..20} workload-type=gpu:NoSchedule
   ```

2. **ä½¿ç”¨ NodeSelector å’Œ Tolerations**ï¼š
   ```yaml
   # GPU ä»»åŠ¡é…ç½®
   apiVersion: v1
   kind: Pod
   metadata:
     name: gpu-training-job
   spec:
     nodeSelector:
       workload-type: gpu  # âœ… ä»…è°ƒåº¦åˆ° GPU èŠ‚ç‚¹æ± 
     tolerations:
     - key: workload-type
       operator: Equal
       value: gpu
       effect: NoSchedule
     containers:
     - name: training
       image: pytorch/pytorch:latest
       resources:
         requests:
           cpu: "8"
           memory: 32Gi
           nvidia.com/gpu: "1"
   ```

3. **å¯ç”¨ Cluster Autoscaler**ï¼š
   ```yaml
   # è‡ªåŠ¨æ‰©å±•èŠ‚ç‚¹æ± 
   apiVersion: v1
   kind: ConfigMap
   metadata:
     name: cluster-autoscaler-config
     namespace: kube-system
   data:
     config.yaml: |
       nodePools:
       - name: gpu-pool
         minSize: 5
         maxSize: 50
         machineType: n1-standard-16-gpu
         autoscaling:
           scaleDownUtilizationThreshold: 0.5
           scaleDownUnneededTime: 10m
   ```

4. **èµ„æºé¢„ç•™ç­–ç•¥**ï¼š
   ```yaml
   # ä¸ºç³»ç»Ÿç»„ä»¶é¢„ç•™èµ„æº
   apiVersion: kubelet.config.k8s.io/v1beta1
   kind: KubeletConfiguration
   kubeReserved:
     cpu: "1000m"  # ä¸º kubelet/ç³»ç»Ÿé¢„ç•™ 1 æ ¸
     memory: "4Gi"  # é¢„ç•™ 4GB
   systemReserved:
     cpu: "500m"
     memory: "2Gi"
   evictionHard:
     memory.available: "2Gi"  # è§¦å‘é©±é€çš„é˜ˆå€¼
   ```

5. **ç›‘æ§èµ„æºç¢ç‰‡åŒ–**ï¼š
   ```promql
   # è®¡ç®—èŠ‚ç‚¹èµ„æºç¢ç‰‡ç‡
   # (å·²åˆ†é… Pod æ•° Ã— å¹³å‡ Pod å¤§å°) / èŠ‚ç‚¹æ€»èµ„æº
   (count(kube_pod_info) by (node) * 0.1) / 
   (kube_node_status_allocatable{resource="cpu"}) * 100
   
   # å‘Šè­¦ï¼šç¢ç‰‡ç‡ > 80%
   - alert: NodeResourceFragmentation
     expr: (sum by(node) (kube_pod_container_resource_requests{resource="cpu"}) / kube_node_status_allocatable{resource="cpu"}) > 0.8 and
           (kube_node_status_allocatable{resource="cpu"} - sum by(node) (kube_pod_container_resource_requests{resource="cpu"})) < 2
     labels:
       severity: warning
     annotations:
       summary: "èŠ‚ç‚¹èµ„æºç¢ç‰‡åŒ–ä¸¥é‡"
       description: "èŠ‚ç‚¹ {{ $labels.node }} å·²ç”¨ 80%+ èµ„æºä½†å‰©ä½™ä¸è¶³ 2 æ ¸ï¼Œæ— æ³•è°ƒåº¦å¤§ Pod"
   ```

#### ğŸ’¡ ç»éªŒæ€»ç»“
- **èµ„æºè§„åˆ’ä¸å½“**ï¼šæœªåŒºåˆ†å¤§å° Pod çš„è°ƒåº¦éœ€æ±‚
- **èŠ‚ç‚¹åŒè´¨åŒ–**ï¼šæ‰€æœ‰èŠ‚ç‚¹è§„æ ¼ç›¸åŒï¼Œç¼ºä¹çµæ´»æ€§
- **ç¼ºä¹é¢„ç•™**ï¼šæœªä¸ºå¤§ Pod é¢„ç•™ä¸“ç”¨èŠ‚ç‚¹æ± 
- **æ”¹è¿›æ–¹å‘**ï¼šèŠ‚ç‚¹æ± åˆ†å±‚ã€è‡ªåŠ¨æ‰©ç¼©å®¹ã€èµ„æºé¢„ç•™ã€ç¢ç‰‡åŒ–ç›‘æ§
| `didn't have free ports` | ç«¯å£å†²çª | ä¿®æ”¹ hostPort |

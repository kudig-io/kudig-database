# ä¼ä¸šçº§AIæµ‹è¯•ä¸è´¨é‡ä¿éšœæ·±åº¦å®è·µ

> **ä½œè€…**: ä¼ä¸šçº§æµ‹è¯•æ¶æ„ä¸“å®¶ | **ç‰ˆæœ¬**: v1.0 | **æ›´æ–°æ—¶é—´**: 2026-02-07
> **é€‚ç”¨åœºæ™¯**: ä¼ä¸šçº§AIç³»ç»Ÿæµ‹è¯•ä¸è´¨é‡ä¿éšœ | **å¤æ‚åº¦**: â­â­â­â­â­

## ğŸ¯ æ‘˜è¦

æœ¬æ–‡æ¡£æ·±å…¥æ¢è®¨ä¼ä¸šçº§AIç³»ç»Ÿæµ‹è¯•ç­–ç•¥ã€è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶å’Œè´¨é‡ä¿éšœä½“ç³»ï¼ŒåŸºäºå¤§æ¨¡å‹ã€æœºå™¨å­¦ä¹ å¹³å°ç­‰AIç³»ç»Ÿçš„å®è·µç»éªŒï¼Œæä¾›ä»æ¨¡å‹éªŒè¯åˆ°ç³»ç»Ÿé›†æˆçš„å®Œæ•´æµ‹è¯•æŠ€æœ¯æŒ‡å—ã€‚

## 1. AIç³»ç»Ÿæµ‹è¯•æ¶æ„

### 1.1 æµ‹è¯•å±‚æ¬¡è®¾è®¡

```mermaid
graph TB
    subgraph "æ¨¡å‹å±‚æµ‹è¯•"
        A[æ¨¡å‹å‡†ç¡®æ€§æµ‹è¯•] --> B[æ¨¡å‹æ€§èƒ½æµ‹è¯•]
        C[æ¨¡å‹é²æ£’æ€§æµ‹è¯•] --> D[æ¨¡å‹å…¬å¹³æ€§æµ‹è¯•]
        E[æ¨¡å‹å¯è§£é‡Šæ€§æµ‹è¯•] --> F[æ¨¡å‹å®‰å…¨æ€§æµ‹è¯•]
    end
    
    subgraph "æ•°æ®å±‚æµ‹è¯•"
        G[æ•°æ®è´¨é‡æµ‹è¯•] --> H[æ•°æ®åè§æ£€æµ‹]
        I[æ•°æ®å®Œæ•´æ€§æµ‹è¯•] --> J[æ•°æ®ä¸€è‡´æ€§æµ‹è¯•]
        K[æ•°æ®éšç§æµ‹è¯•] --> L[æ•°æ®åˆè§„æ€§æµ‹è¯•]
    end
    
    subgraph "åº”ç”¨å±‚æµ‹è¯•"
        M[åŠŸèƒ½é›†æˆæµ‹è¯•] --> N[æ€§èƒ½å‹åŠ›æµ‹è¯•]
        O[ç”¨æˆ·ä½“éªŒæµ‹è¯•] --> P[å®‰å…¨æ¸—é€æµ‹è¯•]
        Q[åˆè§„æ€§æµ‹è¯•] --> R[å›å½’æµ‹è¯•]
    end
    
    subgraph "è¿ç»´å±‚æµ‹è¯•"
        S[éƒ¨ç½²éªŒè¯æµ‹è¯•] --> T[ç›‘æ§å‘Šè­¦æµ‹è¯•]
        U[ç¾å¤‡æ¢å¤æµ‹è¯•] --> V[å®¹é‡è§„åˆ’æµ‹è¯•]
        W[æˆæœ¬ä¼˜åŒ–æµ‹è¯•] --> X[å¯æ‰©å±•æ€§æµ‹è¯•]
    end
```

### 1.2 æ™ºèƒ½æµ‹è¯•æ¡†æ¶

```python
# ai-test-framework.py
import unittest
import pytest
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from typing import Dict, List, Tuple, Any
import json
import time
from datetime import datetime

class AITestFramework:
    def __init__(self):
        self.test_results = {}
        self.model_registry = {}
        self.test_datasets = {}
        
    def register_model(self, model_name: str, model_obj: Any, metadata: Dict):
        """æ³¨å†Œå¾…æµ‹è¯•æ¨¡å‹"""
        self.model_registry[model_name] = {
            'model': model_obj,
            'metadata': metadata,
            'registration_time': datetime.now().isoformat()
        }
        
    def load_test_dataset(self, dataset_name: str, data: pd.DataFrame, labels: List = None):
        """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
        self.test_datasets[dataset_name] = {
            'data': data,
            'labels': labels,
            'loaded_time': datetime.now().isoformat()
        }
    
    def model_accuracy_test(self, model_name: str, dataset_name: str, 
                          threshold: float = 0.8) -> Dict:
        """æ¨¡å‹å‡†ç¡®æ€§æµ‹è¯•"""
        if model_name not in self.model_registry:
            raise ValueError(f"Model {model_name} not registered")
            
        if dataset_name not in self.test_datasets:
            raise ValueError(f"Dataset {dataset_name} not loaded")
            
        model = self.model_registry[model_name]['model']
        test_data = self.test_datasets[dataset_name]['data']
        test_labels = self.test_datasets[dataset_name]['labels']
        
        # æ‰§è¡Œé¢„æµ‹
        start_time = time.time()
        predictions = model.predict(test_data)
        inference_time = time.time() - start_time
        
        # è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡
        accuracy = accuracy_score(test_labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            test_labels, predictions, average='weighted'
        )
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        test_result = {
            'test_name': 'model_accuracy_test',
            'model_name': model_name,
            'dataset_name': dataset_name,
            'timestamp': datetime.now().isoformat(),
            'metrics': {
                'accuracy': round(accuracy, 4),
                'precision': round(precision, 4),
                'recall': round(recall, 4),
                'f1_score': round(f1, 4),
                'inference_time_per_sample': round(inference_time / len(test_data), 6)
            },
            'passed': accuracy >= threshold,
            'threshold': threshold,
            'details': {
                'total_samples': len(test_data),
                'correct_predictions': int(accuracy * len(test_data)),
                'incorrect_predictions': int((1 - accuracy) * len(test_data))
            }
        }
        
        self._store_test_result(model_name, 'accuracy', test_result)
        return test_result
    
    def model_robustness_test(self, model_name: str, dataset_name: str,
                            noise_levels: List[float] = [0.1, 0.2, 0.3]) -> Dict:
        """æ¨¡å‹é²æ£’æ€§æµ‹è¯•"""
        if model_name not in self.model_registry:
            raise ValueError(f"Model {model_name} not registered")
            
        model = self.model_registry[model_name]['model']
        test_data = self.test_datasets[dataset_name]['data']
        test_labels = self.test_datasets[dataset_name]['labels']
        
        robustness_results = []
        
        for noise_level in noise_levels:
            # æ·»åŠ å™ªå£°
            noisy_data = self._add_noise(test_data, noise_level)
            
            # æ‰§è¡Œé¢„æµ‹
            predictions = model.predict(noisy_data)
            accuracy = accuracy_score(test_labels, predictions)
            
            robustness_results.append({
                'noise_level': noise_level,
                'accuracy': round(accuracy, 4),
                'degradation': round(1 - accuracy, 4)
            })
        
        # è®¡ç®—é²æ£’æ€§æŒ‡æ ‡
        baseline_accuracy = robustness_results[0]['accuracy']
        avg_degradation = np.mean([r['degradation'] for r in robustness_results])
        
        test_result = {
            'test_name': 'model_robustness_test',
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'noise_test_results': robustness_results,
            'robustness_metrics': {
                'baseline_accuracy': baseline_accuracy,
                'average_degradation': round(avg_degradation, 4),
                'robustness_score': round(1 - avg_degradation, 4)
            },
            'passed': avg_degradation < 0.2,  # é€€åŒ–å°äº20%è®¤ä¸ºé€šè¿‡
            'details': {
                'tested_noise_levels': noise_levels,
                'total_test_cases': len(noise_levels)
            }
        }
        
        self._store_test_result(model_name, 'robustness', test_result)
        return test_result
    
    def bias_detection_test(self, model_name: str, dataset_name: str,
                          protected_attributes: List[str]) -> Dict:
        """åè§æ£€æµ‹æµ‹è¯•"""
        model = self.model_registry[model_name]['model']
        test_data = self.test_datasets[dataset_name]['data']
        test_labels = self.test_datasets[dataset_name]['labels']
        
        bias_results = {}
        
        for attribute in protected_attributes:
            if attribute in test_data.columns:
                # æŒ‰å±æ€§åˆ†ç»„åˆ†æ
                grouped_results = {}
                
                for attr_value in test_data[attribute].unique():
                    mask = test_data[attribute] == attr_value
                    subset_data = test_data[mask]
                    subset_labels = [test_labels[i] for i in range(len(test_labels)) if mask.iloc[i]]
                    
                    if len(subset_labels) > 0:
                        subset_predictions = model.predict(subset_data)
                        subset_accuracy = accuracy_score(subset_labels, subset_predictions)
                        
                        grouped_results[attr_value] = {
                            'sample_count': len(subset_labels),
                            'accuracy': round(subset_accuracy, 4)
                        }
                
                # è®¡ç®—åå·®æŒ‡æ ‡
                accuracies = [result['accuracy'] for result in grouped_results.values()]
                max_diff = max(accuracies) - min(accuracies) if accuracies else 0
                
                bias_results[attribute] = {
                    'group_results': grouped_results,
                    'bias_metrics': {
                        'max_accuracy_difference': round(max_diff, 4),
                        'fairness_score': round(1 - max_diff, 4)
                    },
                    'passed': max_diff < 0.1  # å·®å¼‚å°äº10%è®¤ä¸ºå…¬å¹³
                }
        
        test_result = {
            'test_name': 'bias_detection_test',
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'bias_analysis': bias_results,
            'overall_passed': all(result['passed'] for result in bias_results.values()),
            'recommendations': self._generate_bias_recommendations(bias_results)
        }
        
        self._store_test_result(model_name, 'bias', test_result)
        return test_result
    
    def adversarial_attack_test(self, model_name: str, dataset_name: str,
                              attack_types: List[str] = ['fgsm', 'pgd']) -> Dict:
        """å¯¹æŠ—æ”»å‡»æµ‹è¯•"""
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ä¸“é—¨çš„å¯¹æŠ—æ”»å‡»åº“
        model = self.model_registry[model_name]['model']
        test_data = self.test_datasets[dataset_name]['data']
        test_labels = self.test_datasets[dataset_name]['labels']
        
        attack_results = {}
        
        for attack_type in attack_types:
            # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            adversarial_samples = self._generate_adversarial_examples(
                model, test_data, test_labels, attack_type
            )
            
            # æµ‹è¯•æ¨¡å‹å¯¹å¯¹æŠ—æ ·æœ¬çš„é²æ£’æ€§
            adv_predictions = model.predict(adversarial_samples)
            adv_accuracy = accuracy_score(test_labels, adv_predictions)
            
            attack_results[attack_type] = {
                'attack_success_rate': round(1 - adv_accuracy, 4),
                'model_accuracy_under_attack': round(adv_accuracy, 4),
                'passed': adv_accuracy > 0.7  # æ”»å‡»åå‡†ç¡®ç‡ä»é«˜äº70%è®¤ä¸ºé€šè¿‡
            }
        
        test_result = {
            'test_name': 'adversarial_attack_test',
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'attack_results': attack_results,
            'security_score': round(np.mean([r['model_accuracy_under_attack'] 
                                           for r in attack_results.values()]), 4),
            'overall_passed': all(result['passed'] for result in attack_results.values())
        }
        
        self._store_test_result(model_name, 'adversarial', test_result)
        return test_result
    
    def generate_test_report(self, model_name: str) -> Dict:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        if model_name not in self.test_results:
            return {'error': f'No test results found for model: {model_name}'}
            
        model_tests = self.test_results[model_name]
        all_passed = all(test['passed'] for test in model_tests.values())
        
        report = {
            'model_name': model_name,
            'report_generated': datetime.now().isoformat(),
            'model_metadata': self.model_registry[model_name]['metadata'],
            'test_summary': {
                'total_tests': len(model_tests),
                'passed_tests': sum(1 for test in model_tests.values() if test['passed']),
                'failed_tests': sum(1 for test in model_tests.values() if not test['passed']),
                'overall_status': 'PASSED' if all_passed else 'FAILED'
            },
            'detailed_results': model_tests,
            'quality_score': self._calculate_quality_score(model_tests),
            'recommendations': self._generate_final_recommendations(model_tests)
        }
        
        return report
    
    def _add_noise(self, data: pd.DataFrame, noise_level: float) -> pd.DataFrame:
        """å‘æ•°æ®æ·»åŠ å™ªå£°"""
        noisy_data = data.copy()
        numeric_columns = noisy_data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            noise = np.random.normal(0, noise_level * noisy_data[col].std(), len(noisy_data))
            noisy_data[col] = noisy_data[col] + noise
            
        return noisy_data
    
    def _generate_adversarial_examples(self, model: Any, data: pd.DataFrame, 
                                     labels: List, attack_type: str) -> pd.DataFrame:
        """ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨ä¸“ä¸šçš„å¯¹æŠ—æ”»å‡»åº“å¦‚ARTã€Foolboxç­‰
        adversarial_data = data.copy()
        
        if attack_type == 'fgsm':
            # ç®€åŒ–çš„FGSMå®ç°
            epsilon = 0.1
            for col in adversarial_data.columns:
                if adversarial_data[col].dtype in ['int64', 'float64']:
                    adversarial_data[col] = adversarial_data[col] + np.random.uniform(
                        -epsilon, epsilon, len(adversarial_data)
                    )
        
        return adversarial_data
    
    def _store_test_result(self, model_name: str, test_type: str, result: Dict):
        """å­˜å‚¨æµ‹è¯•ç»“æœ"""
        if model_name not in self.test_results:
            self.test_results[model_name] = {}
        self.test_results[model_name][test_type] = result
    
    def _calculate_quality_score(self, test_results: Dict) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""
        scores = []
        weights = {
            'accuracy': 0.4,
            'robustness': 0.25,
            'bias': 0.2,
            'adversarial': 0.15
        }
        
        for test_type, weight in weights.items():
            if test_type in test_results:
                score = 1.0 if test_results[test_type]['passed'] else 0.0
                scores.append(score * weight)
        
        return round(sum(scores), 4)
    
    def _generate_bias_recommendations(self, bias_results: Dict) -> List[str]:
        """ç”Ÿæˆåè§ç¼“è§£å»ºè®®"""
        recommendations = []
        
        for attribute, result in bias_results.items():
            if not result['passed']:
                max_diff = result['bias_metrics']['max_accuracy_difference']
                recommendations.append(
                    f"å±æ€§ '{attribute}' å­˜åœ¨åè§é—®é¢˜ï¼Œæœ€å¤§å‡†ç¡®ç‡å·®å¼‚è¾¾åˆ° {max_diff:.2%}ï¼Œ"
                    f"å»ºè®®æ£€æŸ¥è®­ç»ƒæ•°æ®åˆ†å¸ƒå’Œç‰¹å¾å·¥ç¨‹"
                )
        
        return recommendations
    
    def _generate_final_recommendations(self, test_results: Dict) -> List[str]:
        """ç”Ÿæˆæœ€ç»ˆå»ºè®®"""
        recommendations = []
        
        if not test_results.get('accuracy', {}).get('passed', True):
            recommendations.append("æ¨¡å‹å‡†ç¡®æ€§æœªè¾¾æ ‡ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´è¶…å‚æ•°")
        
        if not test_results.get('robustness', {}).get('passed', True):
            recommendations.append("æ¨¡å‹é²æ£’æ€§ä¸è¶³ï¼Œå»ºè®®å¢åŠ æ•°æ®å¢å¼ºå’Œæ­£åˆ™åŒ–")
        
        if not test_results.get('bias', {}).get('passed', True):
            recommendations.append("æ¨¡å‹å­˜åœ¨åè§é—®é¢˜ï¼Œå»ºè®®è¿›è¡Œå…¬å¹³æ€§ä¼˜åŒ–")
        
        if not test_results.get('adversarial', {}).get('passed', True):
            recommendations.append("æ¨¡å‹å®‰å…¨æ€§ä¸è¶³ï¼Œå»ºè®®å®æ–½å¯¹æŠ—è®­ç»ƒ")
        
        if not recommendations:
            recommendations.append("æ¨¡å‹è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è¿›å…¥ç”Ÿäº§ç¯å¢ƒ")
        
        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
framework = AITestFramework()

# æ³¨å†Œæ¨¡å‹å’Œæ•°æ®
# framework.register_model("chatbot_model", model_obj, {"version": "1.0", "type": "NLP"})
# framework.load_test_dataset("validation_data", test_df, test_labels)

# æ‰§è¡Œå„ç±»æµ‹è¯•
# accuracy_result = framework.model_accuracy_test("chatbot_model", "validation_data")
# robustness_result = framework.model_robustness_test("chatbot_model", "validation_data")
# bias_result = framework.bias_detection_test("chatbot_model", "validation_data", ["gender", "age"])
# adversarial_result = framework.adversarial_attack_test("chatbot_model", "validation_data")

# ç”ŸæˆæŠ¥å‘Š
# report = framework.generate_test_report("chatbot_model")
# print(json.dumps(report, indent=2, ensure_ascii=False))
```

é€šè¿‡ä»¥ä¸Šä¼ä¸šçº§AIæµ‹è¯•ä¸è´¨é‡ä¿éšœæ·±åº¦å®è·µï¼Œä¼ä¸šå¯ä»¥å»ºç«‹å®Œå–„çš„AIç³»ç»Ÿæµ‹è¯•ä½“ç³»ï¼Œç¡®ä¿AIäº§å“çš„è´¨é‡å’Œå¯é æ€§ã€‚
# 26 - PriorityClass / RuntimeClass YAML é…ç½®å‚è€ƒ

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-02  
> **ä¸»é¢˜**: PriorityClass ä¼˜å…ˆçº§ã€RuntimeClass è¿è¡Œæ—¶ã€ResourceClaim åŠ¨æ€èµ„æºåˆ†é…

## ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [PriorityClass å®Œæ•´é…ç½®](#priorityclass-å®Œæ•´é…ç½®)
- [RuntimeClass å®Œæ•´é…ç½®](#runtimeclass-å®Œæ•´é…ç½®)
- [ResourceClaim / ResourceClaimTemplate](#resourceclaim--resourceclaimtemplate)
- [å†…éƒ¨åŸç†](#å†…éƒ¨åŸç†)
- [ç”Ÿäº§æ¡ˆä¾‹](#ç”Ÿäº§æ¡ˆä¾‹)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## æ¦‚è¿°

### PriorityClass
ç”¨äºå®šä¹‰ Pod çš„ä¼˜å…ˆçº§ï¼Œå½±å“è°ƒåº¦é¡ºåºå’ŒæŠ¢å è¡Œä¸ºã€‚

**æ ¸å¿ƒèƒ½åŠ›**:
- **ä¼˜å…ˆçº§å€¼**: -2Â³Â¹ åˆ° 10â¹
- **å…¨å±€é»˜è®¤**: `globalDefault`
- **æŠ¢å ç­–ç•¥**: `PreemptLowerPriority` / `Never`
- **ç³»ç»Ÿä¿ç•™**: å€¼ â‰¥ 10â¹ ä»…é™ç³»ç»Ÿä½¿ç”¨

### RuntimeClass
å®šä¹‰å®¹å™¨è¿è¡Œæ—¶å¤„ç†å™¨ï¼ˆå¦‚ Kataã€gVisorï¼‰ï¼Œæ”¯æŒæ²™ç®±éš”ç¦»ã€‚

**æ ¸å¿ƒèƒ½åŠ›**:
- **è¿è¡Œæ—¶å¤„ç†å™¨**: `handler` å­—æ®µ
- **èµ„æºå¼€é”€**: `overhead.podFixed`
- **è°ƒåº¦çº¦æŸ**: `nodeSelector` / `tolerations`

### ResourceClaimï¼ˆDynamic Resource Allocationï¼‰
åŠ¨æ€èµ„æºåˆ†é…ï¼ˆDRAï¼‰ç”¨äº GPUã€FPGA ç­‰è®¾å¤‡çš„åŠ¨æ€ç®¡ç†ã€‚

**ç‰ˆæœ¬å…¼å®¹æ€§**:
- v1.26+: Alphaï¼ˆéœ€å¯ç”¨ `DynamicResourceAllocation` feature gateï¼‰
- v1.30+: Betaï¼ˆé»˜è®¤å¯ç”¨ï¼‰

---

## PriorityClass å®Œæ•´é…ç½®

### åŸºç¡€ç¤ºä¾‹

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
# ä¼˜å…ˆçº§å€¼: -2147483648 åˆ° 1000000000
# å€¼è¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜,ç³»ç»Ÿå…³é”®ç»„ä»¶é€šå¸¸ä½¿ç”¨ 2000000000
value: 1000000
# å…¨å±€é»˜è®¤ä¼˜å…ˆçº§ç±»(é›†ç¾¤ä¸­åªèƒ½æœ‰ä¸€ä¸ª globalDefault: true)
globalDefault: false
# æŠ¢å ç­–ç•¥: PreemptLowerPriority(é»˜è®¤) æˆ– Never
preemptionPolicy: PreemptLowerPriority
# äººç±»å¯è¯»çš„æè¿°
description: "é«˜ä¼˜å…ˆçº§ä¸šåŠ¡åº”ç”¨,å¯ä»¥æŠ¢å ä½ä¼˜å…ˆçº§ Pod"
```

### ç³»ç»Ÿçº§ä¼˜å…ˆçº§ï¼ˆä¿ç•™èŒƒå›´ï¼‰

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: system-cluster-critical
# ç³»ç»Ÿçº§ä¼˜å…ˆçº§(>= 1000000000,ä»…é™ç³»ç»Ÿç»„ä»¶)
value: 2000000000
globalDefault: false
preemptionPolicy: PreemptLowerPriority
description: "é›†ç¾¤å…³é”®ç»„ä»¶(å¦‚ kube-dns, metrics-server)"
```

**Kubernetes å†…ç½® PriorityClass**:
- `system-cluster-critical`: 2000000000ï¼ˆé›†ç¾¤å…³é”®ç»„ä»¶ï¼‰
- `system-node-critical`: 2000001000ï¼ˆèŠ‚ç‚¹å…³é”®ç»„ä»¶ï¼Œå¦‚ kubeletï¼‰

### ç¦æ­¢æŠ¢å ç­–ç•¥

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority-no-preemption
value: 500000
# Never: å³ä½¿ä¼˜å…ˆçº§é«˜ä¹Ÿä¸ä¼šè§¦å‘æŠ¢å 
# é€‚ç”¨äº: é‡è¦ä½†å¯ä»¥ç­‰å¾…çš„ä»»åŠ¡
preemptionPolicy: Never
description: "é«˜ä¼˜å…ˆçº§ä½†ä¸ä¼šé©±é€å…¶ä»– Pod"
```

### å¤šå±‚çº§ä¼˜å…ˆçº§ä½“ç³»

```yaml
---
# ç”Ÿäº§ä¸šåŠ¡ - æœ€é«˜ä¼˜å…ˆçº§
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: production-critical
value: 900000
preemptionPolicy: PreemptLowerPriority
description: "ç”Ÿäº§ç¯å¢ƒæ ¸å¿ƒä¸šåŠ¡"
---
# ç”Ÿäº§ä¸šåŠ¡ - æ™®é€šä¼˜å…ˆçº§
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: production-normal
value: 500000
preemptionPolicy: PreemptLowerPriority
description: "ç”Ÿäº§ç¯å¢ƒæ™®é€šä¸šåŠ¡"
---
# æµ‹è¯•/å¼€å‘ç¯å¢ƒ
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: dev-environment
value: 100000
preemptionPolicy: Never
description: "å¼€å‘/æµ‹è¯•ç¯å¢ƒ,ä¸è§¦å‘æŠ¢å "
---
# ç¦»çº¿ä»»åŠ¡ - æœ€ä½ä¼˜å…ˆçº§
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: batch-jobs
value: 0
# å…¨å±€é»˜è®¤: æœªæŒ‡å®š priorityClassName çš„ Pod ä½¿ç”¨æ­¤ä¼˜å…ˆçº§
globalDefault: true
preemptionPolicy: Never
description: "æ‰¹å¤„ç†ä»»åŠ¡,æœ€ä½ä¼˜å…ˆçº§"
```

### Pod ä¸­ä½¿ç”¨ PriorityClass

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority-app
spec:
  # æŒ‡å®šä¼˜å…ˆçº§ç±»
  priorityClassName: high-priority
  containers:
  - name: app
    image: nginx:1.25
    resources:
      requests:
        cpu: "1"
        memory: "1Gi"
```

---

## RuntimeClass å®Œæ•´é…ç½®

### åŸºç¡€ç¤ºä¾‹ï¼ˆé»˜è®¤è¿è¡Œæ—¶ï¼‰

```yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: runc
# handler: èŠ‚ç‚¹ä¸Š CRI è¿è¡Œæ—¶çš„åç§°
# å¿…é¡»ä¸ containerd/cri-o é…ç½®ä¸­çš„ runtime handler åŒ¹é…
handler: runc
```

### Kata Containers æ²™ç®±è¿è¡Œæ—¶

```yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: kata-containers
# Kata è¿è¡Œæ—¶å¤„ç†å™¨(éœ€åœ¨èŠ‚ç‚¹ containerd ä¸­é…ç½®)
handler: kata
# èµ„æºå¼€é”€: RuntimeClass è¿è¡Œæ—¶é¢å¤–æ¶ˆè€—çš„èµ„æº
# è°ƒåº¦å™¨ä¼šå°†æ­¤å¼€é”€åŠ åˆ° Pod çš„èµ„æºè¯·æ±‚ä¸Š
overhead:
  podFixed:
    cpu: "200m"      # Kata VM å¯åŠ¨å¼€é”€
    memory: "256Mi"  # Kata VM å†…å­˜å¼€é”€
# è°ƒåº¦çº¦æŸ: ä»…è°ƒåº¦åˆ°æ”¯æŒ Kata çš„èŠ‚ç‚¹
scheduling:
  nodeSelector:
    runtime: kata
  # å®¹å¿èŠ‚ç‚¹æ±¡ç‚¹
  tolerations:
  - key: "kata-only"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
```

### gVisor æ²™ç®±è¿è¡Œæ—¶

```yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: gvisor
handler: runsc
overhead:
  podFixed:
    cpu: "100m"      # gVisor ç”¨æˆ·æ€å†…æ ¸å¼€é”€è¾ƒå°
    memory: "128Mi"
scheduling:
  nodeSelector:
    runtime: gvisor
  tolerations:
  - key: "sandbox"
    operator: "Exists"
    effect: "NoSchedule"
```

### NVIDIA GPU è¿è¡Œæ—¶

```yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
# nvidia-container-runtime å¤„ç†å™¨
handler: nvidia
scheduling:
  nodeSelector:
    accelerator: nvidia-gpu
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
```

### Pod ä¸­ä½¿ç”¨ RuntimeClass

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-workload
spec:
  # æŒ‡å®šè¿è¡Œæ—¶ç±»
  runtimeClassName: kata-containers
  containers:
  - name: app
    image: myapp:1.0
    resources:
      requests:
        # å®é™…è°ƒåº¦æ—¶ä¼šåŠ ä¸Š overhead çš„ 200m CPU + 256Mi Memory
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1"
        memory: "1Gi"
```

---

## ResourceClaim / ResourceClaimTemplate

> **Feature Gate**: `DynamicResourceAllocation`  
> **çŠ¶æ€**: v1.26 Alpha â†’ v1.30 Beta â†’ v1.32 ç¨³å®šä¸­

### ResourceClaim åŸºç¡€ç¤ºä¾‹

```yaml
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: gpu-claim
  namespace: default
spec:
  # èµ„æºç±» (ç”±è®¾å¤‡æ’ä»¶æä¾›)
  resourceClassName: nvidia-gpu.resource.k8s.io
  # å‚æ•°å¼•ç”¨ (å¯é€‰)
  parametersRef:
    apiGroup: gpu.resource.k8s.io
    kind: GpuClaimParameters
    name: high-performance
  # åˆ†é…æ¨¡å¼
  allocationMode: WaitForFirstConsumer  # å»¶è¿Ÿç»‘å®š
```

### ResourceClaimTemplateï¼ˆåŠ¨æ€åˆ›å»ºï¼‰

```yaml
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaimTemplate
metadata:
  name: gpu-claim-template
  namespace: default
spec:
  # æ¨¡æ¿è§„èŒƒ (ä¸ ResourceClaim.spec ç›¸åŒ)
  spec:
    resourceClassName: nvidia-gpu.resource.k8s.io
    parametersRef:
      apiGroup: gpu.resource.k8s.io
      kind: GpuClaimParameters
      name: ml-training
```

### Pod ä¸­ä½¿ç”¨ ResourceClaim

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-workload
spec:
  # æ–¹å¼1: å¼•ç”¨å·²å­˜åœ¨çš„ ResourceClaim
  resourceClaims:
  - name: gpu
    source:
      resourceClaimName: gpu-claim
  containers:
  - name: training
    image: tensorflow/tensorflow:latest-gpu
    # å®¹å™¨å†…ä½¿ç”¨èµ„æºå£°æ˜
    resources:
      claims:
      - name: gpu
        request: "1"  # è¯·æ±‚ 1 ä¸ª GPU
```

### ä½¿ç”¨ ResourceClaimTemplate

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-dynamic
spec:
  # æ–¹å¼2: ä½¿ç”¨æ¨¡æ¿åŠ¨æ€åˆ›å»º
  resourceClaims:
  - name: gpu
    source:
      resourceClaimTemplateName: gpu-claim-template
  containers:
  - name: inference
    image: pytorch/pytorch:2.0-cuda11.8
    resources:
      claims:
      - name: gpu
```

### ResourceClass é…ç½®ï¼ˆé©±åŠ¨ä¾§ï¼‰

```yaml
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: nvidia-gpu.resource.k8s.io
# DRA é©±åŠ¨åç§°
driverName: gpu.nvidia.com
# å‚æ•°å¼•ç”¨ (å¯é€‰, å…¨å±€é…ç½®)
parametersRef:
  apiGroup: gpu.resource.k8s.io
  kind: DeviceClassParameters
  name: default-gpu-config
# é€‚ç”¨èŠ‚ç‚¹é€‰æ‹©å™¨
suitableNodes:
  nodeSelectorTerms:
  - matchExpressions:
    - key: gpu.nvidia.com/present
      operator: In
      values: ["true"]
```

---

## å†…éƒ¨åŸç†

### PriorityClass ä¼˜å…ˆçº§æŠ¢å ç®—æ³•

#### è°ƒåº¦æµç¨‹

```
1. Pod è¿›å…¥è°ƒåº¦é˜Ÿåˆ—
   â”œâ”€ æŒ‰ Priority å€¼æ’åº (é«˜ä¼˜å…ˆçº§ä¼˜å…ˆ)
   â””â”€ ç›¸åŒä¼˜å…ˆçº§æŒ‰åˆ›å»ºæ—¶é—´æ’åº (FIFO)

2. è°ƒåº¦å™¨å°è¯•è°ƒåº¦
   â”œâ”€ é¢„é€‰ (Predicates): æ‰¾åˆ°å¯ç”¨èŠ‚ç‚¹
   â””â”€ ä¼˜é€‰ (Priorities): é€‰æ‹©æœ€ä½³èŠ‚ç‚¹

3. æ— å¯ç”¨èŠ‚ç‚¹æ—¶è§¦å‘æŠ¢å  (preemptionPolicy: PreemptLowerPriority)
   â”œâ”€ æ‰¾åˆ°å€™é€‰èŠ‚ç‚¹
   â”œâ”€ æ¨¡æ‹Ÿé©±é€ä½ä¼˜å…ˆçº§ Pod
   â”œâ”€ éªŒè¯æ˜¯å¦æ»¡è¶³è°ƒåº¦æ¡ä»¶
   â””â”€ æ‰§è¡Œé©±é€å¹¶ç­‰å¾…èµ„æºé‡Šæ”¾
```

#### æŠ¢å ç®—æ³•

**å…³é”®ä»£ç é€»è¾‘** (`pkg/scheduler/framework/plugins/defaultpreemption`):
```go
// ä¼ªä»£ç 
func SelectNodesForPreemption(pod *Pod) {
  if pod.Spec.PreemptionPolicy == Never {
    return nil  // ä¸è§¦å‘æŠ¢å 
  }
  
  for node in allNodes {
    // æ‰¾åˆ°å¯é©±é€çš„ä½ä¼˜å…ˆçº§ Pod
    victimsOnNode = findVictims(node, pod.Priority)
    if canScheduleAfterEviction(pod, node, victimsOnNode) {
      candidates = append(candidates, node)
    }
  }
  
  // é€‰æ‹©å½±å“æœ€å°çš„èŠ‚ç‚¹ (é©±é€ Pod æ•°é‡æœ€å°‘ã€ä¼˜å…ˆçº§æ€»å’Œæœ€ä½)
  return selectBestCandidate(candidates)
}
```

**æŠ¢å ä¿æŠ¤**:
- PDB (PodDisruptionBudget) ä¼šé˜»æ­¢æŠ¢å 
- `system-cluster-critical` å’Œ `system-node-critical` ä¸ä¼šè¢«æŠ¢å 
- åŒä¼˜å…ˆçº§ Pod ä¹‹é—´ä¸ä¼šäº’ç›¸æŠ¢å 

### RuntimeClass Admission Controller

#### å·¥ä½œæµç¨‹

```
1. API Server æ¥æ”¶ Pod åˆ›å»ºè¯·æ±‚
   â””â”€ RuntimeClass Admission Controller æ‹¦æˆª

2. è¯»å– RuntimeClass å¯¹è±¡
   â”œâ”€ éªŒè¯ handler æœ‰æ•ˆæ€§
   â””â”€ æ³¨å…¥ overhead å’Œ scheduling å­—æ®µ

3. Mutating Webhook ä¿®æ”¹ Pod Spec
   â”œâ”€ pod.spec.overhead = runtimeClass.overhead.podFixed
   â”œâ”€ pod.spec.nodeSelector += runtimeClass.scheduling.nodeSelector
   â””â”€ pod.spec.tolerations += runtimeClass.scheduling.tolerations

4. è°ƒåº¦å™¨è°ƒåº¦æ—¶
   â”œâ”€ èµ„æºè¯·æ±‚ = container.requests + pod.spec.overhead
   â””â”€ èŠ‚ç‚¹è¿‡æ»¤è€ƒè™‘ nodeSelector å’Œ tolerations

5. Kubelet åˆ›å»ºå®¹å™¨
   â””â”€ CRI è°ƒç”¨: RunPodSandbox(handler=runtimeClass.handler)
```

#### Containerd é…ç½®ç¤ºä¾‹

```toml
# /etc/containerd/config.toml
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
  # é»˜è®¤ runc
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
    runtime_type = "io.containerd.runc.v2"
  
  # Kata Containers
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.kata]
    runtime_type = "io.containerd.kata.v2"
  
  # gVisor runsc
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runsc]
    runtime_type = "io.containerd.runsc.v1"
```

### Dynamic Resource Allocation (DRA) è®¾å¤‡åˆ†é…

#### æ¶æ„ç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Kubernetes API Server                â”‚
â”‚  ResourceClaim, ResourceClass, ResourceClaimTemplate   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Scheduler     â”‚         â”‚ DRA Controller    â”‚
        â”‚  (èµ„æºæ„ŸçŸ¥è°ƒåº¦)   â”‚         â”‚ (èµ„æºåˆ†é…åè°ƒå™¨)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             DRA Driver (è®¾å¤‡æ’ä»¶)              â”‚
        â”‚   - è®¾å¤‡å‘ç°å’Œæ³¨å†Œ                              â”‚
        â”‚   - åˆ†é…ç®—æ³•å®ç°                                â”‚
        â”‚   - è®¾å¤‡å‡†å¤‡ (Prepare/Unprepare)               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚     Kubelet     â”‚
                   â”‚   - æŒ‚è½½è®¾å¤‡     â”‚
                   â”‚   - å®¹å™¨å¯åŠ¨     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### åˆ†é…æµç¨‹

```
1. ç”¨æˆ·åˆ›å»º ResourceClaim
   â””â”€ çŠ¶æ€: Pending

2. DRA Controller ç›‘å¬ ResourceClaim
   â”œâ”€ è°ƒç”¨ DRA Driver Allocate API
   â””â”€ Driver è¿”å›åˆ†é…ç»“æœ (è®¾å¤‡ ID, æ‹“æ‰‘ä¿¡æ¯)

3. ResourceClaim çŠ¶æ€æ›´æ–°
   â”œâ”€ status.allocation = {...}
   â””â”€ çŠ¶æ€: Allocated

4. Pod å¼•ç”¨ ResourceClaim
   â””â”€ è°ƒåº¦å™¨éªŒè¯èŠ‚ç‚¹äº²å’Œæ€§

5. Pod ç»‘å®šåˆ°èŠ‚ç‚¹å
   â”œâ”€ Kubelet è°ƒç”¨ DRA Driver NodePrepareResource
   â”œâ”€ Driver æŒ‚è½½è®¾å¤‡ (å¦‚ /dev/nvidia0)
   â””â”€ Kubelet å¯åŠ¨å®¹å™¨å¹¶æ³¨å…¥è®¾å¤‡è·¯å¾„

6. Pod åˆ é™¤æ—¶
   â””â”€ Kubelet è°ƒç”¨ NodeUnprepareResource æ¸…ç†
```

---

## ç”Ÿäº§æ¡ˆä¾‹

### æ¡ˆä¾‹1: å¤šç§Ÿæˆ·é›†ç¾¤ä¼˜å…ˆçº§ä½“ç³»

**åœºæ™¯**: ä¼ä¸šå¤šç§Ÿæˆ·é›†ç¾¤ï¼Œéœ€ç¡®ä¿ç”Ÿäº§ä¸šåŠ¡ä¼˜å…ˆçº§ã€‚

```yaml
# 1. ç³»ç»Ÿç»„ä»¶ (æœ€é«˜ä¼˜å…ˆçº§)
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: system-critical
value: 2000000000
description: "ç³»ç»Ÿå…³é”®ç»„ä»¶: kube-dns, metrics-server, ingress-controller"
---
# 2. ç”Ÿäº§ä¸šåŠ¡ - åœ¨çº¿æœåŠ¡
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: prod-online
value: 900000
preemptionPolicy: PreemptLowerPriority
description: "ç”Ÿäº§åœ¨çº¿æœåŠ¡,å¯æŠ¢å ç¦»çº¿ä»»åŠ¡"
---
# 3. ç”Ÿäº§ä¸šåŠ¡ - ç¦»çº¿ä»»åŠ¡
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: prod-batch
value: 500000
preemptionPolicy: Never
description: "ç”Ÿäº§ç¦»çº¿ä»»åŠ¡,ä¸è§¦å‘æŠ¢å "
---
# 4. æµ‹è¯•ç¯å¢ƒ
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: staging
value: 100000
preemptionPolicy: Never
description: "é¢„å‘å¸ƒç¯å¢ƒ"
---
# 5. å¼€å‘ç¯å¢ƒ (é»˜è®¤)
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: dev-default
value: 0
globalDefault: true  # æœªæŒ‡å®šçš„ Pod ä½¿ç”¨æ­¤ä¼˜å…ˆçº§
preemptionPolicy: Never
description: "å¼€å‘ç¯å¢ƒé»˜è®¤ä¼˜å…ˆçº§"
```

**éƒ¨ç½²ç¤ºä¾‹**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: online-api
  namespace: production
spec:
  replicas: 3
  template:
    spec:
      # ç”Ÿäº§åœ¨çº¿æœåŠ¡ä½¿ç”¨é«˜ä¼˜å…ˆçº§
      priorityClassName: prod-online
      containers:
      - name: api
        image: api:v1.0
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing
  namespace: production
spec:
  template:
    spec:
      # ç¦»çº¿ä»»åŠ¡ä½¿ç”¨è¾ƒä½ä¼˜å…ˆçº§
      priorityClassName: prod-batch
      containers:
      - name: processor
        image: processor:v1.0
```

**æ•ˆæœ**:
- èµ„æºç´§å¼ æ—¶ï¼Œ`online-api` ä¼šæŠ¢å  `data-processing`
- å¼€å‘ç¯å¢ƒ Pod æœ€å…ˆè¢«é©±é€
- ç³»ç»Ÿç»„ä»¶å§‹ç»ˆå—ä¿æŠ¤

### æ¡ˆä¾‹2: é‡‘èä¸šåŠ¡å®‰å…¨éš”ç¦» (Kata Containers)

**åœºæ™¯**: é‡‘èæœåŠ¡éœ€å¼ºéš”ç¦»æ²™ç®±è¿è¡Œæ—¶ã€‚

```yaml
# 1. é…ç½® RuntimeClass
---
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: kata-secure
handler: kata
overhead:
  podFixed:
    cpu: "250m"
    memory: "512Mi"  # Kata VM é¢å¤–å¼€é”€
scheduling:
  nodeSelector:
    node-role.kubernetes.io/kata: ""
  tolerations:
  - key: "kata-only"
    operator: "Exists"
    effect: "NoSchedule"
---
# 2. PriorityClass ç¡®ä¿å…³é”®ä¸šåŠ¡ä¼˜å…ˆ
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: finance-critical
value: 950000
description: "é‡‘èæ ¸å¿ƒä¸šåŠ¡"
```

**åº”ç”¨éƒ¨ç½²**:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: payment-service
  namespace: finance
spec:
  # ä½¿ç”¨ Kata æ²™ç®±è¿è¡Œæ—¶
  runtimeClassName: kata-secure
  # é«˜ä¼˜å…ˆçº§
  priorityClassName: finance-critical
  containers:
  - name: payment
    image: payment-service:secure
    resources:
      requests:
        # å®é™…è¯·æ±‚ = 500m + 250m(overhead) = 750m CPU
        cpu: "500m"
        memory: "1Gi"   # å®é™… = 1Gi + 512Mi = 1.5Gi
      limits:
        cpu: "2"
        memory: "4Gi"
    securityContext:
      runAsNonRoot: true
      readOnlyRootFilesystem: true
```

**èŠ‚ç‚¹æ ‡ç­¾é…ç½®**:

```bash
# ä¸ºæ”¯æŒ Kata çš„èŠ‚ç‚¹æ‰“æ ‡ç­¾
kubectl label nodes node-01 node-role.kubernetes.io/kata=""
kubectl taint nodes node-01 kata-only=true:NoSchedule
```

### æ¡ˆä¾‹3: GPU åŠ¨æ€åˆ†é… (DRA)

**åœºæ™¯**: AI è®­ç»ƒå¹³å°ï¼ŒåŠ¨æ€åˆ†é… NVIDIA GPUã€‚

```yaml
# 1. ResourceClass é…ç½®
---
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClass
metadata:
  name: nvidia-a100
driverName: gpu.nvidia.com
parametersRef:
  apiGroup: gpu.nvidia.com
  kind: GpuConfig
  name: a100-80gb
suitableNodes:
  nodeSelectorTerms:
  - matchExpressions:
    - key: nvidia.com/gpu.product
      operator: In
      values: ["NVIDIA-A100-SXM4-80GB"]
---
# 2. ResourceClaimTemplate ç”¨äºè®­ç»ƒä»»åŠ¡
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaimTemplate
metadata:
  name: ml-training-gpu
  namespace: ml-platform
spec:
  spec:
    resourceClassName: nvidia-a100
    parametersRef:
      apiGroup: gpu.nvidia.com
      kind: GpuClaimParameters
      name: training-config
---
# 3. GPU é…ç½®å‚æ•°
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: training-config
spec:
  # å…±äº«æ¨¡å¼ (MIG / Time-Slicing)
  sharing: false
  # GPU å†…å­˜éœ€æ±‚
  memory: "40Gi"
  # å¤š GPU æ‹“æ‰‘
  count: 4
  topology: nvlink  # è¦æ±‚ NVLink äº’è”
```

**è®­ç»ƒä»»åŠ¡ä½¿ç”¨ GPU**:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: bert-training
  namespace: ml-platform
spec:
  template:
    spec:
      # é«˜ä¼˜å…ˆçº§ç¡®ä¿è®­ç»ƒä»»åŠ¡ä¸è¢«æŠ¢å 
      priorityClassName: ml-training
      # å¼•ç”¨ GPU èµ„æºå£°æ˜æ¨¡æ¿
      resourceClaims:
      - name: gpu
        source:
          resourceClaimTemplateName: ml-training-gpu
      containers:
      - name: trainer
        image: nvcr.io/nvidia/pytorch:23.12-py3
        command: ["python", "train.py"]
        resources:
          # å£°æ˜ä½¿ç”¨ 4 ä¸ª GPU
          claims:
          - name: gpu
            request: "4"
        env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"  # DRA é©±åŠ¨ä¼šè‡ªåŠ¨æ³¨å…¥æ­£ç¡®çš„ GPU ID
```

**æ¨ç†æœåŠ¡ (å…±äº« GPU)**:

```yaml
---
apiVersion: gpu.nvidia.com/v1alpha1
kind: GpuClaimParameters
metadata:
  name: inference-config
spec:
  # å¯ç”¨ Time-Slicing å…±äº«
  sharing: true
  # æ¯ä¸ªå®¹å™¨æœ€å¤§ GPU åˆ©ç”¨ç‡
  maxUtilization: 25
---
apiVersion: v1
kind: Pod
metadata:
  name: inference-service
spec:
  resourceClaims:
  - name: gpu
    source:
      resourceClaimName: inference-gpu  # é¢„åˆ›å»ºçš„ ResourceClaim
  containers:
  - name: inference
    image: tritonserver:23.12
    resources:
      claims:
      - name: gpu
        request: "1"  # å…±äº« 1/4 GPU èµ„æº
```

---

## æœ€ä½³å®è·µ

### PriorityClass è®¾è®¡åŸåˆ™

1. **æ˜ç¡®åˆ†å±‚ä½“ç³»**
   ```
   ç³»ç»Ÿç»„ä»¶ (2000000000+)
   â”œâ”€ system-node-critical (2000001000): kubelet, kube-proxy
   â””â”€ system-cluster-critical (2000000000): DNS, metrics-server
   
   ç”Ÿäº§ä¸šåŠ¡ (500000-1000000)
   â”œâ”€ åœ¨çº¿æœåŠ¡ (800000-1000000): API, å‰ç«¯åº”ç”¨
   â””â”€ ç¦»çº¿ä»»åŠ¡ (500000-700000): æ•°æ®å¤„ç†, å®šæ—¶ä»»åŠ¡
   
   éç”Ÿäº§ (0-100000)
   â”œâ”€ é¢„å‘å¸ƒ (50000-100000)
   â””â”€ å¼€å‘æµ‹è¯• (0, globalDefault: true)
   ```

2. **æŠ¢å ç­–ç•¥é€‰æ‹©**
   - **PreemptLowerPriority**: åœ¨çº¿æœåŠ¡ã€å®æ—¶ä»»åŠ¡
   - **Never**: ç¦»çº¿ä»»åŠ¡ã€å¼€å‘ç¯å¢ƒï¼ˆé¿å…é›ªå´©ï¼‰

3. **é…é¢é™åˆ¶**
   ```yaml
   # é˜²æ­¢æ»¥ç”¨é«˜ä¼˜å…ˆçº§
   apiVersion: v1
   kind: ResourceQuota
   metadata:
     name: high-priority-quota
     namespace: production
   spec:
     hard:
       pods: "100"
     scopeSelector:
       matchExpressions:
       - operator: In
         scopeName: PriorityClass
         values: ["prod-online"]
   ```

### RuntimeClass ä½¿ç”¨å»ºè®®

1. **åˆç†è¯„ä¼° Overhead**
   - Kata: 200-500m CPU, 256-512Mi Memory
   - gVisor: 50-100m CPU, 64-128Mi Memory
   - å®é™…æµ‹è¯•åè°ƒæ•´

2. **èŠ‚ç‚¹éš”ç¦»ç­–ç•¥**
   ```bash
   # ä¸“ç”¨èŠ‚ç‚¹ç»„
   kubectl label nodes kata-node-{1..3} runtime=kata
   kubectl taint nodes kata-node-{1..3} kata-only=true:NoSchedule
   ```

3. **æ€§èƒ½ç›‘æ§**
   ```yaml
   # å¯¹æ¯”ç›‘æ§
   metrics:
   - pod_overhead_cpu{runtime_class="kata"}
   - pod_overhead_memory{runtime_class="kata"}
   - pod_startup_duration{runtime_class="kata"} vs {runtime_class="runc"}
   ```

### ResourceClaim DRA æ³¨æ„äº‹é¡¹

1. **ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥**
   ```bash
   # æ£€æŸ¥ DRA æ˜¯å¦å¯ç”¨
   kubectl api-resources | grep resource.k8s.io
   
   # æŸ¥çœ‹ Feature Gate
   kubectl get --raw /metrics | grep dynamic_resource_allocation
   ```

2. **é©±åŠ¨å¥åº·ç›‘æ§**
   ```yaml
   # DRA é©±åŠ¨ DaemonSet å¥åº·æ£€æŸ¥
   apiVersion: apps/v1
   kind: DaemonSet
   metadata:
     name: gpu-dra-driver
   spec:
     template:
       spec:
         containers:
         - name: driver
           livenessProbe:
             httpGet:
               path: /healthz
               port: 8080
           readinessProbe:
             grpc:
               port: 9090  # DRA gRPC ç«¯å£
   ```

3. **èµ„æºæ³„æ¼é˜²æŠ¤**
   ```yaml
   # ResourceClaim è‡ªåŠ¨æ¸…ç†
   apiVersion: v1
   kind: Pod
   metadata:
     ownerReferences:
     - apiVersion: batch/v1
       kind: Job
       name: training-job
       # Job åˆ é™¤æ—¶è‡ªåŠ¨æ¸…ç† ResourceClaim
       blockOwnerDeletion: true
   ```

---

## å¸¸è§é—®é¢˜

### PriorityClass FAQ

**Q: Pod æ— æ³•è°ƒåº¦ï¼Œæç¤ºä¼˜å…ˆçº§ä¸è¶³ï¼Ÿ**

```bash
# æ£€æŸ¥äº‹ä»¶
kubectl describe pod <pod-name>
# Events:
#   Warning  FailedScheduling  pod has insufficient priority to preempt

# è§£å†³æ–¹æ¡ˆ: æå‡ä¼˜å…ˆçº§æˆ–ç­‰å¾…èµ„æºé‡Šæ”¾
kubectl get priorityclasses
kubectl edit pod <pod-name>  # ä¿®æ”¹ priorityClassName
```

**Q: é«˜ä¼˜å…ˆçº§ Pod é¢‘ç¹è§¦å‘æŠ¢å ï¼Œå¯¼è‡´é›†ç¾¤ä¸ç¨³å®šï¼Ÿ**

**è§£å†³æ–¹æ¡ˆ**:
1. æ·»åŠ  PDB ä¿æŠ¤ä½ä¼˜å…ˆçº§å…³é”®æœåŠ¡
   ```yaml
   apiVersion: policy/v1
   kind: PodDisruptionBudget
   metadata:
     name: batch-job-pdb
   spec:
     minAvailable: 50%
     selector:
       matchLabels:
         app: batch-job
   ```

2. ä½¿ç”¨ `preemptionPolicy: Never`
3. å¢åŠ é›†ç¾¤èŠ‚ç‚¹å®¹é‡

**Q: å¦‚ä½•ç¦æ­¢æ™®é€šç”¨æˆ·ä½¿ç”¨é«˜ä¼˜å…ˆçº§ï¼Ÿ**

**RBAC é™åˆ¶**:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: restrict-high-priority
  namespace: dev-team
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["create", "update"]
  # é™åˆ¶åªèƒ½ä½¿ç”¨ç‰¹å®š PriorityClass
  resourceNames: ["dev-default", "staging"]
```

### RuntimeClass FAQ

**Q: Pod æŠ¥é”™ `RuntimeClass not found`ï¼Ÿ**

```bash
# æ£€æŸ¥ RuntimeClass æ˜¯å¦å­˜åœ¨
kubectl get runtimeclass
kubectl describe runtimeclass kata-containers

# æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ”¯æŒ
kubectl describe node <node-name> | grep -A 10 "Container Runtime"
```

**Q: RuntimeClass æ— æ³•æ­£ç¡®æ³¨å…¥ overheadï¼Ÿ**

```bash
# éªŒè¯ Admission Controller æ˜¯å¦å¯ç”¨
kubectl get pod -o jsonpath='{.spec.overhead}' <pod-name>

# è¾“å‡ºåº”ä¸º: {"cpu":"200m","memory":"256Mi"}

# å¦‚æœä¸ºç©º, æ£€æŸ¥ API Server é…ç½®
--enable-admission-plugins=...,RuntimeClass,...
```

**Q: Containerd æŠ¥é”™ `unknown handler "kata"`ï¼Ÿ**

**æ’æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥ containerd é…ç½®
cat /etc/containerd/config.toml | grep -A 5 "kata"

# 2. éªŒè¯ Kata å®‰è£…
kata-runtime --version

# 3. é‡å¯ containerd
systemctl restart containerd

# 4. æµ‹è¯•è¿è¡Œæ—¶
ctr run --runtime io.containerd.kata.v2 docker.io/library/busybox:latest test-kata
```

### ResourceClaim FAQ

**Q: ResourceClaim ä¸€ç›´ Pendingï¼Ÿ**

```bash
# æ£€æŸ¥çŠ¶æ€
kubectl describe resourceclaim <claim-name>

# å¸¸è§åŸå› :
# 1. DRA Driver æœªå®‰è£…
kubectl get daemonset -n kube-system | grep dra-driver

# 2. ResourceClass ä¸å­˜åœ¨
kubectl get resourceclass

# 3. èŠ‚ç‚¹ä¸æ»¡è¶³ suitableNodes æ¡ä»¶
kubectl get nodes -L gpu.nvidia.com/present
```

**Q: Pod å¯åŠ¨å¤±è´¥ `failed to prepare resources`ï¼Ÿ**

**è°ƒè¯•æ­¥éª¤**:
```bash
# 1. æŸ¥çœ‹ Kubelet æ—¥å¿—
journalctl -u kubelet -f | grep DRA

# 2. æ£€æŸ¥ DRA Driver æ—¥å¿—
kubectl logs -n kube-system <dra-driver-pod> --tail=100

# 3. éªŒè¯è®¾å¤‡å¯ç”¨æ€§
kubectl get resourceclaim <claim-name> -o yaml | grep allocation -A 20
```

**Q: å¦‚ä½•é‡Šæ”¾å·²åˆ†é…ä½†æœªä½¿ç”¨çš„ ResourceClaimï¼Ÿ**

```bash
# æ‰‹åŠ¨åˆ é™¤ (å¦‚æœ Pod å·²åˆ é™¤)
kubectl delete resourceclaim <claim-name>

# æ£€æŸ¥æ³„æ¼çš„ Claim
kubectl get resourceclaim --all-namespaces -o json | \
  jq '.items[] | select(.status.reservedFor == null) | .metadata.name'
```

---

## ç‰ˆæœ¬å…¼å®¹æ€§å¯¹ç…§

| åŠŸèƒ½                | v1.25 | v1.26 | v1.27 | v1.28 | v1.30 | v1.32 |
|---------------------|-------|-------|-------|-------|-------|-------|
| PriorityClass       | âœ… GA | âœ…    | âœ…    | âœ…    | âœ…    | âœ…    |
| preemptionPolicy    | âœ… GA | âœ…    | âœ…    | âœ…    | âœ…    | âœ…    |
| RuntimeClass        | âœ… GA | âœ…    | âœ…    | âœ…    | âœ…    | âœ…    |
| overhead.podFixed   | âœ… GA | âœ…    | âœ…    | âœ…    | âœ…    | âœ…    |
| ResourceClaim (DRA) | âŒ    | ğŸ§ª Alpha | ğŸ§ª Alpha | ğŸ§ª Alpha | ğŸŸ¡ Beta | âœ… æ¥è¿‘ GA |

**å›¾ä¾‹**: âœ… GA (ç¨³å®š) | ğŸŸ¡ Beta (é»˜è®¤å¯ç”¨) | ğŸ§ª Alpha (éœ€å¯ç”¨ Feature Gate) | âŒ ä¸æ”¯æŒ

---

## å‚è€ƒèµ„æ–™

- [PriorityClass å®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/)
- [RuntimeClass å®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/concepts/containers/runtime-class/)
- [Dynamic Resource Allocation KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation)
- [Kata Containers å®˜ç½‘](https://katacontainers.io/)
- [gVisor å®˜ç½‘](https://gvisor.dev/)

---

**æ–‡æ¡£ç»´æŠ¤**: å»ºè®®æ¯å­£åº¦æ›´æ–°ä¸€æ¬¡ï¼Œå…³æ³¨ DRA æ­£å¼ GA çš„ç‰ˆæœ¬å˜åŒ–ã€‚

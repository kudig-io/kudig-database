# Commvault ä¼ä¸šçº§ç¾å¤‡ä¸ä¸šåŠ¡è¿ç»­æ€§æ·±åº¦å®è·µ

> **ä½œè€…**: ç¾å¤‡æ¶æ„å¸ˆ | **ç‰ˆæœ¬**: v1.0 | **æ›´æ–°æ—¶é—´**: 2026-02-07
> **åœºæ™¯**: ä¼ä¸šçº§æ•°æ®ä¿æŠ¤å’Œç¾éš¾æ¢å¤è§£å†³æ–¹æ¡ˆ | **å¤æ‚åº¦**: â­â­â­â­

## ğŸ¯ æ‘˜è¦

æœ¬æ–‡æ¡£å…¨é¢æ¢è®¨äº†Commvaultä¼ä¸šçº§éƒ¨ç½²æ¶æ„ã€ç¾å¤‡ç­–ç•¥å®æ–½å’Œä¸šåŠ¡è¿ç»­æ€§ç®¡ç†å®è·µã€‚åŸºäºå¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒç»éªŒï¼Œæä¾›ä»å¤‡ä»½æ¶æ„è®¾è®¡åˆ°ç¾éš¾æ¢å¤æ¼”ç»ƒçš„å®Œæ•´æŠ€æœ¯æŒ‡å¯¼ï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºç»Ÿä¸€ã€å¯é çš„æ•°æ®ä¿æŠ¤å¹³å°ï¼Œå®ç°RTO/RPOç›®æ ‡ï¼Œç¡®ä¿å…³é”®ä¸šåŠ¡ç³»ç»Ÿåœ¨å„ç§ç¾éš¾åœºæ™¯ä¸‹çš„å¿«é€Ÿæ¢å¤èƒ½åŠ›ã€‚

## 1. Commvault ä¼ä¸šæ¶æ„

### 1.1 æ ¸å¿ƒç»„ä»¶æ¶æ„

```mermaid
graph TB
    subgraph "Commvault åŸºç¡€è®¾æ–½å±‚"
        A[CommServe æœåŠ¡å™¨]
        B[MediaAgents]
        C[æ•°æ®åº“æœåŠ¡å™¨]
        D[ç´¢å¼•æœåŠ¡å™¨]
        E[Web æ§åˆ¶å°]
    end
    
    subgraph "æ•°æ®ä¿æŠ¤å±‚"
        F[æ–‡ä»¶ç³»ç»Ÿå¤‡ä»½]
        G[æ•°æ®åº“å¤‡ä»½]
        H[è™šæ‹Ÿæœºå¤‡ä»½]
        I[åº”ç”¨ç¨‹åºå¤‡ä»½]
        J[äº‘å­˜å‚¨å¤‡ä»½]
    end
    
    subgraph "ç¾å¤‡ç®¡ç†"
        K[å¤‡ä»½ç­–ç•¥]
        L[æ¢å¤ç‚¹ç›®æ ‡]
        M[æ¢å¤æ—¶é—´ç›®æ ‡]
        N[ç¾éš¾æ¢å¤è®¡åˆ’]
        O[ä¸šåŠ¡å½±å“åˆ†æ]
    end
    
    subgraph "å­˜å‚¨ç®¡ç†å±‚"
        P[ç£å¸¦åº“]
        Q[ç£ç›˜é˜µåˆ—]
        R[å¯¹è±¡å­˜å‚¨]
        S[äº‘å­˜å‚¨]
        T[é‡å¤æ•°æ®åˆ é™¤]
    end
    
    subgraph "ç›‘æ§ä¸æŠ¥å‘Š"
        U[å¤‡ä»½ç›‘æ§]
        V[æ€§èƒ½æŠ¥å‘Š]
        W[å®¹é‡è§„åˆ’]
        X[åˆè§„æŠ¥å‘Š]
        Y[å®¡è®¡æ—¥å¿—]
    end
    
    subgraph "å®‰å…¨ä¸åˆè§„"
        Z[æ•°æ®åŠ å¯†]
        AA[è®¿é—®æ§åˆ¶]
        AB[å®¡è®¡è¿½è¸ª]
        AC[åˆè§„æ£€æŸ¥]
        AD[å¯†é’¥ç®¡ç†]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    
    F --> G
    G --> H
    H --> I
    I --> J
    
    K --> L
    L --> M
    M --> N
    N --> O
    
    P --> Q
    Q --> R
    R --> S
    S --> T
    
    U --> V
    V --> W
    W --> X
    X --> Y
    
    Z --> AA
    AA --> AB
    AB --> AC
    AC --> AD
```

### 1.2 ä¼ä¸šçº§éƒ¨ç½²æ¶æ„

```yaml
commvault_enterprise_deployment:
  commserve_configuration:
    production_commserve:
      hostname: "commserve-prod.company.com"
      ip_address: "192.168.1.100"
      operating_system: "Windows Server 2019"
      cpu_cores: 16
      memory_gb: 32
      storage_gb: 1000
      database:
        type: "Microsoft SQL Server 2019"
        edition: "Enterprise"
        collation: "SQL_Latin1_General_CP1_CI_AS"
        backup_retention_days: 30
      
      network_configuration:
        management_interface:
          ip: "192.168.1.100"
          subnet_mask: "255.255.255.0"
          gateway: "192.168.1.1"
          dns_servers:
            - "192.168.1.10"
            - "192.168.1.11"
        
        backup_interface:
          ip: "10.0.1.100"
          subnet_mask: "255.255.255.0"
          mtu: 9000  # Jumbo Frames for better performance
    
    high_availability:
      cluster_type: "Windows Failover Cluster"
      nodes:
        - hostname: "commserve-node1"
          ip: "192.168.1.101"
        - hostname: "commserve-node2"
          ip: "192.168.1.102"
      
      shared_storage:
        type: "SAN"
        lun_id: "LUN001"
        size_gb: 2000
        filesystem: "NTFS"
  
  mediaagent_configuration:
    primary_mediaagents:
      - hostname: "ma-prod-01"
        ip_address: "192.168.1.110"
        operating_system: "Windows Server 2019"
        cpu_cores: 12
        memory_gb: 24
        network_interfaces:
          - name: "Management"
            ip: "192.168.1.110"
          - name: "Backup"
            ip: "10.0.1.110"
            mtu: 9000
        
        storage_libraries:
          - library_name: "Tape Library A"
            type: "IBM TS4500"
            drive_count: 8
            slot_count: 2000
          
          - library_name: "Disk Library"
            type: "Dell EMC PowerVault"
            capacity_tb: 500
            raid_level: "RAID 6"
    
    secondary_mediaagents:
      - hostname: "ma-dr-01"
        ip_address: "192.168.2.110"
        location: "å¼‚åœ°æ•°æ®ä¸­å¿ƒ"
        network_bandwidth_mbps: 1000
        purpose: "ç¾éš¾æ¢å¤ç«™ç‚¹"
  
  storage_policies:
    tiered_storage:
      primary_storage:
        type: "Disk"
        retention_days: 30
        deduplication_ratio: "20:1"
        encryption: "AES-256"
      
      secondary_storage:
        type: "Tape"
        retention_weeks: 52
        compression: "Hardware"
        encryption: "AES-256"
      
      tertiary_storage:
        type: "Cloud"
        provider: "Amazon S3 Glacier"
        retention_years: 7
        transfer_protocol: "HTTPS"
        encryption: "AES-256"
  
  security_configuration:
    authentication:
      method: "Active Directory"
      domain: "company.com"
      service_account: "svc-commvault"
    
    authorization:
      admin_groups:
        - "Commvault Admins"
        - "Backup Operators"
      user_groups:
        - "Department A Users"
        - "Department B Users"
    
    encryption:
      in_transit:
        protocol: "TLS 1.3"
        certificate_validity_days: 365
      at_rest:
        method: "Hardware Encryption"
        key_length: 256
        key_rotation_days: 90
    
    auditing:
      log_retention_days: 180
      alert_thresholds:
        failed_logins: 5
        unauthorized_access: 1
        policy_changes: 1
```

## 2. é«˜çº§å¤‡ä»½ç­–ç•¥

### 2.1 åˆ†å±‚å¤‡ä»½é…ç½®

```powershell
# Commvault PowerShell è„šæœ¬ - åˆ†å±‚å¤‡ä»½ç­–ç•¥é…ç½®

# 1. åˆ›å»ºå­˜å‚¨ç­–ç•¥
New-CVStoragePolicy -Name "Tiered-Backup-Policy" -Description "ä¼ä¸šçº§åˆ†å±‚å¤‡ä»½ç­–ç•¥" `
    -RetentionRules @{
        "Daily" = @{ RetentionDays = 30; BackupType = "Full" }
        "Weekly" = @{ RetentionWeeks = 12; BackupType = "Full" }
        "Monthly" = @{ RetentionMonths = 12; BackupType = "Full" }
        "Yearly" = @{ RetentionYears = 7; BackupType = "Full" }
    } `
    -DeduplicationEnabled $true `
    -GlobalDeduplication $true `
    -EncryptionEnabled $true

# 2. é…ç½®ä¸»å­˜å‚¨å±‚ï¼ˆç£ç›˜ï¼‰
Add-CVStoragePool -StoragePolicy "Tiered-Backup-Policy" `
    -PoolName "Primary-Disk-Pool" `
    -MediaType "Disk" `
    -Path "\\storage-array\backup-pool" `
    -BlockSizeKB 1024 `
    -DeduplicationRatio 20 `
    -RetentionDays 30

# 3. é…ç½®äºŒçº§å­˜å‚¨å±‚ï¼ˆç£å¸¦ï¼‰
Add-CVStoragePool -StoragePolicy "Tiered-Backup-Policy" `
    -PoolName "Secondary-Tape-Pool" `
    -MediaType "Tape" `
    -LibraryName "IBM-TS4500-Library" `
    -DriveCount 8 `
    -SlotCount 2000 `
    -RetentionWeeks 52

# 4. é…ç½®ä¸‰çº§å­˜å‚¨å±‚ï¼ˆäº‘ï¼‰
Add-CVStoragePool -StoragePolicy "Tiered-Backup-Policy" `
    -PoolName "Tertiary-Cloud-Pool" `
    -MediaType "Cloud" `
    -CloudProvider "Amazon S3" `
    -BucketName "company-backup-archive" `
    -Region "us-west-2" `
    -RetentionYears 7

# 5. åˆ›å»ºå¤‡ä»½é›†
New-CVBackupSet -ClientGroup "Production-Servers" `
    -BackupSetName "Critical-Systems-Backup" `
    -StoragePolicy "Tiered-Backup-Policy" `
    -SubclientPolicy @{
        "Database-Servers" = @{
            Schedule = "æ¯å¤© 23:00"
            Type = "Full"
            Throttle = "Medium"
        }
        "File-Servers" = @{
            Schedule = "æ¯å‘¨æ—¥ 22:00"
            Type = "Incremental"
            Throttle = "Low"
        }
        "Virtual-Machines" = @{
            Schedule = "æ¯4å°æ—¶"
            Type = "SnapShot"
            Throttle = "High"
        }
    }
```

### 2.2 åº”ç”¨ç¨‹åºä¸€è‡´æ€§å¤‡ä»½

```xml
<!-- åº”ç”¨ç¨‹åºä¸€è‡´æ€§å¤‡ä»½é…ç½® -->
<ApplicationConsistentBackup>
    <Applications>
        <!-- Microsoft SQL Server é…ç½® -->
        <Application name="SQL Server">
            <PreScript>
                <Command>powershell.exe -File "C:\Scripts\PreBackup-SQL.ps1"</Command>
                <TimeoutMinutes>30</TimeoutMinutes>
                <RunAsUser>DOMAIN\sqlservice</RunAsUser>
            </PreScript>
            
            <PostScript>
                <Command>powershell.exe -File "C:\Scripts\PostBackup-SQL.ps1"</Command>
                <TimeoutMinutes>15</TimeoutMinutes>
            </PostScript>
            
            <VSSConfiguration>
                <WriterName>SqlServerWriter</WriterName>
                <ComponentSelection>All</ComponentSelection>
                <TransactionLogBackup>Enabled</TransactionLogBackup>
                <LogTruncation>AfterBackup</LogTruncation>
            </VSSConfiguration>
        </Application>
        
        <!-- Oracle æ•°æ®åº“é…ç½® -->
        <Application name="Oracle">
            <PreScript>
                <Command>rman target / @C:\Scripts\PreBackup-Oracle.sql</Command>
                <TimeoutMinutes>45</TimeoutMinutes>
            </PreScript>
            
            <ArchiveLogMode>ARCHIVELOG</ArchiveLogMode>
            <ControlFileAutobackup>Enabled</ControlFileAutobackup>
            <BackupValidation>Enabled</BackupValidation>
        </Application>
        
        <!-- Exchange Server é…ç½® -->
        <Application name="Exchange">
            <VSSConfiguration>
                <WriterName>Microsoft Exchange Writer</WriterName>
                <GranularRecovery>Enabled</GranularRecovery>
                <MailboxRecovery>Enabled</MailboxRecovery>
            </VSSConfiguration>
        </Application>
        
        <!-- SharePoint é…ç½® -->
        <Application name="SharePoint">
            <PreScript>
                <Command>stsadm -o quiesceservice -allowupdates 0</Command>
                <TimeoutMinutes>10</TimeoutMinutes>
            </PreScript>
            
            <PostScript>
                <Command>stsadm -o quiesceservice -allowupdates 1</Command>
                <TimeoutMinutes>10</TimeoutMinutes>
            </PostScript>
        </Application>
    </Applications>
    
    <!-- è™šæ‹Ÿæœºåº”ç”¨ç¨‹åºä¸€è‡´æ€§ -->
    <VirtualMachineBackup>
        <VMware>
            <GuestQuiescing>Enabled</GuestQuiescing>
            <FileSystemQuiescing>Enabled</FileSystemQuiescing>
            <ApplicationQuiescing>
                <SQLServer>Enabled</SQLServer>
                <Exchange>Enabled</Exchange>
                <ActiveDirectory>Enabled</ActiveDirectory>
            </ApplicationQuiescing>
        </VMware>
        
        <HyperV>
            <ChildIntegrationService>Enabled</ChildIntegrationService>
            <BackupIntegration>Enabled</BackupIntegration>
            <GuestVSSProvider>Enabled</GuestVSSProvider>
        </HyperV>
    </VirtualMachineBackup>
</ApplicationConsistentBackup>
```

## 3. ç¾éš¾æ¢å¤ç­–ç•¥

### 3.1 å¤šç«™ç‚¹ç¾å¤‡æ¶æ„

```yaml
disaster_recovery_architecture:
  primary_site:
    location: "åŒ—äº¬æ•°æ®ä¸­å¿ƒ"
    commserve: "commserve-beijing"
    mediaagents:
      - "ma-beijing-01"
      - "ma-beijing-02"
    storage:
      local_disk_tb: 500
      tape_library: "IBM-TS4500-Local"
    network_bandwidth_gbps: 10
    rpo_hours: 4
    rto_hours: 2
  
  secondary_site:
    location: "ä¸Šæµ·æ•°æ®ä¸­å¿ƒ"
    commserve: "commserve-shanghai"
    mediaagents:
      - "ma-shanghai-01"
      - "ma-shanghai-02"
    storage:
      local_disk_tb: 300
      tape_library: "IBM-TS4500-DR"
    network_bandwidth_gbps: 1
    rpo_hours: 24
    rto_hours: 8
    synchronization_schedule: "æ¯4å°æ—¶å¢é‡åŒæ­¥"
  
  tertiary_site:
    location: "å¹¿å·å¼‚åœ°å¤‡ä»½ä¸­å¿ƒ"
    storage_type: "äº‘å­˜å‚¨"
    provider: "é˜¿é‡Œäº‘ OSS"
    bucket_name: "company-dr-archive"
    rpo_days: 7
    rto_days: 3
    data_sync_schedule: "æ¯æ—¥åŒæ­¥"
  
  failover_scenarios:
    site_failure:
      detection_time_minutes: 30
      failover_procedure:
        - å¯åŠ¨å¤‡ç”¨ CommServe
        - æ¿€æ´»è¿œç¨‹ MediaAgents
        - é‡å®šå‘å¤‡ä»½æµé‡
        - éªŒè¯æ•°æ®å®Œæ•´æ€§
        - é€šçŸ¥ç›¸å…³äººå‘˜
    
    regional_disaster:
      scope: "æ•´ä¸ªåŒºåŸŸç”µåŠ›ä¸­æ–­"
      recovery_steps:
        - åˆ‡æ¢åˆ°ç¬¬ä¸‰ç«™ç‚¹
        - ä»äº‘å­˜å‚¨æ¢å¤å…³é”®æ•°æ®
        - é‡å»ºæ ¸å¿ƒä¸šåŠ¡ç³»ç»Ÿ
        - é€æ­¥æ¢å¤å…¶ä»–æœåŠ¡
```

### 3.2 è‡ªåŠ¨åŒ–æ•…éšœè½¬ç§»é…ç½®

```powershell
# Commvault è‡ªåŠ¨åŒ–æ•…éšœè½¬ç§»è„šæœ¬

param(
    [Parameter(Mandatory=$true)]
    [string]$PrimarySite,
    
    [Parameter(Mandatory=$true)]
    [string]$SecondarySite,
    
    [Parameter(Mandatory=$false)]
    [int]$HealthCheckInterval = 300  # 5åˆ†é’Ÿæ£€æŸ¥é—´éš”
)

class DisasterRecoveryOrchestrator {
    [string]$PrimaryCommServe
    [string]$SecondaryCommServe
    [hashtable]$SiteStatus
    [bool]$FailoverInProgress
    
    DisasterRecoveryOrchestrator($primary, $secondary) {
        $this.PrimaryCommServe = $primary
        $this.SecondaryCommServe = $secondary
        $this.SiteStatus = @{}
        $this.FailoverInProgress = $false
    }
    
    [bool] CheckSiteHealth($site) {
        try {
            $response = Invoke-RestMethod -Uri "https://$site/HealthCheck" -Method Get -TimeoutSec 30
            return $response.Status -eq "Healthy"
        }
        catch {
            Write-Warning "æ— æ³•è¿æ¥åˆ°ç«™ç‚¹ $site : $($_.Exception.Message)"
            return $false
        }
    }
    
    [void] PerformFailover() {
        if ($this.FailoverInProgress) {
            Write-Warning "æ•…éšœè½¬ç§»å·²åœ¨è¿›è¡Œä¸­"
            return
        }
        
        $this.FailoverInProgress = $true
        Write-Host "å¼€å§‹æ‰§è¡Œæ•…éšœè½¬ç§»..." -ForegroundColor Yellow
        
        try {
            # 1. åœæ­¢ä¸»ç«™ç‚¹æœåŠ¡
            Write-Host "åœæ­¢ä¸»ç«™ç‚¹å¤‡ä»½ä½œä¸š..." -ForegroundColor Cyan
            Stop-CVBackupJobs -CommServe $this.PrimaryCommServe
            
            # 2. æ¿€æ´»å¤‡ç”¨ç«™ç‚¹
            Write-Host "æ¿€æ´»å¤‡ç”¨ç«™ç‚¹..." -ForegroundColor Cyan
            Enable-CVDRSite -CommServe $this.SecondaryCommServe
            
            # 3. é‡å®šå‘å®¢æˆ·ç«¯
            Write-Host "é‡å®šå‘å¤‡ä»½å®¢æˆ·ç«¯..." -ForegroundColor Cyan
            $clients = Get-CVClients -CommServe $this.PrimaryCommServe
            foreach ($client in $clients) {
                Move-CVClient -ClientName $client.Name -TargetCommServe $this.SecondaryCommServe
            }
            
            # 4. å¯åŠ¨å¤‡ä»½ä½œä¸š
            Write-Host "å¯åŠ¨å¤‡ç”¨ç«™ç‚¹å¤‡ä»½ä½œä¸š..." -ForegroundColor Cyan
            Start-CVBackupJobs -CommServe $this.SecondaryCommServe
            
            # 5. éªŒè¯æ¢å¤
            Write-Host "éªŒè¯æ•…éšœè½¬ç§»çŠ¶æ€..." -ForegroundColor Cyan
            $validationResult = $this.ValidateFailover()
            
            if ($validationResult) {
                Write-Host "æ•…éšœè½¬ç§»æˆåŠŸå®Œæˆï¼" -ForegroundColor Green
            } else {
                Write-Error "æ•…éšœè½¬ç§»éªŒè¯å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥"
            }
        }
        catch {
            Write-Error "æ•…éšœè½¬ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: $($_.Exception.Message)"
            $this.InitiateRollback()
        }
        finally {
            $this.FailoverInProgress = $false
        }
    }
    
    [bool] ValidateFailover() {
        $maxWaitTime = 1800  # 30åˆ†é’Ÿè¶…æ—¶
        $startTime = Get-Date
        
        do {
            Start-Sleep -Seconds 60
            $backupStatus = Get-CVBackupStatus -CommServe $this.SecondaryCommServe
            
            if ($backupStatus.RunningJobs -gt 0 -and $backupStatus.FailedJobs -eq 0) {
                return $true
            }
            
            if ((Get-Date) - $startTime).TotalSeconds -gt $maxWaitTime {
                break
            }
        } while ($true)
        
        return $false
    }
    
    [void] InitiateRollback() {
        Write-Warning "å¼€å§‹å›æ»šæ“ä½œ..."
        # å›æ»šé€»è¾‘å®ç°
    }
    
    [void] MonitorSites() {
        while ($true) {
            $primaryHealthy = $this.CheckSiteHealth($this.PrimaryCommServe)
            $secondaryHealthy = $this.CheckSiteHealth($this.SecondaryCommServe)
            
            $this.SiteStatus.Primary = $primaryHealthy
            $this.SiteStatus.Secondary = $secondaryHealthy
            
            if (-not $primaryHealthy -and $secondaryHealthy -and -not $this.FailoverInProgress) {
                Write-Host "æ£€æµ‹åˆ°ä¸»ç«™ç‚¹æ•…éšœï¼Œå‡†å¤‡æ‰§è¡Œæ•…éšœè½¬ç§»..." -ForegroundColor Red
                $this.PerformFailover()
            }
            
            Start-Sleep -Seconds $HealthCheckInterval
        }
    }
}

# ä¸»ç¨‹åºæ‰§è¡Œ
$orchestrator = [DisasterRecoveryOrchestrator]::new($PrimarySite, $SecondarySite)

# å¯åŠ¨ç›‘æ§
Write-Host "å¯åŠ¨ç¾å¤‡ç›‘æ§æœåŠ¡..." -ForegroundColor Green
$orchestrator.MonitorSites()
```

## 4. æ€§èƒ½ä¼˜åŒ–ä¸å®¹é‡è§„åˆ’

### 4.1 å¤‡ä»½æ€§èƒ½è°ƒä¼˜

```bash
#!/bin/bash
# commvault_performance_optimization.sh

# 1. ç³»ç»Ÿçº§æ€§èƒ½ä¼˜åŒ–
optimize_system_performance() {
    echo "=== ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ– ==="
    
    # è°ƒæ•´TCPå‚æ•°
    echo "ä¼˜åŒ–ç½‘ç»œTCPå‚æ•°..."
    cat >> /etc/sysctl.conf << EOF
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
net.ipv4.tcp_congestion_control = bbr
EOF
    
    sysctl -p
    
    # è°ƒæ•´æ–‡ä»¶ç³»ç»Ÿå‚æ•°
    echo "ä¼˜åŒ–æ–‡ä»¶ç³»ç»Ÿå‚æ•°..."
    tune2fs -o journal_data_writeback /dev/sdb1
    
    # è°ƒæ•´IOè°ƒåº¦å™¨
    echo "è®¾ç½®IOè°ƒåº¦å™¨ä¸ºdeadline..."
    echo deadline > /sys/block/sdb/queue/scheduler
}

# 2. Commvaultç‰¹å®šä¼˜åŒ–
optimize_commvault_settings() {
    echo "=== Commvault å‚æ•°ä¼˜åŒ– ==="
    
    # æ•°æ®åº“ä¼˜åŒ–
    cat > /opt/commvault/optimize_db.sql << 'EOF'
-- SQL Server æ€§èƒ½ä¼˜åŒ–
USE Commvault;
GO

-- åˆ›å»ºæ€§èƒ½ç´¢å¼•
CREATE INDEX IX_JobHistory_StartTime ON JobHistory(StartTime);
CREATE INDEX IX_JobHistory_ClientId ON JobHistory(ClientId);
CREATE INDEX IX_BackupInfo_BackupTime ON BackupInfo(BackupTime);

-- æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
UPDATE STATISTICS JobHistory;
UPDATE STATISTICS BackupInfo;

-- é…ç½®å†…å­˜ä¼˜åŒ–
EXEC sp_configure 'max server memory (MB)', 24576;
EXEC sp_configure 'min server memory (MB)', 4096;
RECONFIGURE;

-- å¯ç”¨å³æ—¶æ–‡ä»¶åˆå§‹åŒ–
EXEC xp_cmdshell 'sc config SQLSERVERAGENT binpath= "C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\Binn\SQLAGENT.EXE" -sSQLSERVERAGENT -i"C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL" -d"C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\DATA\master.mdf" -l"C:\Program Files\Microsoft SQL Server\MSSQL15.MSSQLSERVER\MSSQL\DATA\mastlog.ldf" -T2704';
EOF
    
    # æ‰§è¡Œæ•°æ®åº“ä¼˜åŒ–
    sqlcmd -S localhost -E -i /opt/commvault/optimize_db.sql
}

# 3. å­˜å‚¨æ€§èƒ½ä¼˜åŒ–
optimize_storage_performance() {
    echo "=== å­˜å‚¨æ€§èƒ½ä¼˜åŒ– ==="
    
    # ç£ç›˜é˜Ÿåˆ—æ·±åº¦ä¼˜åŒ–
    echo "ä¼˜åŒ–ç£ç›˜é˜Ÿåˆ—æ·±åº¦..."
    cat > /etc/udev/rules.d/99-commvault-storage.rules << 'EOF'
ACTION=="add", SUBSYSTEM=="block", KERNEL=="sd*", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="noop", ATTR{queue/nr_requests}="1024", ATTR{queue/read_ahead_kb}="4096"
EOF
    
    # é‡å¯udevæœåŠ¡
    udevadm control --reload-rules
    udevadm trigger
    
    # åˆ›å»ºä¼˜åŒ–çš„æŒ‚è½½é€‰é¡¹
    echo "ä¼˜åŒ–å­˜å‚¨æŒ‚è½½é€‰é¡¹..."
    sed -i '/backup-storage/d' /etc/fstab
    echo "/dev/sdb1 /backup ext4 defaults,noatime,nobarrier,data=writeback 0 2" >> /etc/fstab
    mount -o remount /backup
}

# 4. ç½‘ç»œæ€§èƒ½ä¼˜åŒ–
optimize_network_performance() {
    echo "=== ç½‘ç»œæ€§èƒ½ä¼˜åŒ– ==="
    
    # é…ç½®å·¨å¸§
    echo "å¯ç”¨å·¨å¸§æ”¯æŒ..."
    for interface in eth0 eth1; do
        if ip link show $interface >/dev/null 2>&1; then
            ip link set $interface mtu 9000
            ethtool -K $interface gso on tso on gro on
        fi
    done
    
    # ä¼˜åŒ–ç½‘ç»œç¼“å†²åŒº
    echo "ä¼˜åŒ–ç½‘ç»œç¼“å†²åŒº..."
    cat >> /etc/sysctl.conf << EOF
net.core.netdev_max_backlog = 5000
net.core.rmem_default = 262144
net.core.wmem_default = 262144
net.core.optmem_max = 20480
EOF
    
    sysctl -p
}

# 5. ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•
performance_benchmarking() {
    echo "=== æ€§èƒ½åŸºå‡†æµ‹è¯• ==="
    
    # åˆ›å»ºæµ‹è¯•è„šæœ¬
    cat > /opt/commvault/benchmark.sh << 'EOF'
#!/bin/bash

TEST_DIR="/backup/benchmark"
TEST_SIZE="10G"
RESULTS_FILE="/var/log/commvault/benchmark_results.txt"

mkdir -p $TEST_DIR
mkdir -p /var/log/commvault

echo "å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•..." | tee -a $RESULTS_FILE
echo "æµ‹è¯•æ—¶é—´: $(date)" | tee -a $RESULTS_FILE

# ç£ç›˜IOæµ‹è¯•
echo "=== ç£ç›˜IOæ€§èƒ½æµ‹è¯• ===" | tee -a $RESULTS_FILE
dd if=/dev/zero of=$TEST_DIR/testfile bs=1M count=10240 oflag=direct 2>&1 | tee -a $RESULTS_FILE

# ç½‘ç»œååé‡æµ‹è¯•
echo "=== ç½‘ç»œååé‡æµ‹è¯• ===" | tee -a $RESULTS_FILE
iperf3 -c backup-server -t 60 -P 4 2>&1 | tee -a $RESULTS_FILE

# å¤‡ä»½æ€§èƒ½æµ‹è¯•
echo "=== å¤‡ä»½æ€§èƒ½æµ‹è¯• ===" | tee -a $RESULTS_FILE
# è¿™é‡Œå¯ä»¥é›†æˆCommvaultçš„å¤‡ä»½æ€§èƒ½æµ‹è¯•å‘½ä»¤

# æ¸…ç†æµ‹è¯•æ–‡ä»¶
rm -f $TEST_DIR/testfile
EOF
    
    chmod +x /opt/commvault/benchmark.sh
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    /opt/commvault/benchmark.sh
}

# ä¸»æ‰§è¡Œå‡½æ•°
main() {
    echo "å¼€å§‹Commvaultæ€§èƒ½ä¼˜åŒ–..."
    
    optimize_system_performance
    optimize_commvault_settings
    optimize_storage_performance
    optimize_network_performance
    performance_benchmarking
    
    echo "æ€§èƒ½ä¼˜åŒ–å®Œæˆï¼"
    echo "è¯·é‡å¯CommvaultæœåŠ¡ä»¥ä½¿æ›´æ”¹ç”Ÿæ•ˆ"
}

main
```

### 4.2 å®¹é‡è§„åˆ’å·¥å…·

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Commvault å®¹é‡è§„åˆ’å’Œé¢„æµ‹å·¥å…·
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
import json
import argparse

class CommvaultCapacityPlanner:
    def __init__(self):
        self.current_data = {}
        self.forecast_data = {}
        self.growth_rate = 0.15  # é»˜è®¤å¹´å¢é•¿ç‡15%
        
    def load_current_inventory(self, inventory_file):
        """åŠ è½½å½“å‰å¤‡ä»½ç¯å¢ƒæ¸…å•"""
        try:
            with open(inventory_file, 'r', encoding='utf-8') as f:
                self.current_data = json.load(f)
            print(f"æˆåŠŸåŠ è½½æ¸…å•æ–‡ä»¶: {inventory_file}")
        except FileNotFoundError:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¸…å•æ–‡ä»¶ {inventory_file}")
            return False
        except json.JSONDecodeError:
            print(f"é”™è¯¯: æ¸…å•æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
            return False
        return True
    
    def calculate_current_capacity(self):
        """è®¡ç®—å½“å‰å®¹é‡ä½¿ç”¨æƒ…å†µ"""
        total_protected_data = 0
        total_backup_size = 0
        total_deduplicated_size = 0
        
        for client in self.current_data.get('clients', []):
            client_size = client.get('data_size_gb', 0)
            total_protected_data += client_size
            
            # è®¡ç®—å¤‡ä»½å¤§å°ï¼ˆè€ƒè™‘å‹ç¼©å’Œå»é‡ï¼‰
            compression_ratio = client.get('compression_ratio', 2.0)
            deduplication_ratio = client.get('deduplication_ratio', 10.0)
            
            backup_size = client_size / compression_ratio
            deduplicated_size = backup_size / deduplication_ratio
            
            total_backup_size += backup_size
            total_deduplicated_size += deduplicated_size
        
        capacity_metrics = {
            'protected_data_tb': round(total_protected_data / 1024, 2),
            'raw_backup_tb': round(total_backup_size / 1024, 2),
            'deduplicated_backup_tb': round(total_deduplicated_size / 1024, 2),
            'effective_compression_ratio': round(total_protected_data / total_deduplicated_size, 2) if total_deduplicated_size > 0 else 0,
            'total_clients': len(self.current_data.get('clients', []))
        }
        
        return capacity_metrics
    
    def forecast_growth(self, months_ahead=36):
        """é¢„æµ‹æœªæ¥å®¹é‡å¢é•¿"""
        current_metrics = self.calculate_current_capacity()
        historical_data = []
        
        # ç”Ÿæˆå†å²æ•°æ®ç‚¹ï¼ˆå‡è®¾è¿‡å»2å¹´çš„æœˆåº¦æ•°æ®ï¼‰
        base_size = current_metrics['deduplicated_backup_tb']
        current_date = datetime.now()
        
        for i in range(24, -1, -1):  # è¿‡å»24ä¸ªæœˆåˆ°å½“å‰
            date = current_date - timedelta(days=i*30)
            months_back = 24 - i
            
            # æ¨¡æ‹Ÿå†å²å¢é•¿ï¼ˆå¸¦ä¸€äº›éšæœºæ³¢åŠ¨ï¼‰
            growth_factor = (1 + self.growth_rate/12) ** months_back
            size = base_size / growth_factor * (0.95 + 0.1*np.random.random())
            
            historical_data.append({
                'date': date.strftime('%Y-%m'),
                'months_ago': months_back,
                'size_tb': round(size, 2)
            })
        
        # é¢„æµ‹æœªæ¥æ•°æ®
        forecast_data = []
        for i in range(1, months_ahead + 1):
            date = current_date + timedelta(days=i*30)
            growth_factor = (1 + self.growth_rate/12) ** i
            size = base_size * growth_factor * (0.98 + 0.04*np.random.random())
            
            forecast_data.append({
                'date': date.strftime('%Y-%m'),
                'months_ahead': i,
                'size_tb': round(size, 2),
                'growth_rate': f"{self.growth_rate*100:.1f}%"
            })
        
        self.forecast_data = {
            'historical': historical_data,
            'forecast': forecast_data,
            'current_metrics': current_metrics
        }
        
        return self.forecast_data
    
    def generate_recommendations(self):
        """ç”Ÿæˆå®¹é‡è§„åˆ’å»ºè®®"""
        if not self.forecast_data:
            self.forecast_growth()
        
        current = self.forecast_data['current_metrics']
        forecast = self.forecast_data['forecast']
        
        recommendations = {
            'immediate_needs': {},
            'short_term_planning': {},
            'long_term_strategy': {}
        }
        
        # 1å¹´åçš„é¢„æµ‹å®¹é‡
        one_year_forecast = next((item for item in forecast if item['months_ahead'] == 12), None)
        three_year_forecast = next((item for item in forecast if item['months_ahead'] == 36), None)
        
        if one_year_forecast:
            # ç«‹å³éœ€æ±‚
            current_capacity = current['deduplicated_backup_tb']
            projected_capacity_1y = one_year_forecast['size_tb']
            growth_needed_1y = projected_capacity_1y - current_capacity
            
            recommendations['immediate_needs'] = {
                'additional_capacity_tb': round(growth_needed_1y, 2),
                'recommended_action': 'æ‰©å±•å½“å‰å­˜å‚¨æ± ',
                'timeline': '3-6ä¸ªæœˆå†…'
            }
        
        if three_year_forecast:
            # é•¿æœŸè§„åˆ’
            projected_capacity_3y = three_year_forecast['size_tb']
            total_growth_3y = projected_capacity_3y - current['deduplicated_backup_tb']
            
            recommendations['long_term_strategy'] = {
                'total_growth_needed_tb': round(total_growth_3y, 2),
                'annual_growth_tb': round(total_growth_3y/3, 2),
                'recommended_approach': 'æ¸è¿›å¼æ‰©å±• + äº‘å­˜å‚¨å½’æ¡£',
                'investment_timeline': 'åˆ†é˜¶æ®µå®æ–½'
            }
        
        # æŠ€æœ¯å»ºè®®
        recommendations['technical_considerations'] = {
            'storage_tiering': 'å»ºè®®é‡‡ç”¨ä¸‰å±‚å­˜å‚¨æ¶æ„ï¼ˆçƒ­/æ¸©/å†·ï¼‰',
            'cloud_integration': 'è€ƒè™‘å°†é•¿æœŸå½’æ¡£è¿ç§»åˆ°äº‘ç«¯',
            'performance_scaling': 'æå‰è§„åˆ’ç½‘ç»œå¸¦å®½å’Œå¤„ç†èƒ½åŠ›',
            'monitoring_alerts': 'å»ºç«‹å®¹é‡é¢„è­¦æœºåˆ¶'
        }
        
        return recommendations
    
    def create_visualization(self, output_file='capacity_forecast.png'):
        """åˆ›å»ºå®¹é‡é¢„æµ‹å¯è§†åŒ–å›¾è¡¨"""
        if not self.forecast_data:
            self.forecast_growth()
        
        # å‡†å¤‡æ•°æ®
        historical_df = pd.DataFrame(self.forecast_data['historical'])
        forecast_df = pd.DataFrame(self.forecast_data['forecast'])
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 8))
        sns.set_style("whitegrid")
        
        # å†å²æ•°æ®
        plt.plot(historical_df['months_ago'], historical_df['size_tb'], 
                marker='o', linewidth=2, label='å†å²æ•°æ®', color='#2E86AB')
        
        # é¢„æµ‹æ•°æ®
        months_future = [item['months_ahead'] for item in self.forecast_data['forecast']]
        sizes_future = [item['size_tb'] for item in self.forecast_data['forecast']]
        plt.plot(months_future, sizes_future, 
                marker='s', linewidth=2, label='é¢„æµ‹æ•°æ®', color='#A23B72')
        
        # å½“å‰ç‚¹æ ‡è®°
        current_months = 0
        current_size = self.forecast_data['current_metrics']['deduplicated_backup_tb']
        plt.scatter([current_months], [current_size], 
                   s=100, color='#F18F01', zorder=5, label='å½“å‰çŠ¶æ€')
        
        # å…³é”®é‡Œç¨‹ç¢‘
        milestones = [12, 24, 36]  # 1å¹´, 2å¹´, 3å¹´
        for milestone in milestones:
            if milestone <= len(sizes_future):
                size_at_milestone = sizes_future[milestone-1]
                plt.axvline(x=milestone, color='gray', linestyle='--', alpha=0.7)
                plt.annotate(f'{milestone}ä¸ªæœˆ\n{size_at_milestone:.1f}TB', 
                           xy=(milestone, size_at_milestone),
                           xytext=(5, 10), textcoords='offset points',
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
        
        plt.xlabel('æœˆä»½')
        plt.ylabel('å®¹é‡ (TB)')
        plt.title('Commvault å®¹é‡å¢é•¿é¢„æµ‹')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜å›¾è¡¨
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"å®¹é‡é¢„æµ‹å›¾è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    
    def generate_report(self, output_file='capacity_planning_report.json'):
        """ç”Ÿæˆå®Œæ•´çš„å®¹é‡è§„åˆ’æŠ¥å‘Š"""
        if not self.forecast_data:
            self.forecast_growth()
        
        report = {
            'generated_at': datetime.now().isoformat(),
            'current_capacity': self.forecast_data['current_metrics'],
            'forecast_data': self.forecast_data,
            'recommendations': self.generate_recommendations(),
            'assumptions': {
                'annual_growth_rate': f"{self.growth_rate*100}%",
                'data_retention_period': 'é»˜è®¤ä¿ç•™ç­–ç•¥',
                'compression_assumption': 'åŸºäºå†å²å¹³å‡å‹ç¼©æ¯”',
                'reporting_period': 'æœˆåº¦åˆ†æ'
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"å®¹é‡è§„åˆ’æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return report

def main():
    parser = argparse.ArgumentParser(description='Commvault å®¹é‡è§„åˆ’å·¥å…·')
    parser.add_argument('--inventory', '-i', required=True, 
                       help='å½“å‰ç¯å¢ƒæ¸…å•æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--growth-rate', '-g', type=float, default=0.15,
                       help='å¹´å¢é•¿ç‡ (é»˜è®¤: 0.15 = 15%)')
    parser.add_argument('--months', '-m', type=int, default=36,
                       help='é¢„æµ‹æœˆæ•° (é»˜è®¤: 36ä¸ªæœˆ)')
    parser.add_argument('--output', '-o', default='capacity_report',
                       help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè§„åˆ’å™¨å®ä¾‹
    planner = CommvaultCapacityPlanner()
    planner.growth_rate = args.growth_rate
    
    # åŠ è½½æ•°æ®
    if not planner.load_current_inventory(args.inventory):
        return 1
    
    # æ‰§è¡Œåˆ†æ
    print("å¼€å§‹å®¹é‡è§„åˆ’åˆ†æ...")
    forecast_data = planner.forecast_growth(args.months)
    recommendations = planner.generate_recommendations()
    
    # ç”Ÿæˆè¾“å‡º
    planner.create_visualization(f"{args.output}_forecast.png")
    planner.generate_report(f"{args.output}_report.json")
    
    # æ‰“å°æ‘˜è¦
    print("\n=== å®¹é‡è§„åˆ’æ‘˜è¦ ===")
    current = forecast_data['current_metrics']
    print(f"å½“å‰ä¿æŠ¤æ•°æ®: {current['protected_data_tb']} TB")
    print(f"å½“å‰å¤‡ä»½å®¹é‡: {current['deduplicated_backup_tb']} TB")
    print(f"æœ‰æ•ˆå‹ç¼©æ¯”: {current['effective_compression_ratio']}:1")
    
    one_year = next((item for item in forecast_data['forecast'] if item['months_ahead'] == 12), None)
    if one_year:
        print(f"1å¹´åé¢„è®¡å®¹é‡: {one_year['size_tb']} TB")
    
    print(f"\nä¸»è¦å»ºè®®:")
    recs = recommendations
    if 'immediate_needs' in recs:
        print(f"- ç«‹å³éœ€æ±‚: {recs['immediate_needs'].get('additional_capacity_tb', 0)} TB")
    if 'long_term_strategy' in recs:
        print(f"- é•¿æœŸè§„åˆ’: {recs['long_term_strategy'].get('total_growth_needed_tb', 0)} TB æ€»å¢é•¿")
    
    return 0

if __name__ == "__main__":
    exit(main())
```

## 5. ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ

### 5.1 ç»¼åˆç›‘æ§ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "name": "Commvault ä¼ä¸šçº§ç›‘æ§ä»ªè¡¨æ¿",
    "refresh_interval": "30s",
    "timezone": "Asia/Shanghai",
    "panels": [
      {
        "title": "å¤‡ä»½ä½œä¸šçŠ¶æ€æ¦‚è§ˆ",
        "type": "stat",
        "datasource": "Commvault",
        "targets": [
          {
            "query": "SELECT COUNT(*) as total_jobs FROM JobHistory WHERE StartTime >= DATEADD(day, -1, GETDATE())",
            "legendFormat": "æ€»ä½œä¸šæ•°"
          },
          {
            "query": "SELECT COUNT(*) as successful_jobs FROM JobHistory WHERE Status = 'Completed' AND StartTime >= DATEADD(day, -1, GETDATE())",
            "legendFormat": "æˆåŠŸä½œä¸š"
          },
          {
            "query": "SELECT COUNT(*) as failed_jobs FROM JobHistory WHERE Status IN ('Failed', 'Error') AND StartTime >= DATEADD(day, -1, GETDATE())",
            "legendFormat": "å¤±è´¥ä½œä¸š"
          }
        ],
        "thresholds": {
          "failed_jobs": {
            "warning": 5,
            "critical": 10
          }
        }
      },
      {
        "title": "å­˜å‚¨å®¹é‡ä½¿ç”¨æƒ…å†µ",
        "type": "gauge",
        "targets": [
          {
            "query": "SELECT (UsedSpaceGB/TotalSpaceGB)*100 as utilization FROM StoragePools",
            "legendFormat": "{{pool_name}}"
          }
        ],
        "thresholds": {
          "utilization": {
            "normal": 0,
            "warning": 80,
            "critical": 95
          }
        }
      },
      {
        "title": "å¤‡ä»½æ€§èƒ½è¶‹åŠ¿",
        "type": "graph",
        "targets": [
          {
            "query": "SELECT AVG(DurationMinutes) as avg_duration, DATE(Date) as day FROM JobHistory WHERE JobType = 'Backup' GROUP BY DATE(Date) ORDER BY Date DESC LIMIT 30",
            "legendFormat": "å¹³å‡å¤‡ä»½æ—¶é•¿(åˆ†é’Ÿ)"
          },
          {
            "query": "SELECT AVG(DataSizeGB) as avg_data_size, DATE(Date) as day FROM JobHistory WHERE JobType = 'Backup' GROUP BY DATE(Date) ORDER BY Date DESC LIMIT 30",
            "legendFormat": "å¹³å‡æ•°æ®é‡(GB)"
          }
        ]
      },
      {
        "title": "å®¢æˆ·ç«¯ä¿æŠ¤çŠ¶æ€",
        "type": "table",
        "targets": [
          {
            "query": "SELECT ClientName, LastBackupTime, BackupStatus, DaysSinceLastBackup FROM Clients ORDER BY DaysSinceLastBackup DESC",
            "legendFormat": "å®¢æˆ·ç«¯ä¿æŠ¤çŠ¶æ€"
          }
        ],
        "thresholds": {
          "DaysSinceLastBackup": {
            "warning": 2,
            "critical": 7
          }
        }
      }
    ]
  }
}
```

### 5.2 æ™ºèƒ½å‘Šè­¦è§„åˆ™

```yaml
# commvault_alerting_rules.yaml
alerting_rules:
  backup_job_failures:
    name: "å¤‡ä»½ä½œä¸šå¤±è´¥å‘Šè­¦"
    description: "ç›‘æ§å¤‡ä»½ä½œä¸šå¤±è´¥æƒ…å†µ"
    severity: "high"
    frequency: "5m"
    conditions:
      - metric: "failed_backup_jobs"
        operator: ">"
        threshold: 3
        duration: "15m"
    actions:
      - type: "email"
        recipients:
          - "backup-admin@company.com"
          - "noc@company.com"
      - type: "sms"
        recipients:
          - "+86-138-0000-0001"
      - type: "webhook"
        url: "https://monitoring.company.com/webhook/commvault"
    
  storage_capacity_warning:
    name: "å­˜å‚¨å®¹é‡è­¦å‘Š"
    description: "ç›‘æ§å­˜å‚¨æ± å®¹é‡ä½¿ç”¨æƒ…å†µ"
    severity: "warning"
    frequency: "1h"
    conditions:
      - metric: "storage_utilization"
        operator: ">"
        threshold: 85
        duration: "1h"
      - label_filters:
          pool_type: "disk"
    actions:
      - type: "email"
        recipients:
          - "storage-admin@company.com"
      - type: "ticket"
        system: "ServiceNow"
        priority: "3"
    
  ransomware_detection:
    name: "å‹’ç´¢è½¯ä»¶æ£€æµ‹"
    description: "æ£€æµ‹å¼‚å¸¸æ–‡ä»¶ä¿®æ”¹æ¨¡å¼"
    severity: "critical"
    frequency: "1m"
    conditions:
      - metric: "file_modification_rate"
        operator: ">"
        threshold: 1000  # æ¯åˆ†é’Ÿæ–‡ä»¶ä¿®æ”¹æ¬¡æ•°
        duration: "5m"
      - metric: "unusual_file_extensions"
        operator: ">"
        threshold: 50
        duration: "10m"
    actions:
      - type: "immediate_shutdown"
        target: "affected_clients"
      - type: "isolation"
        target: "network_segments"
      - type: "notification"
        recipients:
          - "security-team@company.com"
          - "management@company.com"
    
  compliance_violation:
    name: "åˆè§„æ€§è¿è§„å‘Šè­¦"
    description: "ç›‘æ§å¤‡ä»½ä¿ç•™ç­–ç•¥åˆè§„æ€§"
    severity: "medium"
    frequency: "1d"
    conditions:
      - metric: "expired_backups_not_deleted"
        operator: ">"
        threshold: 0
        duration: "1d"
      - metric: "missing_required_backups"
        operator: ">"
        threshold: 0
        duration: "1d"
    actions:
      - type: "email"
        recipients:
          - "compliance@company.com"
          - "audit@company.com"
      - type: "report_generation"
        template: "compliance_violation_report"
    
  performance_degradation:
    name: "æ€§èƒ½ä¸‹é™å‘Šè­¦"
    description: "ç›‘æ§å¤‡ä»½æ€§èƒ½æŒ‡æ ‡"
    severity: "warning"
    frequency: "10m"
    conditions:
      - metric: "average_backup_duration"
        operator: ">"
        threshold: 1.5  # ç›¸æ¯”åŸºçº¿å¢åŠ 50%
        duration: "30m"
      - metric: "throughput_mb_per_second"
        operator: "<"
        threshold: 50  # MB/s
        duration: "15m"
    actions:
      - type: "email"
        recipients:
          - "performance-team@company.com"
      - type: "auto_scaling"
        target: "media_agents"
        action: "scale_up"
```

## 6. åˆè§„æ€§ä¸å®¡è®¡

### 6.1 æ•°æ®ä¿æŠ¤åˆè§„æ¡†æ¶

```xml
<!-- æ•°æ®ä¿æŠ¤åˆè§„æ€§é…ç½® -->
<DataProtectionCompliance>
    <Regulations>
        <!-- GDPR åˆè§„é…ç½® -->
        <GDPR>
            <DataSubjectRights>
                <RightToAccess>Enabled</RightToAccess>
                <RightToErasure>Enabled</RightToErasure>
                <RightToDataPortability>Enabled</RightToDataPortability>
                <RightToObject>Enabled</RightToObject>
            </DataSubjectRights>
            
            <RetentionPolicies>
                <PurposeBasedRetention>Enabled</PurposeBasedRetention>
                <MaximumRetentionPeriod>2555</MaximumRetentionPeriod> <!-- 7 years -->
                <RegularReviewInterval>90</RegularReviewInterval> <!-- 90 days -->
            </RetentionPolicies>
            
            <DataProcessing>
                <ConsentManagement>Enabled</ConsentManagement>
                <PrivacyByDefault>Enabled</PrivacyByDefault>
                <PrivacyByDesign>Enabled</PrivacyByDesign>
            </DataProcessing>
        </GDPR>
        
        <!-- ç½‘ç»œå®‰å…¨æ³•åˆè§„ -->
        <CyberSecurityLaw>
            <DataLocalization>Required</DataLocalization>
            <SecurityAssessment>Mandatory</SecurityAssessment>
            <IncidentReporting>Within24Hours</IncidentReporting>
            <CrossBorderTransfer>Restricted</CrossBorderTransfer>
        </CyberSecurityLaw>
        
        <!-- ç­‰ä¿2.0åˆè§„ -->
        <LevelProtection2>
            <SecurityLevel>ä¸‰çº§</SecurityLevel>
            <TechnicalRequirements>
                <IdentityAuthentication>MultiFactor</IdentityAuthentication>
                <AccessControl>FineGrained</AccessControl>
                <SecurityAudit>FullCoverage</SecurityAudit>
                <IntrusionPrevention>RealTime</IntrusionPrevention>
                <MaliciousCodePrevention>MultiLayer</MaliciousCodePrevention>
            </TechnicalRequirements>
            
            <ManagementRequirements>
                <SecurityManagementSystem>Established</SecurityManagementSystem>
                <PersonnelSecurity>AwarenessTraining</PersonnelSecurity>
                <SystemConstruction>SecureDevelopment</SystemConstruction>
                <SystemOperation>MaintenancePlan</SystemOperation>
            </ManagementRequirements>
        </LevelProtection2>
    </Regulations>
    
    <AuditTrail>
        <Logging>
            <UserActivities>Full</UserActivities>
            <SystemEvents>Full</SystemEvents>
            <DataAccess>Full</DataAccess>
            <PolicyChanges>Full</PolicyChanges>
        </Logging>
        
        <LogRetention>
            <SecurityLogs>180days</SecurityLogs>
            <OperationalLogs>90days</OperationalLogs>
            <AuditLogs>365days</AuditLogs>
        </LogRetention>
        
        <Reporting>
            <ScheduledReports>
                <Daily>BackupStatus,DiskUsage</Daily>
                <Weekly>PerformanceMetrics,ComplianceStatus</Weekly>
                <Monthly>CapacityPlanning,SecurityAssessment</Monthly>
                <Quarterly>RiskAnalysis,RegulatoryCompliance</Quarterly>
            </ScheduledReports>
            
            <AdhocReports>
                <IncidentInvestigation>OnDemand</IncidentInvestigation>
                <ComplianceAudit>OnRequest</ComplianceAudit>
                <ManagementReview>Monthly</ManagementReview>
            </AdhocReports>
        </Reporting>
    </AuditTrail>
</DataProtectionCompliance>
```

### 6.2 è‡ªåŠ¨åŒ–åˆè§„æ£€æŸ¥è„šæœ¬

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–åˆè§„æ€§æ£€æŸ¥å’ŒæŠ¥å‘Šç”Ÿæˆå·¥å…·
"""

import json
import sqlite3
from datetime import datetime, timedelta
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.application import MIMEApplication
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

class ComplianceChecker:
    def __init__(self, commvault_db_path):
        self.db_path = commvault_db_path
        self.compliance_rules = self.load_compliance_rules()
        self.violations = []
        
    def load_compliance_rules(self):
        """åŠ è½½åˆè§„æ€§è§„åˆ™"""
        return {
            'backup_frequency': {
                'critical_systems': {'max_gap_hours': 24},
                'important_systems': {'max_gap_hours': 168},  # 1å‘¨
                'standard_systems': {'max_gap_hours': 336}    # 2å‘¨
            },
            'retention_compliance': {
                'minimum_retention_days': 30,
                'maximum_retention_years': 7
            },
            'security_compliance': {
                'encryption_required': True,
                'access_logging': True,
                'regular_audits': True
            }
        }
    
    def check_backup_frequency_compliance(self):
        """æ£€æŸ¥å¤‡ä»½é¢‘ç‡åˆè§„æ€§"""
        conn = sqlite3.connect(self.db_path)
        
        query = """
        SELECT 
            c.ClientName,
            c.ClientGroupName,
            MAX(j.EndTime) as LastBackupTime,
            CASE 
                WHEN c.ClientGroupName LIKE '%Critical%' THEN 'critical'
                WHEN c.ClientGroupName LIKE '%Important%' THEN 'important'
                ELSE 'standard'
            END as system_type
        FROM Clients c
        LEFT JOIN JobHistory j ON c.ClientId = j.ClientId 
            AND j.JobType = 'Backup' 
            AND j.Status = 'Completed'
        GROUP BY c.ClientId
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        current_time = datetime.now()
        violations = []
        
        for _, row in df.iterrows():
            if pd.isna(row['LastBackupTime']):
                violations.append({
                    'type': 'missing_backup',
                    'client': row['ClientName'],
                    'severity': 'critical',
                    'description': 'ä»æœªæ‰§è¡Œè¿‡å¤‡ä»½'
                })
                continue
            
            last_backup = datetime.strptime(row['LastBackupTime'], '%Y-%m-%d %H:%M:%S')
            gap_hours = (current_time - last_backup).total_seconds() / 3600
            
            system_type = row['system_type']
            max_gap = self.compliance_rules['backup_frequency'][f'{system_type}_systems']['max_gap_hours']
            
            if gap_hours > max_gap:
                violations.append({
                    'type': 'backup_gap',
                    'client': row['ClientName'],
                    'system_type': system_type,
                    'gap_hours': round(gap_hours, 2),
                    'max_allowed_hours': max_gap,
                    'severity': 'high' if system_type == 'critical' else 'medium',
                    'description': f'å¤‡ä»½é—´éš”è¶…è¿‡è§„å®šæ—¶é—´ {gap_hours:.1f}å°æ—¶ > {max_gap}å°æ—¶'
                })
        
        self.violations.extend(violations)
        return violations
    
    def check_retention_compliance(self):
        """æ£€æŸ¥æ•°æ®ä¿ç•™åˆè§„æ€§"""
        conn = sqlite3.connect(self.db_path)
        
        query = """
        SELECT 
            bp.BackupSetName,
            sp.StoragePolicyName,
            sp.RetentionDays,
            COUNT(*) as backup_count
        FROM BackupInfo bp
        JOIN StoragePolicies sp ON bp.StoragePolicyId = sp.StoragePolicyId
        WHERE bp.BackupTime >= datetime('now', '-2 years')
        GROUP BY bp.BackupSetId
        """
        
        df = pd.read_sql_query(query, conn)
        conn.close()
        
        violations = []
        
        for _, row in df.iterrows():
            retention_days = row['RetentionDays']
            
            # æ£€æŸ¥æœ€å°ä¿ç•™æœŸ
            if retention_days < self.compliance_rules['retention_compliance']['minimum_retention_days']:
                violations.append({
                    'type': 'insufficient_retention',
                    'backup_set': row['BackupSetName'],
                    'current_retention': retention_days,
                    'minimum_required': self.compliance_rules['retention_compliance']['minimum_retention_days'],
                    'severity': 'medium',
                    'description': f'ä¿ç•™æœŸä¸è¶³: {retention_days}å¤© < {self.compliance_rules["retention_compliance"]["minimum_retention_days"]}å¤©'
                })
            
            # æ£€æŸ¥æœ€å¤§ä¿ç•™æœŸ
            if retention_days > (self.compliance_rules['retention_compliance']['maximum_retention_years'] * 365):
                violations.append({
                    'type': 'excessive_retention',
                    'backup_set': row['BackupSetName'],
                    'current_retention': retention_days,
                    'maximum_allowed': self.compliance_rules['retention_compliance']['maximum_retention_years'] * 365,
                    'severity': 'low',
                    'description': f'ä¿ç•™æœŸè¿‡é•¿: {retention_days}å¤© > {self.compliance_rules["retention_compliance"]["maximum_retention_years"] * 365}å¤©'
                })
        
        self.violations.extend(violations)
        return violations
    
    def check_security_compliance(self):
        """æ£€æŸ¥å®‰å…¨åˆè§„æ€§"""
        conn = sqlite3.connect(self.db_path)
        
        # æ£€æŸ¥åŠ å¯†çŠ¶æ€
        encryption_query = """
        SELECT 
            COUNT(*) as total_backups,
            SUM(CASE WHEN IsEncrypted = 1 THEN 1 ELSE 0 END) as encrypted_backups
        FROM BackupInfo
        WHERE BackupTime >= datetime('now', '-1 month')
        """
        
        enc_df = pd.read_sql_query(encryption_query, conn)
        
        # æ£€æŸ¥è®¿é—®æ—¥å¿—
        audit_query = """
        SELECT COUNT(*) as audit_entries
        FROM AuditLog
        WHERE EventTime >= datetime('now', '-24 hours')
        """
        
        audit_df = pd.read_sql_query(audit_query, conn)
        conn.close()
        
        violations = []
        
        # åŠ å¯†åˆè§„æ£€æŸ¥
        if self.compliance_rules['security_compliance']['encryption_required']:
            encryption_rate = (enc_df.iloc[0]['encrypted_backups'] / enc_df.iloc[0]['total_backups']) * 100
            if encryption_rate < 95:  # è¦æ±‚95%ä»¥ä¸Šçš„å¤‡ä»½åŠ å¯†
                violations.append({
                    'type': 'encryption_non_compliance',
                    'encryption_rate': round(encryption_rate, 2),
                    'required_rate': 95,
                    'severity': 'high',
                    'description': f'åŠ å¯†ç‡ä¸è¶³: {encryption_rate:.1f}% < 95%'
                })
        
        # å®¡è®¡æ—¥å¿—æ£€æŸ¥
        if self.compliance_rules['security_compliance']['access_logging']:
            daily_audit_entries = audit_df.iloc[0]['audit_entries']
            if daily_audit_entries < 1000:  # è¦æ±‚æ¯æ—¥è‡³å°‘1000æ¡å®¡è®¡è®°å½•
                violations.append({
                    'type': 'insufficient_audit_logging',
                    'daily_entries': daily_audit_entries,
                    'minimum_required': 1000,
                    'severity': 'medium',
                    'description': f'å®¡è®¡æ—¥å¿—ä¸è¶³: {daily_audit_entries}æ¡ < 1000æ¡'
                })
        
        self.violations.extend(violations)
        return violations
    
    def generate_compliance_report(self):
        """ç”Ÿæˆåˆè§„æ€§æŠ¥å‘Š"""
        # æ‰§è¡Œæ‰€æœ‰åˆè§„æ£€æŸ¥
        self.check_backup_frequency_compliance()
        self.check_retention_compliance()
        self.check_security_compliance()
        
        # ç”ŸæˆæŠ¥å‘Šæ•°æ®
        report_data = {
            'generated_at': datetime.now().isoformat(),
            'total_violations': len(self.violations),
            'violations_by_severity': {
                'critical': len([v for v in self.violations if v['severity'] == 'critical']),
                'high': len([v for v in self.violations if v['severity'] == 'high']),
                'medium': len([v for v in self.violations if v['severity'] == 'medium']),
                'low': len([v for v in self.violations if v['severity'] == 'low'])
            },
            'violations_by_type': {},
            'detailed_violations': self.violations
        }
        
        # æŒ‰ç±»å‹ç»Ÿè®¡è¿è§„
        for violation in self.violations:
            v_type = violation['type']
            if v_type not in report_data['violations_by_type']:
                report_data['violations_by_type'][v_type] = 0
            report_data['violations_by_type'][v_type] += 1
        
        return report_data
    
    def create_visual_dashboard(self, report_data):
        """åˆ›å»ºå¯è§†åŒ–ä»ªè¡¨æ¿"""
        # åˆ›å»ºè¿è§„ä¸¥é‡ç¨‹åº¦é¥¼å›¾
        plt.figure(figsize=(12, 5))
        
        # å­å›¾1: ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        plt.subplot(1, 2, 1)
        severity_counts = [
            report_data['violations_by_severity']['critical'],
            report_data['violations_by_severity']['high'],
            report_data['violations_by_severity']['medium'],
            report_data['violations_by_severity']['low']
        ]
        severity_labels = ['ä¸¥é‡', 'é«˜', 'ä¸­', 'ä½']
        colors = ['#FF6B6B', '#FFE66D', '#4ECDC4', '#45B7D1']
        
        plt.pie(severity_counts, labels=severity_labels, colors=colors, autopct='%1.1f%%')
        plt.title('è¿è§„ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ')
        
        # å­å›¾2: è¿è§„ç±»å‹åˆ†å¸ƒ
        plt.subplot(1, 2, 2)
        type_counts = list(report_data['violations_by_type'].values())
        type_labels = list(report_data['violations_by_type'].keys())
        
        bars = plt.bar(range(len(type_counts)), type_counts, color=colors[:len(type_counts)])
        plt.xticks(range(len(type_labels)), type_labels, rotation=45, ha='right')
        plt.ylabel('è¿è§„æ•°é‡')
        plt.title('è¿è§„ç±»å‹åˆ†å¸ƒ')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, type_counts):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                    str(count), ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('compliance_dashboard.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def send_email_report(self, report_data, recipients):
        """å‘é€é‚®ä»¶æŠ¥å‘Š"""
        # åˆ›å»ºé‚®ä»¶
        msg = MIMEMultipart()
        msg['From'] = 'compliance@company.com'
        msg['To'] = ', '.join(recipients)
        msg['Subject'] = f'Commvault åˆè§„æ€§æŠ¥å‘Š - {datetime.now().strftime("%Y-%m-%d")}'
        
        # é‚®ä»¶æ­£æ–‡
        body = f"""
        <html>
        <body>
        <h2>Commvault åˆè§„æ€§æ£€æŸ¥æŠ¥å‘Š</h2>
        <p><strong>æŠ¥å‘Šç”Ÿæˆæ—¶é—´:</strong> {report_data['generated_at']}</p>
        <p><strong>æ€»è¿è§„æ•°é‡:</strong> {report_data['total_violations']}</p>
        
        <h3>è¿è§„ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡</h3>
        <ul>
        <li>ä¸¥é‡: {report_data['violations_by_severity']['critical']}</li>
        <li>é«˜: {report_data['violations_by_severity']['high']}</li>
        <li>ä¸­: {report_data['violations_by_severity']['medium']}</li>
        <li>ä½: {report_data['violations_by_severity']['low']}</li>
        </ul>
        
        <h3>ä¸»è¦è¿è§„ç±»å‹</h3>
        <ul>
        """
        
        for v_type, count in report_data['violations_by_type'].items():
            body += f"<li>{v_type}: {count}</li>"
        
        body += """
        </ul>
        <p>è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹é™„ä»¶ã€‚</p>
        <p>æ­¤é‚®ä»¶ç”±ç³»ç»Ÿè‡ªåŠ¨å‘é€ï¼Œè¯·å‹¿å›å¤ã€‚</p>
        </body>
        </html>
        """
        
        msg.attach(MIMEText(body, 'html'))
        
        # é™„åŠ è¯¦ç»†æŠ¥å‘Š
        report_json = json.dumps(report_data, indent=2, ensure_ascii=False)
        attachment = MIMEApplication(report_json.encode('utf-8'))
        attachment.add_header('Content-Disposition', 'attachment', filename='detailed_compliance_report.json')
        msg.attach(attachment)
        
        # å‘é€é‚®ä»¶
        try:
            server = smtplib.SMTP('smtp.company.com', 587)
            server.starttls()
            server.login('compliance@company.com', 'password')
            server.send_message(msg)
            server.quit()
            print("åˆè§„æ€§æŠ¥å‘Šé‚®ä»¶å‘é€æˆåŠŸ")
        except Exception as e:
            print(f"å‘é€é‚®ä»¶å¤±è´¥: {e}")

def main():
    # åˆå§‹åŒ–åˆè§„æ£€æŸ¥å™¨
    checker = ComplianceChecker('/opt/commvault/database/CommServ.db')
    
    # ç”ŸæˆæŠ¥å‘Š
    report = checker.generate_compliance_report()
    
    # åˆ›å»ºå¯è§†åŒ–
    checker.create_visual_dashboard(report)
    
    # å‘é€æŠ¥å‘Š
    recipients = ['compliance@company.com', 'audit@company.com', 'management@company.com']
    checker.send_email_report(report, recipients)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    with open('compliance_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print("åˆè§„æ€§æ£€æŸ¥å®Œæˆï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆ")

if __name__ == "__main__":
    main()
```

---
*æœ¬æ–‡æ¡£åŸºäºä¼ä¸šçº§Commvaultå®è·µç»éªŒç¼–å†™ï¼Œå¹¶æŒç»­æ›´æ–°æœ€æ–°çš„æŠ€æœ¯å’Œæœ€ä½³å®è·µã€‚*
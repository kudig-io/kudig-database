# 09 - Job ä¸ CronJob æ‰¹å¤„ç†äº‹ä»¶

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä½œè€…**: Allen Galler

> **æœ¬æ–‡æ¡£è¯¦ç»†è®°å½• Job å’Œ CronJob æ§åˆ¶å™¨äº§ç”Ÿçš„æ‰€æœ‰æ‰¹å¤„ç†ç›¸å…³äº‹ä»¶ã€‚**

---

## ğŸ“‹ ç›®å½•

- [äº‹ä»¶ç´¢å¼•è¡¨](#äº‹ä»¶ç´¢å¼•è¡¨)
- [Job æ§åˆ¶å™¨äº‹ä»¶](#job-æ§åˆ¶å™¨äº‹ä»¶)
- [CronJob æ§åˆ¶å™¨äº‹ä»¶](#cronjob-æ§åˆ¶å™¨äº‹ä»¶)
- [æ‰¹å¤„ç†æ‰§è¡Œç”Ÿå‘½å‘¨æœŸ](#æ‰¹å¤„ç†æ‰§è¡Œç”Ÿå‘½å‘¨æœŸ)
- [æ·±åº¦åˆ†æ](#æ·±åº¦åˆ†æ)
- [æ•…éšœæ’æŸ¥æ¨¡å¼](#æ•…éšœæ’æŸ¥æ¨¡å¼)
- [ç›¸å…³å‚è€ƒ](#ç›¸å…³å‚è€ƒ)

---

## äº‹ä»¶ç´¢å¼•è¡¨

### Job Controller Events

| Event Reason | Type | é¢‘ç‡ | èµ·å§‹ç‰ˆæœ¬ | æè¿° |
|--------------|------|------|----------|------|
| SuccessfulCreate | Normal | é«˜é¢‘ | v1.0+ | æˆåŠŸåˆ›å»º Pod |
| SuccessfulDelete | Normal | ä¸­é¢‘ | v1.0+ | æˆåŠŸåˆ é™¤ Pod |
| FailedCreate | Warning | ä¸­é¢‘ | v1.0+ | åˆ›å»º Pod å¤±è´¥ |
| Completed | Normal | é«˜é¢‘ | v1.0+ | Job å®Œæˆ |
| BackoffLimitExceeded | Warning | ä¸­é¢‘ | v1.0+ | è¾¾åˆ°é‡è¯•ä¸Šé™ |
| DeadlineExceeded | Warning | ä½é¢‘ | v1.2+ | è¶…è¿‡æ´»è·ƒæˆªæ­¢æ—¶é—´ |
| TooManyActivePods | Warning | ç½•è§ | v1.3+ | æ´»è·ƒ Pod è¿‡å¤š |
| TooManySucceededPods | Warning | ç½•è§ | v1.3+ | æˆåŠŸ Pod è¿‡å¤š |
| Suspended | Normal | ä½é¢‘ | v1.22+ | Job å·²æš‚åœ |
| Resumed | Normal | ä½é¢‘ | v1.22+ | Job å·²æ¢å¤ |
| FailedJob | Warning | ä½é¢‘ | v1.26+ | Indexed Job å¤±è´¥ |
| SuccessCriteriaMet | Normal | ä½é¢‘ | v1.28+ | æ»¡è¶³æˆåŠŸç­–ç•¥ |

### CronJob Controller Events

| Event Reason | Type | é¢‘ç‡ | èµ·å§‹ç‰ˆæœ¬ | æè¿° |
|--------------|------|------|----------|------|
| SuccessfulCreate | Normal | é«˜é¢‘ | v1.4+ | æˆåŠŸåˆ›å»º Job |
| SuccessfulDelete | Normal | ä¸­é¢‘ | v1.4+ | æˆåŠŸåˆ é™¤ Job |
| SawCompletedJob | Normal | é«˜é¢‘ | v1.4+ | å‘ç°å·²å®Œæˆ Job |
| UnexpectedJob | Warning | ç½•è§ | v1.4+ | å‘ç°æœªé¢„æœŸçš„ Job |
| MissingJob | Normal | ç½•è§ | v1.4+ | é¢„æœŸä½†æœªæ‰¾åˆ° Job |
| TooManyMissedTimes | Warning | ä½é¢‘ | v1.4+ | é”™è¿‡å¤ªå¤šæ‰§è¡Œæ—¶é—´ |
| FailedCreate | Warning | ä¸­é¢‘ | v1.4+ | åˆ›å»º Job å¤±è´¥ |
| ForbidConcurrent | Warning | ä½é¢‘ | v1.4+ | å¹¶å‘ç­–ç•¥ç¦æ­¢ |

---

## Job æ§åˆ¶å™¨äº‹ä»¶

### 1. SuccessfulCreate (Pod åˆ›å»ºæˆåŠŸ)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: SuccessfulCreate
Message: "Created pod: <pod-name>"
Source: job-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:00:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- Job æ§åˆ¶å™¨æˆåŠŸåˆ›å»ºæ–°çš„ Pod å‰¯æœ¬
- Job å¹¶è¡Œåº¦è¦æ±‚åˆ›å»ºå¤šä¸ª Pod
- Pod å¤±è´¥åé‡æ–°åˆ›å»ºï¼ˆåœ¨ backoffLimit å†…ï¼‰

**å­—æ®µè¯¦è§£:**
- `<pod-name>`: æ–°åˆ›å»ºçš„ Pod åç§°ï¼Œæ ¼å¼ `<job-name>-<random>`

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.0
- **æœ€åå˜æ›´**: v1.24ï¼ˆIndexed Jobs å¢å¼ºï¼‰

**ç”Ÿäº§å½±å“:**
- âœ… æ­£å¸¸æ‰§è¡Œæµç¨‹
- ğŸ“Š å¯ç”¨äºè¿½è¸ª Job æ‰§è¡Œè¿›åº¦
- ğŸ” é…åˆ `.spec.completions` ç›‘æ§å®Œæˆåº¦

**è§‚æµ‹ç¤ºä¾‹:**
```bash
# æŸ¥çœ‹ Job Pod åˆ›å»ºäº‹ä»¶
kubectl describe job batch-processor | grep SuccessfulCreate

# ç»Ÿè®¡å·²åˆ›å»º Pod æ•°é‡
kubectl get pods -l job-name=batch-processor --no-headers | wc -l
```

**å…³è”é…ç½®:**
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-processor
spec:
  completions: 5      # éœ€è¦æˆåŠŸå®Œæˆ 5 ä¸ª Pod
  parallelism: 2      # å¹¶è¡Œè¿è¡Œ 2 ä¸ª Pod
  backoffLimit: 3     # æœ€å¤šé‡è¯• 3 æ¬¡
  template:
    spec:
      containers:
      - name: worker
        image: busybox
        command: ["sh", "-c", "echo Processing && sleep 10"]
      restartPolicy: Never
```

---

### 2. SuccessfulDelete (Pod åˆ é™¤æˆåŠŸ)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: SuccessfulDelete
Message: "Deleted pod: <pod-name>"
Source: job-controller
First Seen: 2026-02-10T10:05:00Z
Last Seen: 2026-02-10T10:05:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- Job å®Œæˆåæ¸…ç† Podï¼ˆæ ¹æ® `ttlSecondsAfterFinished`ï¼‰
- Job è¢«åˆ é™¤æ—¶çº§è”åˆ é™¤ Pod
- æ‰‹åŠ¨åˆ é™¤ Job æ—¶æ¸…ç†

**å­—æ®µè¯¦è§£:**
- `<pod-name>`: è¢«åˆ é™¤çš„ Pod åç§°

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.0
- **æœ€åå˜æ›´**: v1.21ï¼ˆTTL æ¸…ç†å¢å¼ºï¼‰

**ç”Ÿäº§å½±å“:**
- âœ… èµ„æºè‡ªåŠ¨æ¸…ç†
- ğŸ—‘ï¸ é˜²æ­¢ Pod å †ç§¯
- âš ï¸ å¦‚æœé¢‘ç¹å‡ºç°å¯èƒ½æ˜¯ Job è¢«æ„å¤–åˆ é™¤

**è§‚æµ‹ç¤ºä¾‹:**
```bash
# æŸ¥çœ‹ Pod åˆ é™¤äº‹ä»¶
kubectl describe job batch-processor | grep SuccessfulDelete

# æ£€æŸ¥ TTL é…ç½®
kubectl get job batch-processor -o jsonpath='{.spec.ttlSecondsAfterFinished}'
```

**å…³è”é…ç½®:**
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-processor
spec:
  ttlSecondsAfterFinished: 300  # å®Œæˆå 5 åˆ†é’Ÿè‡ªåŠ¨åˆ é™¤
  template:
    spec:
      containers:
      - name: worker
        image: busybox
      restartPolicy: Never
```

---

### 3. FailedCreate (Pod åˆ›å»ºå¤±è´¥)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: FailedCreate
Message: "Error creating: pods \"<pod-name>\" is forbidden: exceeded quota: compute-resources"
Source: job-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:00:30Z
Count: 5
```

**è§¦å‘æ¡ä»¶:**
- ResourceQuota é™åˆ¶å¯¼è‡´æ— æ³•åˆ›å»º Pod
- Pod å®‰å…¨ç­–ç•¥é˜»æ­¢åˆ›å»º
- èŠ‚ç‚¹èµ„æºä¸è¶³ï¼ˆé—´æ¥åŸå› ï¼‰
- é•œåƒæ‹‰å–ç­–ç•¥é—®é¢˜

**å¸¸è§é”™è¯¯æ¶ˆæ¯:**
```
# é…é¢è¶…é™
Error creating: pods "xxx" is forbidden: exceeded quota: compute-resources

# å®‰å…¨ç­–ç•¥é˜»æ­¢
Error creating: pods "xxx" is forbidden: violates PodSecurity "restricted:latest"

# æœåŠ¡è´¦å·é—®é¢˜
Error creating: pods "xxx" is forbidden: error looking up service account default/job-sa

# èŠ‚ç‚¹é€‰æ‹©å™¨é—®é¢˜
Error creating: No nodes are available that match all of the following predicates
```

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.0
- **æœ€åå˜æ›´**: v1.25ï¼ˆPSS é”™è¯¯ä¿¡æ¯æ”¹è¿›ï¼‰

**ç”Ÿäº§å½±å“:**
- â›” **ä¸¥é‡**: Job æ— æ³•æ‰§è¡Œ
- ğŸš¨ éœ€è¦ç«‹å³ä»‹å…¥
- ğŸ“ˆ å¯èƒ½å¯¼è‡´ CronJob ç§¯å‹

**æ•…éšœæ’æŸ¥:**
```bash
# 1. æ£€æŸ¥è¯¦ç»†é”™è¯¯
kubectl describe job <job-name>

# 2. æ£€æŸ¥ ResourceQuota
kubectl get resourcequota -A
kubectl describe resourcequota <quota-name> -n <namespace>

# 3. æ£€æŸ¥ Pod å®‰å…¨ç­–ç•¥
kubectl get psp
kubectl auth can-i use podsecuritypolicies/<psp-name> --as=system:serviceaccount:<namespace>:<sa>

# 4. æ£€æŸ¥æœåŠ¡è´¦å·
kubectl get sa <sa-name> -n <namespace>

# 5. æ¨¡æ‹Ÿ Pod åˆ›å»º
kubectl run test-pod --image=busybox --dry-run=server
```

**ä¿®å¤æ–¹æ¡ˆ:**
```yaml
# æ–¹æ¡ˆ 1: è°ƒæ•´ ResourceQuota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
spec:
  hard:
    requests.cpu: "100"      # å¢åŠ é…é¢
    requests.memory: "200Gi"

# æ–¹æ¡ˆ 2: é™ä½ Job èµ„æºè¯·æ±‚
apiVersion: batch/v1
kind: Job
spec:
  template:
    spec:
      containers:
      - name: worker
        resources:
          requests:
            cpu: "100m"      # é™ä½èµ„æºè¯·æ±‚
            memory: "128Mi"

# æ–¹æ¡ˆ 3: è°ƒæ•´ Pod å®‰å…¨ä¸Šä¸‹æ–‡
apiVersion: batch/v1
kind: Job
spec:
  template:
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
```

---

### 4. Completed (Job å®Œæˆ)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: Completed
Message: "Job completed"
Source: job-controller
First Seen: 2026-02-10T10:10:00Z
Last Seen: 2026-02-10T10:10:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- æˆåŠŸå®Œæˆçš„ Pod æ•°é‡è¾¾åˆ° `.spec.completions`
- æ‰€æœ‰å¿…éœ€çš„ Pod æˆåŠŸæ‰§è¡Œ
- æ»¡è¶³ `successPolicy` æ¡ä»¶ï¼ˆv1.28+ï¼‰

**å­—æ®µè¯¦è§£:**
- æ— é™„åŠ å­—æ®µï¼Œç®€å•å®Œæˆé€šçŸ¥

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.0
- **æœ€åå˜æ›´**: v1.28ï¼ˆsuccessPolicy æ”¯æŒï¼‰

**ç”Ÿäº§å½±å“:**
- âœ… Job æˆåŠŸå®Œæˆ
- ğŸ“Š å¯ç”¨äºç›‘æ§å’Œå‘Šè­¦
- ğŸ”„ è§¦å‘åç»­æµç¨‹ï¼ˆå¦‚ CronJob ä¸‹æ¬¡è°ƒåº¦ï¼‰

**è§‚æµ‹ç¤ºä¾‹:**
```bash
# æŸ¥çœ‹ Job å®ŒæˆçŠ¶æ€
kubectl get job batch-processor -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}'

# æŸ¥çœ‹å®Œæˆæ—¶é—´
kubectl get job batch-processor -o jsonpath='{.status.completionTime}'

# æŸ¥çœ‹æˆåŠŸ Pod æ•°é‡
kubectl get job batch-processor -o jsonpath='{.status.succeeded}'
```

**çŠ¶æ€æ£€æŸ¥:**
```yaml
# Job å®Œæˆåçš„çŠ¶æ€
status:
  conditions:
  - type: Complete
    status: "True"
    lastProbeTime: 2026-02-10T10:10:00Z
    lastTransitionTime: 2026-02-10T10:10:00Z
  succeeded: 5              # æˆåŠŸ Pod æ•°é‡
  completionTime: 2026-02-10T10:10:00Z
  startTime: 2026-02-10T10:00:00Z
```

---

### 5. BackoffLimitExceeded (é‡è¯•ä¸Šé™å·²è¾¾)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: BackoffLimitExceeded
Message: "Job has reached the specified backoff limit"
Source: job-controller
First Seen: 2026-02-10T10:15:00Z
Last Seen: 2026-02-10T10:15:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- Pod å¤±è´¥æ¬¡æ•°è¾¾åˆ° `.spec.backoffLimit`ï¼ˆé»˜è®¤ 6ï¼‰
- é‡è¯•é—´éš”é‡‡ç”¨æŒ‡æ•°é€€é¿ç®—æ³•
- Job æœ€ç»ˆæ ‡è®°ä¸ºå¤±è´¥

**å­—æ®µè¯¦è§£:**
- æ— é™„åŠ å­—æ®µï¼Œè¡¨ç¤ºé‡è¯•è€—å°½

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.0
- **æœ€åå˜æ›´**: v1.26ï¼ˆPod Failure Policy æ”¯æŒï¼‰

**ç”Ÿäº§å½±å“:**
- â›” **ä¸¥é‡**: Job æ°¸ä¹…å¤±è´¥
- ğŸš¨ éœ€è¦äººå·¥ä»‹å…¥åˆ†æåŸå› 
- ğŸ“Š å½±å“ CronJob ä¸‹æ¬¡æ‰§è¡Œ

**é‡è¯•æœºåˆ¶è¯´æ˜:**
```
Pod å¤±è´¥æ¬¡æ•°ä¸é€€é¿æ—¶é—´:
  å¤±è´¥ 1 æ¬¡: 10s åé‡è¯•
  å¤±è´¥ 2 æ¬¡: 20s åé‡è¯•
  å¤±è´¥ 3 æ¬¡: 40s åé‡è¯•
  å¤±è´¥ 4 æ¬¡: 80s åé‡è¯•
  å¤±è´¥ 5 æ¬¡: 160s åé‡è¯•
  å¤±è´¥ 6 æ¬¡: è¾¾åˆ° backoffLimitï¼ŒJob å¤±è´¥

æœ€å¤§é€€é¿æ—¶é—´: 6 åˆ†é’Ÿ
```

**è§‚æµ‹ç¤ºä¾‹:**
```bash
# æŸ¥çœ‹å¤±è´¥åŸå› 
kubectl describe job batch-processor

# æŸ¥çœ‹å¤±è´¥ Pod æ—¥å¿—
kubectl logs -l job-name=batch-processor --tail=100

# æŸ¥çœ‹å¤±è´¥ Pod çŠ¶æ€
kubectl get pods -l job-name=batch-processor -o wide

# æŸ¥çœ‹é‡è¯•æ¬¡æ•°
kubectl get job batch-processor -o jsonpath='{.status.failed}'
```

**æ·±åº¦åˆ†æ:**
```bash
# åˆ†ææ‰€æœ‰å¤±è´¥ Pod çš„é€€å‡ºç 
kubectl get pods -l job-name=batch-processor -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.containerStatuses[0].state.terminated.exitCode}{"\n"}{end}'

# æŸ¥çœ‹ Pod å¤±è´¥æ—¶é—´çº¿
kubectl get events --field-selector involvedObject.kind=Pod --sort-by='.lastTimestamp' | grep batch-processor
```

**æ•…éšœæ’æŸ¥æ¨¡å¼:**
```yaml
# å¸¸è§å¤±è´¥åŸå› ä¸è§£å†³æ–¹æ¡ˆ

# 1. é€€å‡ºç  1 - åº”ç”¨ç¨‹åºé”™è¯¯
status:
  containerStatuses:
  - state:
      terminated:
        exitCode: 1
        reason: Error
# è§£å†³: æ£€æŸ¥åº”ç”¨æ—¥å¿—ï¼Œä¿®å¤ä»£ç é€»è¾‘

# 2. é€€å‡ºç  137 - å†…å­˜ OOM
status:
  containerStatuses:
  - state:
      terminated:
        exitCode: 137
        reason: OOMKilled
# è§£å†³: å¢åŠ å†…å­˜é™åˆ¶æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨

# 3. é€€å‡ºç  143 - SIGTERM ç»ˆæ­¢
status:
  containerStatuses:
  - state:
      terminated:
        exitCode: 143
        reason: Error
# è§£å†³: æ£€æŸ¥ activeDeadlineSeconds é…ç½®
```

**é…ç½®ä¼˜åŒ–:**
```yaml
# æ–¹æ¡ˆ 1: å¢åŠ é‡è¯•æ¬¡æ•°
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-processor
spec:
  backoffLimit: 10          # å…è®¸ 10 æ¬¡é‡è¯•
  template:
    spec:
      containers:
      - name: worker
        image: myapp:v1

# æ–¹æ¡ˆ 2: ä½¿ç”¨ Pod Failure Policy (v1.26+)
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-processor
spec:
  backoffLimit: 6
  podFailurePolicy:
    rules:
    - action: FailJob      # ç«‹å³å¤±è´¥ï¼Œä¸é‡è¯•
      onExitCodes:
        containerName: worker
        operator: In
        values: [42]       # ä¸šåŠ¡é€»è¾‘é”™è¯¯
    - action: Ignore       # å¿½ç•¥æ­¤ç±»å¤±è´¥ï¼Œä¸è®¡å…¥ backoffLimit
      onExitCodes:
        containerName: worker
        operator: In
        values: [2]        # ä¸´æ—¶ç½‘ç»œé”™è¯¯
    - action: Count        # è®¡å…¥ backoffLimit ä½†ç»§ç»­é‡è¯•
      onPodConditions:
      - type: DisruptionTarget
  template:
    spec:
      containers:
      - name: worker
        image: myapp:v1

# æ–¹æ¡ˆ 3: ç»“åˆ activeDeadlineSeconds
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-processor
spec:
  backoffLimit: 6
  activeDeadlineSeconds: 1800  # 30 åˆ†é’Ÿæ€»è¶…æ—¶
  template:
    spec:
      containers:
      - name: worker
        image: myapp:v1
```

---

### 6. DeadlineExceeded (æ´»è·ƒæˆªæ­¢æ—¶é—´è¶…æ—¶)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: DeadlineExceeded
Message: "Job was active longer than specified deadline"
Source: job-controller
First Seen: 2026-02-10T10:30:00Z
Last Seen: 2026-02-10T10:30:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- Job è¿è¡Œæ—¶é—´è¶…è¿‡ `.spec.activeDeadlineSeconds`
- ä» Job å¼€å§‹æ‰§è¡Œåˆ°è¶…æ—¶æ—¶é—´åˆ°è¾¾
- æ‰€æœ‰è¿è¡Œä¸­çš„ Pod å°†è¢«ç»ˆæ­¢

**å­—æ®µè¯¦è§£:**
- æ— é™„åŠ å­—æ®µï¼Œè¡¨ç¤ºæ€»è¶…æ—¶

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.2
- **æœ€åå˜æ›´**: v1.21ï¼ˆæ¸…ç†å¢å¼ºï¼‰

**ç”Ÿäº§å½±å“:**
- â›” **ä¸¥é‡**: Job è¢«å¼ºåˆ¶ç»ˆæ­¢
- ğŸš¨ å¯èƒ½å¯¼è‡´æ•°æ®ä¸ä¸€è‡´
- â±ï¸ éœ€è¦è¯„ä¼°åˆç†çš„è¶…æ—¶æ—¶é—´

**è§‚æµ‹ç¤ºä¾‹:**
```bash
# æŸ¥çœ‹ Job è¿è¡Œæ—¶é•¿
kubectl get job batch-processor -o jsonpath='{.status.startTime}'
kubectl get job batch-processor -o jsonpath='{.status.completionTime}'

# æŸ¥çœ‹è¶…æ—¶é…ç½®
kubectl get job batch-processor -o jsonpath='{.spec.activeDeadlineSeconds}'
```

**é…ç½®å»ºè®®:**
```yaml
# ç¤ºä¾‹: æ‰¹é‡æ•°æ®å¤„ç† Job
apiVersion: batch/v1
kind: Job
metadata:
  name: data-import
spec:
  activeDeadlineSeconds: 3600  # 1 å°æ—¶è¶…æ—¶
  backoffLimit: 3
  template:
    spec:
      containers:
      - name: importer
        image: data-importer:v1
        resources:
          limits:
            cpu: "2"
            memory: "4Gi"
      restartPolicy: Never

# è®¡ç®—å…¬å¼:
# activeDeadlineSeconds = (å•æ¬¡æ‰§è¡Œæ—¶é—´ Ã— completions Ã— å®‰å…¨ç³»æ•°) + å¯åŠ¨å¼€é”€
# ç¤ºä¾‹: (600s Ã— 5 Ã— 1.5) + 300s = 4800s
```

---

### 7. TooManyActivePods (æ´»è·ƒ Pod è¿‡å¤š)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: TooManyActivePods
Message: "too many active pods running for the job"
Source: job-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:05:00Z
Count: 10
```

**è§¦å‘æ¡ä»¶:**
- æ´»è·ƒ Pod æ•°é‡è¶…è¿‡ `.spec.parallelism`
- æ§åˆ¶å™¨å¼‚å¸¸å¯¼è‡´ Pod åˆ›å»ºå¤±æ§
- å¤–éƒ¨ç›´æ¥åˆ›å»ºäº†ç›¸åŒ label çš„ Pod

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.3
- **è§¦å‘åœºæ™¯**: ç½•è§ï¼ˆé€šå¸¸æ˜¯ Bugï¼‰

**ç”Ÿäº§å½±å“:**
- âš ï¸ **ä¸­ç­‰**: å¯èƒ½å¯¼è‡´èµ„æºæµªè´¹
- ğŸ” éœ€è¦æ£€æŸ¥æ§åˆ¶å™¨å¥åº·çŠ¶æ€
- ğŸ› å¯èƒ½æ˜¯ç³»ç»Ÿ Bug

**æ•…éšœæ’æŸ¥:**
```bash
# æ£€æŸ¥æ´»è·ƒ Pod æ•°é‡
kubectl get pods -l job-name=batch-processor --field-selector=status.phase=Running --no-headers | wc -l

# æ£€æŸ¥ parallelism é…ç½®
kubectl get job batch-processor -o jsonpath='{.spec.parallelism}'

# æ£€æŸ¥ Job Controller æ—¥å¿—
kubectl logs -n kube-system -l component=kube-controller-manager --tail=200 | grep job-controller
```

---

### 8. TooManySucceededPods (æˆåŠŸ Pod è¿‡å¤š)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: TooManySucceededPods
Message: "too many succeeded pods running for the job"
Source: job-controller
First Seen: 2026-02-10T10:10:00Z
Last Seen: 2026-02-10T10:10:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- æˆåŠŸ Pod æ•°é‡è¶…è¿‡ `.spec.completions`
- å¤–éƒ¨æ‰‹åŠ¨åˆ›å»ºäº†é¢å¤–çš„ Pod å¹¶æˆåŠŸå®Œæˆ
- æ§åˆ¶å™¨çŠ¶æ€åŒæ­¥é—®é¢˜

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.3
- **è§¦å‘åœºæ™¯**: ç½•è§ï¼ˆå¼‚å¸¸æƒ…å†µï¼‰

**ç”Ÿäº§å½±å“:**
- âš ï¸ **ä½**: é€šå¸¸ä¸å½±å“åŠŸèƒ½
- ğŸ” è¡¨ç¤ºå¯èƒ½å­˜åœ¨åŒæ­¥é—®é¢˜
- ğŸ—‘ï¸ å¯èƒ½å¯¼è‡´èµ„æºæ¸…ç†å»¶è¿Ÿ

---

### 9. Suspended (Job å·²æš‚åœ)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: Suspended
Message: "Job suspended"
Source: job-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:00:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- `.spec.suspend` è®¾ç½®ä¸º `true`
- æ‰€æœ‰æ´»è·ƒ Pod è¢«åˆ é™¤
- Job ä¿æŒæš‚åœçŠ¶æ€ç›´åˆ°æ¢å¤

**å­—æ®µè¯¦è§£:**
- æ— é™„åŠ å­—æ®µï¼Œç®€å•æš‚åœé€šçŸ¥

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.22 (Beta)
- **GA ç‰ˆæœ¬**: v1.24

**ç”Ÿäº§å½±å“:**
- âœ… æ­£å¸¸æ“ä½œæµç¨‹
- â¸ï¸ ç”¨äºä¸´æ—¶åœæ­¢æ‰§è¡Œ
- ğŸ’¾ ä¿ç•™ Job çŠ¶æ€å’Œé…ç½®

**ä½¿ç”¨åœºæ™¯:**
```bash
# 1. æš‚åœ Job
kubectl patch job batch-processor -p '{"spec":{"suspend":true}}'

# 2. æ£€æŸ¥æš‚åœçŠ¶æ€
kubectl get job batch-processor -o jsonpath='{.spec.suspend}'

# 3. æŸ¥çœ‹æ´»è·ƒ Podï¼ˆåº”ä¸º 0ï¼‰
kubectl get pods -l job-name=batch-processor --field-selector=status.phase=Running
```

**åº”ç”¨åœºæ™¯:**
```yaml
# åœºæ™¯ 1: ç»´æŠ¤çª—å£æœŸæš‚åœæ‰¹å¤„ç†
apiVersion: batch/v1
kind: Job
metadata:
  name: data-sync
spec:
  suspend: true          # åˆ›å»ºæ—¶å³æš‚åœ
  completions: 100
  parallelism: 10
  template:
    spec:
      containers:
      - name: syncer
        image: data-sync:v1

# åœºæ™¯ 2: åŠ¨æ€æµé‡æ§åˆ¶
# é«˜å³°æœŸæš‚åœéå…³é”® Job
kubectl patch job non-critical-batch -p '{"spec":{"suspend":true}}'

# ä½å³°æœŸæ¢å¤
kubectl patch job non-critical-batch -p '{"spec":{"suspend":false}}'
```

---

### 10. Resumed (Job å·²æ¢å¤)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: Resumed
Message: "Job resumed"
Source: job-controller
First Seen: 2026-02-10T10:05:00Z
Last Seen: 2026-02-10T10:05:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- `.spec.suspend` ä» `true` å˜ä¸º `false`
- Job æ§åˆ¶å™¨å¼€å§‹é‡æ–°åˆ›å»º Pod
- ä»ä¸Šæ¬¡æš‚åœä½ç½®ç»§ç»­æ‰§è¡Œ

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.22 (Beta)
- **GA ç‰ˆæœ¬**: v1.24

**ç”Ÿäº§å½±å“:**
- âœ… æ­£å¸¸æ¢å¤æµç¨‹
- â–¶ï¸ Job ç»§ç»­æ‰§è¡Œ
- ğŸ“Š å¯é…åˆç›‘æ§å’Œè‡ªåŠ¨åŒ–

**è§‚æµ‹ç¤ºä¾‹:**
```bash
# æ¢å¤ Job
kubectl patch job batch-processor -p '{"spec":{"suspend":false}}'

# æŸ¥çœ‹æ¢å¤å Pod åˆ›å»º
kubectl get events --field-selector involvedObject.name=batch-processor --sort-by='.lastTimestamp'
```

---

### 11. FailedJob (Indexed Job å¤±è´¥)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: FailedJob
Message: "Job failed: index X failed"
Source: job-controller
First Seen: 2026-02-10T10:20:00Z
Last Seen: 2026-02-10T10:20:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- Indexed Job ä¸­æŸä¸ªç´¢å¼•çš„ Pod å¤±è´¥
- é…åˆ `podFailurePolicy` ä½¿ç”¨
- ç´¢å¼•ä»»åŠ¡ä¸å¯æ¢å¤å¤±è´¥

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.26
- **ç‰¹æ€§**: Indexed Jobs + Pod Failure Policy

**Indexed Jobs è¯´æ˜:**
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: indexed-processor
spec:
  completions: 10           # 10 ä¸ªç´¢å¼•ä»»åŠ¡ (0-9)
  parallelism: 3            # å¹¶è¡Œ 3 ä¸ª
  completionMode: Indexed   # ç´¢å¼•æ¨¡å¼
  template:
    spec:
      containers:
      - name: worker
        image: processor:v1
        env:
        - name: JOB_COMPLETION_INDEX  # è‡ªåŠ¨æ³¨å…¥ç´¢å¼• (0-9)
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
        command:
        - sh
        - -c
        - |
          echo "Processing index: $JOB_COMPLETION_INDEX"
          # æ ¹æ®ç´¢å¼•å¤„ç†ä¸åŒæ•°æ®åˆ†ç‰‡
      restartPolicy: Never
```

**åº”ç”¨åœºæ™¯:**
- å¤§è§„æ¨¡æ•°æ®åˆ†ç‰‡å¤„ç†
- å‚æ•°åŒ–æ‰¹é‡ä»»åŠ¡
- MapReduce é£æ ¼ä½œä¸š

---

### 12. SuccessCriteriaMet (æ»¡è¶³æˆåŠŸç­–ç•¥)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: SuccessCriteriaMet
Message: "Pods satisfied success criteria"
Source: job-controller
First Seen: 2026-02-10T10:15:00Z
Last Seen: 2026-02-10T10:15:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- æ»¡è¶³è‡ªå®šä¹‰ `successPolicy` æ¡ä»¶
- è¾¾åˆ°æˆåŠŸé˜ˆå€¼å³å¯å®Œæˆ Job
- æ— éœ€ç­‰å¾…æ‰€æœ‰ Pod æˆåŠŸ

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.28 (Alpha)
- **ç‰¹æ€§é—¨æ§**: `JobSuccessPolicy`

**successPolicy åŠŸèƒ½:**
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training
spec:
  completions: 10           # æ€»å…± 10 ä¸ªè®­ç»ƒå‰¯æœ¬
  parallelism: 10
  successPolicy:
    rules:
    - succeededIndexes: "0-2"      # ç´¢å¼• 0ã€1ã€2 æˆåŠŸå³å¯
      succeededCount: 3             # æˆ–ä»»æ„ 3 ä¸ªæˆåŠŸ
  completionMode: Indexed
  template:
    spec:
      containers:
      - name: trainer
        image: ml-trainer:v1
```

**ä½¿ç”¨åœºæ™¯:**
- æœºå™¨å­¦ä¹ åˆ†å¸ƒå¼è®­ç»ƒï¼ˆéƒ¨åˆ†æˆåŠŸå³å¯ï¼‰
- å†—ä½™è®¡ç®—ä»»åŠ¡
- é‡‡æ ·å¼æ‰¹å¤„ç†

---

## CronJob æ§åˆ¶å™¨äº‹ä»¶

### 13. SuccessfulCreate (Job åˆ›å»ºæˆåŠŸ)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: SuccessfulCreate
Message: "Created job <job-name>"
Source: cronjob-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:00:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- CronJob æŒ‰ schedule è§¦å‘æ–°çš„ Job
- è°ƒåº¦æ—¶é—´åˆ°è¾¾ä¸”æ»¡è¶³æ‰§è¡Œæ¡ä»¶
- å¹¶å‘ç­–ç•¥å…è®¸åˆ›å»ºæ–° Job

**å­—æ®µè¯¦è§£:**
- `<job-name>`: æ–°åˆ›å»ºçš„ Job åç§°ï¼Œæ ¼å¼ `<cronjob-name>-<timestamp>`

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.4 (batch/v1beta1)
- **GA ç‰ˆæœ¬**: v1.21 (batch/v1)

**ç”Ÿäº§å½±å“:**
- âœ… æ­£å¸¸è°ƒåº¦æµç¨‹
- ğŸ“… å¯è¿½è¸ªæ‰§è¡Œå†å²
- ğŸ” ç”¨äºå®¡è®¡å’Œç›‘æ§

**è§‚æµ‹ç¤ºä¾‹:**
```bash
# æŸ¥çœ‹ CronJob æœ€è¿‘åˆ›å»ºçš„ Job
kubectl get jobs -l cronjob-name=hourly-backup --sort-by=.metadata.creationTimestamp

# æŸ¥çœ‹è°ƒåº¦å†å²
kubectl describe cronjob hourly-backup | grep "Last Schedule Time"

# æŸ¥çœ‹æ´»è·ƒ Job
kubectl get cronjob hourly-backup -o jsonpath='{.status.active}'
```

---

### 14. SuccessfulDelete (Job åˆ é™¤æˆåŠŸ)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: SuccessfulDelete
Message: "Deleted job <job-name>"
Source: cronjob-controller
First Seen: 2026-02-10T11:00:00Z
Last Seen: 2026-02-10T11:00:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- è¶…è¿‡ `.spec.successfulJobsHistoryLimit` ä¿ç•™æ•°é‡
- è¶…è¿‡ `.spec.failedJobsHistoryLimit` ä¿ç•™æ•°é‡
- CronJob è‡ªåŠ¨æ¸…ç†å†å² Job

**å­—æ®µè¯¦è§£:**
- `<job-name>`: è¢«åˆ é™¤çš„å†å² Job åç§°

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.4
- **é»˜è®¤å€¼**: `successfulJobsHistoryLimit: 3`, `failedJobsHistoryLimit: 1`

**ç”Ÿäº§å½±å“:**
- âœ… è‡ªåŠ¨èµ„æºæ¸…ç†
- ğŸ—‘ï¸ é˜²æ­¢ Job å †ç§¯
- ğŸ“Š ä¿ç•™é€‚é‡å†å²ç”¨äºå®¡è®¡

**é…ç½®å»ºè®®:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hourly-backup
spec:
  schedule: "0 * * * *"
  successfulJobsHistoryLimit: 5   # ä¿ç•™æœ€è¿‘ 5 æ¬¡æˆåŠŸ Job
  failedJobsHistoryLimit: 3       # ä¿ç•™æœ€è¿‘ 3 æ¬¡å¤±è´¥ Job
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: backup-tool:v1
          restartPolicy: OnFailure
```

---

### 15. SawCompletedJob (å‘ç°å·²å®Œæˆ Job)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: SawCompletedJob
Message: "Saw completed job: <job-name>, status: Complete"
Source: cronjob-controller
First Seen: 2026-02-10T10:30:00Z
Last Seen: 2026-02-10T10:30:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- CronJob æ§åˆ¶å™¨æ£€æµ‹åˆ° Job å®Œæˆ
- ç”¨äºæ›´æ–° `.status.lastSuccessfulTime`
- è§¦å‘å†å²æ¸…ç†é€»è¾‘

**å­—æ®µè¯¦è§£:**
- `<job-name>`: å·²å®Œæˆçš„ Job åç§°
- `status`: Completeï¼ˆæˆåŠŸï¼‰æˆ– Failedï¼ˆå¤±è´¥ï¼‰

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.4

**ç”Ÿäº§å½±å“:**
- âœ… æ­£å¸¸çŠ¶æ€åŒæ­¥
- ğŸ“Š æ›´æ–°æ‰§è¡Œç»Ÿè®¡
- ğŸ”„ å‡†å¤‡ä¸‹æ¬¡è°ƒåº¦

---

### 16. UnexpectedJob (å‘ç°æœªé¢„æœŸ Job)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: UnexpectedJob
Message: "Saw unexpected active job: <job-name>"
Source: cronjob-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:00:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- å‘ç°ä¸åœ¨ CronJob ç®¡ç†åˆ—è¡¨ä¸­çš„æ´»è·ƒ Job
- Job çš„ label åŒ¹é…ä½† ownerReference ä¸åŒ¹é…
- æ‰‹åŠ¨åˆ›å»ºäº†åŒå Job

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.4
- **è§¦å‘åœºæ™¯**: ç½•è§ï¼ˆå¼‚å¸¸æƒ…å†µï¼‰

**ç”Ÿäº§å½±å“:**
- âš ï¸ **ä¸­ç­‰**: è¡¨ç¤ºçŠ¶æ€ä¸ä¸€è‡´
- ğŸ” éœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰æ‰‹åŠ¨æ“ä½œ
- ğŸ› å¯èƒ½æ˜¯æ§åˆ¶å™¨åŒæ­¥é—®é¢˜

**æ•…éšœæ’æŸ¥:**
```bash
# æ£€æŸ¥æ‰€æœ‰ç›¸å…³ Job
kubectl get jobs -l cronjob-name=hourly-backup

# æ£€æŸ¥ Job çš„ ownerReference
kubectl get job <job-name> -o jsonpath='{.metadata.ownerReferences}'

# æ£€æŸ¥ CronJob çŠ¶æ€
kubectl get cronjob hourly-backup -o yaml
```

---

### 17. MissingJob (é¢„æœŸ Job æœªæ‰¾åˆ°)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Normal
Reason: MissingJob
Message: "Expected but did not find job: <expected-job-name>"
Source: cronjob-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:00:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- CronJob çŠ¶æ€ä¸­è®°å½•äº†æŸä¸ª Job ä½†å®é™…ä¸å­˜åœ¨
- Job è¢«å¤–éƒ¨åˆ é™¤
- æ§åˆ¶å™¨æ¢å¤åçŠ¶æ€åŒæ­¥

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.4
- **è§¦å‘åœºæ™¯**: ç½•è§

**ç”Ÿäº§å½±å“:**
- â„¹ï¸ **ä½**: æ§åˆ¶å™¨è‡ªåŠ¨ä¿®æ­£çŠ¶æ€
- ğŸ”„ é€šå¸¸è‡ªåŠ¨æ¢å¤
- ğŸ“ è®°å½•å¼‚å¸¸åˆ é™¤æ“ä½œ

---

### 18. TooManyMissedTimes (é”™è¿‡å¤ªå¤šæ‰§è¡Œæ—¶é—´)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: TooManyMissedTimes
Message: "Too many missed start times (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew."
Source: cronjob-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:00:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- CronJob é•¿æ—¶é—´æœªè¢«è°ƒåº¦ï¼ˆå¦‚ Controller å®•æœºï¼‰
- æ¢å¤åå‘ç°é”™è¿‡è¶…è¿‡ 100 æ¬¡è°ƒåº¦æ—¶é—´
- é¿å…åˆ›å»ºå¤§é‡å†å² Job

**å­—æ®µè¯¦è§£:**
- é”™è¿‡æ¬¡æ•°é˜ˆå€¼: 100 æ¬¡

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.4
- **æœ€åå˜æ›´**: v1.21ï¼ˆé”™è¯¯ä¿¡æ¯æ”¹è¿›ï¼‰

**ç”Ÿäº§å½±å“:**
- âš ï¸ **é«˜**: è¡¨ç¤ºè°ƒåº¦ä¸­æ–­
- ğŸš¨ éœ€è¦æ£€æŸ¥æ§åˆ¶å™¨å¥åº·çŠ¶æ€
- â° å¯èƒ½é”™è¿‡é‡è¦ä»»åŠ¡

**æ•…éšœæ’æŸ¥:**
```bash
# 1. æ£€æŸ¥ CronJob æœ€åè°ƒåº¦æ—¶é—´
kubectl get cronjob hourly-backup -o jsonpath='{.status.lastScheduleTime}'

# 2. æ£€æŸ¥æ§åˆ¶å™¨æ—¥å¿—
kubectl logs -n kube-system -l component=kube-controller-manager | grep cronjob

# 3. æ£€æŸ¥æ—¶é’ŸåŒæ­¥
date
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.conditions[?(@.type=="Ready")].lastHeartbeatTime}{"\n"}{end}'

# 4. æ£€æŸ¥ startingDeadlineSeconds
kubectl get cronjob hourly-backup -o jsonpath='{.spec.startingDeadlineSeconds}'
```

**é…ç½®ä¼˜åŒ–:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: critical-backup
spec:
  schedule: "*/5 * * * *"         # æ¯ 5 åˆ†é’Ÿ
  startingDeadlineSeconds: 300    # 5 åˆ†é’Ÿå†…å¿…é¡»å¯åŠ¨
  concurrencyPolicy: Replace      # æ›¿æ¢æ—§çš„æœªå®Œæˆ Job
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      activeDeadlineSeconds: 600  # å•æ¬¡ Job 10 åˆ†é’Ÿè¶…æ—¶
      template:
        spec:
          containers:
          - name: backup
            image: backup:v1
          restartPolicy: OnFailure

# startingDeadlineSeconds è®¡ç®—:
# åº”ç•¥å¤§äº schedule é—´éš”ï¼Œä»¥å®¹å¿çŸ­æš‚å»¶è¿Ÿ
# ç¤ºä¾‹: schedule é—´éš” 5 åˆ†é’Ÿï¼Œè®¾ç½® 300 ç§’ï¼ˆ5 åˆ†é’Ÿï¼‰æˆ– 600 ç§’ï¼ˆ10 åˆ†é’Ÿï¼‰
```

---

### 19. FailedCreate (Job åˆ›å»ºå¤±è´¥)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: FailedCreate
Message: "Error creating job: Job.batch \"xxx\" is invalid: spec.template.spec.restartPolicy: Invalid value: \"Always\": Unsupported value"
Source: cronjob-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:05:00Z
Count: 5
```

**è§¦å‘æ¡ä»¶:**
- Job æ¨¡æ¿é…ç½®é”™è¯¯
- ResourceQuota é™åˆ¶
- RBAC æƒé™ä¸è¶³
- éªŒè¯ webhook æ‹’ç»

**å¸¸è§é”™è¯¯æ¶ˆæ¯:**
```
# é…ç½®é”™è¯¯
Error creating job: Job.batch "xxx" is invalid: spec.template.spec.restartPolicy: Invalid value: "Always"

# é…é¢è¶…é™
Error creating job: forbidden: exceeded quota: compute-resources

# æƒé™ä¸è¶³
Error creating job: jobs.batch is forbidden: User "system:serviceaccount:default:cronjob-controller" cannot create resource "jobs"

# Webhook æ‹’ç»
Error creating job: admission webhook "validate.job" denied the request: invalid configuration
```

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.4

**ç”Ÿäº§å½±å“:**
- â›” **ä¸¥é‡**: CronJob æ— æ³•æ‰§è¡Œ
- ğŸš¨ éœ€è¦ç«‹å³ä¿®å¤é…ç½®
- ğŸ“Š æŒç»­å¤±è´¥ä¼šç§¯å‹è°ƒåº¦

**æ•…éšœæ’æŸ¥:**
```bash
# 1. éªŒè¯ Job æ¨¡æ¿
kubectl create job test-job --from=cronjob/hourly-backup --dry-run=server

# 2. æ£€æŸ¥ RBAC æƒé™
kubectl auth can-i create jobs --as=system:serviceaccount:default:cronjob-controller

# 3. æ£€æŸ¥ ResourceQuota
kubectl describe resourcequota -A

# 4. æ£€æŸ¥ ValidatingWebhookConfiguration
kubectl get validatingwebhookconfiguration
```

**ä¿®å¤æ–¹æ¡ˆ:**
```yaml
# å¸¸è§é”™è¯¯ä¿®å¤

# é”™è¯¯ 1: restartPolicy å¿…é¡»æ˜¯ OnFailure æˆ– Never
apiVersion: batch/v1
kind: CronJob
spec:
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure  # ä¸èƒ½æ˜¯ Always

# é”™è¯¯ 2: æˆäºˆ RBAC æƒé™
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cronjob-executor
rules:
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["create", "get", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cronjob-executor-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cronjob-executor
subjects:
- kind: ServiceAccount
  name: cronjob-sa
  namespace: default
```

---

### 20. ForbidConcurrent (å¹¶å‘ç­–ç•¥ç¦æ­¢)

**äº‹ä»¶æ¨¡æ¿:**
```yaml
Type: Warning
Reason: ForbidConcurrent
Message: "Cannot create job: too many jobs running (xxx) for the CronJob, concurrencyPolicy is Forbid"
Source: cronjob-controller
First Seen: 2026-02-10T10:00:00Z
Last Seen: 2026-02-10T10:00:00Z
Count: 1
```

**è§¦å‘æ¡ä»¶:**
- `.spec.concurrencyPolicy` è®¾ç½®ä¸º `Forbid`
- ä¸Šæ¬¡ Job å°šæœªå®Œæˆ
- æ–°çš„è°ƒåº¦æ—¶é—´åˆ°è¾¾ä½†è¢«é˜»æ­¢

**å­—æ®µè¯¦è§£:**
- `xxx`: å½“å‰è¿è¡Œä¸­çš„ Job æ•°é‡
- `concurrencyPolicy`: å¹¶å‘ç­–ç•¥

**ç‰ˆæœ¬ä¿¡æ¯:**
- **èµ·å§‹ç‰ˆæœ¬**: v1.4

**ç”Ÿäº§å½±å“:**
- âš ï¸ **ä¸­ç­‰**: è·³è¿‡å½“å‰è°ƒåº¦
- ğŸ“Š å¯èƒ½å¯¼è‡´ä»»åŠ¡ç§¯å‹
- â±ï¸ éœ€è¦è¯„ä¼° Job æ‰§è¡Œæ—¶é—´

**å¹¶å‘ç­–ç•¥è¯¦è§£:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: data-sync
spec:
  schedule: "*/10 * * * *"        # æ¯ 10 åˆ†é’Ÿ
  concurrencyPolicy: Forbid       # Allow | Forbid | Replace
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: syncer
            image: syncer:v1

# ä¸‰ç§å¹¶å‘ç­–ç•¥:

# 1. Allow (é»˜è®¤) - å…è®¸å¹¶å‘è¿è¡Œ
#    é€‚ç”¨åœºæ™¯: æ— çŠ¶æ€ã€æ— èµ„æºç«äº‰çš„ä»»åŠ¡
#    é£é™©: å¯èƒ½å¯¼è‡´èµ„æºäº‰æŠ¢

# 2. Forbid - ç¦æ­¢å¹¶å‘ï¼Œè·³è¿‡æ–°è°ƒåº¦
#    é€‚ç”¨åœºæ™¯: æœ‰çŠ¶æ€ã€èµ„æºç‹¬å çš„ä»»åŠ¡
#    é£é™©: å¦‚æœ Job æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œä¼šæŒç»­è·³è¿‡è°ƒåº¦

# 3. Replace - æ›¿æ¢æ—§çš„è¿è¡Œä¸­ Job
#    é€‚ç”¨åœºæ™¯: åªéœ€æœ€æ–°ç»“æœçš„ä»»åŠ¡
#    é£é™©: å¯èƒ½ä¸­æ–­æ­£åœ¨æ‰§è¡Œçš„é‡è¦æ“ä½œ
```

**è§‚æµ‹ç¤ºä¾‹:**
```bash
# æŸ¥çœ‹æ´»è·ƒ Job
kubectl get jobs -l cronjob-name=data-sync --field-selector=status.successful!=1

# æŸ¥çœ‹ Job è¿è¡Œæ—¶é•¿
kubectl get jobs -l cronjob-name=data-sync -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.startTime}{"\t"}{.status.completionTime}{"\n"}{end}'

# ç»Ÿè®¡è·³è¿‡æ¬¡æ•°
kubectl get events --field-selector reason=ForbidConcurrent,involvedObject.name=data-sync --sort-by='.lastTimestamp'
```

**ä¼˜åŒ–æ–¹æ¡ˆ:**
```yaml
# æ–¹æ¡ˆ 1: ä¼˜åŒ– Job æ‰§è¡Œæ—¶é—´
apiVersion: batch/v1
kind: CronJob
spec:
  schedule: "*/10 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      activeDeadlineSeconds: 480   # 8 åˆ†é’Ÿè¶…æ—¶ï¼ˆå°äºè°ƒåº¦é—´éš”ï¼‰
      template:
        spec:
          containers:
          - name: syncer
            resources:
              requests:
                cpu: "1"             # å¢åŠ èµ„æºåŠ é€Ÿæ‰§è¡Œ
                memory: "2Gi"

# æ–¹æ¡ˆ 2: è°ƒæ•´è°ƒåº¦é—´éš”
apiVersion: batch/v1
kind: CronJob
spec:
  schedule: "*/15 * * * *"          # å¢åŠ åˆ° 15 åˆ†é’Ÿ
  concurrencyPolicy: Forbid

# æ–¹æ¡ˆ 3: æ”¹ç”¨ Replace ç­–ç•¥ï¼ˆé€‚ç”¨äºå¹‚ç­‰ä»»åŠ¡ï¼‰
apiVersion: batch/v1
kind: CronJob
spec:
  schedule: "*/10 * * * *"
  concurrencyPolicy: Replace         # è‡ªåŠ¨ç»ˆæ­¢æ—§ Job

# æ–¹æ¡ˆ 4: æ”¹ç”¨ Allow + åˆ†å¸ƒå¼é”ï¼ˆåº”ç”¨å±‚æ§åˆ¶ï¼‰
apiVersion: batch/v1
kind: CronJob
spec:
  schedule: "*/10 * * * *"
  concurrencyPolicy: Allow
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: syncer
            image: syncer-with-lock:v1  # åº”ç”¨å†…å®ç°åˆ†å¸ƒå¼é”
```

---

## æ‰¹å¤„ç†æ‰§è¡Œç”Ÿå‘½å‘¨æœŸ

### Job æ‰§è¡Œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Job Execution Lifecycle                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

åˆ›å»ºé˜¶æ®µ:
  kubectl apply -f job.yaml
         â”‚
         â–¼
  [Job Controller æ¥æ”¶]
         â”‚
         â”œâ”€â”€â”€ éªŒè¯é…ç½®
         â”œâ”€â”€â”€ è®¡ç®—æ‰€éœ€ Pod æ•°é‡
         â”‚    (min(parallelism, completions - succeeded))
         â””â”€â”€â”€ ç”Ÿæˆ Pod æ¨¡æ¿
                â”‚
                â–¼
         [SuccessfulCreate Event]
         åˆ›å»º Pod: job-xxx-abc123


æ‰§è¡Œé˜¶æ®µ:
  [Pod è¿è¡Œä¸­]
         â”‚
         â”œâ”€â”€â”€ æ­£å¸¸å®Œæˆ (exitCode=0)
         â”‚         â”‚
         â”‚         â–¼
         â”‚    succeeded++
         â”‚         â”‚
         â”‚         â”œâ”€â”€â”€ succeeded < completions
         â”‚         â”‚         â”‚
         â”‚         â”‚         â””â”€â”€> [åˆ›å»ºæ–° Pod]
         â”‚         â”‚
         â”‚         â””â”€â”€â”€ succeeded == completions
         â”‚                   â”‚
         â”‚                   â–¼
         â”‚            [Completed Event]
         â”‚            Job å®Œæˆ
         â”‚
         â””â”€â”€â”€ å¤±è´¥ (exitCode != 0)
                   â”‚
                   â–¼
              failed++
                   â”‚
                   â”œâ”€â”€â”€ failed < backoffLimit
                   â”‚         â”‚
                   â”‚         â”œâ”€â”€â”€ ç­‰å¾…é€€é¿æ—¶é—´ (æŒ‡æ•°é€€é¿)
                   â”‚         â””â”€â”€> [SuccessfulCreate Event]
                   â”‚              é‡æ–°åˆ›å»º Pod
                   â”‚
                   â””â”€â”€â”€ failed >= backoffLimit
                             â”‚
                             â–¼
                      [BackoffLimitExceeded Event]
                      Job å¤±è´¥


è¶…æ—¶æ£€æŸ¥:
  [Job Controller å®šæœŸæ£€æŸ¥]
         â”‚
         â””â”€â”€â”€ (now - startTime) > activeDeadlineSeconds
                   â”‚
                   â–¼
            [DeadlineExceeded Event]
            ç»ˆæ­¢æ‰€æœ‰ Podï¼ŒJob å¤±è´¥


æ¸…ç†é˜¶æ®µ:
  [Job å®Œæˆæˆ–å¤±è´¥]
         â”‚
         â””â”€â”€â”€ (now - completionTime) > ttlSecondsAfterFinished
                   â”‚
                   â–¼
            [SuccessfulDelete Event]
            åˆ é™¤ Job åŠå…¶ Pod


æš‚åœ/æ¢å¤:
  kubectl patch job xxx -p '{"spec":{"suspend":true}}'
         â”‚
         â–¼
  [Suspended Event]
  åˆ é™¤æ‰€æœ‰æ´»è·ƒ Pod
         â”‚
         â””â”€â”€â”€ kubectl patch job xxx -p '{"spec":{"suspend":false}}'
                   â”‚
                   â–¼
              [Resumed Event]
              é‡æ–°åˆ›å»º Pod
```

### CronJob è°ƒåº¦æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CronJob Scheduling Lifecycle                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è°ƒåº¦å‘¨æœŸ:
  [CronJob Controller æ¯ 10 ç§’åŒæ­¥ä¸€æ¬¡]
         â”‚
         â””â”€â”€â”€ è®¡ç®—ä¸‹ä¸€æ¬¡è°ƒåº¦æ—¶é—´
                   â”‚
                   â”œâ”€â”€â”€ æœªåˆ°è°ƒåº¦æ—¶é—´
                   â”‚         â””â”€â”€> ç­‰å¾…
                   â”‚
                   â””â”€â”€â”€ åˆ°è¾¾è°ƒåº¦æ—¶é—´
                             â”‚
                             â–¼
                      [æ£€æŸ¥ startingDeadlineSeconds]
                             â”‚
                             â”œâ”€â”€â”€ è¶…è¿‡æˆªæ­¢æ—¶é—´
                             â”‚         â””â”€â”€> è·³è¿‡æ­¤æ¬¡è°ƒåº¦
                             â”‚
                             â””â”€â”€â”€ åœ¨æˆªæ­¢æ—¶é—´å†…
                                       â”‚
                                       â–¼
                                [æ£€æŸ¥ concurrencyPolicy]
                                       â”‚
                                       â”œâ”€â”€â”€ Forbid + æœ‰æ´»è·ƒ Job
                                       â”‚         â”‚
                                       â”‚         â–¼
                                       â”‚    [ForbidConcurrent Event]
                                       â”‚    è·³è¿‡æ­¤æ¬¡è°ƒåº¦
                                       â”‚
                                       â”œâ”€â”€â”€ Replace + æœ‰æ´»è·ƒ Job
                                       â”‚         â”‚
                                       â”‚         â–¼
                                       â”‚    åˆ é™¤æ—§ Jobï¼Œåˆ›å»ºæ–° Job
                                       â”‚
                                       â””â”€â”€â”€ Allow æˆ–æ— æ´»è·ƒ Job
                                                 â”‚
                                                 â–¼
                                          [SuccessfulCreate Event]
                                          åˆ›å»ºæ–° Job: cronjob-xxx-1234567890


Job ç›‘æ§:
  [CronJob Controller ç›‘æ§ Job çŠ¶æ€]
         â”‚
         â”œâ”€â”€â”€ Job å®Œæˆ
         â”‚         â”‚
         â”‚         â–¼
         â”‚    [SawCompletedJob Event]
         â”‚    æ›´æ–° lastSuccessfulTime
         â”‚         â”‚
         â”‚         â””â”€â”€â”€ æ£€æŸ¥å†å²æ•°é‡
         â”‚                   â”‚
         â”‚                   â””â”€â”€â”€ è¶…è¿‡ successfulJobsHistoryLimit
         â”‚                             â”‚
         â”‚                             â–¼
         â”‚                      [SuccessfulDelete Event]
         â”‚                      åˆ é™¤æ—§ Job
         â”‚
         â””â”€â”€â”€ Job å¤±è´¥
                   â”‚
                   â–¼
              [SawCompletedJob Event]
              status: Failed
                   â”‚
                   â””â”€â”€â”€ æ£€æŸ¥å†å²æ•°é‡
                             â”‚
                             â””â”€â”€â”€ è¶…è¿‡ failedJobsHistoryLimit
                                       â”‚
                                       â–¼
                                [SuccessfulDelete Event]
                                åˆ é™¤æ—§ Job


å¼‚å¸¸å¤„ç†:
  [æ§åˆ¶å™¨å®•æœºæ¢å¤]
         â”‚
         â””â”€â”€â”€ è®¡ç®—é”™è¿‡çš„è°ƒåº¦æ¬¡æ•°
                   â”‚
                   â”œâ”€â”€â”€ é”™è¿‡æ¬¡æ•° <= 100
                   â”‚         â””â”€â”€> åˆ›å»ºæœ€è¿‘ä¸€æ¬¡çš„ Job
                   â”‚
                   â””â”€â”€â”€ é”™è¿‡æ¬¡æ•° > 100
                             â”‚
                             â–¼
                      [TooManyMissedTimes Event]
                      è·³è¿‡æ‰€æœ‰é”™è¿‡çš„è°ƒåº¦


æ—¶é—´è®¡ç®—ç¤ºä¾‹:
  schedule: "*/5 * * * *"  (æ¯ 5 åˆ†é’Ÿ)

  å½“å‰æ—¶é—´: 10:03:00
  ä¸Šæ¬¡è°ƒåº¦: 10:00:00
  ä¸‹æ¬¡è°ƒåº¦: 10:05:00
  ç­‰å¾…æ—¶é—´: 2 åˆ†é’Ÿ

  startingDeadlineSeconds: 300 (5 åˆ†é’Ÿ)
  è°ƒåº¦çª—å£: [10:00:00, 10:05:00]

  å¦‚æœ 10:06:00 æ‰æ‰§è¡Œæ£€æŸ¥:
    - è¶…è¿‡ startingDeadlineSeconds: No (10:06 - 10:05 = 1 åˆ†é’Ÿ < 5 åˆ†é’Ÿ)
    - ç»§ç»­åˆ›å»º Job

  å¦‚æœ 10:11:00 æ‰æ‰§è¡Œæ£€æŸ¥:
    - è¶…è¿‡ startingDeadlineSeconds: Yes (10:11 - 10:05 = 6 åˆ†é’Ÿ > 5 åˆ†é’Ÿ)
    - è·³è¿‡æ­¤æ¬¡è°ƒåº¦ï¼Œç­‰å¾…ä¸‹æ¬¡ 10:10:00
```

---

## æ·±åº¦åˆ†æ

### 1. BackoffLimitExceeded æ·±åº¦å‰–æ

**é€€é¿ç®—æ³•å®ç°:**
```go
// Kubernetes Job Controller é€€é¿ç®—æ³•ä¼ªä»£ç 

func getBackoffDuration(failureCount int) time.Duration {
    // åŸºç¡€é€€é¿æ—¶é—´: 10 ç§’
    baseDelay := 10 * time.Second
    
    // æŒ‡æ•°é€€é¿: 10s * 2^(failures-1)
    // å¤±è´¥ 1 æ¬¡: 10s * 2^0 = 10s
    // å¤±è´¥ 2 æ¬¡: 10s * 2^1 = 20s
    // å¤±è´¥ 3 æ¬¡: 10s * 2^2 = 40s
    // å¤±è´¥ 4 æ¬¡: 10s * 2^3 = 80s
    // å¤±è´¥ 5 æ¬¡: 10s * 2^4 = 160s
    delay := baseDelay * (1 << (failureCount - 1))
    
    // æœ€å¤§é€€é¿æ—¶é—´: 6 åˆ†é’Ÿ
    maxDelay := 6 * time.Minute
    if delay > maxDelay {
        return maxDelay
    }
    
    return delay
}

// è®¡ç®—ä¸‹æ¬¡é‡è¯•æ—¶é—´
nextRetryTime := podFailureTime.Add(getBackoffDuration(failureCount))
```

**é€€é¿æ—¶é—´è¡¨:**
| å¤±è´¥æ¬¡æ•° | é€€é¿æ—¶é—´ | ç´¯è®¡ç­‰å¾…æ—¶é—´ | è¯´æ˜ |
|---------|---------|-------------|------|
| 1 | 10s | 10s | 2^0 Ã— 10s |
| 2 | 20s | 30s | 2^1 Ã— 10s |
| 3 | 40s | 70s | 2^2 Ã— 10s |
| 4 | 80s | 150s | 2^3 Ã— 10s |
| 5 | 160s | 310s | 2^4 Ã— 10s |
| 6 | 320s (5m20s) | 630s (10m30s) | 2^5 Ã— 10s |
| 7+ | 360s (6m) | - | è¾¾åˆ°æœ€å¤§å€¼ |

**å¤±è´¥åœºæ™¯åˆ†ç±»:**

```yaml
# åœºæ™¯ 1: ä¸´æ—¶ç½‘ç»œé”™è¯¯ï¼ˆåº”è¯¥é‡è¯•ï¼‰
apiVersion: batch/v1
kind: Job
metadata:
  name: api-caller
spec:
  backoffLimit: 6            # å…è®¸å¤šæ¬¡é‡è¯•
  template:
    spec:
      containers:
      - name: caller
        image: curl:latest
        command: ["curl", "https://api.example.com"]
      restartPolicy: Never

# åœºæ™¯ 2: æ•°æ®éªŒè¯é”™è¯¯ï¼ˆä¸åº”é‡è¯•ï¼‰
apiVersion: batch/v1
kind: Job
metadata:
  name: data-validator
spec:
  backoffLimit: 0            # ä¸é‡è¯•ï¼Œç«‹å³å¤±è´¥
  podFailurePolicy:          # v1.26+
    rules:
    - action: FailJob        # æ•°æ®é”™è¯¯ç›´æ¥å¤±è´¥
      onExitCodes:
        containerName: validator
        operator: In
        values: [1]          # é€€å‡ºç  1 è¡¨ç¤ºæ•°æ®æ— æ•ˆ
  template:
    spec:
      containers:
      - name: validator
        image: validator:v1
      restartPolicy: Never

# åœºæ™¯ 3: æ··åˆåœºæ™¯ï¼ˆé€‰æ‹©æ€§é‡è¯•ï¼‰
apiVersion: batch/v1
kind: Job
metadata:
  name: smart-processor
spec:
  backoffLimit: 5
  podFailurePolicy:
    rules:
    - action: FailJob              # é…ç½®é”™è¯¯ä¸é‡è¯•
      onExitCodes:
        operator: In
        values: [2]
    - action: Ignore               # èŠ‚ç‚¹é©±é€ä¸è®¡å…¥å¤±è´¥
      onPodConditions:
      - type: DisruptionTarget
    - action: Count                # å…¶ä»–é”™è¯¯æ­£å¸¸è®¡å…¥
      onExitCodes:
        operator: NotIn
        values: [0, 2]
  template:
    spec:
      containers:
      - name: processor
        image: processor:v1
      restartPolicy: Never
```

**ç›‘æ§å’Œå‘Šè­¦:**
```bash
# Prometheus æŸ¥è¯¢ç¤ºä¾‹

# 1. Job å¤±è´¥ç‡
sum(rate(kube_job_status_failed{namespace="production"}[5m]))
  /
sum(rate(kube_job_status_succeeded{namespace="production"}[5m]) + rate(kube_job_status_failed{namespace="production"}[5m]))

# 2. BackoffLimitExceeded äº‹ä»¶æ•°é‡
sum(increase(kube_events_total{reason="BackoffLimitExceeded"}[1h]))

# 3. Job é‡è¯•æ¬¡æ•°åˆ†å¸ƒ
histogram_quantile(0.95, 
  sum(rate(kube_job_status_failed[5m])) by (job_name, le)
)

# 4. å¹³å‡å¤±è´¥ç­‰å¾…æ—¶é—´
avg(kube_job_complete_time - kube_job_start_time) 
  by (job_name)
  where kube_job_status_failed > 0
```

**å‘Šè­¦è§„åˆ™:**
```yaml
# Prometheus AlertManager è§„åˆ™
groups:
- name: job_alerts
  rules:
  - alert: JobBackoffLimitExceeded
    expr: |
      sum(increase(kube_events_total{reason="BackoffLimitExceeded"}[5m])) > 0
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "Job {{ $labels.name }} è¾¾åˆ°é‡è¯•ä¸Šé™"
      description: "Job åœ¨è¿‡å» 5 åˆ†é’Ÿå†…è¾¾åˆ° backoffLimitï¼Œéœ€è¦äººå·¥ä»‹å…¥"

  - alert: JobHighFailureRate
    expr: |
      sum(rate(kube_job_status_failed[5m])) by (namespace, job_name)
        /
      sum(rate(kube_job_status_succeeded[5m] + kube_job_status_failed[5m])) by (namespace, job_name)
      > 0.5
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: "Job {{ $labels.job_name }} å¤±è´¥ç‡è¿‡é«˜"
      description: "å¤±è´¥ç‡è¶…è¿‡ 50%ï¼Œå½“å‰å€¼: {{ $value }}"
```

---

### 2. Job Suspend/Resume ç‰¹æ€§ (v1.22+)

**ä½¿ç”¨åœºæ™¯:**

```yaml
# åœºæ™¯ 1: è®¡åˆ’ç»´æŠ¤çª—å£
apiVersion: batch/v1
kind: Job
metadata:
  name: db-migration
spec:
  suspend: true              # åˆ›å»ºæ—¶æš‚åœ
  completions: 100
  parallelism: 10
  template:
    spec:
      containers:
      - name: migrator
        image: db-migrator:v1

# è¿ç»´æµç¨‹:
# 1. åˆ›å»º Jobï¼ˆæš‚åœçŠ¶æ€ï¼‰
kubectl apply -f db-migration.yaml

# 2. ç»´æŠ¤çª—å£å¼€å§‹ï¼Œæ¢å¤ Job
kubectl patch job db-migration -p '{"spec":{"suspend":false}}'

# 3. ç´§æ€¥æƒ…å†µéœ€è¦æš‚åœ
kubectl patch job db-migration -p '{"spec":{"suspend":true}}'

# 4. é—®é¢˜è§£å†³åæ¢å¤
kubectl patch job db-migration -p '{"spec":{"suspend":false}}'
```

```yaml
# åœºæ™¯ 2: èµ„æºåŠ¨æ€è°ƒåº¦
apiVersion: v1
kind: ConfigMap
metadata:
  name: job-scheduler-config
data:
  peak_hours: "09:00-18:00"    # é«˜å³°æœŸæš‚åœä½ä¼˜å…ˆçº§ Job
  off_hours: "18:00-09:00"     # ä½å³°æœŸæ¢å¤

---
# è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆä¼ªä»£ç ï¼‰
# é«˜å³°æœŸè‡ªåŠ¨æš‚åœéå…³é”® Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-suspender
spec:
  schedule: "0 9 * * *"        # æ¯å¤© 9:00
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: suspender
            image: kubectl:latest
            command:
            - sh
            - -c
            - |
              kubectl patch job non-critical-batch -p '{"spec":{"suspend":true}}'

---
# ä½å³°æœŸè‡ªåŠ¨æ¢å¤
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-resumer
spec:
  schedule: "0 18 * * *"       # æ¯å¤© 18:00
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: resumer
            image: kubectl:latest
            command:
            - sh
            - -c
            - |
              kubectl patch job non-critical-batch -p '{"spec":{"suspend":false}}'
```

**çŠ¶æ€å˜åŒ–è·Ÿè¸ª:**
```bash
# ç›‘æ§ suspend çŠ¶æ€å˜åŒ–
kubectl get events --watch | grep -E 'Suspended|Resumed'

# æŸ¥çœ‹ Job æš‚åœå†å²
kubectl describe job db-migration | grep -A 5 "Suspended\|Resumed"

# ç»Ÿè®¡æš‚åœæ—¶é•¿
# ä½¿ç”¨è‡ªå®šä¹‰è„šæœ¬æˆ– Prometheus æŸ¥è¯¢
```

**é™åˆ¶å’Œæ³¨æ„äº‹é¡¹:**
- æš‚åœæ—¶æ‰€æœ‰æ´»è·ƒ Pod å°†è¢«åˆ é™¤ï¼ˆæ•°æ®å¯èƒ½ä¸¢å¤±ï¼‰
- æ¢å¤åä»å¤´å¼€å§‹æ‰§è¡Œï¼ˆéæ–­ç‚¹ç»­ä¼ ï¼‰
- `.status.succeeded` è®¡æ•°ä¿ç•™
- é€‚ç”¨äºå¹‚ç­‰æ€§ä»»åŠ¡

---

### 3. Indexed Jobs (v1.24 GA)

**å®Œæ•´ç¤ºä¾‹:**
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processor
spec:
  completions: 10              # 10 ä¸ªç´¢å¼•ä»»åŠ¡ (0-9)
  parallelism: 3               # å¹¶è¡Œ 3 ä¸ª
  completionMode: Indexed      # ç´¢å¼•æ¨¡å¼
  template:
    metadata:
      labels:
        app: data-processor
    spec:
      restartPolicy: Never
      containers:
      - name: processor
        image: python:3.9
        env:
        - name: JOB_COMPLETION_INDEX
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
        - name: DATA_BUCKET
          value: "s3://my-data-bucket"
        command:
        - python
        - -c
        - |
          import os
          import sys
          
          # è·å–å½“å‰ç´¢å¼•
          index = int(os.environ['JOB_COMPLETION_INDEX'])
          total = 10
          
          print(f"Processing index: {index}/{total}")
          
          # æ•°æ®åˆ†ç‰‡é€»è¾‘
          start_id = index * 1000
          end_id = (index + 1) * 1000
          
          print(f"Processing records {start_id} to {end_id}")
          
          # æ¨¡æ‹Ÿå¤„ç†
          # process_data_shard(start_id, end_id)
          
          print(f"Index {index} completed successfully")
```

**æ•°æ®åˆ†ç‰‡ç­–ç•¥:**
```python
# ç¤ºä¾‹: å¤§è§„æ¨¡æ•°æ®å¤„ç†

# æ–¹æ¡ˆ 1: å‡åŒ€åˆ†ç‰‡
def get_shard_range(index, total_tasks, total_records):
    shard_size = total_records // total_tasks
    start = index * shard_size
    end = start + shard_size if index < total_tasks - 1 else total_records
    return start, end

# ä½¿ç”¨:
# Total: 1000ä¸‡æ¡è®°å½•, 100 ä¸ªä»»åŠ¡
# Index 0: 0 - 100,000
# Index 1: 100,000 - 200,000
# Index 99: 9,900,000 - 10,000,000

# æ–¹æ¡ˆ 2: å“ˆå¸Œåˆ†ç‰‡
def get_shard_key(index, total_tasks):
    return lambda key: hash(key) % total_tasks == index

# ä½¿ç”¨:
# åªå¤„ç† hash(record_id) % 100 == index çš„è®°å½•

# æ–¹æ¡ˆ 3: èŒƒå›´åˆ†ç‰‡ï¼ˆé€‚ç”¨äºæœ‰åºæ•°æ®ï¼‰
def get_date_range(index, total_tasks):
    start_date = datetime(2024, 1, 1) + timedelta(days=index*30)
    end_date = start_date + timedelta(days=30)
    return start_date, end_date

# ä½¿ç”¨:
# Index 0: 2024-01-01 to 2024-01-31
# Index 1: 2024-02-01 to 2024-03-02
```

**MapReduce é£æ ¼å®ç°:**
```yaml
# Map é˜¶æ®µ: Indexed Job å¤„ç†æ•°æ®åˆ†ç‰‡
apiVersion: batch/v1
kind: Job
metadata:
  name: map-job
spec:
  completions: 100
  parallelism: 20
  completionMode: Indexed
  template:
    spec:
      containers:
      - name: mapper
        image: map-processor:v1
        env:
        - name: INDEX
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
        - name: OUTPUT_PATH
          value: "/data/map-output"
        volumeMounts:
        - name: shared-data
          mountPath: /data
      volumes:
      - name: shared-data
        persistentVolumeClaim:
          claimName: map-reduce-pvc
      restartPolicy: Never

---
# Reduce é˜¶æ®µ: å•ä¸ª Job æ±‡æ€»ç»“æœ
apiVersion: batch/v1
kind: Job
metadata:
  name: reduce-job
spec:
  completions: 1
  template:
    spec:
      containers:
      - name: reducer
        image: reduce-processor:v1
        env:
        - name: INPUT_PATH
          value: "/data/map-output"
        - name: OUTPUT_PATH
          value: "/data/final-result"
        volumeMounts:
        - name: shared-data
          mountPath: /data
      volumes:
      - name: shared-data
        persistentVolumeClaim:
          claimName: map-reduce-pvc
      restartPolicy: Never
```

**ç›‘æ§ Indexed Jobs:**
```bash
# æŸ¥çœ‹å„ç´¢å¼•å®Œæˆæƒ…å†µ
kubectl get pods -l job-name=data-processor -o custom-columns=\
  NAME:.metadata.name,\
  INDEX:.metadata.annotations.batch\.kubernetes\.io/job-completion-index,\
  STATUS:.status.phase

# è¾“å‡ºç¤ºä¾‹:
# NAME                      INDEX   STATUS
# data-processor-0-abc123   0       Succeeded
# data-processor-1-def456   1       Running
# data-processor-2-ghi789   2       Succeeded
# data-processor-3-jkl012   3       Pending

# ç»Ÿè®¡å®Œæˆåº¦
kubectl get job data-processor -o jsonpath='{.status.succeeded}/{.spec.completions}'
# è¾“å‡º: 7/10

# æŸ¥çœ‹ç‰¹å®šç´¢å¼•çš„æ—¥å¿—
INDEX=5
kubectl logs -l job-name=data-processor,batch.kubernetes.io/job-completion-index=$INDEX
```

---

### 4. CronJob æ—¶åŒºæ”¯æŒ (v1.25+)

**æ—¶åŒºé…ç½®:**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-report
spec:
  schedule: "0 9 * * *"              # æ¯å¤© 9:00
  timeZone: "Asia/Shanghai"          # ä¸œå…«åŒºæ—¶é—´ (v1.25+)
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: reporter
            image: report-generator:v1
          restartPolicy: OnFailure

# å¸¸ç”¨æ—¶åŒº:
# "Asia/Shanghai"      - ä¸­å›½æ ‡å‡†æ—¶é—´ (UTC+8)
# "America/New_York"   - ç¾å›½ä¸œéƒ¨æ—¶é—´ (UTC-5/-4)
# "Europe/London"      - è‹±å›½æ—¶é—´ (UTC+0/+1)
# "UTC"                - åè°ƒä¸–ç•Œæ—¶
```

**ç‰ˆæœ¬å…¼å®¹æ€§:**
- **v1.24 åŠæ›´æ—©**: ä¸æ”¯æŒ `timeZone`ï¼Œä½¿ç”¨æ§åˆ¶å™¨æ‰€åœ¨èŠ‚ç‚¹æ—¶åŒº
- **v1.25+**: æ”¯æŒ `timeZone` å­—æ®µï¼ˆBetaï¼‰
- **v1.27**: `timeZone` å­—æ®µå‡çº§ä¸º GA

---

## æ•…éšœæ’æŸ¥æ¨¡å¼

### é—®é¢˜ 1: Job é•¿æ—¶é—´ä¸åˆ›å»º Pod

**ç—‡çŠ¶:**
```bash
kubectl get job my-job
# NAME     COMPLETIONS   DURATION   AGE
# my-job   0/5           0s         5m

kubectl get pods -l job-name=my-job
# No resources found
```

**æ’æŸ¥æ­¥éª¤:**
```bash
# 1. æ£€æŸ¥ Job äº‹ä»¶
kubectl describe job my-job | grep Events -A 20

# å¯èƒ½çœ‹åˆ°:
# Type     Reason        Message
# Warning  FailedCreate  Error creating: pods "my-job-xxx" is forbidden: exceeded quota

# 2. æ£€æŸ¥ ResourceQuota
kubectl describe resourcequota -n <namespace>

# 3. æ£€æŸ¥ Job é…ç½®
kubectl get job my-job -o yaml | grep -A 10 "resources:"

# 4. æ£€æŸ¥èŠ‚ç‚¹èµ„æº
kubectl top nodes

# 5. æ¨¡æ‹Ÿåˆ›å»º
kubectl create job test --image=busybox --dry-run=server
```

**è§£å†³æ–¹æ¡ˆ:**
- å¢åŠ  ResourceQuota é™åˆ¶
- é™ä½ Job èµ„æºè¯·æ±‚
- æ¸…ç†æ—§çš„ Job é‡Šæ”¾é…é¢
- æ£€æŸ¥ LimitRange é…ç½®

---

### é—®é¢˜ 2: CronJob ä¸æŒ‰æ—¶æ‰§è¡Œ

**ç—‡çŠ¶:**
```bash
kubectl get cronjob hourly-backup
# NAME            SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
# hourly-backup   0 * * * *     False     0        62m             5h

# æœ€åè°ƒåº¦æ—¶é—´æ˜¯ 62 åˆ†é’Ÿå‰ï¼ˆåº”è¯¥æ˜¯æœ€è¿‘ 1 å°æ—¶å†…ï¼‰
```

**æ’æŸ¥æ­¥éª¤:**
```bash
# 1. æ£€æŸ¥ CronJob äº‹ä»¶
kubectl describe cronjob hourly-backup | grep Events -A 20

# å¯èƒ½çœ‹åˆ°:
# Type     Reason            Message
# Warning  FailedCreate      Error creating job: ...
# Warning  ForbidConcurrent  Cannot create job: too many jobs running

# 2. æ£€æŸ¥æ´»è·ƒ Job
kubectl get jobs -l cronjob-name=hourly-backup --field-selector=status.successful!=1

# 3. æ£€æŸ¥ concurrencyPolicy
kubectl get cronjob hourly-backup -o jsonpath='{.spec.concurrencyPolicy}'

# 4. æ£€æŸ¥ startingDeadlineSeconds
kubectl get cronjob hourly-backup -o jsonpath='{.spec.startingDeadlineSeconds}'

# 5. æ£€æŸ¥æ§åˆ¶å™¨å¥åº·
kubectl get pods -n kube-system -l component=kube-controller-manager
kubectl logs -n kube-system -l component=kube-controller-manager --tail=100 | grep cronjob
```

**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ–¹æ¡ˆ 1: æ¸…ç†å¡ä½çš„ Job
kubectl delete job -l cronjob-name=hourly-backup --field-selector=status.successful!=1

# æ–¹æ¡ˆ 2: è°ƒæ•´å¹¶å‘ç­–ç•¥
kubectl patch cronjob hourly-backup -p '{"spec":{"concurrencyPolicy":"Replace"}}'

# æ–¹æ¡ˆ 3: å¢åŠ è¶…æ—¶æ—¶é—´
kubectl patch cronjob hourly-backup -p '{"spec":{"startingDeadlineSeconds":600}}'

# æ–¹æ¡ˆ 4: ä¼˜åŒ– Job æ‰§è¡Œæ—¶é—´
kubectl patch cronjob hourly-backup -p '{"spec":{"jobTemplate":{"spec":{"activeDeadlineSeconds":1800}}}}'
```

---

### é—®é¢˜ 3: Job é¢‘ç¹è¾¾åˆ° BackoffLimitExceeded

**ç—‡çŠ¶:**
```bash
kubectl get job data-import
# NAME          COMPLETIONS   DURATION   AGE
# data-import   0/1           5m         5m

kubectl describe job data-import
# Type     Reason                  Message
# Warning  BackoffLimitExceeded    Job has reached the specified backoff limit
```

**æ·±åº¦åˆ†æ:**
```bash
# 1. æŸ¥çœ‹æ‰€æœ‰å¤±è´¥ Pod çš„é€€å‡ºç 
kubectl get pods -l job-name=data-import \
  -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.containerStatuses[0].state.terminated.exitCode}{"\t"}{.status.containerStatuses[0].state.terminated.reason}{"\n"}{end}'

# è¾“å‡ºç¤ºä¾‹:
# data-import-abc123   137   OOMKilled
# data-import-def456   137   OOMKilled
# data-import-ghi789   137   OOMKilled

# 2. æŸ¥çœ‹å¤±è´¥ Pod æ—¥å¿—
kubectl logs -l job-name=data-import --tail=100 --prefix=true

# 3. åˆ†æå¤±è´¥æ¨¡å¼
# é€€å‡ºç  137: OOMKilled - éœ€è¦å¢åŠ å†…å­˜
# é€€å‡ºç  1: åº”ç”¨é”™è¯¯ - æ£€æŸ¥ä»£ç é€»è¾‘
# é€€å‡ºç  143: SIGTERM - æ£€æŸ¥è¶…æ—¶é…ç½®
```

**é’ˆå¯¹æ€§è§£å†³:**
```yaml
# åœºæ™¯ 1: OOM å¯¼è‡´å¤±è´¥
apiVersion: batch/v1
kind: Job
metadata:
  name: data-import
spec:
  backoffLimit: 3
  template:
    spec:
      containers:
      - name: importer
        image: importer:v1
        resources:
          limits:
            memory: "4Gi"     # å¢åŠ å†…å­˜
          requests:
            memory: "2Gi"

# åœºæ™¯ 2: å¤–éƒ¨ä¾èµ–è¶…æ—¶
apiVersion: batch/v1
kind: Job
metadata:
  name: api-caller
spec:
  backoffLimit: 10            # å¢åŠ é‡è¯•æ¬¡æ•°
  podFailurePolicy:           # v1.26+
    rules:
    - action: Ignore          # ç½‘ç»œé”™è¯¯ä¸è®¡å…¥å¤±è´¥
      onExitCodes:
        operator: In
        values: [7, 28]       # curl é”™è¯¯ç 
  template:
    spec:
      containers:
      - name: caller
        image: curl:latest
        command:
        - sh
        - -c
        - "curl --retry 3 --retry-delay 10 https://api.example.com"

# åœºæ™¯ 3: æ•°æ®é”™è¯¯ç«‹å³å¤±è´¥
apiVersion: batch/v1
kind: Job
metadata:
  name: validator
spec:
  backoffLimit: 0             # ä¸é‡è¯•
  template:
    spec:
      containers:
      - name: validator
        image: validator:v1
```

---

## ç›¸å…³å‚è€ƒ

### å†…éƒ¨æ–‡æ¡£

**Domain-33 Kubernetes Events:**
- [01-pod-lifecycle-events.md](./01-pod-lifecycle-events.md) - Pod ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ï¼ˆFailedScheduling ç­‰ï¼‰
- [02-deployment-events.md](./02-deployment-events.md) - Deployment äº‹ä»¶
- [03-statefulset-events.md](./03-statefulset-events.md) - StatefulSet äº‹ä»¶
- [04-daemonset-events.md](./04-daemonset-events.md) - DaemonSet äº‹ä»¶
- [05-replicaset-events.md](./05-replicaset-events.md) - ReplicaSet äº‹ä»¶
- [06-hpa-events.md](./06-hpa-events.md) - HPA è‡ªåŠ¨æ‰©ç¼©å®¹äº‹ä»¶
- [07-pvc-storage-events.md](./07-pvc-storage-events.md) - PVC/PV å­˜å‚¨äº‹ä»¶
- [08-service-ingress-events.md](./08-service-ingress-events.md) - Service/Ingress ç½‘ç»œäº‹ä»¶

**Troubleshooting æ–‡æ¡£:**
- [topic-structural-trouble-shooting/05-workloads/05-job-cronjob-troubleshooting.md](../topic-structural-trouble-shooting/05-workloads/05-job-cronjob-troubleshooting.md) - Job/CronJob æ•…éšœæ’æŸ¥
- [topic-structural-trouble-shooting/01-control-plane/04-controller-manager-troubleshooting.md](../topic-structural-trouble-shooting/01-control-plane/04-controller-manager-troubleshooting.md) - Controller Manager æ•…éšœæ’æŸ¥

**Domain æ–‡æ¡£:**
- [domain-8-kubernetes-workloads/](../domain-8-kubernetes-workloads/) - Workload å·¥ä½œè´Ÿè½½è¯¦è§£
- [domain-17-batch-processing/](../domain-17-batch-processing/) - æ‰¹å¤„ç†æ¨¡å¼æœ€ä½³å®è·µ

### å®˜æ–¹æ–‡æ¡£

**Job/CronJob:**
- [Jobs - Run to Completion](https://kubernetes.io/docs/concepts/workloads/controllers/job/)
- [CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/)
- [Pod Failure Policy](https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy) (v1.26+)
- [Success Policy](https://kubernetes.io/docs/concepts/workloads/controllers/job/#success-policy) (v1.28+)

**KEPs (Kubernetes Enhancement Proposals):**
- [KEP-2232: Suspend Job](https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2232-suspend-jobs)
- [KEP-2214: Indexed Job](https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/2214-indexed-job)
- [KEP-3329: Pod Failure Policy](https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/3329-retriable-and-non-retriable-failures)
- [KEP-3998: Job Success/Completion Policy](https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/3998-job-success-completion-policy)

### æœ€ä½³å®è·µ

**èµ„æºé…ç½®:**
```yaml
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
apiVersion: batch/v1
kind: Job
metadata:
  name: production-job
  labels:
    app: production-job
    version: v1.0
spec:
  # æ‰§è¡Œé…ç½®
  completions: 1                    # å•æ¬¡æ‰§è¡Œ
  parallelism: 1                    # å•å¹¶å‘
  backoffLimit: 3                   # æœ€å¤šé‡è¯• 3 æ¬¡
  activeDeadlineSeconds: 3600       # 1 å°æ—¶æ€»è¶…æ—¶
  ttlSecondsAfterFinished: 86400    # å®Œæˆå 24 å°æ—¶è‡ªåŠ¨åˆ é™¤
  
  # Pod æ¨¡æ¿
  template:
    metadata:
      labels:
        app: production-job
    spec:
      restartPolicy: OnFailure      # å¤±è´¥æ—¶é‡å¯å®¹å™¨è€Œéé‡å»º Pod
      
      # èµ„æºé…ç½®
      containers:
      - name: worker
        image: worker:v1.0
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"            # é™åˆ¶ CPU é˜²æ­¢æŠ¢å 
            memory: "2Gi"           # é™åˆ¶å†…å­˜é˜²æ­¢ OOM
        
        # å¥åº·æ£€æŸ¥
        livenessProbe:
          exec:
            command: ["pgrep", "-f", "worker"]
          initialDelaySeconds: 30
          periodSeconds: 10
        
        # ç¯å¢ƒå˜é‡
        env:
        - name: LOG_LEVEL
          value: "INFO"
        - name: RETRY_ATTEMPTS
          value: "3"
        
        # å·æŒ‚è½½
        volumeMounts:
        - name: config
          mountPath: /etc/config
        - name: data
          mountPath: /data
      
      # å·é…ç½®
      volumes:
      - name: config
        configMap:
          name: job-config
      - name: data
        persistentVolumeClaim:
          claimName: job-data-pvc
      
      # å®‰å…¨ä¸Šä¸‹æ–‡
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
```

```yaml
# CronJob ç”Ÿäº§é…ç½®
apiVersion: batch/v1
kind: CronJob
metadata:
  name: production-cronjob
  labels:
    app: production-cronjob
    version: v1.0
spec:
  # è°ƒåº¦é…ç½®
  schedule: "0 2 * * *"                # æ¯å¤©å‡Œæ™¨ 2 ç‚¹
  timeZone: "Asia/Shanghai"            # ä¸œå…«åŒºæ—¶é—´
  concurrencyPolicy: Forbid            # ç¦æ­¢å¹¶å‘æ‰§è¡Œ
  startingDeadlineSeconds: 3600        # 1 å°æ—¶è°ƒåº¦çª—å£
  successfulJobsHistoryLimit: 7        # ä¿ç•™æœ€è¿‘ 7 å¤©æˆåŠŸè®°å½•
  failedJobsHistoryLimit: 3            # ä¿ç•™æœ€è¿‘ 3 æ¬¡å¤±è´¥è®°å½•
  
  # Job æ¨¡æ¿
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 7200      # 2 å°æ—¶æ€»è¶…æ—¶
      ttlSecondsAfterFinished: 172800  # å®Œæˆå 48 å°æ—¶åˆ é™¤
      
      template:
        metadata:
          labels:
            app: production-cronjob
        spec:
          restartPolicy: OnFailure
          containers:
          - name: worker
            image: worker:v1.0
            resources:
              requests:
                cpu: "200m"
                memory: "512Mi"
              limits:
                cpu: "500m"
                memory: "1Gi"
```

**ç›‘æ§æŒ‡æ ‡:**
```yaml
# ServiceMonitor ç¤ºä¾‹ï¼ˆPrometheus Operatorï¼‰
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: job-metrics
spec:
  selector:
    matchLabels:
      app: job-exporter
  endpoints:
  - port: metrics
    interval: 30s
    
# å…³é”®æŒ‡æ ‡:
# - kube_job_status_succeeded          # Job æˆåŠŸæ•°
# - kube_job_status_failed             # Job å¤±è´¥æ•°
# - kube_job_complete_duration_seconds # Job æ‰§è¡Œæ—¶é•¿
# - kube_cronjob_next_schedule_time    # CronJob ä¸‹æ¬¡è°ƒåº¦æ—¶é—´
# - kube_cronjob_status_last_schedule_time # æœ€åè°ƒåº¦æ—¶é—´
```

---

> **KUDIG-DATABASE** | Domain-33: Kubernetes Events å…¨åŸŸäº‹ä»¶å¤§å…¨ | æ–‡æ¡£ 09/15
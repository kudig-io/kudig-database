# Kubernetes è‡ªåŠ¨åŒ–è¿ç»´ä¸SREå®è·µ (Automation and SRE Practices)

> **ä½œè€…**: SREä¸“å®¶ | **ç‰ˆæœ¬**: v2.2 | **æ›´æ–°æ—¶é—´**: 2026-02-07
> **é€‚ç”¨åœºæ™¯**: ä¼ä¸šçº§è¿ç»´è‡ªåŠ¨åŒ– | **å¤æ‚åº¦**: â­â­â­â­â­

## ğŸ¯ æ‘˜è¦

æœ¬æ–‡æ¡£æ·±å…¥æ¢è®¨äº†Kubernetesç¯å¢ƒä¸‹çš„è‡ªåŠ¨åŒ–è¿ç»´å’ŒSREï¼ˆç«™ç‚¹å¯é æ€§å·¥ç¨‹ï¼‰å®è·µï¼ŒåŸºäºå¤§å‹äº’è”ç½‘å…¬å¸çš„SREå®è·µç»éªŒï¼Œæä¾›ä»ç›‘æ§å‘Šè­¦ã€æ•…éšœå“åº”åˆ°è‡ªåŠ¨åŒ–è¿ç»´çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ä¼ä¸šå»ºç«‹é«˜æ•ˆã€å¯é çš„è¿ç»´ä½“ç³»ã€‚

## 1. SREæ ¸å¿ƒç†å¿µä¸åŸåˆ™

### 1.1 SREåŸºæœ¬åŸåˆ™

```yaml
SREä¸‰å¤§æ”¯æŸ±:
  1. å¯é æ€§å·¥ç¨‹ (Reliability Engineering)
     - SLI/SLO/Error Budgetç®¡ç†
     - æ•…éšœæ¨¡å¼åˆ†æ
     - å¯é æ€§é‡åŒ–æŒ‡æ ‡
  
  2. è‡ªåŠ¨åŒ–è¿ç»´ (Automation)
     - æ— äººå€¼å®ˆè¿ç»´
     - è‡ªæ„ˆèƒ½åŠ›
     - æ•…éšœè‡ªåŠ¨æ¢å¤
  
  3. é€Ÿåº¦ä¸ç¨³å®šæ€§å¹³è¡¡ (Speed vs Stability)
     - å¿«é€Ÿè¿­ä»£ä¸ç¨³å®šè¿è¡Œ
     - é”™è¯¯é¢„ç®—ç®¡ç†
     - é£é™©æ§åˆ¶ç­–ç•¥
```

### 1.2 SLI/SLO/Error Budgetç®¡ç†

```yaml
æ ¸å¿ƒSLIæŒ‡æ ‡:
  å¯ç”¨æ€§æŒ‡æ ‡:
    - APIå¯ç”¨æ€§: 99.95% (p99.9)
    - é¡µé¢åŠ è½½æ—¶é—´: < 2ç§’ (p95)
    - é”™è¯¯ç‡: < 0.05% (p99.9)
  
  æ€§èƒ½æŒ‡æ ‡:
    - APIå“åº”æ—¶é—´: < 100ms (p95)
    - æ•°æ®åº“æŸ¥è¯¢æ—¶é—´: < 50ms (p95)
    - ç³»ç»Ÿååé‡: > 10000 TPS
  
  åŠŸèƒ½æŒ‡æ ‡:
    - è®¤è¯æˆåŠŸç‡: 99.99%
    - æ”¯ä»˜æˆåŠŸç‡: 99.95%
    - æ•°æ®ä¸€è‡´æ€§: 99.99%
```

## 2. ç›‘æ§å‘Šè­¦ä½“ç³»å»ºè®¾

### 2.1 é»„é‡‘æŒ‡æ ‡ç›‘æ§

```yaml
REDæ–¹æ³• (Rate/Errors/Duration):
  Rate (é€Ÿç‡):
    - HTTPè¯·æ±‚é€Ÿç‡: requests_per_second
    - äº‹åŠ¡å¤„ç†é€Ÿç‡: transactions_per_second
    - æ¶ˆæ¯å¤„ç†é€Ÿç‡: messages_per_second
  
  Errors (é”™è¯¯):
    - HTTPé”™è¯¯ç‡: error_rate
    - ä¸šåŠ¡é”™è¯¯ç‡: business_error_rate
    - ç³»ç»Ÿé”™è¯¯ç‡: system_error_rate
  
  Duration (æŒç»­æ—¶é—´):
    - è¯·æ±‚å“åº”æ—¶é—´: response_time
    - äº‹åŠ¡å¤„ç†æ—¶é—´: transaction_time
    - é˜Ÿåˆ—ç­‰å¾…æ—¶é—´: queue_wait_time

USEæ–¹æ³• (Utilization/Saturation/Errors):
  Utilization (åˆ©ç”¨ç‡):
    - CPUåˆ©ç”¨ç‡: cpu_utilization
    - å†…å­˜åˆ©ç”¨ç‡: memory_utilization
    - ç£ç›˜åˆ©ç”¨ç‡: disk_utilization
  
  Saturation (é¥±å’Œåº¦):
    - é˜Ÿåˆ—é•¿åº¦: queue_length
    - è¿æ¥æ•°: connection_count
    - è´Ÿè½½: system_load
  
  Errors (é”™è¯¯):
    - ç¡¬ä»¶é”™è¯¯: hardware_errors
    - ç³»ç»Ÿé”™è¯¯: system_errors
    - åº”ç”¨é”™è¯¯: application_errors
```

### 2.2 ç›‘æ§ç³»ç»Ÿæ¶æ„

```yaml
# ç›‘æ§ç³»ç»Ÿæ¶æ„é…ç½®
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: main-prometheus
  namespace: monitoring
spec:
  serviceAccountName: prometheus
  serviceMonitorSelector: {}
  ruleSelector:
    matchLabels:
      role: alert-rules
  additionalScrapeConfigs:
    name: additional-scrape-configs
    key: prometheus-additional.yaml
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 500Gi
  resources:
    requests:
      memory: 4Gi
    limits:
      memory: 8Gi
  retention: 30d
  retentionSize: "100GB"
  walCompression: true
---
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: main-alertmanager
  namespace: monitoring
spec:
  replicas: 3
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: fast-ssd
        resources:
          requests:
            storage: 100Gi
  resources:
    requests:
      memory: 512Mi
    limits:
      memory: 1Gi
  configSecret: alertmanager-config
```

### 2.3 é«˜çº§å‘Šè­¦è§„åˆ™

```yaml
# é«˜çº§å‘Šè­¦è§„åˆ™é…ç½®
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sre-alert-rules
  namespace: monitoring
spec:
  groups:
  - name: availability.rules
    rules:
    # APIå¯ç”¨æ€§å‘Šè­¦
    - alert: APIAvailabilityLow
      expr: |
        1 - (sum(rate(http_requests_total{status=~"5..", handler!~"healthz|readyz"}[5m]))
        /
        sum(rate(http_requests_total{handler!~"healthz|readyz"}[5m]))) < 0.9995
      for: 5m
      labels:
        severity: critical
        category: availability
      annotations:
        summary: "APIå¯ç”¨æ€§ä½äºSLAè¦æ±‚ (å½“å‰: {{ $value }}%)"
        description: "APIå¯ç”¨æ€§æŒç»­5åˆ†é’Ÿä½äº99.95% SLAè¦æ±‚"
    
    # é”™è¯¯ç‡å‘Šè­¦
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{status=~"5.."}[5m])
        /
        rate(http_requests_total[5m]) > 0.01
      for: 2m
      labels:
        severity: warning
        category: errors
      annotations:
        summary: "é”™è¯¯ç‡è¶…è¿‡é˜ˆå€¼ (å½“å‰: {{ $value }}%)"
        description: "HTTPé”™è¯¯ç‡è¶…è¿‡1%ï¼Œå¯èƒ½å½±å“ç”¨æˆ·ä½“éªŒ"
    
    # å“åº”æ—¶é—´å‘Šè­¦
    - alert: HighResponseTime
      expr: |
        histogram_quantile(0.95, 
        sum by(le) (rate(http_request_duration_seconds_bucket[5m])))
        > 1.0
      for: 3m
      labels:
        severity: warning
        category: performance
      annotations:
        summary: "APIå“åº”æ—¶é—´è¶…è¿‡é˜ˆå€¼ (p95: {{ $value }}s)"
        description: "APIå“åº”æ—¶é—´p95è¶…è¿‡1ç§’ï¼Œå½±å“ç”¨æˆ·ä½“éªŒ"
  
  - name: infrastructure.rules
    rules:
    # èŠ‚ç‚¹èµ„æºå‘Šè­¦
    - alert: NodeHighCPU
      expr: |
        100 - (avg by(instance) (
        rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
      for: 10m
      labels:
        severity: warning
        category: infrastructure
      annotations:
        summary: "èŠ‚ç‚¹CPUä½¿ç”¨ç‡è¿‡é«˜ ({{ $labels.instance }}: {{ $value }}%)"
        description: "èŠ‚ç‚¹CPUä½¿ç”¨ç‡æŒç»­10åˆ†é’Ÿè¶…è¿‡85%"
    
    # å†…å­˜å‹åŠ›å‘Šè­¦
    - alert: NodeMemoryPressure
      expr: |
        (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 < 15
      for: 5m
      labels:
        severity: critical
        category: infrastructure
      annotations:
        summary: "èŠ‚ç‚¹å†…å­˜å‹åŠ›è¿‡å¤§ ({{ $labels.instance }}: {{ $value }}%)"
        description: "èŠ‚ç‚¹å¯ç”¨å†…å­˜ä¸è¶³15%ï¼Œå¯èƒ½å¯¼è‡´OOM"
    
    # ç£ç›˜ç©ºé—´å‘Šè­¦
    - alert: DiskSpaceCritical
      expr: |
        (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
      for: 10m
      labels:
        severity: critical
        category: infrastructure
      annotations:
        summary: "ç£ç›˜ç©ºé—´ä¸è¶³ ({{ $labels.mountpoint }}: {{ $value }}%)"
        description: "ç£ç›˜å¯ç”¨ç©ºé—´ä¸è¶³10%ï¼Œéœ€è¦åŠæ—¶æ¸…ç†"
```

## 3. è‡ªåŠ¨åŒ–è¿ç»´ç³»ç»Ÿ

### 3.1 è¿ç»´æœºå™¨äººå®ç°

```python
#!/usr/bin/env python3
# sre-automation-bot.py

import asyncio
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass
from kubernetes import client, config
from kubernetes.stream import stream
import requests
import time
from datetime import datetime, timedelta

@dataclass
class Incident:
    id: str
    severity: str
    summary: str
    description: str
    timestamp: datetime
    affected_services: List[str]

class SREAutomationBot:
    def __init__(self):
        config.load_incluster_config()
        self.v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.logger = logging.getLogger(__name__)
        self.alert_manager_url = "http://alertmanager.monitoring.svc:9093"
        self.chat_webhook_url = "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
    
    async def monitor_system_health(self):
        """ç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        while True:
            try:
                # æ£€æŸ¥å…³é”®ç»„ä»¶çŠ¶æ€
                await self.check_kubernetes_components()
                
                # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
                await self.check_node_health()
                
                # æ£€æŸ¥PodçŠ¶æ€
                await self.check_pod_health()
                
                # æ£€æŸ¥å­˜å‚¨çŠ¶æ€
                await self.check_storage_health()
                
                await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(60)
    
    async def check_kubernetes_components(self):
        """æ£€æŸ¥Kubernetesç»„ä»¶çŠ¶æ€"""
        try:
            components = self.v1.list_component_status().items
            for component in components:
                if component.conditions[0].status != "True":
                    self.logger.warning(f"ç»„ä»¶ {component.metadata.name} å¼‚å¸¸: {component.conditions[0].message}")
                    
                    # å‘é€å‘Šè­¦
                    incident = Incident(
                        id=f"component-{component.metadata.name}",
                        severity="critical",
                        summary=f"Kubernetesç»„ä»¶å¼‚å¸¸: {component.metadata.name}",
                        description=component.conditions[0].message,
                        timestamp=datetime.now(),
                        affected_services=["kubernetes-control-plane"]
                    )
                    await self.handle_incident(incident)
                    
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç»„ä»¶çŠ¶æ€å¤±è´¥: {e}")
    
    async def check_node_health(self):
        """æ£€æŸ¥èŠ‚ç‚¹å¥åº·çŠ¶æ€"""
        try:
            nodes = self.v1.list_node().items
            for node in nodes:
                for condition in node.status.conditions:
                    if condition.type == "Ready" and condition.status != "True":
                        self.logger.warning(f"èŠ‚ç‚¹ {node.metadata.name} ä¸å¥åº·: {condition.message}")
                        
                        # å°è¯•è‡ªåŠ¨ä¿®å¤
                        await self.attempt_node_repair(node)
                        
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥èŠ‚ç‚¹å¥åº·å¤±è´¥: {e}")
    
    async def check_pod_health(self):
        """æ£€æŸ¥Podå¥åº·çŠ¶æ€"""
        try:
            pods = self.v1.list_pod_for_all_namespaces().items
            for pod in pods:
                if pod.status.phase == "Failed":
                    self.logger.warning(f"Pod {pod.metadata.namespace}/{pod.metadata.name} å¤±è´¥")
                    
                    # å‘é€å‘Šè­¦
                    incident = Incident(
                        id=f"pod-{pod.metadata.namespace}-{pod.metadata.name}",
                        severity="high",
                        summary=f"Podå¼‚å¸¸: {pod.metadata.namespace}/{pod.metadata.name}",
                        description=f"PodçŠ¶æ€: {pod.status.phase}, åŸå› : {pod.status.reason}",
                        timestamp=datetime.now(),
                        affected_services=[pod.metadata.labels.get('app', 'unknown')]
                    )
                    await self.handle_incident(incident)
                    
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥Podå¥åº·å¤±è´¥: {e}")
    
    async def attempt_node_repair(self, node):
        """å°è¯•è‡ªåŠ¨ä¿®å¤èŠ‚ç‚¹"""
        try:
            # æ£€æŸ¥èŠ‚ç‚¹é—®é¢˜ç±»å‹
            for condition in node.status.conditions:
                if condition.type == "DiskPressure":
                    # æ¸…ç†èŠ‚ç‚¹ä¸Šçš„åƒåœ¾å®¹å™¨
                    await self.cleanup_node_containers(node.metadata.name)
                    
                elif condition.type == "MemoryPressure":
                    # é©±é€éƒ¨åˆ†Podé‡Šæ”¾å†…å­˜
                    await self.drain_node(node.metadata.name)
                    
                elif condition.type == "PIDPressure":
                    # é‡å¯èŠ‚ç‚¹ä¸Šçš„Pod
                    await self.restart_node_pods(node.metadata.name)
                    
        except Exception as e:
            self.logger.error(f"è‡ªåŠ¨ä¿®å¤èŠ‚ç‚¹å¤±è´¥: {e}")
    
    async def cleanup_node_containers(self, node_name):
        """æ¸…ç†èŠ‚ç‚¹ä¸Šçš„å®¹å™¨"""
        try:
            # è·å–èŠ‚ç‚¹ä¸Šçš„Podåˆ—è¡¨
            pods = self.v1.list_pod_for_all_namespaces(
                field_selector=f"spec.nodeName={node_name}"
            ).items
            
            for pod in pods:
                if pod.status.phase in ["Succeeded", "Failed"]:
                    # åˆ é™¤å·²å®Œæˆçš„Pod
                    self.v1.delete_namespaced_pod(
                        pod.metadata.name,
                        pod.metadata.namespace
                    )
                    
        except Exception as e:
            self.logger.error(f"æ¸…ç†èŠ‚ç‚¹å®¹å™¨å¤±è´¥: {e}")
    
    async def drain_node(self, node_name):
        """é©±é€èŠ‚ç‚¹ä¸Šçš„Pod"""
        try:
            # æ ‡è®°èŠ‚ç‚¹ä¸å¯è°ƒåº¦
            body = {
                "spec": {
                    "unschedulable": True
                }
            }
            self.v1.patch_node(node_name, body)
            
            # é©±é€èŠ‚ç‚¹ä¸Šçš„Pod
            pods = self.v1.list_pod_for_all_namespaces(
                field_selector=f"spec.nodeName={node_name}"
            ).items
            
            for pod in pods:
                if pod.metadata.labels.get('critical', 'false') != 'true':
                    # åˆ é™¤éå…³é”®Pod
                    self.v1.delete_namespaced_pod(
                        pod.metadata.name,
                        pod.metadata.namespace,
                        grace_period_seconds=30
                    )
                    
        except Exception as e:
            self.logger.error(f"é©±é€èŠ‚ç‚¹Podå¤±è´¥: {e}")
    
    async def restart_node_pods(self, node_name):
        """é‡å¯èŠ‚ç‚¹ä¸Šçš„Pod"""
        try:
            pods = self.v1.list_pod_for_all_namespaces(
                field_selector=f"spec.nodeName={node_name}"
            ).items
            
            for pod in pods:
                # ä¼˜é›…åˆ é™¤Podï¼Œè®©å…¶é‡æ–°è°ƒåº¦
                self.v1.delete_namespaced_pod(
                    pod.metadata.name,
                    pod.metadata.namespace,
                    grace_period_seconds=30
                )
                
        except Exception as e:
            self.logger.error(f"é‡å¯èŠ‚ç‚¹Podå¤±è´¥: {e}")
    
    async def handle_incident(self, incident: Incident):
        """å¤„ç†å‘Šè­¦äº‹ä»¶"""
        # è®°å½•å‘Šè­¦
        self.logger.info(f"å¤„ç†å‘Šè­¦: {incident.summary}")
        
        # å‘é€åˆ°å‘Šè­¦ç³»ç»Ÿ
        await self.send_alert_to_monitoring(incident)
        
        # å‘é€åˆ°èŠå¤©ç³»ç»Ÿ
        await self.send_alert_to_chat(incident)
        
        # æ ¹æ®ä¸¥é‡ç¨‹åº¦æ‰§è¡Œè‡ªåŠ¨åŒ–ä¿®å¤
        if incident.severity == "critical":
            await self.execute_emergency_procedures(incident)
        elif incident.severity == "high":
            await self.execute_high_priority_procedures(incident)
    
    async def send_alert_to_monitoring(self, incident: Incident):
        """å‘é€å‘Šè­¦åˆ°ç›‘æ§ç³»ç»Ÿ"""
        try:
            alert = {
                "labels": {
                    "alertname": incident.id,
                    "severity": incident.severity,
                    "summary": incident.summary
                },
                "annotations": {
                    "description": incident.description,
                    "timestamp": incident.timestamp.isoformat()
                }
            }
            
            response = requests.post(
                f"{self.alert_manager_url}/api/v2/alerts",
                json=[alert],
                timeout=10
            )
            
            if response.status_code != 200:
                self.logger.error(f"å‘é€å‘Šè­¦å¤±è´¥: {response.text}")
                
        except Exception as e:
            self.logger.error(f"å‘é€å‘Šè­¦åˆ°ç›‘æ§ç³»ç»Ÿå¤±è´¥: {e}")
    
    async def send_alert_to_chat(self, incident: Incident):
        """å‘é€å‘Šè­¦åˆ°èŠå¤©ç³»ç»Ÿ"""
        try:
            message = {
                "text": f"ğŸš¨ {incident.severity.upper()} å‘Šè­¦: {incident.summary}",
                "attachments": [
                    {
                        "color": "danger" if incident.severity == "critical" else "warning",
                        "fields": [
                            {
                                "title": "æè¿°",
                                "value": incident.description,
                                "short": False
                            },
                            {
                                "title": "æ—¶é—´",
                                "value": incident.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                                "short": True
                            },
                            {
                                "title": "å—å½±å“æœåŠ¡",
                                "value": ", ".join(incident.affected_services),
                                "short": True
                            }
                        ]
                    }
                ]
            }
            
            response = requests.post(
                self.chat_webhook_url,
                json=message,
                timeout=10
            )
            
            if response.status_code != 200:
                self.logger.error(f"å‘é€å‘Šè­¦åˆ°èŠå¤©ç³»ç»Ÿå¤±è´¥: {response.text}")
                
        except Exception as e:
            self.logger.error(f"å‘é€å‘Šè­¦åˆ°èŠå¤©ç³»ç»Ÿå¤±è´¥: {e}")
    
    async def execute_emergency_procedures(self, incident: Incident):
        """æ‰§è¡Œç´§æ€¥å¤„ç†ç¨‹åº"""
        try:
            self.logger.info(f"æ‰§è¡Œç´§æ€¥å¤„ç†ç¨‹åº: {incident.id}")
            
            # 1. é€šçŸ¥å€¼ç­äººå‘˜
            await self.notify_oncall_team(incident)
            
            # 2. æ‰§è¡Œåº”æ€¥æ¢å¤
            await self.perform_emergency_recovery(incident)
            
            # 3. å¯åŠ¨å¤‡ç”¨ç³»ç»Ÿ
            await self.activate_backup_systems(incident)
            
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œç´§æ€¥å¤„ç†ç¨‹åºå¤±è´¥: {e}")
    
    async def execute_high_priority_procedures(self, incident: Incident):
        """æ‰§è¡Œé«˜ä¼˜å…ˆçº§å¤„ç†ç¨‹åº"""
        try:
            self.logger.info(f"æ‰§è¡Œé«˜ä¼˜å…ˆçº§å¤„ç†ç¨‹åº: {incident.id}")
            
            # 1. æ£€æŸ¥æ˜¯å¦æœ‰è‡ªåŠ¨ä¿®å¤æ–¹æ¡ˆ
            await self.attempt_automated_fix(incident)
            
            # 2. é€šçŸ¥ç›¸å…³äººå‘˜
            await self.notify_team_members(incident)
            
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œé«˜ä¼˜å…ˆçº§å¤„ç†ç¨‹åºå¤±è´¥: {e}")
    
    async def attempt_automated_fix(self, incident: Incident):
        """å°è¯•è‡ªåŠ¨åŒ–ä¿®å¤"""
        try:
            # æ ¹æ®å‘Šè­¦ç±»å‹æ‰§è¡Œä¸åŒçš„ä¿®å¤ç­–ç•¥
            if "pod" in incident.id and "Failed" in incident.description:
                # é‡å¯å¤±è´¥çš„Pod
                parts = incident.id.split('-')
                if len(parts) >= 4:
                    namespace = parts[2]
                    pod_name = '-'.join(parts[3:])
                    
                    try:
                        self.v1.delete_namespaced_pod(pod_name, namespace)
                        self.logger.info(f"å·²åˆ é™¤å¤±è´¥çš„Pod: {namespace}/{pod_name}")
                    except:
                        pass
            
            elif "memory" in incident.summary.lower():
                # é‡å¯ç›¸å…³Podé‡Šæ”¾å†…å­˜
                await self.restart_related_pods(incident)
                
        except Exception as e:
            self.logger.error(f"è‡ªåŠ¨åŒ–ä¿®å¤å¤±è´¥: {e}")
    
    async def restart_related_pods(self, incident: Incident):
        """é‡å¯ç›¸å…³Pod"""
        try:
            for service in incident.affected_services:
                pods = self.v1.list_pod_for_all_namespaces(
                    label_selector=f"app={service}"
                ).items
                
                for pod in pods:
                    self.v1.delete_namespaced_pod(
                        pod.metadata.name,
                        pod.metadata.namespace
                    )
                    
        except Exception as e:
            self.logger.error(f"é‡å¯ç›¸å…³Podå¤±è´¥: {e}")

async def main():
    bot = SREAutomationBot()
    await bot.monitor_system_health()

if __name__ == "__main__":
    asyncio.run(main())
```

### 3.2 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

```bash
#!/bin/bash
# sre-automation-scripts.sh

# SREè‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬é›†

# 1. é›†ç¾¤å¥åº·æ£€æŸ¥
check_cluster_health() {
    echo "=== é›†ç¾¤å¥åº·æ£€æŸ¥ ==="
    
    # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
    echo "1. æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€:"
    kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type,AGE:.metadata.creationTimestamp
    
    # æ£€æŸ¥å…³é”®ç»„ä»¶
    echo "2. æ£€æŸ¥å…³é”®ç»„ä»¶:"
    kubectl get componentstatuses
    
    # æ£€æŸ¥ç³»ç»ŸPod
    echo "3. æ£€æŸ¥ç³»ç»ŸPodçŠ¶æ€:"
    kubectl get pods -n kube-system --field-selector=status.phase!=Running
    
    # æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
    echo "4. æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ:"
    kubectl top nodes --sort-by=cpu
    kubectl top nodes --sort-by=memory
}

# 2. è‡ªåŠ¨æ‰©å®¹è„šæœ¬
auto_scale_nodes() {
    echo "=== è‡ªåŠ¨èŠ‚ç‚¹æ‰©å®¹ ==="
    
    # è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
    CPU_USAGE=$(kubectl top nodes --no-headers | awk '{sum+=$3} END {print sum/NR}' | sed 's/%//')
    MEMORY_USAGE=$(kubectl top nodes --no-headers | awk '{sum+=$5} END {print sum/NR}' | sed 's/%//')
    
    echo "å½“å‰CPUå¹³å‡ä½¿ç”¨ç‡: $CPU_USAGE%"
    echo "å½“å‰å†…å­˜å¹³å‡ä½¿ç”¨ç‡: $MEMORY_USAGE%"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å®¹
    if [ "$CPU_USAGE" -gt 80 ] || [ "$MEMORY_USAGE" -gt 80 ]; then
        echo "èµ„æºä½¿ç”¨ç‡è¿‡é«˜ï¼Œæ‰§è¡Œæ‰©å®¹..."
        
        # è·å–èŠ‚ç‚¹æ± åç§°
        NODE_POOL=$(kubectl get nodes -o jsonpath='{.items[0].metadata.labels.node\.kubernetes\.io/instance-type}')
        
        # æ‰©å®¹èŠ‚ç‚¹æ±  (å‡è®¾ä½¿ç”¨eksctl)
        # eksctl scale nodegroup --cluster=your-cluster --name=$NODE_POOL --nodes=10
    fi
}

# 3. æ•…éšœæ¢å¤è„šæœ¬
recover_failed_deployments() {
    echo "=== æ•…éšœéƒ¨ç½²æ¢å¤ ==="
    
    # æŸ¥æ‰¾å¤±è´¥çš„Deployment
    FAILED_DEPLOYMENTS=$(kubectl get deployments --all-namespaces -o json | \
        jq -r '.items[] | select(.status.conditions[]?.type=="Progressing" and .status.conditions[]?.status=="False") | "\(.metadata.namespace)/\(.metadata.name)"')
    
    if [ -z "$FAILED_DEPLOYMENTS" ]; then
        echo "æ²¡æœ‰å‘ç°å¤±è´¥çš„éƒ¨ç½²"
        return
    fi
    
    echo "å‘ç°å¤±è´¥çš„éƒ¨ç½²:"
    echo "$FAILED_DEPLOYMENTS"
    
    # å¯¹æ¯ä¸ªå¤±è´¥çš„éƒ¨ç½²æ‰§è¡Œå›æ»š
    for deployment in $FAILED_DEPLOYMENTS; do
        echo "æ­£åœ¨å›æ»š $deployment..."
        kubectl rollout undo deployment/$deployment
        sleep 5
    done
}

# 4. èµ„æºæ¸…ç†è„šæœ¬
cleanup_resources() {
    echo "=== èµ„æºæ¸…ç† ==="
    
    # æ¸…ç†å·²å®Œæˆçš„Job
    COMPLETED_JOBS=$(kubectl get jobs --all-namespaces -o json | \
        jq -r '.items[] | select(.status.succeeded > 0) | "\(.metadata.namespace)/\(.metadata.name)"')
    
    if [ -n "$COMPLETED_JOBS" ]; then
        echo "æ¸…ç†å·²å®Œæˆçš„Jobs:"
        for job in $COMPLETED_JOBS; do
            echo "åˆ é™¤ $job"
            kubectl delete job $job
        done
    fi
    
    # æ¸…ç†å¤±è´¥çš„Pod
    FAILED_PODS=$(kubectl get pods --all-namespaces -o json | \
        jq -r '.items[] | select(.status.phase=="Failed") | "\(.metadata.namespace)/\(.metadata.name)"')
    
    if [ -n "$FAILED_PODS" ]; then
        echo "æ¸…ç†å¤±è´¥çš„Pods:"
        for pod in $FAILED_PODS; do
            echo "åˆ é™¤ $pod"
            kubectl delete pod $pod
        done
    fi
    
    # æ¸…ç†æœªä½¿ç”¨çš„ConfigMap
    kubectl get configmaps --all-namespaces --no-headers | \
        while read namespace name created; do
            if ! kubectl get pods -n $namespace -o yaml | grep -q $name; then
                echo "åˆ é™¤æœªä½¿ç”¨çš„ConfigMap: $namespace/$name"
                kubectl delete configmap $name -n $namespace
            fi
        done
}

# 5. æ€§èƒ½ä¼˜åŒ–è„šæœ¬
optimize_performance() {
    echo "=== æ€§èƒ½ä¼˜åŒ– ==="
    
    # æ£€æŸ¥èµ„æºè¯·æ±‚å’Œé™åˆ¶
    echo "æ£€æŸ¥èµ„æºè¯·æ±‚/é™åˆ¶é…ç½®..."
    
    # åˆ†æPodèµ„æºä½¿ç”¨æƒ…å†µ
    kubectl top pods --all-namespaces --containers
    
    # ç”Ÿæˆèµ„æºä¼˜åŒ–å»ºè®®
    kubectl get pods --all-namespaces -o json | \
        jq -r '.items[] | select(.spec.containers[].resources.requests) | 
        "\(.metadata.namespace)/\(.metadata.name):\(.spec.containers[].name)"'
}

# 6. å¤‡ä»½è„šæœ¬
backup_cluster_config() {
    echo "=== é›†ç¾¤é…ç½®å¤‡ä»½ ==="
    
    BACKUP_DIR="/tmp/cluster-backup-$(date +%Y%m%d_%H%M%S)"
    mkdir -p $BACKUP_DIR
    
    # å¤‡ä»½å‘½åç©ºé—´
    kubectl get namespaces -o yaml > $BACKUP_DIR/namespaces.yaml
    
    # å¤‡ä»½å­˜å‚¨ç±»
    kubectl get storageclasses -o yaml > $BACKUP_DIR/storageclasses.yaml
    
    # å¤‡ä»½RBACé…ç½®
    kubectl get clusterroles,clusterrolebindings,roles,rolebindings --all-namespaces -o yaml > $BACKUP_DIR/rbac.yaml
    
    # å¤‡ä»½å…³é”®é…ç½®
    kubectl get configmaps,secrets --all-namespaces -o yaml > $BACKUP_DIR/configs.yaml
    
    # å¤‡ä»½éƒ¨ç½²é…ç½®
    kubectl get deployments,statefulsets,daemonsets --all-namespaces -o yaml > $BACKUP_DIR/deployments.yaml
    
    echo "å¤‡ä»½å®Œæˆ: $BACKUP_DIR"
}

# ä¸»å‡½æ•°
main() {
    case "$1" in
        health-check)
            check_cluster_health
            ;;
        auto-scale)
            auto_scale_nodes
            ;;
        recover-failed)
            recover_failed_deployments
            ;;
        cleanup)
            cleanup_resources
            ;;
        optimize)
            optimize_performance
            ;;
        backup)
            backup_cluster_config
            ;;
        all)
            check_cluster_health
            auto_scale_nodes
            cleanup_resources
            ;;
        *)
            echo "ç”¨æ³•: $0 {health-check|auto-scale|recover-failed|cleanup|optimize|backup|all}"
            exit 1
            ;;
    esac
}

main "$@"
```

## 4. æ•…éšœå“åº”ä¸æ¢å¤

### 4.1 æ•…éšœå“åº”æµç¨‹

```yaml
# æ•…éšœå“åº”æµç¨‹é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: incident-response-playbook
  namespace: sre
data:
  critical-incident.yaml: |
    # å…³é”®æ•…éšœå“åº”æ‰‹å†Œ
    incident_types:
      api_outage:
        severity: critical
        response_time: 5m
        escalation_path:
          - level: 1
            role: oncall_engineer
            contact: pagerduty
            timeout: 15m
          - level: 2
            role: sre_lead
            contact: phone
            timeout: 30m
          - level: 3
            role: engineering_vp
            contact: phone
            timeout: 60m
        
        immediate_actions:
          - check_api_health
          - restart_api_pods
          - switch_to_backup_region
          - engage_database_team
        
        rollback_procedure:
          - rollback_recent_deployments
          - restore_from_backup
          - scale_down_traffic
        
        postmortem_requirements:
          - timeline_documentation
          - root_cause_analysis
          - action_items_definition
          - follow_up_meeting_schedule
      
      database_outage:
        severity: critical
        response_time: 3m
        escalation_path:
          - level: 1
            role: dba_oncall
            contact: pagerduty
            timeout: 10m
          - level: 2
            role: sre_engineer
            contact: phone
            timeout: 20m
          - level: 3
            role: database_architect
            contact: phone
            timeout: 40m
        
        immediate_actions:
          - check_database_connectivity
          - restart_database_pods
          - failover_to_replica
          - engage_infrastructure_team
        
        rollback_procedure:
          - restore_from_recent_backup
          - replay_transaction_logs
          - verify_data_consistency
```

### 4.2 è‡ªæ„ˆç³»ç»Ÿé…ç½®

```yaml
# è‡ªæ„ˆç³»ç»Ÿé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: self-healing-config
  namespace: sre
data:
  healing-rules.yaml: |
    # è‡ªæ„ˆè§„åˆ™é…ç½®
    healing_rules:
      pod_crash_loop:
        condition: "pod.status.phase == 'Failed' or pod.status.containerStatuses[*].restartCount > 5"
        action: "delete_pod_and_reschedule"
        cooldown: 300s
        max_attempts: 3
      
      node_unresponsive:
        condition: "node.status.conditions[Ready].status == 'False' for 5m"
        action: "cordon_and_drain_node"
        cooldown: 600s
        max_attempts: 1
      
      high_cpu_usage:
        condition: "node.cpu_usage > 85% for 10m"
        action: "scale_up_node_pool"
        cooldown: 300s
        max_attempts: 2
      
      high_memory_usage:
        condition: "pod.memory_usage > 90% for 5m"
        action: "restart_pod"
        cooldown: 120s
        max_attempts: 3
      
      service_unavailable:
        condition: "service.endpoint_count == 0 for 2m"
        action: "restart_deployment"
        cooldown: 300s
        max_attempts: 2
```

## 5. å˜æ›´ç®¡ç†ä¸å‘å¸ƒ

### 5.1 å˜æ›´ç®¡ç†æµç¨‹

```yaml
# å˜æ›´ç®¡ç†é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: change-management-config
  namespace: sre
data:
  change-process.yaml: |
    # å˜æ›´ç®¡ç†æµç¨‹
    change_categories:
      emergency:
        approval_level: engineering_director
        review_time: immediate
        deployment_window: anytime
        rollback_plan: mandatory
      
      standard:
        approval_level: team_lead
        review_time: 24h
        deployment_window: maintenance_window
        rollback_plan: required
      
      low_risk:
        approval_level: peer_review
        review_time: 2h
        deployment_window: business_hours
        rollback_plan: optional
    
    deployment_windows:
      maintenance:
        days: ["Saturday", "Sunday"]
        time: "02:00-06:00 UTC"
        duration: 4h
      
      business_hours:
        days: ["Monday-Friday"]
        time: "09:00-17:00 UTC"
        duration: 8h
    
    approval_workflow:
      - stage: code_review
        approvers: ["senior_engineer", "tech_lead"]
        criteria: ["all_tests_passed", "security_scan_passed"]
      
      - stage: qa_approval
        approvers: ["qa_lead"]
        criteria: ["feature_tested", "regression_tests_passed"]
      
      - stage: ops_approval
        approvers: ["sre_lead"]
        criteria: ["resource_impact_approved", "rollback_plan_verified"]
```

### 5.2 å‘å¸ƒç­–ç•¥é…ç½®

```yaml
# å‘å¸ƒç­–ç•¥é…ç½®
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: progressive-rollout
  namespace: production
spec:
  replicas: 10
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:v2.0.0
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"
  strategy:
    blueGreen:
      activeService: myapp-active
      previewService: myapp-preview
      autoPromotionEnabled: false
      autoPromotionSeconds: 600
      scaleDownDelaySeconds: 30
      prePromotionAnalysis:
        templates:
        - templateName: success-rate
        args:
        - name: service-name
          value: myapp-preview
      postPromotionAnalysis:
        templates:
        - templateName: success-rate
        args:
        - name: service-name
          value: myapp-active
---
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: success-rate
spec:
  args:
  - name: service-name
  metrics:
  - name: success-rate
    interval: 30s
    count: 20
    successCondition: result[0] >= 0.95
    provider:
      prometheus:
        address: http://prometheus-server.monitoring.svc.cluster.local:9090
        query: |
          1 - (
            sum(rate(http_requests_total{service="{{args.service-name}}", code=~"5.."}[5m]))
          /
            sum(rate(http_requests_total{service="{{args.service-name}}"}[5m]))
          )
```

## 6. å®¹é‡è§„åˆ’ä¸æ€§èƒ½ä¼˜åŒ–

### 6.1 å®¹é‡è§„åˆ’å·¥å…·

```python
#!/usr/bin/env python3
# capacity-planning.py

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import warnings
warnings.filterwarnings('ignore')

class CapacityPlanner:
    def __init__(self):
        self.data_history = []
        self.planning_horizon = 90  # 90å¤©è§„åˆ’æœŸ
    
    def collect_resource_usage(self):
        """æ”¶é›†èµ„æºä½¿ç”¨æ•°æ®"""
        import subprocess
        import json
        
        # æ”¶é›†èŠ‚ç‚¹èµ„æºä½¿ç”¨
        cmd = "kubectl top nodes --no-headers"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        node_data = []
        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split()
                if len(parts) >= 5:
                    node_data.append({
                        'timestamp': datetime.now(),
                        'node': parts[0],
                        'cpu_cores': int(parts[1].rstrip('m')),
                        'cpu_percent': int(parts[2].rstrip('%')),
                        'memory_bytes': self.parse_memory(parts[3]),
                        'memory_percent': int(parts[4].rstrip('%'))
                    })
        
        return node_data
    
    def parse_memory(self, mem_str):
        """è§£æå†…å­˜å­—ç¬¦ä¸²"""
        if mem_str.endswith('Ki'):
            return float(mem_str[:-2]) * 1024
        elif mem_str.endswith('Mi'):
            return float(mem_str[:-2]) * 1024 * 1024
        elif mem_str.endswith('Gi'):
            return float(mem_str[:-2]) * 1024 * 1024 * 1024
        else:
            return float(mem_str)
    
    def predict_resource_needs(self, historical_data, days_ahead=90):
        """é¢„æµ‹æœªæ¥èµ„æºéœ€æ±‚"""
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(historical_data)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['days_since_start'] = (df['timestamp'] - df['timestamp'].min()).dt.days
        
        # æŒ‰å¤©èšåˆæ•°æ®
        daily_agg = df.groupby('days_since_start').agg({
            'cpu_percent': 'mean',
            'memory_percent': 'mean'
        }).reset_index()
        
        # ä½¿ç”¨å¤šé¡¹å¼å›å½’è¿›è¡Œé¢„æµ‹
        X = daily_agg[['days_since_start']].values
        y_cpu = daily_agg['cpu_percent'].values
        y_memory = daily_agg['memory_percent'].values
        
        # CPUé¢„æµ‹
        poly_features = PolynomialFeatures(degree=2)
        X_poly = poly_features.fit_transform(X)
        
        model_cpu = LinearRegression()
        model_cpu.fit(X_poly, y_cpu)
        
        model_memory = LinearRegression()
        model_memory.fit(X_poly, y_memory)
        
        # é¢„æµ‹æœªæ¥æ•°æ®
        future_days = np.arange(X.max() + 1, X.max() + days_ahead + 1).reshape(-1, 1)
        future_days_poly = poly_features.transform(future_days)
        
        future_cpu = model_cpu.predict(future_days_poly)
        future_memory = model_memory.predict(future_days_poly)
        
        return {
            'days': future_days.flatten(),
            'predicted_cpu': future_cpu,
            'predicted_memory': future_memory
        }
    
    def calculate_scaling_recommendations(self, predictions, current_capacity):
        """è®¡ç®—æ‰©å®¹å»ºè®®"""
        cpu_threshold = 80  # CPUä½¿ç”¨ç‡é˜ˆå€¼
        memory_threshold = 80  # å†…å­˜ä½¿ç”¨ç‡é˜ˆå€¼
        
        recommendations = []
        
        for i, day in enumerate(predictions['days']):
            cpu_needed = predictions['predicted_cpu'][i]
            memory_needed = predictions['predicted_memory'][i]
            
            if cpu_needed > cpu_threshold or memory_needed > memory_threshold:
                # è®¡ç®—éœ€è¦çš„å®¹é‡å¢é•¿
                cpu_growth = max(0, (cpu_needed - cpu_threshold) / cpu_threshold)
                memory_growth = max(0, (memory_needed - memory_threshold) / memory_threshold)
                
                growth_factor = max(cpu_growth, memory_growth) * 1.2  # 20%ç¼“å†²
                
                recommendations.append({
                    'day': day,
                    'date': datetime.now() + timedelta(days=int(day)),
                    'growth_factor': growth_factor,
                    'reason': f"CPU: {cpu_needed:.1f}%, Memory: {memory_needed:.1f}%"
                })
        
        return recommendations
    
    def generate_capacity_report(self, recommendations, current_capacity):
        """ç”Ÿæˆå®¹é‡è§„åˆ’æŠ¥å‘Š"""
        report = {
            'report_date': datetime.now().isoformat(),
            'current_capacity': current_capacity,
            'recommendations': recommendations,
            'summary': {
                'total_recommendations': len(recommendations),
                'earliest_recommendation': min([r['date'] for r in recommendations]) if recommendations else None,
                'max_growth_factor': max([r['growth_factor'] for r in recommendations]) if recommendations else 0
            }
        }
        
        return report
    
    def plot_predictions(self, predictions, recommendations):
        """ç»˜åˆ¶é¢„æµ‹å›¾è¡¨"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # CPUé¢„æµ‹
        ax1.plot(predictions['days'], predictions['predicted_cpu'], label='Predicted CPU Usage', color='blue')
        ax1.axhline(y=80, color='red', linestyle='--', label='Threshold (80%)')
        ax1.set_title('CPU Usage Prediction')
        ax1.set_xlabel('Days from Now')
        ax1.set_ylabel('CPU Usage (%)')
        ax1.legend()
        ax1.grid(True)
        
        # å†…å­˜é¢„æµ‹
        ax2.plot(predictions['days'], predictions['predicted_memory'], label='Predicted Memory Usage', color='green')
        ax2.axhline(y=80, color='red', linestyle='--', label='Threshold (80%)')
        ax2.set_title('Memory Usage Prediction')
        ax2.set_xlabel('Days from Now')
        ax2.set_ylabel('Memory Usage (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('/tmp/capacity_prediction.png')
        plt.show()

if __name__ == "__main__":
    planner = CapacityPlanner()
    
    # æ¨¡æ‹Ÿå†å²æ•°æ®
    historical_data = [
        {'timestamp': datetime.now() - timedelta(days=i), 'cpu_percent': 60 + i*0.5, 'memory_percent': 55 + i*0.3}
        for i in range(30, 0, -1)
    ]
    
    # é¢„æµ‹æœªæ¥éœ€æ±‚
    predictions = planner.predict_resource_needs(historical_data)
    
    # ç”Ÿæˆæ‰©å®¹å»ºè®®
    current_capacity = {'nodes': 10, 'cpu_cores': 100, 'memory_gb': 400}
    recommendations = planner.calculate_scaling_recommendations(predictions, current_capacity)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = planner.generate_capacity_report(recommendations, current_capacity)
    
    print("å®¹é‡è§„åˆ’æŠ¥å‘Š:")
    print(f"å½“å‰å®¹é‡: {report['current_capacity']}")
    print(f"å»ºè®®æ‰©å®¹æ¬¡æ•°: {report['summary']['total_recommendations']}")
    
    for rec in recommendations[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå»ºè®®
        print(f"  {rec['date'].strftime('%Y-%m-%d')}: å»ºè®®æ‰©å®¹ {rec['growth_factor']:.2f}x ({rec['reason']})")
```

## 7. æœ€ä½³å®è·µä¸å®æ–½æŒ‡å—

### 7.1 SREå®æ–½åŸåˆ™

```markdown
## âš™ï¸ SREå®æ–½åŸåˆ™

### 1. å¯é æ€§ä¼˜å…ˆ
- ä»¥SLI/SLOä¸ºè¡¡é‡æ ‡å‡†
- å»ºç«‹é”™è¯¯é¢„ç®—ç®¡ç†æœºåˆ¶
- å¹³è¡¡æ–°åŠŸèƒ½å¼€å‘ä¸ç³»ç»Ÿç¨³å®šæ€§

### 2. è‡ªåŠ¨åŒ–é©±åŠ¨
- ç”¨ä»£ç ç®¡ç†è¿ç»´ä»»åŠ¡
- å®æ–½æ— äººå€¼å®ˆè¿ç»´
- å»ºç«‹è‡ªæ„ˆèƒ½åŠ›

### 3. æŒç»­æ”¹è¿›
- å®šæœŸè¿›è¡Œäº‹ååˆ†æ
- ä»æ•…éšœä¸­å­¦ä¹ æ”¹è¿›
- æŒç»­ä¼˜åŒ–ç³»ç»Ÿæ¶æ„

### 4. æ–‡åŒ–å»ºè®¾
- å»ºç«‹å­¦ä¹ å‹ç»„ç»‡
- é¼“åŠ±å®éªŒå’Œåˆ›æ–°
- è¥é€ æ— æŒ‡è´£æ–‡åŒ–
```

### 7.2 å®æ–½æ£€æŸ¥æ¸…å•

```yaml
SREå®æ–½æ£€æŸ¥æ¸…å•:
  ç›‘æ§å‘Šè­¦:
    â˜ æ ¸å¿ƒæŒ‡æ ‡ç›‘æ§é…ç½®
    â˜ SLI/SLOå®šä¹‰å®Œæˆ
    â˜ å‘Šè­¦è§„åˆ™é…ç½®å®Œæˆ
    â˜ å‘Šè­¦é€šçŸ¥æ¸ é“å»ºç«‹
  
  è‡ªåŠ¨åŒ–è¿ç»´:
    â˜ åŸºç¡€è®¾æ–½å³ä»£ç å®æ–½
    â˜ è‡ªåŠ¨éƒ¨ç½²æµæ°´çº¿å»ºç«‹
    â˜ è‡ªæ„ˆç³»ç»Ÿé…ç½®å®Œæˆ
    â˜ å¤‡ä»½æ¢å¤æœºåˆ¶å»ºç«‹
  
  æ•…éšœå“åº”:
    â˜ å€¼ç­åˆ¶åº¦å»ºç«‹
    â˜ å“åº”æµç¨‹æ–‡æ¡£åŒ–
    â˜ åº”æ€¥é¢„æ¡ˆåˆ¶å®š
    â˜ æ¼”ç»ƒæœºåˆ¶å»ºç«‹
  
  å˜æ›´ç®¡ç†:
    â˜ å˜æ›´å®¡æ‰¹æµç¨‹å»ºç«‹
    â˜ å‘å¸ƒç­–ç•¥åˆ¶å®š
    â˜ å›æ»šæœºåˆ¶é…ç½®
    â˜ æµ‹è¯•éªŒè¯æµç¨‹å»ºç«‹
  
  å®¹é‡è§„åˆ’:
    â˜ èµ„æºä½¿ç”¨ç›‘æ§å»ºç«‹
    â˜ å®¹é‡é¢„æµ‹æ¨¡å‹å»ºç«‹
    â˜ æ‰©å®¹ç­–ç•¥åˆ¶å®š
    â˜ æˆæœ¬ä¼˜åŒ–æœºåˆ¶å»ºç«‹
```

## 8. æœªæ¥å‘å±•è¶‹åŠ¿

### 8.1 æ™ºèƒ½åŒ–è¿ç»´

```yaml
SREæ™ºèƒ½åŒ–è¶‹åŠ¿:
  1. AIé©±åŠ¨çš„å¼‚å¸¸æ£€æµ‹
     - æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹
     - é¢„æµ‹æ€§æ•…éšœé¢„é˜²
     - æ™ºèƒ½å®¹é‡è§„åˆ’
  
  2. è‡ªä¸»è¿ç»´ç³»ç»Ÿ
     - æ— äººé©¾é©¶è¿ç»´
     - è‡ªé€‚åº”ç³»ç»Ÿè°ƒä¼˜
     - æ™ºèƒ½å†³ç­–æ”¯æŒ
  
  3. æ··åˆäº‘SRE
     - å¤šäº‘ç»Ÿä¸€è¿ç»´
     - è·¨äº‘æ•…éšœå“åº”
     - ç»Ÿä¸€ç›‘æ§æ²»ç†
```

---
*æœ¬æ–‡æ¡£åŸºäºä¼ä¸šçº§SREå®è·µç»éªŒç¼–å†™ï¼ŒæŒç»­æ›´æ–°æœ€æ–°æŠ€æœ¯å’Œæœ€ä½³å®è·µã€‚*
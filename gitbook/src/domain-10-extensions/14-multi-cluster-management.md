# 14 - å¤šé›†ç¾¤ç®¡ç†ä¸è”é‚¦ (Multi-Cluster Management & Federation)

> **é€‚ç”¨ç‰ˆæœ¬**: v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **å‚è€ƒ**: [kubernetes.io/docs/concepts/architecture/multicluster](https://kubernetes.io/docs/concepts/architecture/multicluster/)

## å¤šé›†ç¾¤æ¶æ„æ¨¡å¼

### é›†ç¾¤ç®¡ç†æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | é€‚ç”¨åœºæ™¯ | ç®¡ç†å¤æ‚åº¦ | æ•°æ®åŒæ­¥ | ç½‘ç»œè¦æ±‚ |
|------|----------|------------|----------|----------|
| **ç‹¬ç«‹é›†ç¾¤** | å¼€å‘æµ‹è¯•ç¯å¢ƒ | ä½ | æ—  | ç‹¬ç«‹ç½‘ç»œ |
| **é›†ç¾¤è”é‚¦** | å¤šåœ°åŸŸéƒ¨ç½² | ä¸­ | æœ‰é™åŒæ­¥ | è·¨åŒºåŸŸç½‘ç»œ |
| **é›†ç¾¤æ³¨å†Œä¸­å¿ƒ** | ç»Ÿä¸€ç®¡ç† | é«˜ | é›†ä¸­è§†å›¾ | ç½‘ç»œå¯è¾¾ |
| **è™šæ‹Ÿé›†ç¾¤** | ç§Ÿæˆ·éš”ç¦» | ä¸­ | å®Œå…¨éš”ç¦» | å…±äº«åº•å±‚ |

### ç”Ÿäº§ç¯å¢ƒå¤šé›†ç¾¤æ‹“æ‰‘

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           å¤šé›†ç¾¤ç®¡ç†æ¶æ„                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  ç®¡ç†æ§åˆ¶å¹³é¢    â”‚    â”‚  æ³¨å†Œä¸­å¿ƒé›†ç¾¤    â”‚    â”‚  ç›‘æ§å‘Šè­¦ä¸­å¿ƒ    â”‚    â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚
â”‚  â”‚ Cluster API     â”‚â—„â”€â”€â–ºâ”‚ Cluster Registry â”‚â—„â”€â”€â–ºâ”‚ Observability   â”‚    â”‚
â”‚  â”‚ ArgoCD          â”‚    â”‚ Fleet Manager   â”‚    â”‚ Central System  â”‚    â”‚
â”‚  â”‚ Rancher         â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚           â”‚                       â”‚                       â”‚              â”‚
â”‚           â–¼                       â–¼                       â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  å¼€å‘é›†ç¾¤        â”‚    â”‚  ç”Ÿäº§é›†ç¾¤        â”‚    â”‚  ç¾å¤‡é›†ç¾¤        â”‚    â”‚
â”‚  â”‚  dev-cluster     â”‚    â”‚  prod-cluster    â”‚    â”‚  dr-cluster      â”‚    â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚
â”‚  â”‚ Applications    â”‚    â”‚ Critical Apps   â”‚    â”‚ Backup Systems  â”‚    â”‚
â”‚  â”‚ CI/CD Pipeline  â”‚    â”‚ HA Services     â”‚    â”‚ DR Procedures   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Cluster API ç”Ÿäº§å®è·µ

### åŸºç¡€è®¾æ–½å³ä»£ç é…ç½®

```yaml
# Cluster API é›†ç¾¤å®šä¹‰
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: production-cluster
  namespace: capi-system
spec:
  clusterNetwork:
    pods:
      cidrBlocks: ["192.168.0.0/16"]
    services:
      cidrBlocks: ["10.96.0.0/12"]
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: AWSCluster
    name: production-cluster
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: production-control-plane

---
# AWS åŸºç¡€è®¾æ–½é…ç½®
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSCluster
metadata:
  name: production-cluster
  namespace: capi-system
spec:
  region: us-west-2
  sshKeyName: production-key
  network:
    vpc:
      availabilityZoneUsageLimit: 3
      availabilityZoneSelection: Ordered
  bastion:
    enabled: true
    allowedCIDRBlocks:
    - 10.0.0.0/8

---
# æ§åˆ¶å¹³é¢é…ç½®
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: production-control-plane
  namespace: capi-system
spec:
  replicas: 3
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: AWSMachineTemplate
      name: production-control-plane-machines
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        extraArgs:
          audit-log-path: /var/log/apiserver/audit.log
          audit-policy-file: /etc/kubernetes/policies/audit-policy.yaml
        extraVolumes:
        - name: audit-policies
          hostPath: /etc/kubernetes/policies
          mountPath: /etc/kubernetes/policies
          readOnly: true
      controllerManager:
        extraArgs:
          horizontal-pod-autoscaler-sync-period: 10s
      scheduler:
        extraArgs:
          profiling: "false"
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: aws
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
          cloud-provider: aws
```

### èŠ‚ç‚¹ç»„ç®¡ç†

```yaml
# Worker èŠ‚ç‚¹æ± é…ç½®
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: production-workers
  namespace: capi-system
spec:
  clusterName: production-cluster
  replicas: 6
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: production-cluster
      pool: workers
  template:
    spec:
      clusterName: production-cluster
      version: v1.28.0
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: production-worker-bootstrap
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: AWSMachineTemplate
        name: production-worker-machines

---
# Worker èŠ‚ç‚¹æœºå™¨æ¨¡æ¿
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSMachineTemplate
metadata:
  name: production-worker-machines
  namespace: capi-system
spec:
  template:
    spec:
      instanceType: m5.xlarge
      ami:
        id: ami-0abcdef1234567890
      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
      rootVolume:
        size: 100
        type: gp3
      cloudInit:
        insecureSkipSecretsManager: true
      spotMarketOptions:
        maxPrice: "0.10"  # Spotå®ä¾‹ä»·æ ¼ä¸Šé™

---
# èŠ‚ç‚¹å¯åŠ¨é…ç½®
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: production-worker-bootstrap
  namespace: capi-system
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: aws
            rotate-certificates: "true"
            streaming-connection-idle-timeout: "5m"
            max-pods: "110"
            register-with-taints: ""
      preKubeadmCommands:
      - hostname "{{ ds.meta_data.hostname }}"
      - echo "::1         ipv6-localhost ipv6-loopback" >/etc/hosts
      - echo "127.0.0.1   localhost" >>/etc/hosts
      postKubeadmCommands:
      - systemctl daemon-reload
      - systemctl enable kubelet
      - systemctl start kubelet
```

## å¤šé›†ç¾¤æ³¨å†Œä¸ç®¡ç†

### Rancher å¤šé›†ç¾¤ç®¡ç†

```yaml
# Rancher Server é«˜å¯ç”¨éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rancher
  namespace: cattle-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rancher
  template:
    metadata:
      labels:
        app: rancher
    spec:
      serviceAccountName: rancher
      containers:
      - name: rancher
        image: rancher/rancher:v2.7.5
        args:
        - --http-port=80
        - --https-port=443
        - --audit-log-path=/var/log/auditlog/rancher-api-audit.log
        - --audit-level=2
        - --audit-log-maxage=30
        - --audit-log-maxbackup=10
        - --audit-log-maxsize=100
        - --features=multi-cluster-management=true
        - --features=fleet=false
        ports:
        - containerPort: 80
        - containerPort: 443
        volumeMounts:
        - name: audit-log
          mountPath: /var/log/auditlog
        readinessProbe:
          httpGet:
            path: /healthz
            port: 80
          initialDelaySeconds: 60
          periodSeconds: 30
      volumes:
      - name: audit-log
        emptyDir: {}

---
# é›†ç¾¤å¯¼å…¥é…ç½®
apiVersion: management.cattle.io/v3
kind: Cluster
metadata:
  name: imported-prod-cluster
spec:
  displayName: "Production Cluster"
  description: "Main production Kubernetes cluster"
  importedConfig:
    kubeConfigSecret: prod-cluster-kubeconfig
  clusterAgentDeploymentCustomization:
    overrideAffinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: In
              values:
              - "true"
  fleetWorkspaceName: prod-workspace
```

### Cluster Registry é…ç½®

```yaml
# é›†ç¾¤æ³¨å†Œä¸­å¿ƒ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-registry
  namespace: multicluster-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cluster-registry
  template:
    metadata:
      labels:
        app: cluster-registry
    spec:
      containers:
      - name: registry-server
        image: k8s.gcr.io/cluster-registry:v0.1.0
        ports:
        - containerPort: 8080
        env:
        - name: CLUSTER_REGISTRY_CONFIG
          value: "/etc/cluster-registry/config.yaml"
        volumeMounts:
        - name: config-volume
          mountPath: /etc/cluster-registry
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: config-volume
        configMap:
          name: cluster-registry-config

---
# é›†ç¾¤å…ƒæ•°æ®é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-registry-config
  namespace: multicluster-system
data:
  config.yaml: |
    clusters:
    - name: production-cluster
      endpoint: https://k8s-prod.example.com:6443
      auth:
        type: serviceaccount
        secretName: prod-cluster-sa-token
      labels:
        environment: production
        region: us-west-2
        purpose: customer-facing
      resources:
        cpu: 128
        memory: 512Gi
        nodes: 24
      
    - name: staging-cluster
      endpoint: https://k8s-staging.example.com:6443
      auth:
        type: certificate
        secretName: staging-cluster-cert
      labels:
        environment: staging
        region: us-east-1
        purpose: testing
      resources:
        cpu: 64
        memory: 256Gi
        nodes: 12
```

## è·¨é›†ç¾¤åº”ç”¨éƒ¨ç½²

### ArgoCD å¤šé›†ç¾¤éƒ¨ç½²

```yaml
# å¤šé›†ç¾¤åº”ç”¨é…ç½®
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: multi-cluster-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/example/microservices.git
    targetRevision: HEAD
    path: manifests/
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
  ignoreDifferences:
  - group: apps
    kind: Deployment
    jsonPointers:
    - /spec/replicas
  
  # å¤šé›†ç¾¤ç›®æ ‡é…ç½®
  destinations:
  - name: production-cluster
    namespace: production
    server: https://k8s-prod.example.com:6443
  - name: staging-cluster
    namespace: staging
    server: https://k8s-staging.example.com:6443
  - name: dr-cluster
    namespace: disaster-recovery
    server: https://k8s-dr.example.com:6443

---
# é›†ç¾¤å‡­è¯ç®¡ç†
apiVersion: v1
kind: Secret
metadata:
  name: cluster-credentials
  namespace: argocd
  labels:
    argocd.argoproj.io/secret-type: cluster
type: Opaque
stringData:
  name: production-cluster
  server: https://k8s-prod.example.com:6443
  config: |
    {
      "bearerToken": "<token>",
      "tlsClientConfig": {
        "insecure": false,
        "caData": "<base64-encoded-ca-cert>"
      }
    }
```

### Fleet å¤šé›†ç¾¤ç®¡ç†

```yaml
# Fleet Bundle é…ç½®
apiVersion: fleet.cattle.io/v1alpha1
kind: Bundle
metadata:
  name: monitoring-stack
  namespace: fleet-default
spec:
  resources:
  - content: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: prometheus
        namespace: monitoring
      spec:
        replicas: 2
        selector:
          matchLabels:
            app: prometheus
        template:
          metadata:
            labels:
              app: prometheus
          spec:
            containers:
            - name: prometheus
              image: prom/prometheus:v2.40.0
  targets:
  - clusterSelector:
      matchLabels:
        environment: production
    replicaCount: 3
    kustomize:
      patches:
      - patch: |-
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: prometheus
          spec:
            template:
              spec:
                containers:
                - name: prometheus
                  resources:
                    requests:
                      memory: "2Gi"
                      cpu: "1"
                    limits:
                      memory: "4Gi"
                      cpu: "2"
  - clusterSelector:
      matchLabels:
        environment: staging
    replicaCount: 1
    kustomize:
      patches:
      - patch: |-
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: prometheus
          spec:
            template:
              spec:
                containers:
                - name: prometheus
                  resources:
                    requests:
                      memory: "1Gi"
                      cpu: "500m"
                    limits:
                      memory: "2Gi"
                      cpu: "1"
```

## é›†ç¾¤é—´é€šä¿¡ä¸æœåŠ¡å‘ç°

### å¤šé›†ç¾¤æœåŠ¡ç½‘æ ¼

```yaml
# Istio å¤šé›†ç¾¤é…ç½®
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-multicluster
spec:
  profile: demo
  values:
    global:
      multiCluster:
        clusterName: production-cluster
      meshID: mesh1
      network: network1
    gateways:
      istio-ingressgateway:
        type: LoadBalancer
  components:
    ingressGateways:
    - name: istio-ingressgateway
      enabled: true
      k8s:
        service:
          ports:
          - port: 80
            targetPort: 8080
            name: http2
          - port: 443
            targetPort: 8443
            name: https

---
# æœåŠ¡å¯¼å‡ºé…ç½®
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: remote-services
  namespace: istio-system
spec:
  hosts:
  - "*.remote-cluster.example.com"
  location: MESH_EXTERNAL
  ports:
  - number: 80
    name: http
    protocol: HTTP
  resolution: DNS
  endpoints:
  - address: remote-cluster-gateway.example.com
    ports:
      http: 80
```

### è·¨é›†ç¾¤DNSé…ç½®

```yaml
# CoreDNS è·¨é›†ç¾¤é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
    
    # è·¨é›†ç¾¤DNSè½¬å‘
    remote-cluster.example.com:53 {
        forward . 10.100.10.10 10.100.20.10 {
            health_check 5s
        }
        cache 30
    }
```

## ç›‘æ§ä¸å‘Šè­¦ç»Ÿä¸€

### å¤šé›†ç¾¤Prometheusé…ç½®

```yaml
# Prometheus è”é‚¦é…ç½®
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: federated-prometheus
  namespace: monitoring
spec:
  replicas: 2
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      team: frontend
  ruleSelector:
    matchLabels:
      team: frontend
  externalLabels:
    cluster: production-cluster
    region: us-west-2
  remoteWrite:
  - url: http://central-prometheus.monitoring.svc:9090/api/v1/write
    writeRelabelConfigs:
    - sourceLabels: [__name__]
      regex: (up|scrape_samples_scraped)
      action: keep
  additionalScrapeConfigs:
    name: additional-scrape-configs
    key: prometheus-additional.yaml

---
# é¢å¤–æŠ“å–é…ç½®
apiVersion: v1
kind: Secret
metadata:
  name: additional-scrape-configs
  namespace: monitoring
stringData:
  prometheus-additional.yaml: |
    - job_name: 'federate'
      scrape_interval: 15s
      honor_labels: true
      metrics_path: '/federate'
      params:
        'match[]':
          - '{job=~"kubernetes-.*"}'
          - '{__name__=~"node_.*"}'
      static_configs:
      - targets:
        - 'prometheus-us-east.monitoring.svc:9090'
        - 'prometheus-eu-west.monitoring.svc:9090'
        labels:
          cluster: remote-clusters
```

## å®‰å…¨ä¸è®¿é—®æ§åˆ¶

### å¤šé›†ç¾¤RBACç®¡ç†

```yaml
# é›†ç¾¤é—´RBACåŒæ­¥
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: multicluster-admin
rules:
- apiGroups: [""]
  resources: ["pods", "services", "namespaces"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["networking.k8s.io"]
  resources: ["networkpolicies", "ingresses"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

---
# è·¨é›†ç¾¤è§’è‰²ç»‘å®š
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: multicluster-admin-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: multicluster-admin
subjects:
- kind: User
  name: admin-user
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: cluster-admins
  apiGroup: rbac.authorization.k8s.io
```

## æ•…éšœæ’é™¤ä¸è°ƒè¯•

### å¤šé›†ç¾¤è¯Šæ–­å·¥å…·

```bash
#!/bin/bash
# multicluster-diagnostics.sh

CLUSTERS=("production-cluster" "staging-cluster" "dr-cluster")

diagnose_cluster_connectivity() {
    echo "=== é›†ç¾¤è¿æ¥æ€§è¯Šæ–­ ==="
    for cluster in "${CLUSTERS[@]}"; do
        echo "æ£€æŸ¥é›†ç¾¤: $cluster"
        kubectl config use-context $cluster
        
        # æ£€æŸ¥API Serverå¯è¾¾æ€§
        if kubectl cluster-info >/dev/null 2>&1; then
            echo "âœ… $cluster API Server å¯è¾¾"
        else
            echo "âŒ $cluster API Server ä¸å¯è¾¾"
        fi
        
        # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
        ready_nodes=$(kubectl get nodes --no-headers | grep -c " Ready ")
        total_nodes=$(kubectl get nodes --no-headers | wc -l)
        echo "ğŸ“Š $cluster èŠ‚ç‚¹çŠ¶æ€: $ready_nodes/$total_nodes å°±ç»ª"
    done
}

diagnose_cross_cluster_services() {
    echo "=== è·¨é›†ç¾¤æœåŠ¡è¯Šæ–­ ==="
    # æ£€æŸ¥æœåŠ¡å‘ç°
    for cluster in "${CLUSTERS[@]}"; do
        echo "æ£€æŸ¥ $cluster ä¸­çš„æœåŠ¡..."
        kubectl config use-context $cluster
        kubectl get svc --all-namespaces | grep -E "(LoadBalancer|ClusterIP)" | head -5
    done
}

diagnose_network_connectivity() {
    echo "=== ç½‘ç»œè¿é€šæ€§è¯Šæ–­ ==="
    # æ£€æŸ¥Podé—´é€šä¿¡
    for cluster in "${CLUSTERS[@]}"; do
        echo "æ£€æŸ¥ $cluster ç½‘ç»œè¿é€šæ€§..."
        kubectl config use-context $cluster
        kubectl run debug-pod --image=busybox --restart=Never --rm -it -- sh -c "
            ping -c 3 8.8.8.8
            nslookup kubernetes.default
        " 2>/dev/null || echo "ç½‘ç»œæµ‹è¯•å¤±è´¥"
    done
}

# æ‰§è¡Œè¯Šæ–­
diagnose_cluster_connectivity
diagnose_cross_cluster_services
diagnose_network_connectivity

echo "=== è¯Šæ–­å®Œæˆ ==="
```

## ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### é›†ç¾¤å‘½åè§„èŒƒ

```yaml
# é›†ç¾¤å‘½åçº¦å®š
clusters:
  # ç¯å¢ƒ-ç”¨é€”-åŒºåŸŸ-åºå·
  production-customer-us-west-01:  # ç”Ÿäº§å®¢æˆ·é›†ç¾¤-ç¾å›½è¥¿éƒ¨-01
    purpose: customer-facing
    sla: 99.99%
    
  staging-testing-us-east-01:      # é¢„å‘å¸ƒæµ‹è¯•é›†ç¾¤-ç¾å›½ä¸œéƒ¨-01
    purpose: testing
    sla: 99.9%
    
  development-dev-us-west-01:      # å¼€å‘ç¯å¢ƒé›†ç¾¤-ç¾å›½è¥¿éƒ¨-01
    purpose: development
    sla: 99%
```

### ç‰ˆæœ¬ç®¡ç†ç­–ç•¥

```yaml
# é›†ç¾¤ç‰ˆæœ¬å‡çº§è®¡åˆ’
version_management:
  upgrade_schedule:
    - time: "2024-02-15T02:00:00Z"
      clusters: ["staging-cluster"]
      target_version: "v1.28.2"
      
    - time: "2024-02-22T02:00:00Z"
      clusters: ["production-cluster"]
      target_version: "v1.28.2"
      
  compatibility_matrix:
    kubernetes_versions: ["1.26", "1.27", "1.28"]
    supported_cnis: ["calico", "cilium", "flannel"]
    certified_platforms: ["aws", "gcp", "azure"]
```

---

**è¡¨æ ¼åº•éƒ¨æ ‡è®°**: Kusheet Project, ä½œè€… Allen Galler (allengaller@gmail.com)
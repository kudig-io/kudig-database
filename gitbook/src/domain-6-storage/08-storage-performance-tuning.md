# 08 - å­˜å‚¨æ€§èƒ½è°ƒä¼˜ä¸ä¼˜åŒ–ç­–ç•¥

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25 - v1.32 | **è¿ç»´é‡ç‚¹**: æ€§èƒ½ä¼˜åŒ–ã€è°ƒä¼˜å‚æ•°ã€ç›‘æ§åˆ†æ | **æœ€åæ›´æ–°**: 2026-02

## ç›®å½•

1. [å­˜å‚¨ç±»å‹æ€§èƒ½å¯¹æ¯”](#å­˜å‚¨ç±»å‹æ€§èƒ½å¯¹æ¯”)
2. [æ€§èƒ½è°ƒä¼˜ç­–ç•¥](#æ€§èƒ½è°ƒä¼˜ç­–ç•¥)
3. [æŒ‚è½½å‚æ•°ä¼˜åŒ–](#æŒ‚è½½å‚æ•°ä¼˜åŒ–)
4. [ç›‘æ§æŒ‡æ ‡ä½“ç³»](#ç›‘æ§æŒ‡æ ‡ä½“ç³»)
5. [æ€§èƒ½æµ‹è¯•æ–¹æ³•](#æ€§èƒ½æµ‹è¯•æ–¹æ³•)
6. [æ•…éšœè¯Šæ–­æµç¨‹](#æ•…éšœè¯Šæ–­æµç¨‹)
7. [ä¼ä¸šçº§ä¼˜åŒ–æ¡ˆä¾‹](#ä¼ä¸šçº§ä¼˜åŒ–æ¡ˆä¾‹)
8. [æœ€ä½³å®è·µæ€»ç»“](#æœ€ä½³å®è·µæ€»ç»“)

---

| å­˜å‚¨ç±»å‹ | IOPS | ååé‡ | å»¶è¿Ÿ | é€‚ç”¨åœºæ™¯ |
|----------|------|--------|------|----------|
| Local SSD | 100k+ | 1GB/s+ | <0.1ms | æ•°æ®åº“ã€ç¼“å­˜ |
| äº‘ SSD | 25k-100k | 350MB/s | <1ms | é€šç”¨å·¥ä½œè´Ÿè½½ |
| äº‘é«˜æ•ˆäº‘ç›˜ | 5k-25k | 150MB/s | 1-3ms | å¼€å‘æµ‹è¯• |
| NFS/NAS | å˜åŒ–å¤§ | 100-500MB/s | 1-10ms | å…±äº«å­˜å‚¨ |
| å¯¹è±¡å­˜å‚¨ | N/A | é«˜åå | 50-200ms | å¤§æ–‡ä»¶ã€å¤‡ä»½ |

## StorageClass æ€§èƒ½é…ç½®

```yaml
# é«˜æ€§èƒ½ SSD å­˜å‚¨ç±»
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: high-performance-ssd
provisioner: disk.csi.aliyun.com
parameters:
  type: cloud_essd
  performanceLevel: PL3  # ESSD æ€§èƒ½çº§åˆ«
  fsType: ext4
  encrypted: "true"
reclaimPolicy: Retain
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
---
# é€šç”¨ SSD å­˜å‚¨ç±»
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard-ssd
provisioner: disk.csi.aliyun.com
parameters:
  type: cloud_ssd
  fsType: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
---
# æœ¬åœ°å­˜å‚¨ç±» (é«˜æ€§èƒ½)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-ssd
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
```

## æœ¬åœ°å­˜å‚¨é…ç½®

```yaml
# æœ¬åœ° PV (Local Persistent Volume)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-node1
spec:
  capacity:
    storage: 500Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-ssd
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node-1
---
# æœ¬åœ°å­˜å‚¨ Provisioner (TopoLVM)
apiVersion: topolvm.io/v1
kind: LogicalVolume
metadata:
  name: app-data
spec:
  deviceClass: ssd
  size: 100Gi
```

## CSI é©±åŠ¨æ€§èƒ½ä¼˜åŒ–

```yaml
# CSI é©±åŠ¨å‚æ•°ä¼˜åŒ– (é˜¿é‡Œäº‘)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: alicloud-disk-essd-optimized
provisioner: disk.csi.aliyun.com
parameters:
  type: cloud_essd
  performanceLevel: PL2
  # å¤š Attach (ReadWriteMany åœºæ™¯)
  multiAttach: "true"
  # åŠ å¯†
  encrypted: "true"
  kmsKeyId: "<kms-key-id>"
  # å¿«ç…§
  snapshotId: ""
  # ç£ç›˜ç±»åˆ«
  zoned: "true"
mountOptions:
- noatime
- nodiratime
- barrier=0
reclaimPolicy: Retain
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
```

## æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–

```yaml
# Pod æŒ‚è½½é€‰é¡¹
apiVersion: v1
kind: Pod
metadata:
  name: storage-optimized-pod
spec:
  containers:
  - name: app
    image: myapp:v1
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: data-pvc
---
# PV æŒ‚è½½é€‰é¡¹ (é€šè¿‡ StorageClass)
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: optimized-ext4
provisioner: disk.csi.aliyun.com
parameters:
  type: cloud_essd
  fsType: ext4
mountOptions:
- noatime           # ä¸æ›´æ–°è®¿é—®æ—¶é—´
- nodiratime        # ä¸æ›´æ–°ç›®å½•è®¿é—®æ—¶é—´
- data=ordered      # ext4 æ•°æ®æ¨¡å¼
- barrier=0         # ç¦ç”¨å†™å±éšœ (æœ‰ç”µæ± å¤‡ä»½)
- discard           # SSD TRIM æ”¯æŒ
```

## æ•°æ®åº“å­˜å‚¨ä¼˜åŒ–

```yaml
# MySQL é«˜æ€§èƒ½å­˜å‚¨é…ç½®
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 1
  template:
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
        - name: config
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 2
            memory: 4Gi
          limits:
            cpu: 4
            memory: 8Gi
      volumes:
      - name: config
        configMap:
          name: mysql-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: high-performance-ssd
      resources:
        requests:
          storage: 200Gi
---
# MySQL é…ç½®ä¼˜åŒ–
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql-config
data:
  performance.cnf: |
    [mysqld]
    innodb_buffer_pool_size = 3G
    innodb_log_file_size = 1G
    innodb_flush_log_at_trx_commit = 2
    innodb_flush_method = O_DIRECT
    innodb_io_capacity = 10000
    innodb_io_capacity_max = 20000
    innodb_read_io_threads = 8
    innodb_write_io_threads = 8
    sync_binlog = 0
```

## å­˜å‚¨æ€§èƒ½æµ‹è¯•

```bash
# ä½¿ç”¨ fio æµ‹è¯•å­˜å‚¨æ€§èƒ½
kubectl run fio --image=nixery.dev/fio --rm -it -- fio \
  --name=test \
  --ioengine=libaio \
  --rw=randwrite \
  --bs=4k \
  --direct=1 \
  --size=1G \
  --numjobs=4 \
  --time_based \
  --runtime=60 \
  --group_reporting \
  --filename=/data/test

# é¡ºåºè¯»å†™æµ‹è¯•
fio --name=seq-read --ioengine=libaio --rw=read --bs=1M --direct=1 --size=1G --numjobs=1
fio --name=seq-write --ioengine=libaio --rw=write --bs=1M --direct=1 --size=1G --numjobs=1

# éšæœºè¯»å†™æµ‹è¯•
fio --name=rand-read --ioengine=libaio --rw=randread --bs=4k --direct=1 --size=1G --numjobs=4
fio --name=rand-write --ioengine=libaio --rw=randwrite --bs=4k --direct=1 --size=1G --numjobs=4

# dd å¿«é€Ÿæµ‹è¯•
dd if=/dev/zero of=/data/testfile bs=1G count=1 oflag=direct
dd if=/data/testfile of=/dev/null bs=1G count=1 iflag=direct
```

## å­˜å‚¨ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | å‘Šè­¦é˜ˆå€¼ |
|------|------|----------|
| kubelet_volume_stats_used_bytes | å·ä½¿ç”¨é‡ | > 80% å®¹é‡ |
| kubelet_volume_stats_inodes_used | inode ä½¿ç”¨é‡ | > 80% æ€»é‡ |
| node_disk_io_time_seconds_total | ç£ç›˜ IO æ—¶é—´ | æŒç»­ > 80% |
| node_disk_read_bytes_total | è¯»å–å­—èŠ‚æ•° | æ¥è¿‘é™åˆ¶ |
| node_disk_write_bytes_total | å†™å…¥å­—èŠ‚æ•° | æ¥è¿‘é™åˆ¶ |

## ç›‘æ§å‘Šè­¦è§„åˆ™

```yaml
groups:
- name: storage
  rules:
  - alert: PVCUsageHigh
    expr: |
      kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PVC {{ $labels.persistentvolumeclaim }} ä½¿ç”¨ç‡ > 80%"
      
  - alert: PVCInodeUsageHigh
    expr: |
      kubelet_volume_stats_inodes_used / kubelet_volume_stats_inodes > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "PVC {{ $labels.persistentvolumeclaim }} inode ä½¿ç”¨ç‡ > 80%"
      
  - alert: DiskIOSaturated
    expr: |
      rate(node_disk_io_time_seconds_total[5m]) > 0.8
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "èŠ‚ç‚¹ {{ $labels.instance }} ç£ç›˜ {{ $labels.device }} IO é¥±å’Œ"
      
  - alert: DiskLatencyHigh
    expr: |
      rate(node_disk_read_time_seconds_total[5m]) 
      / rate(node_disk_reads_completed_total[5m]) > 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "èŠ‚ç‚¹ {{ $labels.instance }} ç£ç›˜è¯»å»¶è¿Ÿ > 100ms"
```

## CSI é©±åŠ¨è¯Šæ–­

```bash
# æŸ¥çœ‹ CSI é©±åŠ¨çŠ¶æ€
kubectl get csidrivers
kubectl get csinodes

# æŸ¥çœ‹ CSI æ§åˆ¶å™¨
kubectl get pods -n kube-system -l app=csi-provisioner
kubectl logs -n kube-system -l app=csi-provisioner -c csi-provisioner

# æŸ¥çœ‹èŠ‚ç‚¹ CSI æ’ä»¶
kubectl get pods -n kube-system -l app=csi-plugin
kubectl logs -n kube-system -l app=csi-plugin -c csi-plugin

# VolumeAttachment çŠ¶æ€
kubectl get volumeattachments

# å­˜å‚¨è¯Šæ–­
kubectl describe pvc <pvc-name>
kubectl describe pv <pv-name>
kubectl get events --field-selector reason=ProvisioningFailed
```
---
## æ€§èƒ½è°ƒä¼˜ç­–ç•¥

### å­˜å‚¨åˆ†å±‚ä¼˜åŒ–

```yaml
# ä¼ä¸šçº§å­˜å‚¨åˆ†å±‚ç­–ç•¥
storage_tiering_strategy:
  hot_data_layer:
    storage_type: "Local NVMe SSD"
    performance: "IOPS > 100K, Latency < 0.1ms"
    use_case: "ç¼“å­˜ã€ä¸´æ—¶è®¡ç®—ç»“æœã€é«˜é¢‘è®¿é—®æ•°æ®"
    cost_factor: "é«˜"
    
  warm_data_layer:
    storage_type: "ESSD PL2/PL3"
    performance: "IOPS 50K-100K, Latency < 1ms"
    use_case: "ä¸»æ•°æ®åº“ã€æ ¸å¿ƒåº”ç”¨æ•°æ®"
    cost_factor: "ä¸­é«˜"
    
  cold_data_layer:
    storage_type: "ESSD PL0/PL1"
    performance: "IOPS 10K-30K, Latency < 5ms"
    use_case: "å†å²æ•°æ®ã€æ—¥å¿—å½’æ¡£"
    cost_factor: "ä¸­"
    
  archive_layer:
    storage_type: "OSS Archive"
    performance: "è®¿é—®å»¶è¿Ÿåˆ†é’Ÿçº§"
    use_case: "å¤‡ä»½æ•°æ®ã€åˆè§„å½’æ¡£"
    cost_factor: "ä½"
```

### æŒ‚è½½å‚æ•°ä¼˜åŒ–é…ç½®

```yaml
# é«˜æ€§èƒ½å­˜å‚¨æŒ‚è½½ä¼˜åŒ–
high_performance_mount_configs:
  database_storage:
    mount_options:
      - noatime          # ä¸æ›´æ–°è®¿é—®æ—¶é—´æˆ³
      - nodiratime       # ç›®å½•ä¸æ›´æ–°è®¿é—®æ—¶é—´æˆ³
      - discard          # å¯ç”¨TRIMæ”¯æŒ
      - barrier=0        # ç¦ç”¨å†™å±éšœ(è°¨æ…ä½¿ç”¨)
      - data=ordered     # æ•°æ®å†™å…¥é¡ºåºä¿è¯
      - nobarrier        # è¿›ä¸€æ­¥ç¦ç”¨å±éšœ
    filesystem_tuning:
      scheduler: "deadline"  # IOè°ƒåº¦å™¨
      read_ahead_kb: 4096    # é¢„è¯»å¤§å°
      nr_requests: 1024      # è¯·æ±‚é˜Ÿåˆ—é•¿åº¦
      
  application_storage:
    mount_options:
      - noatime
      - discard
      - relatime         # ç›¸å¯¹è®¿é—®æ—¶é—´æ›´æ–°
    filesystem_tuning:
      scheduler: "noop"
      read_ahead_kb: 2048
      
  shared_storage:
    mount_options:
      - vers=4.1         # NFSç‰ˆæœ¬4.1
      - rsize=1048576    # è¯»å–ç¼“å†²åŒº1MB
      - wsize=1048576    # å†™å…¥ç¼“å†²åŒº1MB
      - hard             # ç¡¬æŒ‚è½½
      - timeo=600        # è¶…æ—¶600ç§’
      - retrans=2        # é‡è¯•2æ¬¡
      - nolock           # ç¦ç”¨æ–‡ä»¶é”å®š
```

---
## ç›‘æ§æŒ‡æ ‡ä½“ç³»

### æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡

```yaml
# å­˜å‚¨æ€§èƒ½ç›‘æ§æŒ‡æ ‡å®šä¹‰
performance_monitoring_metrics:
  iops_metrics:
    - name: "storage_iops_total"
      type: "counter"
      description: "å­˜å‚¨æ¯ç§’IOæ“ä½œæ•°"
      critical_threshold: 90
      warning_threshold: 80
      
    - name: "storage_read_iops"
      type: "gauge"
      description: "è¯»å–IOPS"
      
    - name: "storage_write_iops"
      type: "gauge"
      description: "å†™å…¥IOPS"
      
  throughput_metrics:
    - name: "storage_throughput_bytes"
      type: "counter"
      description: "å­˜å‚¨ååé‡(bytes)"
      units: "bytes/sec"
      
  latency_metrics:
    - name: "storage_operation_duration_seconds"
      type: "histogram"
      description: "å­˜å‚¨æ“ä½œå»¶è¿Ÿåˆ†å¸ƒ"
      buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]
      
  utilization_metrics:
    - name: "storage_utilization_percentage"
      type: "gauge"
      description: "å­˜å‚¨ä½¿ç”¨ç‡ç™¾åˆ†æ¯”"
      critical_threshold: 95
      warning_threshold: 85
```

### Prometheuså‘Šè­¦è§„åˆ™

```yaml
# å­˜å‚¨æ€§èƒ½å‘Šè­¦é…ç½®
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: storage-performance-alerts
  namespace: monitoring
spec:
  groups:
  - name: storage-performance.rules
    rules:
    # é«˜IOPSå‘Šè­¦
    - alert: StorageHighIOPS
      expr: |
        rate(storage_iops_total[5m]) > 80000
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "å­˜å‚¨IOPSè¿‡é«˜"
        description: "å½“å‰IOPS: {{ $value }}, å¯èƒ½å½±å“æ€§èƒ½"
        
    # é«˜å»¶è¿Ÿå‘Šè­¦
    - alert: StorageHighLatency
      expr: |
        histogram_quantile(0.95, rate(storage_operation_duration_seconds_bucket[5m])) > 0.01
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "å­˜å‚¨å»¶è¿Ÿè¿‡é«˜"
        description: "P95å»¶è¿Ÿ: {{ $value }}s, è¶…è¿‡10msé˜ˆå€¼"
        
    # ä½ååé‡å‘Šè­¦ï¼ˆå¯èƒ½è¡¨ç¤ºæ€§èƒ½ç“¶é¢ˆï¼‰
    - alert: StorageLowThroughput
      expr: |
        rate(storage_throughput_bytes[5m]) < 1048576  # 1MB/s
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "å­˜å‚¨ååé‡å¼‚å¸¸åä½"
        description: "å½“å‰ååé‡: {{ $value }} bytes/s"
```

---
## æ€§èƒ½æµ‹è¯•æ–¹æ³•

### åŸºå‡†æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash
# storage-performance-benchmark.sh

# å­˜å‚¨æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·
run_storage_benchmark() {
    local test_pod=$1
    local namespace=${2:-"default"}
    local test_file="/data/benchmark-test"
    
    echo "âš¡ å¼€å§‹å­˜å‚¨æ€§èƒ½åŸºå‡†æµ‹è¯•..."
    echo "æµ‹è¯•Pod: $test_pod"
    echo "å‘½åç©ºé—´: $namespace"
    echo ""
    
    # 1. é¡ºåºå†™å…¥æµ‹è¯•
    echo "=== é¡ºåºå†™å…¥æ€§èƒ½æµ‹è¯• ==="
    kubectl exec -it $test_pod -n $namespace -- \
        dd if=/dev/zero of=$test_file bs=1M count=1000 oflag=direct 2>&1
    
    # 2. é¡ºåºè¯»å–æµ‹è¯•
    echo ""
    echo "=== é¡ºåºè¯»å–æ€§èƒ½æµ‹è¯• ==="
    kubectl exec -it $test_pod -n $namespace -- \
        dd if=$test_file of=/dev/null bs=1M count=1000 iflag=direct 2>&1
    
    # 3. éšæœºè¯»å†™æµ‹è¯•
    echo ""
    echo "=== éšæœºè¯»å†™æ€§èƒ½æµ‹è¯• ==="
    kubectl exec -it $test_pod -n $namespace -- \
        fio --name=randtest --filename=$test_file --rw=randrw \
            --bs=4k --size=1G --numjobs=4 --iodepth=32 --direct=1 \
            --runtime=60 --time_based --group_reporting
    
    # 4. æ¸…ç†æµ‹è¯•æ–‡ä»¶
    kubectl exec -it $test_pod -n $namespace -- rm -f $test_file
    
    echo ""
    echo "âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ"
}

# ä½¿ç”¨ç¤ºä¾‹
# run_storage_benchmark "test-pod" "benchmark-namespace"
```

### æŒç»­æ€§èƒ½ç›‘æ§

```python
# å­˜å‚¨æ€§èƒ½æŒç»­ç›‘æ§ç³»ç»Ÿ
import time
import subprocess
import json
from datetime import datetime

class StoragePerformanceMonitor:
    def __init__(self):
        self.metrics_history = []
        self.thresholds = {
            'iops': 80000,
            'latency_ms': 10,
            'throughput_mb': 100
        }
    
    def collect_metrics(self):
        """æ”¶é›†å­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
        try:
            # ä½¿ç”¨kubectlè·å–å­˜å‚¨æŒ‡æ ‡
            cmd = "kubectl top pods --no-headers"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            
            metrics = {
                'timestamp': datetime.now().isoformat(),
                'cpu_usage': [],
                'memory_usage': [],
                'storage_metrics': self.get_storage_metrics()
            }
            
            return metrics
        except Exception as e:
            print(f"æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            return None
    
    def get_storage_metrics(self):
        """è·å–å­˜å‚¨ç›¸å…³æŒ‡æ ‡"""
        # è¿™é‡Œå¯ä»¥é›†æˆå…·ä½“çš„å­˜å‚¨ç›‘æ§æ•°æ®æº
        return {
            'iops_current': 45000,
            'latency_ms': 2.5,
            'throughput_mb': 150,
            'utilization_pct': 75
        }
    
    def analyze_performance(self, metrics):
        """åˆ†ææ€§èƒ½çŠ¶å†µ"""
        storage = metrics['storage_metrics']
        alerts = []
        
        if storage['iops_current'] > self.thresholds['iops']:
            alerts.append({
                'type': 'high_iops',
                'severity': 'warning',
                'message': f"IOPSè¿‡é«˜: {storage['iops_current']}"
            })
            
        if storage['latency_ms'] > self.thresholds['latency_ms']:
            alerts.append({
                'type': 'high_latency',
                'severity': 'critical',
                'message': f"å»¶è¿Ÿè¿‡é«˜: {storage['latency_ms']}ms"
            })
            
        return alerts
    
    def run_continuous_monitoring(self, interval=300):
        """æŒç»­ç›‘æ§å¾ªç¯"""
        print("å¼€å§‹æŒç»­æ€§èƒ½ç›‘æ§...")
        while True:
            try:
                metrics = self.collect_metrics()
                if metrics:
                    alerts = self.analyze_performance(metrics)
                    if alerts:
                        self.handle_alerts(alerts)
                    self.metrics_history.append(metrics)
                
                time.sleep(interval)
            except KeyboardInterrupt:
                print("ç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                print(f"ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(60)

# ä½¿ç”¨ç¤ºä¾‹
monitor = StoragePerformanceMonitor()
# monitor.run_continuous_monitoring()
```

---
## æ•…éšœè¯Šæ–­æµç¨‹

### æ€§èƒ½é—®é¢˜è¯Šæ–­æ ‘

```mermaid
graph TD
    A[å­˜å‚¨æ€§èƒ½é—®é¢˜] --> B{é—®é¢˜æ˜¯IOPSä¸è¶³?}
    B -->|æ˜¯| C[æ£€æŸ¥å­˜å‚¨ç±»å‹å’ŒPLçº§åˆ«]
    B -->|å¦| D{é—®é¢˜æ˜¯å»¶è¿Ÿé«˜?}
    D -->|æ˜¯| E[æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒCSIé©±åŠ¨]
    D -->|å¦| F{é—®é¢˜æ˜¯ååé‡ä½?}
    F -->|æ˜¯| G[æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿå’ŒæŒ‚è½½å‚æ•°]
    F -->|å¦| H[ç»¼åˆæ€§èƒ½åˆ†æ]
    
    C --> I[å‡çº§å­˜å‚¨ç±»å‹æˆ–PLçº§åˆ«]
    E --> J[ä¼˜åŒ–ç½‘ç»œé…ç½®å’Œé©±åŠ¨ç‰ˆæœ¬]
    G --> K[è°ƒæ•´æ–‡ä»¶ç³»ç»Ÿå‚æ•°å’ŒæŒ‚è½½é€‰é¡¹]
    H --> L[ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·æ·±å…¥è¯Šæ–­]
```

### å¸¸è§æ€§èƒ½é—®é¢˜è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ç±»å‹ | ç—‡çŠ¶è¡¨ç° | è¯Šæ–­æ–¹æ³• | è§£å†³æ–¹æ¡ˆ |
|---------|---------|---------|---------|
| **IOPSç“¶é¢ˆ** | åº”ç”¨å“åº”ç¼“æ…¢ï¼Œæ•°æ®åº“QPSä¸‹é™ | `iostat`, `fio`æµ‹è¯• | å‡çº§åˆ°æ›´é«˜æ€§èƒ½å­˜å‚¨ç±»å‹ |
| **é«˜å»¶è¿Ÿ** | è¯·æ±‚å“åº”æ—¶é—´é•¿ï¼Œç”¨æˆ·ä½“éªŒå·® | `ping`, `traceroute`ç½‘ç»œæµ‹è¯• | ä¼˜åŒ–ç½‘ç»œé…ç½®ï¼Œä½¿ç”¨æœ¬åœ°å­˜å‚¨ |
| **å¸¦å®½é™åˆ¶** | å¤§æ–‡ä»¶ä¼ è¾“æ…¢ï¼Œå¤‡ä»½è€—æ—¶é•¿ | `iperf`ç½‘ç»œå¸¦å®½æµ‹è¯• | è°ƒæ•´æŒ‚è½½å‚æ•°ï¼Œä½¿ç”¨å¹¶è¡Œä¼ è¾“ |
| **æ–‡ä»¶ç³»ç»Ÿé—®é¢˜** | å°æ–‡ä»¶æ€§èƒ½å·®ï¼Œinodeè€—å°½ | `df -i`æ£€æŸ¥inodeä½¿ç”¨ | æ¸…ç†å°æ–‡ä»¶ï¼Œé‡å»ºæ–‡ä»¶ç³»ç»Ÿ |
| **ç¼“å­˜å¤±æ•ˆ** | é‡å¤è¯»å–æ€§èƒ½æ— æå‡ | `free`, `vmstat`æ£€æŸ¥ç¼“å­˜ | è°ƒæ•´ç³»ç»Ÿç¼“å­˜å‚æ•° |

---
## ä¼ä¸šçº§ä¼˜åŒ–æ¡ˆä¾‹

### ç”µå•†å¹³å°æ•°æ®åº“ä¼˜åŒ–æ¡ˆä¾‹

```yaml
# ç”µå•†æ•°æ®åº“å­˜å‚¨ä¼˜åŒ–æ–¹æ¡ˆ
ecommerce_db_optimization:
  scenario: "é«˜å¹¶å‘ç”µå•†æ•°æ®åº“ï¼Œå³°å€¼QPS 50000+"
  challenges:
    - high_iops_requirement: "éœ€è¦æ”¯æŒ10ä¸‡+ IOPS"
    - low_latency_demand: "æŸ¥è¯¢å»¶è¿Ÿè¦æ±‚ < 2ms"
    - data_consistency: "å¼ºä¸€è‡´æ€§è¦æ±‚"
    
  solution:
    storage_configuration:
      type: "ESSD PL3"
      size: "2Ti"
      iops_guaranteed: 1000000
      latency_target: "< 1ms"
      
    mount_optimization:
      options:
        - noatime
        - nodiratime
        - discard
        - barrier=0
      filesystem: "ext4 with optimized parameters"
      
    monitoring_setup:
      tools:
        - prometheus_for_metrics
        - grafana_for_visualization
        - alertmanager_for_notifications
      key_metrics:
        - iops_real_time
        - latency_p95
        - queue_depth
        - utilization_percentage
        
  results:
    performance_improvement:
      iops_increase: "200% æå‡"
      latency_reduction: "60% é™ä½"
      cost_optimization: "é€šè¿‡åˆ†å±‚å­˜å‚¨èŠ‚çœ30%æˆæœ¬"
```

### å¤§æ•°æ®åˆ†æå¹³å°ä¼˜åŒ–æ¡ˆä¾‹

```yaml
# å¤§æ•°æ®å¹³å°å­˜å‚¨ä¼˜åŒ–
big_data_platform_optimization:
  scenario: "PBçº§æ•°æ®å­˜å‚¨å’Œåˆ†æå¹³å°"
  requirements:
    - massive_storage: "éœ€è¦å­˜å‚¨æ•°ç™¾TBæ•°æ®"
    - sequential_io: "ä¸»è¦æ˜¯å¤§æ–‡ä»¶é¡ºåºè¯»å†™"
    - cost_effective: "æˆæœ¬æ§åˆ¶è¦æ±‚ä¸¥æ ¼"
    
  tiered_storage_solution:
    hot_tier:
      storage: "Local NVMe for active computation"
      size: "50TB"
      performance: "æœ€é«˜æ€§èƒ½"
      
    warm_tier:
      storage: "ESSD PL1 for recent data"
      size: "200TB"
      performance: "è‰¯å¥½æ€§èƒ½"
      
    cold_tier:
      storage: "OSS for archived data"
      size: "1000TB+"
      performance: "æˆæœ¬ä¼˜åŒ–"
      
  data_lifecycle_management:
    policies:
      - move_to_warm_after: "30å¤©"
      - move_to_cold_after: "180å¤©"
      - delete_after: "7å¹´(åˆè§„è¦æ±‚)"
```

---
## æœ€ä½³å®è·µæ€»ç»“

### ğŸ”§ æ ¸å¿ƒä¼˜åŒ–åŸåˆ™

1. **æ€§èƒ½ä¸æˆæœ¬å¹³è¡¡**: æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„å­˜å‚¨å±‚çº§
2. **ç›‘æ§é©±åŠ¨ä¼˜åŒ–**: åŸºäºå®é™…ç›‘æ§æ•°æ®è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–
3. **æ¸è¿›å¼æ”¹è¿›**: ä»å°èŒƒå›´è¯•ç‚¹å¼€å§‹ï¼Œé€æ­¥æ¨å¹¿ä¼˜åŒ–æªæ–½
4. **è‡ªåŠ¨åŒ–è¿ç»´**: å»ºç«‹è‡ªåŠ¨åŒ–çš„ç›‘æ§ã€å‘Šè­¦å’Œå“åº”æœºåˆ¶

### ğŸ“Š æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

```markdown
## å­˜å‚¨æ€§èƒ½ä¼˜åŒ–å®æ–½æ¸…å•

### åŸºç¡€é…ç½®æ£€æŸ¥
- [ ] é€‰æ‹©äº†åˆé€‚çš„å­˜å‚¨ç±»å‹å’Œæ€§èƒ½ç­‰çº§
- [ ] é…ç½®äº†ä¼˜åŒ–çš„æŒ‚è½½å‚æ•°
- [ ] è®¾ç½®äº†é€‚å½“çš„æ–‡ä»¶ç³»ç»Ÿå‚æ•°
- [ ] å»ºç«‹äº†åˆ†å±‚å­˜å‚¨ç­–ç•¥

### ç›‘æ§ä½“ç³»å»ºç«‹
- [ ] éƒ¨ç½²äº†æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ç›‘æ§
- [ ] é…ç½®äº†å¤šå±‚çº§å‘Šè­¦ç­–ç•¥
- [ ] å»ºç«‹äº†æ€§èƒ½åŸºçº¿å’Œè¶‹åŠ¿åˆ†æ
- [ ] å®ç°äº†è‡ªåŠ¨åŒ–çš„æ€§èƒ½æŠ¥å‘Š

### ä¼˜åŒ–æ•ˆæœéªŒè¯
- [ ] å®šæœŸè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½æ•°æ®
- [ ] æ”¶é›†ç”¨æˆ·åé¦ˆå’Œåº”ç”¨æ€§èƒ½æŒ‡æ ‡
- [ ] æŒç»­è¿­ä»£ä¼˜åŒ–ç­–ç•¥

### æˆæœ¬æ•ˆç›Šåˆ†æ
- [ ] å®šæœŸè¯„ä¼°å­˜å‚¨æˆæœ¬æ•ˆç›Š
- [ ] åˆ†ææ€§èƒ½æå‡çš„æŠ•èµ„å›æŠ¥ç‡
- [ ] ä¼˜åŒ–å­˜å‚¨èµ„æºåˆ©ç”¨ç‡
- [ ] åˆ¶å®šé•¿æœŸçš„æˆæœ¬æ§åˆ¶ç­–ç•¥
```

---

**è¡¨æ ¼åº•éƒ¨æ ‡è®°**: Kusheet Project, ä½œè€… Allen Galler (allengaller@gmail.com)
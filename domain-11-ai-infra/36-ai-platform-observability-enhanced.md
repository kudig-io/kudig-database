# 36 - AIå¹³å°å¢å¼ºå¯è§‚æµ‹æ€§

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25 - v1.32 | **AIæ ˆç‰ˆæœ¬**: Prometheus 2.40+ | **æœ€åæ›´æ–°**: 2026-02 | **è´¨é‡ç­‰çº§**: ä¸“å®¶çº§

## ä¸€ã€AIå¹³å°å¯è§‚æµ‹æ€§å…¨æ™¯æ¶æ„

### 1.1 äº”ç»´å¯è§‚æµ‹æ€§æ¨¡å‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI Platform Observability Framework                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  ğŸ“Š æŒ‡æ ‡ç›‘æ§ (Metrics)                                                 â”‚
â”‚  â”œâ”€ ç³»ç»ŸæŒ‡æ ‡: CPUã€å†…å­˜ã€GPUã€ç½‘ç»œ                                      â”‚
â”‚  â”œâ”€ åº”ç”¨æŒ‡æ ‡: QPSã€å»¶è¿Ÿã€é”™è¯¯ç‡                                         â”‚
â”‚  â”œâ”€ AIæŒ‡æ ‡: å‡†ç¡®ç‡ã€å›°æƒ‘åº¦ã€ç”Ÿæˆè´¨é‡                                    â”‚
â”‚  â”œâ”€ ä¸šåŠ¡æŒ‡æ ‡: æ”¶å…¥ã€è½¬åŒ–ç‡ã€ç”¨æˆ·æ»¡æ„åº¦                                  â”‚
â”‚  â””â”€ æˆæœ¬æŒ‡æ ‡: èµ„æºæ¶ˆè€—ã€å•ä½æˆæœ¬                                        â”‚
â”‚                                                                         â”‚
â”‚  ğŸªµ æ—¥å¿—åˆ†æ (Logs)                                                    â”‚
â”‚  â”œâ”€ åº”ç”¨æ—¥å¿—: ä¸šåŠ¡é€»è¾‘ã€é”™è¯¯ä¿¡æ¯                                        â”‚
â”‚  â”œâ”€ ç³»ç»Ÿæ—¥å¿—: å†…æ ¸ã€å®¹å™¨è¿è¡Œæ—¶                                          â”‚
â”‚  â”œâ”€ å®‰å…¨æ—¥å¿—: è®¿é—®æ§åˆ¶ã€å¨èƒæ£€æµ‹                                        â”‚
â”‚  â”œâ”€ å®¡è®¡æ—¥å¿—: æ“ä½œè®°å½•ã€åˆè§„è¿½è¸ª                                        â”‚
â”‚  â””â”€ è°ƒè¯•æ—¥å¿—: å¼€å‘è°ƒè¯•ã€æ€§èƒ½åˆ†æ                                        â”‚
â”‚                                                                         â”‚
â”‚  ğŸ” é“¾è·¯è¿½è¸ª (Tracing)                                                 â”‚
â”‚  â”œâ”€ è¯·æ±‚é“¾è·¯: ç«¯åˆ°ç«¯è°ƒç”¨è½¨è¿¹                                            â”‚
â”‚  â”œâ”€ ä¾èµ–å…³ç³»: æœåŠ¡é—´è°ƒç”¨å›¾                                              â”‚
â”‚  â”œâ”€ æ€§èƒ½ç“¶é¢ˆ: çƒ­ç‚¹åˆ†æã€å»¶è¿Ÿåˆ†è§£                                        â”‚
â”‚  â”œâ”€ é”™è¯¯ä¼ æ’­: å¼‚å¸¸æº¯æºã€å½±å“åˆ†æ                                        â”‚
â”‚  â””â”€ AIé“¾è·¯: Promptå¤„ç†ã€æ¨ç†è¿‡ç¨‹                                        â”‚
â”‚                                                                         â”‚
â”‚  ğŸš¨ å‘Šè­¦ç®¡ç† (Alerting)                                                â”‚
â”‚  â”œâ”€ é˜ˆå€¼å‘Šè­¦: é™æ€é˜ˆå€¼ç›‘æ§                                              â”‚
â”‚  â”œâ”€ å¼‚å¸¸æ£€æµ‹: æœºå™¨å­¦ä¹ å¼‚å¸¸è¯†åˆ«                                          â”‚
â”‚  â”œâ”€ é¢„æµ‹å‘Šè­¦: è¶‹åŠ¿é¢„æµ‹ã€å®¹é‡é¢„è­¦                                        â”‚
â”‚  â”œâ”€ æ™ºèƒ½å‘Šè­¦: æ ¹å› åˆ†æã€å…³è”å‘Šè­¦                                        â”‚
â”‚  â””â”€ è‡ªæ„ˆèƒ½åŠ›: è‡ªåŠ¨ä¿®å¤ã€é™çº§å¤„ç†                                        â”‚
â”‚                                                                         â”‚
â”‚  ğŸ“ˆ å¯è§†åŒ–å±•ç¤º (Visualization)                                         â”‚
â”‚  â”œâ”€ å®æ—¶ä»ªè¡¨æ¿: è¿è¡ŒçŠ¶æ€ã€å…³é”®æŒ‡æ ‡                                      â”‚
â”‚  â”œâ”€ å†å²è¶‹åŠ¿: æ€§èƒ½æ¼”å˜ã€å®¹é‡è§„åˆ’                                        â”‚
â”‚  â”œâ”€ å¯¹æ¯”åˆ†æ: ABæµ‹è¯•ã€ç‰ˆæœ¬å¯¹æ¯”                                          â”‚
â”‚  â”œâ”€ ä¸‹é’»åˆ†æ: é—®é¢˜å®šä½ã€æ ¹å› æŸ¥æ‰¾                                        â”‚
â”‚  â””â”€ æŠ¥å‘Šç”Ÿæˆ: è‡ªåŠ¨æŠ¥è¡¨ã€åˆè§„æŠ¥å‘Š                                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 AIå¯è§‚æµ‹æ€§ç‰¹æ®ŠæŒ‘æˆ˜

| æŒ‘æˆ˜ç±»å‹ | å…·ä½“é—®é¢˜ | å½±å“ | è§£å†³æ€è·¯ |
|---------|---------|------|---------|
| **ç»´åº¦å¤æ‚æ€§** | å¤šæ¨¡æ€è¾“å…¥è¾“å‡º | ç›‘æ§æŒ‡æ ‡çˆ†ç‚¸ | ç»Ÿä¸€æŒ‡æ ‡æ¡†æ¶ã€ç»´åº¦æŠ½è±¡ |
| **å®æ—¶æ€§è¦æ±‚** | æ¨ç†å»¶è¿Ÿæ•æ„Ÿ | ç”¨æˆ·ä½“éªŒå½±å“å¤§ | è¾¹ç¼˜è®¡ç®—ã€æµå¼å¤„ç† |
| **è¯­ä¹‰ç†è§£** | éç»“æ„åŒ–æ•°æ® | ä¼ ç»Ÿç›‘æ§å¤±æ•ˆ | AIè¾…åŠ©åˆ†æã€è¯­ä¹‰ç›‘æ§ |
| **åŠ¨æ€ç‰¹æ€§** | æ¨¡å‹æŒç»­æ¼”è¿› | åŸºçº¿ä¸æ–­å˜åŒ– | è‡ªé€‚åº”é˜ˆå€¼ã€åœ¨çº¿å­¦ä¹  |
| **æˆæœ¬æ•æ„Ÿ** | å¤§è§„æ¨¡éƒ¨ç½² | ç›‘æ§æˆæœ¬é«˜æ˜‚ | æ™ºèƒ½é‡‡æ ·ã€åˆ†å±‚ç›‘æ§ |

## äºŒã€ä¼ä¸šçº§AIç›‘æ§æŒ‡æ ‡ä½“ç³»

### 2.1 æ ¸å¿ƒæŒ‡æ ‡åˆ†ç±»

```yaml
# ai-monitoring-metrics.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-platform-metrics-spec
  namespace: monitoring
data:
  metrics-specification.yaml: |
    # AIå¹³å°ç›‘æ§æŒ‡æ ‡è§„èŒƒ
    metrics:
      system_level:
        - name: "node_gpu_utilization"
          type: "gauge"
          description: "GPUåˆ©ç”¨ç‡ç™¾åˆ†æ¯”"
          labels: ["node", "gpu_id", "model"]
          sampling_interval: "15s"
          retention: "30d"
        
        - name: "node_gpu_temperature"
          type: "gauge"
          description: "GPUæ¸©åº¦(æ‘„æ°åº¦)"
          labels: ["node", "gpu_id"]
          sampling_interval: "30s"
          retention: "7d"
        
        - name: "container_memory_working_set_bytes"
          type: "gauge"
          description: "å®¹å™¨å®é™…ä½¿ç”¨å†…å­˜"
          labels: ["namespace", "pod", "container"]
          sampling_interval: "15s"
          retention: "14d"
      
      application_level:
        - name: "http_requests_total"
          type: "counter"
          description: "HTTPè¯·æ±‚æ•°æ€»é‡"
          labels: ["service", "method", "status_code", "model_name"]
          sampling_interval: "5s"
          retention: "90d"
        
        - name: "http_request_duration_seconds"
          type: "histogram"
          description: "HTTPè¯·æ±‚å»¶è¿Ÿåˆ†å¸ƒ"
          labels: ["service", "method", "model_name"]
          buckets: [0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
          sampling_interval: "5s"
          retention: "30d"
        
        - name: "model_inference_duration_seconds"
          type: "histogram"
          description: "æ¨¡å‹æ¨ç†å»¶è¿Ÿ"
          labels: ["model_name", "batch_size", "hardware_type"]
          buckets: [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0]
          sampling_interval: "1s"
          retention: "15d"
      
      ai_specific:
        - name: "model_accuracy"
          type: "gauge"
          description: "æ¨¡å‹å‡†ç¡®ç‡"
          labels: ["model_name", "dataset", "version"]
          sampling_interval: "1h"
          retention: "365d"
        
        - name: "prompt_tokens_total"
          type: "counter"
          description: "Prompt tokenæ¶ˆè€—æ€»é‡"
          labels: ["model_name", "user_id", "application"]
          sampling_interval: "1m"
          retention: "90d"
        
        - name: "generation_tokens_total"
          type: "counter"
          description: "ç”Ÿæˆtokenæ¶ˆè€—æ€»é‡"
          labels: ["model_name", "user_id", "application"]
          sampling_interval: "1m"
          retention: "90d"
        
        - name: "model_drift_score"
          type: "gauge"
          description: "æ¨¡å‹æ¼‚ç§»æ£€æµ‹åˆ†æ•°"
          labels: ["model_name", "feature_name", "drift_type"]
          sampling_interval: "1h"
          retention: "180d"
      
      business_level:
        - name: "api_cost_usd"
          type: "counter"
          description: "APIè°ƒç”¨æˆæœ¬(ç¾å…ƒ)"
          labels: ["service", "model_name", "customer_tier"]
          sampling_interval: "1m"
          retention: "365d"
        
        - name: "user_satisfaction_score"
          type: "gauge"
          description: "ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†"
          labels: ["application", "model_name", "user_segment"]
          sampling_interval: "1d"
          retention: "365d"
        
        - name: "revenue_generated_usd"
          type: "counter"
          description: "äº§ç”Ÿçš„æ”¶å…¥(ç¾å…ƒ)"
          labels: ["product", "model_name", "region"]
          sampling_interval: "1h"
          retention: "365d"
```

### 2.2 Prometheusç›‘æ§è§„åˆ™

```yaml
# ai-prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ai-platform-monitoring-rules
  namespace: monitoring
spec:
  groups:
  - name: ai-platform.rules
    rules:
    # ç³»ç»Ÿå¥åº·æ£€æŸ¥
    - alert: HighGPUMemoryUsage
      expr: |
        avg by(node, gpu_id) (
          nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes * 100
        ) > 90
      for: 5m
      labels:
        severity: critical
        team: ai-platform
      annotations:
        summary: "GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({{ $labels.node }}-{{ $labels.gpu_id }})"
        description: "GPUå†…å­˜ä½¿ç”¨ç‡è¾¾åˆ° {{ $value }}%ï¼Œå¯èƒ½å¯¼è‡´OOMé”™è¯¯"
        runbook_url: "https://internal/wiki/gpu-troubleshooting"
    
    - alert: GPUPowerAnomaly
      expr: |
        stddev_over_time(nvidia_gpu_power_usage_watts[10m]) > 50
      for: 15m
      labels:
        severity: warning
        team: ai-platform
      annotations:
        summary: "GPUåŠŸè€—å‡ºç°å¼‚å¸¸æ³¢åŠ¨"
        description: "GPUåŠŸè€—æ ‡å‡†å·®è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨ç¡¬ä»¶é—®é¢˜"
    
    # åº”ç”¨æ€§èƒ½ç›‘æ§
    - alert: HighInferenceLatency
      expr: |
        histogram_quantile(0.95, 
          sum by(model_name) (
            rate(model_inference_duration_seconds_bucket[5m])
          )
        ) > 2.0
      for: 2m
      labels:
        severity: warning
        team: ml-engineering
      annotations:
        summary: "æ¨¡å‹æ¨ç†å»¶è¿Ÿè¿‡é«˜ ({{ $labels.model_name }})"
        description: "P95æ¨ç†å»¶è¿Ÿè¾¾åˆ° {{ $value }}ç§’ï¼Œè¶…å‡ºSLAè¦æ±‚"
    
    - alert: LowModelAccuracy
      expr: |
        model_accuracy < 0.85
      for: 1h
      labels:
        severity: critical
        team: ml-engineering
      annotations:
        summary: "æ¨¡å‹å‡†ç¡®ç‡ä¸‹é™ ({{ $labels.model_name }})"
        description: "æ¨¡å‹å‡†ç¡®ç‡ {{ $value }} ä½äºé˜ˆå€¼0.85ï¼Œéœ€è¦è°ƒæŸ¥"
    
    # ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§
    - alert: HighAPICost
      expr: |
        sum by(service) (
          rate(api_cost_usd[1h])
        ) > 100
      for: 30m
      labels:
        severity: warning
        team: finance
      annotations:
        summary: "APIæˆæœ¬è¿‡é«˜ ({{ $labels.service }})"
        description: "å°æ—¶APIæˆæœ¬è¾¾åˆ° ${{ $value }}ï¼Œè¶…å‡ºé¢„ç®—"
    
    - alert: UserSatisfactionDrop
      expr: |
        user_satisfaction_score < 3.5
      for: 24h
      labels:
        severity: critical
        team: product
      annotations:
        summary: "ç”¨æˆ·æ»¡æ„åº¦ä¸‹é™"
        description: "ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ† {{ $value }} ä½äºé˜ˆå€¼3.5"
    
    # AIç‰¹æœ‰é—®é¢˜
    - alert: ModelDriftDetected
      expr: |
        model_drift_score > 0.1
      for: 6h
      labels:
        severity: warning
        team: ml-engineering
      annotations:
        summary: "æ£€æµ‹åˆ°æ¨¡å‹æ¼‚ç§» ({{ $labels.model_name }})"
        description: "æ¨¡å‹æ¼‚ç§»åˆ†æ•° {{ $value }}ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹"
    
    - alert: TokenConsumptionSpike
      expr: |
        rate(prompt_tokens_total[5m]) > 100000
      for: 10m
      labels:
        severity: info
        team: ml-engineering
      annotations:
        summary: "Tokenæ¶ˆè€—æ¿€å¢"
        description: "Prompt tokenæ¶ˆè€—é€Ÿç‡å¼‚å¸¸å¢é•¿ï¼Œå½“å‰ä¸º {{ $value }}/sec"

  - name: ai-platform-recording.rules
    rules:
    # é¢„è®¡ç®—å¸¸ç”¨æŒ‡æ ‡
    - record: "node:gpu_utilization:avg5m"
      expr: |
        avg_over_time(nvidia_gpu_utilization[5m])
    
    - record: "model:inference_p95_latency:1h"
      expr: |
        histogram_quantile(0.95, 
          sum by(model_name) (
            rate(model_inference_duration_seconds_bucket[1h])
          )
        )
    
    - record: "service:daily_cost:usd"
      expr: |
        sum by(service) (
          increase(api_cost_usd[24h])
        )
    
    - record: "model:drift_trend:7d"
      expr: |
        avg_over_time(model_drift_score[7d])
```

## ä¸‰ã€åˆ†å¸ƒå¼è¿½è¸ªä¸é“¾è·¯åˆ†æ

### 3.1 AIæœåŠ¡é“¾è·¯è¿½è¸ªæ¶æ„

```python
# ai-tracing-instrumentation.py
import asyncio
import time
from typing import Dict, List, Optional, Any
import uuid
from dataclasses import dataclass, field
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.trace import SpanKind
import logging
import json

# åˆå§‹åŒ–OpenTelemetry
trace.set_tracer_provider(TracerProvider())
otlp_exporter = OTLPSpanExporter(endpoint="http://jaeger-collector:4317", insecure=True)
span_processor = BatchSpanProcessor(otlp_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

tracer = trace.get_tracer(__name__)

@dataclass
class AIRequestContext:
    request_id: str
    model_name: str
    user_id: str
    prompt: str
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)

class AITracingInstrumentation:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def trace_model_inference(self, context: AIRequestContext, batch_size: int = 1):
        """è¿½è¸ªæ¨¡å‹æ¨ç†å…¨è¿‡ç¨‹"""
        with tracer.start_as_current_span(
            f"model_inference_{context.model_name}",
            kind=SpanKind.SERVER,
            attributes={
                "ai.request_id": context.request_id,
                "ai.model_name": context.model_name,
                "ai.user_id": context.user_id,
                "ai.batch_size": batch_size,
                "ai.prompt_length": len(context.prompt),
            }
        ) as span:
            
            # é¢„å¤„ç†é˜¶æ®µ
            preprocessing_result = self._trace_preprocessing(context, span)
            
            # æ¨¡å‹æ¨ç†é˜¶æ®µ
            inference_result = self._trace_model_inference(preprocessing_result, span)
            
            # åå¤„ç†é˜¶æ®µ
            postprocessing_result = self._trace_postprocessing(inference_result, span)
            
            # è®¾ç½®æœ€ç»ˆå±æ€§
            span.set_attribute("ai.response_length", len(postprocessing_result.get("response", "")))
            span.set_attribute("ai.tokens_input", preprocessing_result.get("input_tokens", 0))
            span.set_attribute("ai.tokens_output", postprocessing_result.get("output_tokens", 0))
            
            return postprocessing_result
    
    def _trace_preprocessing(self, context: AIRequestContext, parent_span) -> Dict:
        """è¿½è¸ªé¢„å¤„ç†é˜¶æ®µ"""
        with tracer.start_as_current_span(
            "preprocessing",
            context=trace.set_span_in_context(parent_span),
            attributes={
                "component": "tokenizer",
                "ai.model_name": context.model_name,
            }
        ) as span:
            
            start_time = time.time()
            
            # æ¨¡æ‹Ÿé¢„å¤„ç†é€»è¾‘
            tokens = self._tokenize_prompt(context.prompt, context.model_name)
            input_ids = tokens["input_ids"]
            
            processing_time = time.time() - start_time
            
            span.set_attribute("ai.input_tokens", len(input_ids))
            span.set_attribute("ai.preprocessing_time", processing_time)
            span.set_status(trace.Status(trace.StatusCode.OK))
            
            return {
                "input_ids": input_ids,
                "attention_mask": tokens["attention_mask"],
                "input_tokens": len(input_ids),
                "processing_time": processing_time
            }
    
    def _trace_model_inference(self, preprocessed_data: Dict, parent_span) -> Dict:
        """è¿½è¸ªæ¨¡å‹æ¨ç†é˜¶æ®µ"""
        with tracer.start_as_current_span(
            "model_inference",
            context=trace.set_span_in_context(parent_span),
            attributes={
                "component": "model",
                "ai.model_name": preprocessed_data.get("model_name", "unknown"),
                "ai.input_tokens": preprocessed_data.get("input_tokens", 0),
            }
        ) as span:
            
            start_time = time.time()
            
            # æ¨¡æ‹Ÿæ¨¡å‹æ¨ç†
            logits, hidden_states = self._run_model_inference(
                preprocessed_data["input_ids"],
                preprocessed_data["attention_mask"]
            )
            
            inference_time = time.time() - start_time
            
            span.set_attribute("ai.inference_time", inference_time)
            span.set_attribute("ai.output_logits_shape", str(logits.shape))
            span.set_status(trace.Status(trace.StatusCode.OK))
            
            return {
                "logits": logits,
                "hidden_states": hidden_states,
                "inference_time": inference_time
            }
    
    def _trace_postprocessing(self, inference_result: Dict, parent_span) -> Dict:
        """è¿½è¸ªåå¤„ç†é˜¶æ®µ"""
        with tracer.start_as_current_span(
            "postprocessing",
            context=trace.set_span_in_context(parent_span),
            attributes={
                "component": "decoder",
                "ai.model_name": inference_result.get("model_name", "unknown"),
            }
        ) as span:
            
            start_time = time.time()
            
            # æ¨¡æ‹Ÿåå¤„ç†é€»è¾‘
            response_text, output_tokens = self._decode_output(inference_result["logits"])
            
            processing_time = time.time() - start_time
            
            span.set_attribute("ai.output_tokens", output_tokens)
            span.set_attribute("ai.postprocessing_time", processing_time)
            span.set_status(trace.Status(trace.StatusCode.OK))
            
            return {
                "response": response_text,
                "output_tokens": output_tokens,
                "processing_time": processing_time
            }
    
    def _tokenize_prompt(self, prompt: str, model_name: str) -> Dict:
        """æ¨¡æ‹Ÿåˆ†è¯"""
        # ç®€åŒ–çš„åˆ†è¯é€»è¾‘
        tokens = prompt.split()
        return {
            "input_ids": list(range(len(tokens))),
            "attention_mask": [1] * len(tokens)
        }
    
    def _run_model_inference(self, input_ids: List[int], attention_mask: List[int]):
        """æ¨¡æ‹Ÿæ¨¡å‹æ¨ç†"""
        # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
        time.sleep(0.1 + len(input_ids) * 0.001)
        
        # æ¨¡æ‹Ÿè¾“å‡º
        import numpy as np
        batch_size = 1
        seq_len = len(input_ids)
        vocab_size = 32000
        
        logits = np.random.randn(batch_size, seq_len, vocab_size).astype(np.float32)
        hidden_states = np.random.randn(batch_size, seq_len, 4096).astype(np.float32)
        
        return logits, hidden_states
    
    def _decode_output(self, logits) -> tuple:
        """æ¨¡æ‹Ÿè§£ç è¾“å‡º"""
        # ç®€åŒ–çš„è§£ç é€»è¾‘
        import numpy as np
        batch_size, seq_len, vocab_size = logits.shape
        
        # é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„token
        output_ids = np.argmax(logits, axis=-1)[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        output_tokens = len(output_ids)
        
        # è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆç®€åŒ–ï¼‰
        response_text = " ".join([f"token_{token_id}" for token_id in output_ids[:50]])
        
        return response_text, output_tokens

class AIPerformanceAnalyzer:
    def __init__(self):
        self.tracer = trace.get_tracer(__name__)
        self.logger = logging.getLogger(__name__)
    
    def analyze_inference_performance(self, traces: List[Dict]) -> Dict:
        """åˆ†ææ¨ç†æ€§èƒ½"""
        if not traces:
            return {}
        
        # æå–å…³é”®æ€§èƒ½æŒ‡æ ‡
        preprocessing_times = []
        inference_times = []
        postprocessing_times = []
        total_times = []
        
        for trace_data in traces:
            spans = trace_data.get("spans", [])
            
            # æå–å„é˜¶æ®µè€—æ—¶
            for span in spans:
                if span.get("name") == "preprocessing":
                    preprocessing_times.append(span.get("attributes", {}).get("ai.preprocessing_time", 0))
                elif span.get("name") == "model_inference":
                    inference_times.append(span.get("attributes", {}).get("ai.inference_time", 0))
                elif span.get("name") == "postprocessing":
                    postprocessing_times.append(span.get("attributes", {}).get("ai.postprocessing_time", 0))
                elif span.get("name", "").startswith("model_inference_"):
                    total_times.append(span.get("duration", 0) / 1_000_000_000)  # çº³ç§’è½¬ç§’
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        analysis = {
            "total_requests": len(traces),
            "performance_metrics": {
                "preprocessing": self._calculate_stats(preprocessing_times),
                "inference": self._calculate_stats(inference_times),
                "postprocessing": self._calculate_stats(postprocessing_times),
                "total": self._calculate_stats(total_times)
            },
            "bottlenecks": self._identify_bottlenecks({
                "preprocessing": preprocessing_times,
                "inference": inference_times,
                "postprocessing": postprocessing_times
            }),
            "recommendations": self._generate_recommendations({
                "preprocessing": preprocessing_times,
                "inference": inference_times,
                "postprocessing": postprocessing_times
            })
        }
        
        return analysis
    
    def _calculate_stats(self, times: List[float]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        if not times:
            return {}
        
        import numpy as np
        times_array = np.array(times)
        
        return {
            "count": len(times),
            "mean": float(np.mean(times_array)),
            "median": float(np.median(times_array)),
            "p95": float(np.percentile(times_array, 95)),
            "p99": float(np.percentile(times_array, 99)),
            "min": float(np.min(times_array)),
            "max": float(np.max(times_array)),
            "std": float(np.std(times_array))
        }
    
    def _identify_bottlenecks(self, timing_data: Dict) -> List[str]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # è®¡ç®—å„é˜¶æ®µå¹³å‡è€—æ—¶å æ¯”
        total_avg = sum(np.mean(times) for times in timing_data.values() if times)
        
        if total_avg > 0:
            for stage, times in timing_data.items():
                if times:
                    avg_time = np.mean(times)
                    percentage = (avg_time / total_avg) * 100
                    if percentage > 50:  # å¦‚æœæŸé˜¶æ®µå æ€»æ—¶é—´è¶…è¿‡50%
                        bottlenecks.append(f"{stage}é˜¶æ®µæ˜¯ä¸»è¦ç“¶é¢ˆ ({percentage:.1f}%)")
        
        return bottlenecks
    
    def _generate_recommendations(self, timing_data: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # é¢„å¤„ç†ä¼˜åŒ–å»ºè®®
        if timing_data.get("preprocessing") and np.mean(timing_data["preprocessing"]) > 0.05:
            recommendations.append("é¢„å¤„ç†è€—æ—¶è¾ƒé•¿ï¼Œè€ƒè™‘ä¼˜åŒ–åˆ†è¯ç®—æ³•æˆ–ä½¿ç”¨æ›´å¿«çš„tokenizer")
        
        # æ¨ç†ä¼˜åŒ–å»ºè®®
        if timing_data.get("inference") and np.mean(timing_data["inference"]) > 0.5:
            recommendations.append("æ¨¡å‹æ¨ç†æ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘æ¨¡å‹é‡åŒ–ã€æ‰¹å¤„ç†æˆ–ä½¿ç”¨æ›´é«˜æ•ˆçš„æ¨ç†å¼•æ“")
        
        # åå¤„ç†ä¼˜åŒ–å»ºè®®
        if timing_data.get("postprocessing") and np.mean(timing_data["postprocessing"]) > 0.05:
            recommendations.append("åå¤„ç†è€—æ—¶è¾ƒé«˜ï¼Œè€ƒè™‘ä¼˜åŒ–è§£ç ç®—æ³•æˆ–å¹¶è¡Œå¤„ç†")
        
        # ä¸€èˆ¬å»ºè®®
        recommendations.append("å¯ç”¨æŒç»­æ€§èƒ½ç›‘æ§ï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿")
        recommendations.append("å®æ–½è‡ªåŠ¨æ‰©ç¼©å®¹ä»¥åº”å¯¹è´Ÿè½½å˜åŒ–")
        
        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    instrumentation = AITracingInstrumentation()
    analyzer = AIPerformanceAnalyzer()
    
    # æ¨¡æ‹Ÿå¤šä¸ªæ¨ç†è¯·æ±‚
    traces_collected = []
    
    for i in range(10):
        request_context = AIRequestContext(
            request_id=str(uuid.uuid4()),
            model_name="llama2-7b",
            user_id=f"user_{i}",
            prompt="Write a short story about AI observability"
        )
        
        # æ‰§è¡Œå¸¦è¿½è¸ªçš„æ¨ç†
        result = instrumentation.trace_model_inference(request_context, batch_size=1)
        traces_collected.append(result)
        
        print(f"Request {i+1} completed in {result.get('total_time', 0):.3f}s")
    
    # åˆ†ææ€§èƒ½
    performance_analysis = analyzer.analyze_inference_performance(traces_collected)
    print("\nPerformance Analysis:")
    print(json.dumps(performance_analysis, indent=2))

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
```

### 3.2 Jaegerè¿½è¸ªé…ç½®

```yaml
# jaeger-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-all-in-one
  namespace: observability
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "14269"
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.42
        ports:
        - containerPort: 5775
          protocol: UDP
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        - containerPort: 5778
          protocol: TCP
        - containerPort: 16686
          protocol: TCP
        - containerPort: 14250
          protocol: TCP
        - containerPort: 14268
          protocol: TCP
        - containerPort: 14269
          protocol: TCP
        - containerPort: 4317
          protocol: TCP
        - containerPort: 4318
          protocol: TCP
        env:
        - name: SPAN_STORAGE_TYPE
          value: badger
        - name: BADGER_EPHEMERAL
          value: "false"
        - name: BADGER_DIRECTORY_VALUE
          value: "/badger/data"
        - name: BADGER_DIRECTORY_KEY
          value: "/badger/key"
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: LOG_LEVEL
          value: info
        livenessProbe:
          httpGet:
            path: "/"
            port: 14269
          initialDelaySeconds: 5
        readinessProbe:
          httpGet:
            path: "/"
            port: 14269
          initialDelaySeconds: 1
        volumeMounts:
        - name: badger-data
          mountPath: /badger
      volumes:
      - name: badger-data
        persistentVolumeClaim:
          claimName: jaeger-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-query
  namespace: observability
spec:
  ports:
  - name: query-http
    port: 16686
    protocol: TCP
    targetPort: 16686
  selector:
    app: jaeger
  type: ClusterIP

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: jaeger-pvc
  namespace: observability
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: fast-ssd
```

## å››ã€æ™ºèƒ½å‘Šè­¦ä¸è‡ªæ„ˆç³»ç»Ÿ

### 4.1 æœºå™¨å­¦ä¹ é©±åŠ¨çš„å¼‚å¸¸æ£€æµ‹

```python
# ml-anomaly-detection.py
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
import joblib
from typing import Dict, List, Tuple, Optional
import asyncio
import logging
from datetime import datetime, timedelta
import json

class MLAnomalyDetector:
    def __init__(self, model_name: str = "isolation_forest"):
        self.model_name = model_name
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = []
        self.is_fitted = False
        self.logger = logging.getLogger(__name__)
        
    def prepare_features(self, metrics_data: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        # é€‰æ‹©æ•°å€¼å‹æŒ‡æ ‡
        numeric_columns = metrics_data.select_dtypes(include=[np.number]).columns.tolist()
        
        # ç§»é™¤æ—¶é—´æˆ³åˆ—
        feature_columns = [col for col in numeric_columns if col != 'timestamp']
        self.feature_columns = feature_columns
        
        # æå–ç‰¹å¾
        features = metrics_data[feature_columns].copy()
        
        # æ·»åŠ æ´¾ç”Ÿç‰¹å¾
        for col in feature_columns:
            if col.startswith(('cpu_', 'memory_', 'gpu_')):
                # è®¡ç®—å˜åŒ–ç‡
                features[f'{col}_rate'] = features[col].diff().fillna(0)
                # è®¡ç®—ç§»åŠ¨å¹³å‡
                features[f'{col}_ma_5'] = features[col].rolling(window=5, min_periods=1).mean()
                features[f'{col}_ma_15'] = features[col].rolling(window=15, min_periods=1).mean()
        
        return features
    
    def train(self, training_data: pd.DataFrame, contamination: float = 0.1) -> Dict:
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        try:
            # å‡†å¤‡ç‰¹å¾
            features = self.prepare_features(training_data)
            
            # æ ‡å‡†åŒ–
            scaled_features = self.scaler.fit_transform(features)
            
            # è®­ç»ƒæ¨¡å‹
            self.model = IsolationForest(
                contamination=contamination,
                random_state=42,
                n_estimators=100
            )
            self.model.fit(scaled_features)
            
            self.is_fitted = True
            
            # è¯„ä¼°æ¨¡å‹
            predictions = self.model.predict(scaled_features)
            anomaly_rate = np.sum(predictions == -1) / len(predictions)
            
            return {
                "status": "success",
                "anomaly_rate": float(anomaly_rate),
                "training_samples": len(training_data),
                "features_used": len(self.feature_columns),
                "model_parameters": {
                    "contamination": contamination,
                    "n_estimators": 100
                }
            }
            
        except Exception as e:
            self.logger.error(f"Training failed: {e}")
            return {"status": "error", "message": str(e)}
    
    def predict(self, test_data: pd.DataFrame) -> Dict:
        """é¢„æµ‹å¼‚å¸¸"""
        if not self.is_fitted:
            raise ValueError("Model not trained yet")
        
        try:
            # å‡†å¤‡ç‰¹å¾
            features = self.prepare_features(test_data)
            
            # æ ‡å‡†åŒ–
            scaled_features = self.scaler.transform(features)
            
            # é¢„æµ‹
            predictions = self.model.predict(scaled_features)
            anomaly_scores = self.model.decision_function(scaled_features)
            
            # è¿”å›ç»“æœ
            results = []
            for i, (pred, score) in enumerate(zip(predictions, anomaly_scores)):
                results.append({
                    "index": i,
                    "is_anomaly": pred == -1,
                    "anomaly_score": float(score),
                    "confidence": float(abs(score)),
                    "timestamp": test_data.iloc[i]['timestamp'] if 'timestamp' in test_data.columns else None
                })
            
            return {
                "predictions": results,
                "anomaly_count": int(np.sum(predictions == -1)),
                "anomaly_percentage": float(np.mean(predictions == -1) * 100)
            }
            
        except Exception as e:
            self.logger.error(f"Prediction failed: {e}")
            return {"status": "error", "message": str(e)}
    
    def save_model(self, filepath: str) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            model_data = {
                'model': self.model,
                'scaler': self.scaler,
                'feature_columns': self.feature_columns,
                'is_fitted': self.is_fitted
            }
            joblib.dump(model_data, filepath)
            return True
        except Exception as e:
            self.logger.error(f"Failed to save model: {e}")
            return False
    
    def load_model(self, filepath: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            model_data = joblib.load(filepath)
            self.model = model_data['model']
            self.scaler = model_data['scaler']
            self.feature_columns = model_data['feature_columns']
            self.is_fitted = model_data['is_fitted']
            return True
        except Exception as e:
            self.logger.error(f"Failed to load model: {e}")
            return False

class AlertCorrelationEngine:
    def __init__(self):
        self.alert_history = []
        self.correlation_window = timedelta(hours=1)
        self.logger = logging.getLogger(__name__)
    
    def correlate_alerts(self, new_alerts: List[Dict]) -> List[Dict]:
        """å…³è”å‘Šè­¦"""
        correlated_alerts = []
        
        for alert in new_alerts:
            # æŸ¥æ‰¾ç›¸å…³å†å²å‘Šè­¦
            related_alerts = self._find_related_alerts(alert)
            
            if related_alerts:
                # åˆ›å»ºå…³è”å‘Šè­¦
                correlated_alert = {
                    "alert_id": alert.get("alert_id"),
                    "type": "correlated",
                    "primary_alert": alert,
                    "related_alerts": related_alerts,
                    "correlation_score": self._calculate_correlation_score(alert, related_alerts),
                    "root_cause_analysis": self._analyze_root_cause(alert, related_alerts),
                    "recommended_actions": self._suggest_actions(alert, related_alerts)
                }
                correlated_alerts.append(correlated_alert)
            else:
                # å•ç‹¬å‘Šè­¦
                correlated_alerts.append({
                    "alert_id": alert.get("alert_id"),
                    "type": "isolated",
                    "alert": alert
                })
        
        # æ›´æ–°å†å²è®°å½•
        self.alert_history.extend(new_alerts)
        self._cleanup_old_alerts()
        
        return correlated_alerts
    
    def _find_related_alerts(self, target_alert: Dict) -> List[Dict]:
        """æŸ¥æ‰¾ç›¸å…³å‘Šè­¦"""
        related = []
        target_time = datetime.fromisoformat(target_alert.get("timestamp", datetime.now().isoformat()))
        
        for historical_alert in self.alert_history:
            hist_time = datetime.fromisoformat(historical_alert.get("timestamp", datetime.now().isoformat()))
            
            # æ—¶é—´çª—å£æ£€æŸ¥
            if abs((target_time - hist_time).total_seconds()) <= self.correlation_window.total_seconds():
                # ç›¸å…³æ€§æ£€æŸ¥
                if self._are_alerts_related(target_alert, historical_alert):
                    related.append(historical_alert)
        
        return related
    
    def _are_alerts_related(self, alert1: Dict, alert2: Dict) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå‘Šè­¦æ˜¯å¦ç›¸å…³"""
        # åŸºäºæ ‡ç­¾çš„ç›¸å…³æ€§
        labels1 = set(alert1.get("labels", {}).keys())
        labels2 = set(alert2.get("labels", {}).keys())
        
        common_labels = labels1.intersection(labels2)
        if len(common_labels) >= 2:  # è‡³å°‘æœ‰ä¸¤ä¸ªå…±åŒæ ‡ç­¾
            return True
        
        # åŸºäºæœåŠ¡çš„ç›¸å…³æ€§
        service1 = alert1.get("labels", {}).get("service")
        service2 = alert2.get("labels", {}).get("service")
        if service1 and service2 and service1 == service2:
            return True
        
        # åŸºäºèŠ‚ç‚¹çš„ç›¸å…³æ€§
        node1 = alert1.get("labels", {}).get("node")
        node2 = alert2.get("labels", {}).get("node")
        if node1 and node2 and node1 == node2:
            return True
        
        return False
    
    def _calculate_correlation_score(self, target_alert: Dict, related_alerts: List[Dict]) -> float:
        """è®¡ç®—å…³è”åˆ†æ•°"""
        if not related_alerts:
            return 0.0
        
        scores = []
        target_labels = set(target_alert.get("labels", {}).keys())
        
        for related_alert in related_alerts:
            related_labels = set(related_alert.get("labels", {}).keys())
            common_labels = len(target_labels.intersection(related_labels))
            total_labels = len(target_labels.union(related_labels))
            
            if total_labels > 0:
                jaccard_similarity = common_labels / total_labels
                scores.append(jaccard_similarity)
        
        return float(np.mean(scores)) if scores else 0.0
    
    def _analyze_root_cause(self, target_alert: Dict, related_alerts: List[Dict]) -> Dict:
        """åˆ†ææ ¹æœ¬åŸå› """
        analysis = {
            "primary_indicators": [],
            "supporting_evidence": [],
            "likely_root_causes": []
        }
        
        # åˆ†æä¸»è¦æŒ‡æ ‡
        target_severity = target_alert.get("labels", {}).get("severity", "")
        if target_severity == "critical":
            analysis["primary_indicators"].append("å…³é”®çº§åˆ«å‘Šè­¦")
        
        # æ”¶é›†æ”¯æŒè¯æ®
        for related in related_alerts:
            evidence = {
                "alert": related.get("alertname", "Unknown"),
                "severity": related.get("labels", {}).get("severity", ""),
                "description": related.get("annotations", {}).get("description", "")
            }
            analysis["supporting_evidence"].append(evidence)
        
        # æ¨æ–­å¯èƒ½çš„æ ¹æœ¬åŸå› 
        services_involved = set()
        for alert in [target_alert] + related_alerts:
            service = alert.get("labels", {}).get("service")
            if service:
                services_involved.add(service)
        
        if len(services_involved) == 1:
            analysis["likely_root_causes"].append(f"æœåŠ¡ {list(services_involved)[0]} çš„é—®é¢˜")
        else:
            analysis["likely_root_causes"].append("åŸºç¡€è®¾æ–½å±‚é¢çš„é—®é¢˜")
        
        return analysis
    
    def _suggest_actions(self, target_alert: Dict, related_alerts: List[Dict]) -> List[str]:
        """å»ºè®®è¡ŒåŠ¨æ–¹æ¡ˆ"""
        actions = []
        
        # åŸºäºä¸¥é‡ç¨‹åº¦çš„å»ºè®®
        severity = target_alert.get("labels", {}).get("severity", "")
        if severity == "critical":
            actions.append("ç«‹å³è°ƒæŸ¥å¹¶é‡‡å–ç´§æ€¥æªæ–½")
            actions.append("é€šçŸ¥ç›¸å…³å›¢é˜Ÿè´Ÿè´£äºº")
        
        # åŸºäºå‘Šè­¦ç±»å‹çš„å»ºè®®
        alert_name = target_alert.get("alertname", "")
        if "HighLatency" in alert_name:
            actions.append("æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒæœåŠ¡ä¾èµ–")
        elif "HighErrorRate" in alert_name:
            actions.append("æŸ¥çœ‹åº”ç”¨æ—¥å¿—å’Œé”™è¯¯å †æ ˆ")
        elif "HighCPULoad" in alert_name:
            actions.append("åˆ†æCPUä½¿ç”¨æ¨¡å¼å’Œè¿›ç¨‹æ´»åŠ¨")
        
        # åŸºäºå…³è”å‘Šè­¦çš„å»ºè®®
        if related_alerts:
            actions.append("æ‰§è¡Œå…³è”å‘Šè­¦çš„ç»¼åˆåˆ†æ")
            actions.append("æ£€æŸ¥å…±äº«èµ„æºçš„ä½¿ç”¨æƒ…å†µ")
        
        return actions
    
    def _cleanup_old_alerts(self):
        """æ¸…ç†æ—§å‘Šè­¦è®°å½•"""
        cutoff_time = datetime.now() - timedelta(days=7)
        self.alert_history = [
            alert for alert in self.alert_history
            if datetime.fromisoformat(alert.get("timestamp", datetime.now().isoformat())) > cutoff_time
        ]

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # æ¨¡æ‹Ÿç›‘æ§æ•°æ®
    timestamps = pd.date_range(start='2024-01-01', periods=1000, freq='5T')
    metrics_data = pd.DataFrame({
        'timestamp': timestamps,
        'cpu_utilization': np.random.normal(50, 15, 1000),
        'memory_utilization': np.random.normal(60, 20, 1000),
        'gpu_utilization': np.random.normal(70, 25, 1000),
        'request_latency': np.random.exponential(0.1, 1000),
        'error_rate': np.random.beta(1, 50, 1000)  # ä½é”™è¯¯ç‡
    })
    
    # æ³¨å…¥ä¸€äº›å¼‚å¸¸æ•°æ®
    anomaly_indices = np.random.choice(1000, 20, replace=False)
    metrics_data.loc[anomaly_indices, 'cpu_utilization'] += 40
    metrics_data.loc[anomaly_indices, 'request_latency'] *= 5
    
    # è®­ç»ƒæ¨¡å‹
    detector = MLAnomalyDetector()
    training_result = detector.train(metrics_data.head(800))
    print(f"Training result: {training_result}")
    
    # é¢„æµ‹å¼‚å¸¸
    test_data = metrics_data.tail(200)
    prediction_result = detector.predict(test_data)
    print(f"Anomalies detected: {prediction_result['anomaly_count']} "
          f"({prediction_result['anomaly_percentage']:.1f}%)")
    
    # å‘Šè­¦å…³è”åˆ†æ
    engine = AlertCorrelationEngine()
    
    sample_alerts = [
        {
            "alert_id": "alert_001",
            "alertname": "HighCPULoad",
            "labels": {"severity": "warning", "service": "llm-inference", "node": "node-01"},
            "annotations": {"description": "CPUä½¿ç”¨ç‡è¶…è¿‡80%"},
            "timestamp": datetime.now().isoformat()
        },
        {
            "alert_id": "alert_002",
            "alertname": "HighLatency",
            "labels": {"severity": "warning", "service": "llm-inference", "node": "node-01"},
            "annotations": {"description": "è¯·æ±‚å»¶è¿Ÿè¶…è¿‡1ç§’"},
            "timestamp": (datetime.now() - timedelta(minutes=5)).isoformat()
        }
    ]
    
    correlated_results = engine.correlate_alerts(sample_alerts)
    print(f"\nCorrelated alerts: {len(correlated_results)}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
```

## äº”ã€ç”Ÿäº§çº§å¯è§‚æµ‹æ€§æœ€ä½³å®è·µ
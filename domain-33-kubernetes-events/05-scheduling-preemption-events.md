# 05 - è°ƒåº¦ä¸æŠ¢å äº‹ä»¶

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25 - v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä½œè€…**: Allen Galler

> **æœ¬æ–‡æ¡£è¯¦ç»†è®°å½• kube-scheduler å’Œ node-controller äº§ç”Ÿçš„æ‰€æœ‰è°ƒåº¦ä¸æŠ¢å ç›¸å…³äº‹ä»¶,å¸®åŠ©è¿ç»´äººå‘˜å¿«é€Ÿå®šä½å’Œè§£å†³è°ƒåº¦é—®é¢˜ã€‚**

---

## ğŸ“‹ äº‹ä»¶é€ŸæŸ¥è¡¨

| äº‹ä»¶åŸå›  | ä¸­æ–‡å | ç±»å‹ | æ¥æºç»„ä»¶ | ç”Ÿäº§é¢‘ç‡ | ç‰ˆæœ¬ | å…¸å‹åœºæ™¯ |
|:--------|:------|:-----|:--------|:--------|:-----|:--------|
| `Scheduled` | è°ƒåº¦æˆåŠŸ | Normal | scheduler | é«˜é¢‘ | v1.0+ | Pod æˆåŠŸç»‘å®šåˆ°èŠ‚ç‚¹ |
| `FailedScheduling` | è°ƒåº¦å¤±è´¥ | Warning | scheduler | ä¸­é¢‘ | v1.0+ | æ— å¯ç”¨èŠ‚ç‚¹/èµ„æºä¸è¶³ |
| `Preempted` | è¢«æŠ¢å  | Normal | scheduler | ä½é¢‘ | v1.11+ | é«˜ä¼˜å…ˆçº§ Pod æŠ¢å  |
| `WaitingForGates` | ç­‰å¾…è°ƒåº¦é—¨ | Normal | scheduler | ä½é¢‘ | v1.26+ | è‡ªå®šä¹‰è°ƒåº¦æ§åˆ¶ |
| `TaintManagerEviction` | æ±¡ç‚¹é©±é€ | Normal | node-controller | ä½é¢‘ | v1.13+ | èŠ‚ç‚¹çŠ¶æ€å¼‚å¸¸é©±é€ |
| `FailedBinding` | ç»‘å®šå¤±è´¥ | Warning | scheduler | ç½•è§ | v1.0+ | è°ƒåº¦ç»‘å®šå†²çª |

---

## ğŸ”„ è°ƒåº¦å™¨å·¥ä½œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    kube-scheduler è°ƒåº¦æµç¨‹                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Pod åˆ›å»º(Pending) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  è°ƒåº¦å‰æ£€æŸ¥         â”‚
                    â”‚  - SchedulingGates â”‚
                    â”‚  - PriorityClass   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”œâ”€[æœ‰ Gates]â”€â”€> [WaitingForGates]
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  è¿‡æ»¤é˜¶æ®µ (Filter)  â”‚
                    â”‚  - Resource Fit    â”‚
                    â”‚  - Node Affinity   â”‚
                    â”‚  - Taints/Tolerationsâ”‚
                    â”‚  - Topology Spread â”‚
                    â”‚  - Volume Binding  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  æ‰“åˆ†é˜¶æ®µ (Score)   â”‚
                    â”‚  - è´Ÿè½½å‡è¡¡         â”‚
                    â”‚  - èµ„æºåˆ†å¸ƒ         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  æŠ¢å æ£€æŸ¥(è‹¥å¤±è´¥)   â”‚
                    â”‚  - ä¼˜å…ˆçº§æ¯”è¾ƒ       â”‚
                    â”‚  - æŠ¢å å¯è¡Œæ€§       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚             â”‚             â”‚
        [æ— å¯ç”¨èŠ‚ç‚¹]      [æŠ¢å æˆåŠŸ]      [è°ƒåº¦æˆåŠŸ]
                â”‚             â”‚             â”‚
                â–¼             â–¼             â–¼
      [FailedScheduling] [Preempted]  [Scheduled]
                                           â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  ç»‘å®šåˆ°èŠ‚ç‚¹ (Bind)       â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                    [Pod Running]
```

---

## ğŸ“Œ äº‹ä»¶è¯¦ç»†è¯´æ˜

### `Scheduled` - è°ƒåº¦æˆåŠŸ

| å±æ€§ | è¯´æ˜ |
|:---|:---|
| **äº‹ä»¶ç±»å‹** | Normal |
| **æ¥æºç»„ä»¶** | default-scheduler |
| **å…³è”èµ„æº** | Pod |
| **é€‚ç”¨ç‰ˆæœ¬** | v1.0+ |
| **ç”Ÿäº§é¢‘ç‡** | é«˜é¢‘ |

#### äº‹ä»¶å«ä¹‰
è¡¨ç¤º Pod å·²æˆåŠŸé€šè¿‡è°ƒåº¦å™¨çš„æ‰€æœ‰è¿‡æ»¤å’Œæ‰“åˆ†é˜¶æ®µ,å¹¶è¢«ç»‘å®šåˆ°ç‰¹å®šçš„èŠ‚ç‚¹ä¸Šã€‚è¿™æ˜¯ Pod ç”Ÿå‘½å‘¨æœŸä¸­æœ€å…³é”®çš„äº‹ä»¶ä¹‹ä¸€,æ ‡å¿—ç€ Pod ä» Pending çŠ¶æ€è¿›å…¥ Running çŠ¶æ€çš„è½¬æŠ˜ç‚¹ã€‚è°ƒåº¦å™¨ä¼šè®°å½•è°ƒåº¦å†³ç­–çš„èŠ‚ç‚¹åç§°å’Œè°ƒåº¦è€—æ—¶ã€‚

æ­¤äº‹ä»¶äº§ç”Ÿå,kubelet å°†æ¥ç®¡ Pod çš„åç»­ç”Ÿå‘½å‘¨æœŸç®¡ç†,åŒ…æ‹¬é•œåƒæ‹‰å–ã€å®¹å™¨åˆ›å»ºã€å¥åº·æ£€æŸ¥ç­‰ã€‚è°ƒåº¦æˆåŠŸå¹¶ä¸ä»£è¡¨ Pod ä¸€å®šèƒ½æˆåŠŸè¿è¡Œ,åç»­ä»å¯èƒ½å› é•œåƒæ‹‰å–å¤±è´¥ã€èµ„æºé™åˆ¶ç­‰é—®é¢˜å¯¼è‡´å¯åŠ¨å¤±è´¥ã€‚

#### å…¸å‹äº‹ä»¶æ¶ˆæ¯
```yaml
Type:    Normal
Reason:  Scheduled
Message: Successfully assigned default/nginx-deployment-7d4c8c6d9f-xkj2m to node-10-0-1-15
Source:  default-scheduler
```

#### å½±å“é¢è¯´æ˜
- **èµ„æºé¢„ç•™**: èŠ‚ç‚¹èµ„æºå·²è¢«é¢„ç•™,å½±å“åç»­ Pod è°ƒåº¦
- **è°ƒåº¦å»¶è¿Ÿ**: è°ƒåº¦è€—æ—¶å½±å“ Pod å¯åŠ¨é€Ÿåº¦(æ­£å¸¸ < 100ms)
- **ç»‘å®šä¸å¯é€†**: è°ƒåº¦ç»‘å®šåæ— æ³•æ›´æ”¹,é™¤éåˆ é™¤ Pod é‡å»º

#### æ’æŸ¥å»ºè®®
1. **æ£€æŸ¥è°ƒåº¦å»¶è¿Ÿ**
   ```bash
   # æŸ¥çœ‹è°ƒåº¦è€—æ—¶(CreationTimestamp -> Scheduled Event)
   kubectl get events --field-selector involvedObject.name=<pod-name> \
     --sort-by='.lastTimestamp' | grep Scheduled
   
   # è°ƒåº¦è€—æ—¶åˆ†æ
   kubectl describe pod <pod-name> | grep -A5 Events
   ```

2. **éªŒè¯èŠ‚ç‚¹é€‰æ‹©åˆç†æ€§**
   ```bash
   # æŸ¥çœ‹ Pod è°ƒåº¦åˆ°çš„èŠ‚ç‚¹
   kubectl get pod <pod-name> -o wide
   
   # æ£€æŸ¥èŠ‚ç‚¹èµ„æºä½¿ç”¨ç‡
   kubectl top node <node-name>
   kubectl describe node <node-name> | grep -A5 "Allocated resources"
   ```

3. **æ£€æŸ¥è°ƒåº¦çº¦æŸæ˜¯å¦ç”Ÿæ•ˆ**
   ```bash
   # æŸ¥çœ‹ Pod çš„ nodeSelector/affinity/tolerations
   kubectl get pod <pod-name> -o yaml | grep -A20 "nodeSelector\|affinity\|tolerations"
   ```

#### è§£å†³å»ºè®®
| åŸå›  | è§£å†³æ–¹æ¡ˆ | ä¼˜å…ˆçº§ |
|:----|:--------|:------|
| è°ƒåº¦å»¶è¿Ÿè¿‡é«˜ (>1s) | æ£€æŸ¥è°ƒåº¦å™¨æ€§èƒ½ã€å‡å°‘èŠ‚ç‚¹æ•°é‡ã€ä¼˜åŒ–è°ƒåº¦ç­–ç•¥ | P2 |
| è°ƒåº¦åˆ°ä¸æœŸæœ›èŠ‚ç‚¹ | æ·»åŠ  nodeSelector/affinity çº¦æŸã€æ›´æ–°èŠ‚ç‚¹æ ‡ç­¾ | P3 |
| èŠ‚ç‚¹èµ„æºä¸å‡è¡¡ | è°ƒæ•´ Scheduler æ‰“åˆ†ç­–ç•¥ã€ä½¿ç”¨ Descheduler é‡å¹³è¡¡ | P3 |
| è°ƒåº¦å™¨é…ç½®é—®é¢˜ | æ£€æŸ¥è°ƒåº¦å™¨é…ç½®æ–‡ä»¶ã€æ’ä»¶å¯ç”¨çŠ¶æ€ | P1 |

---

### `FailedScheduling` - è°ƒåº¦å¤±è´¥

| å±æ€§ | è¯´æ˜ |
|:---|:---|
| **äº‹ä»¶ç±»å‹** | Warning |
| **æ¥æºç»„ä»¶** | default-scheduler |
| **å…³è”èµ„æº** | Pod |
| **é€‚ç”¨ç‰ˆæœ¬** | v1.0+ |
| **ç”Ÿäº§é¢‘ç‡** | ä¸­é¢‘ |

#### äº‹ä»¶å«ä¹‰
è¡¨ç¤ºè°ƒåº¦å™¨æ— æ³•ä¸º Pod æ‰¾åˆ°æ»¡è¶³æ‰€æœ‰è°ƒåº¦çº¦æŸçš„èŠ‚ç‚¹ã€‚è¿™æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­æœ€å¸¸è§çš„è°ƒåº¦é—®é¢˜,åŸå› å¤šæ ·ä¸”å¤æ‚,åŒ…æ‹¬èµ„æºä¸è¶³ã€èŠ‚ç‚¹æ±¡ç‚¹ã€äº²å’Œæ€§çº¦æŸã€æ‹“æ‰‘åˆ†å¸ƒçº¦æŸã€å­˜å‚¨å·ç»‘å®šå¤±è´¥ã€ç«¯å£å†²çªç­‰ã€‚Pod å°†ä¿æŒåœ¨ Pending çŠ¶æ€,è°ƒåº¦å™¨ä¼šæŒç»­é‡è¯•(é»˜è®¤é€€é¿é—´éš” 1s-60s)ã€‚

è°ƒåº¦å¤±è´¥çš„æ ¹æœ¬åŸå› æ˜¯**æ‰€æœ‰èŠ‚ç‚¹éƒ½æ— æ³•é€šè¿‡ Filter é˜¶æ®µçš„æŸä¸ªæˆ–å¤šä¸ªè¿‡æ»¤æ’ä»¶**ã€‚ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦æ ¹æ®å¤±è´¥åŸå› å¿«é€Ÿå®šä½é—®é¢˜,é¿å…å½±å“ä¸šåŠ¡éƒ¨ç½²ã€‚è°ƒåº¦å™¨ä¼šè®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„å¤±è´¥åŸå› ,æ ¼å¼ä¸º `0/N nodes are available: <reason1>, <reason2>...`ã€‚

#### å…¸å‹äº‹ä»¶æ¶ˆæ¯
```yaml
# èµ„æºä¸è¶³ç¤ºä¾‹
Type:    Warning
Reason:  FailedScheduling
Message: 0/3 nodes are available: 1 Insufficient cpu, 2 Insufficient memory. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.

# æ±¡ç‚¹å®¹å¿ç¤ºä¾‹
Type:    Warning
Reason:  FailedScheduling
Message: 0/5 nodes are available: 5 node(s) had untolerated taint {node-role.kubernetes.io/master: }.

# äº²å’Œæ€§çº¦æŸç¤ºä¾‹
Type:    Warning
Reason:  FailedScheduling
Message: 0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector.

# æ‹“æ‰‘åˆ†å¸ƒçº¦æŸç¤ºä¾‹
Type:    Warning
Reason:  FailedScheduling
Message: 0/6 nodes are available: 6 node(s) didn't match pod topology spread constraints.

# PVC ç»‘å®šå¤±è´¥ç¤ºä¾‹
Type:    Warning
Reason:  FailedScheduling
Message: 0/3 nodes are available: 3 node(s) had volume node affinity conflict.

# ç«¯å£å†²çªç¤ºä¾‹
Type:    Warning
Reason:  FailedScheduling
Message: 0/2 nodes are available: 2 node(s) didn't have free ports for the requested pod ports.
```

#### FailedScheduling å¤±è´¥åŸå› åˆ†ç±»è¡¨

| å¤±è´¥åŸå› å…³é”®å­— | ä¸­æ–‡è¯´æ˜ | åŸå› åˆ†ç±» | ç”Ÿäº§é¢‘ç‡ | è§£å†³éš¾åº¦ |
|:-------------|:--------|:--------|:--------|:--------|
| `Insufficient cpu` | CPU èµ„æºä¸è¶³ | èµ„æºä¸è¶³ | é«˜é¢‘ | ä¸­ |
| `Insufficient memory` | å†…å­˜èµ„æºä¸è¶³ | èµ„æºä¸è¶³ | é«˜é¢‘ | ä¸­ |
| `Insufficient pods` | èŠ‚ç‚¹ Pod æ•°é‡è¶…é™ | èµ„æºä¸è¶³ | ä¸­é¢‘ | ä½ |
| `Insufficient ephemeral-storage` | ä¸´æ—¶å­˜å‚¨ä¸è¶³ | èµ„æºä¸è¶³ | ä¸­é¢‘ | ä¸­ |
| `Insufficient nvidia.com/gpu` | GPU èµ„æºä¸è¶³ | æ‰©å±•èµ„æºä¸è¶³ | ä¸­é¢‘ | é«˜ |
| `untolerated taint` | èŠ‚ç‚¹æ±¡ç‚¹ä¸å®¹å¿ | æ±¡ç‚¹çº¦æŸ | é«˜é¢‘ | ä½ |
| `didn't match Pod's node affinity/selector` | èŠ‚ç‚¹äº²å’Œæ€§ä¸åŒ¹é… | äº²å’Œæ€§çº¦æŸ | ä¸­é¢‘ | ä½ |
| `didn't match pod affinity rules` | Pod äº²å’Œæ€§ä¸æ»¡è¶³ | äº²å’Œæ€§çº¦æŸ | ä½é¢‘ | ä¸­ |
| `didn't match pod anti-affinity rules` | Pod åäº²å’Œæ€§å†²çª | åäº²å’Œæ€§çº¦æŸ | ä¸­é¢‘ | ä¸­ |
| `didn't match pod topology spread constraints` | æ‹“æ‰‘åˆ†å¸ƒçº¦æŸä¸æ»¡è¶³ | æ‹“æ‰‘çº¦æŸ | ä¸­é¢‘ | ä¸­ |
| `had volume node affinity conflict` | å­˜å‚¨å·èŠ‚ç‚¹äº²å’Œæ€§å†²çª | å­˜å‚¨ç»‘å®š | ä¸­é¢‘ | ä¸­ |
| `persistentvolumeclaim "xxx" not found` | PVC ä¸å­˜åœ¨ | å­˜å‚¨ç»‘å®š | ä½é¢‘ | ä½ |
| `didn't have free ports for the requested pod ports` | ç«¯å£å†²çª | ç«¯å£å ç”¨ | ä½é¢‘ | ä¸­ |
| `node(s) had taint {key: value}, that the pod didn't tolerate` | ç‰¹å®šæ±¡ç‚¹ä¸å®¹å¿ | æ±¡ç‚¹çº¦æŸ | é«˜é¢‘ | ä½ |
| `node(s) not ready` | èŠ‚ç‚¹æœªå°±ç»ª | èŠ‚ç‚¹çŠ¶æ€ | ä¸­é¢‘ | é«˜ |
| `node(s) were unschedulable` | èŠ‚ç‚¹ä¸å¯è°ƒåº¦ | èŠ‚ç‚¹çŠ¶æ€ | ä¸­é¢‘ | ä¸­ |
| `No preemption victims found` | æ— æ³•æŠ¢å ä½ä¼˜å…ˆçº§ Pod | æŠ¢å å¤±è´¥ | ä½é¢‘ | é«˜ |
| `didn't find available persistent volumes to bind` | æ— å¯ç”¨ PV ç»‘å®š | å­˜å‚¨ç»‘å®š | ä¸­é¢‘ | ä¸­ |

#### å½±å“é¢è¯´æ˜
- **ä¸šåŠ¡ä¸­æ–­**: Pod æ— æ³•å¯åŠ¨,å½±å“æœåŠ¡å¯ç”¨æ€§
- **è°ƒåº¦é‡è¯•**: è°ƒåº¦å™¨æŒç»­é‡è¯•,æ¶ˆè€— API Server èµ„æº
- **èµ„æºæµªè´¹**: èµ„æºé…ç½®ä¸åˆç†å¯¼è‡´ç¢ç‰‡åŒ–
- **çº§è”å¤±è´¥**: äº²å’Œæ€§çº¦æŸå¯èƒ½å¯¼è‡´å…³è” Pod å…¨éƒ¨å¤±è´¥

#### æ’æŸ¥å»ºè®®

1. **å¿«é€Ÿå®šä½å¤±è´¥åŸå› **
   ```bash
   # æŸ¥çœ‹ Pod äº‹ä»¶è·å–å¤±è´¥è¯¦æƒ…
   kubectl describe pod <pod-name> | grep -A10 "Events:"
   
   # è¿‡æ»¤è°ƒåº¦å¤±è´¥äº‹ä»¶
   kubectl get events --field-selector reason=FailedScheduling,involvedObject.name=<pod-name>
   
   # æŸ¥çœ‹æ‰€æœ‰ Pending Pod çš„è°ƒåº¦å¤±è´¥åŸå› 
   kubectl get pods --field-selector status.phase=Pending -A
   kubectl describe pod -A | grep -B5 "FailedScheduling"
   ```

2. **èµ„æºä¸è¶³æ’æŸ¥**
   ```bash
   # æ£€æŸ¥é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ
   kubectl top nodes
   kubectl describe nodes | grep -A5 "Allocated resources"
   
   # æŸ¥çœ‹èŠ‚ç‚¹å¯åˆ†é…èµ„æº
   kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, allocatable: .status.allocatable}'
   
   # è®¡ç®—é›†ç¾¤æ€»å¯ç”¨èµ„æº
   kubectl get nodes -o json | jq '[.items[] | .status.allocatable | {cpu: .cpu, memory: .memory}]'
   
   # æŸ¥çœ‹ Pod èµ„æºè¯·æ±‚
   kubectl get pod <pod-name> -o json | jq '.spec.containers[] | {name: .name, resources: .resources.requests}'
   ```

3. **æ±¡ç‚¹å®¹å¿æ’æŸ¥**
   ```bash
   # æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹æ±¡ç‚¹
   kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, taints: .spec.taints}'
   
   # æŸ¥çœ‹ Pod æ±¡ç‚¹å®¹å¿é…ç½®
   kubectl get pod <pod-name> -o yaml | grep -A10 tolerations
   
   # æ£€æŸ¥ç‰¹å®šæ±¡ç‚¹çš„èŠ‚ç‚¹
   kubectl get nodes -o json | jq '.items[] | select(.spec.taints != null) | select(.spec.taints[] | .key == "node-role.kubernetes.io/master") | .metadata.name'
   ```

4. **äº²å’Œæ€§çº¦æŸæ’æŸ¥**
   ```bash
   # æŸ¥çœ‹ Pod äº²å’Œæ€§é…ç½®
   kubectl get pod <pod-name> -o yaml | grep -A20 affinity
   
   # æ£€æŸ¥èŠ‚ç‚¹æ ‡ç­¾æ˜¯å¦åŒ¹é…
   kubectl get nodes --show-labels
   kubectl get pod <pod-name> -o yaml | grep -A5 nodeSelector
   
   # æ£€æŸ¥ Pod åäº²å’Œæ€§å†²çª
   kubectl get pods -o wide --all-namespaces | grep <node-name>
   kubectl get pod <pod-name> -o yaml | grep -A15 podAntiAffinity
   ```

5. **æ‹“æ‰‘åˆ†å¸ƒçº¦æŸæ’æŸ¥**
   ```bash
   # æŸ¥çœ‹ Pod æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
   kubectl get pod <pod-name> -o yaml | grep -A10 topologySpreadConstraints
   
   # æ£€æŸ¥æ‹“æ‰‘åŸŸåˆ†å¸ƒæƒ…å†µ
   kubectl get pods -o wide --all-namespaces | grep <app-label>
   kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, zone: .metadata.labels["topology.kubernetes.io/zone"]}'
   ```

6. **å­˜å‚¨å·ç»‘å®šæ’æŸ¥**
   ```bash
   # æ£€æŸ¥ PVC çŠ¶æ€
   kubectl get pvc -A
   kubectl describe pvc <pvc-name>
   
   # æŸ¥çœ‹ PV çš„èŠ‚ç‚¹äº²å’Œæ€§
   kubectl get pv -o yaml | grep -A10 nodeAffinity
   
   # æ£€æŸ¥ StorageClass é…ç½®
   kubectl get storageclass
   kubectl describe storageclass <storageclass-name>
   ```

7. **ç«¯å£å†²çªæ’æŸ¥**
   ```bash
   # æŸ¥çœ‹ Pod è¯·æ±‚çš„ç«¯å£
   kubectl get pod <pod-name> -o yaml | grep -A5 "hostPort"
   
   # æ£€æŸ¥èŠ‚ç‚¹ä¸Šå·²å ç”¨çš„ç«¯å£
   kubectl get pods -o wide --all-namespaces | grep <node-name>
   kubectl get pods -A -o json | jq '.items[] | select(.spec.nodeName == "<node-name>") | .spec.containers[] | select(.ports != null) | .ports[] | select(.hostPort != null) | .hostPort'
   ```

8. **è°ƒåº¦å™¨æ—¥å¿—åˆ†æ**
   ```bash
   # æŸ¥çœ‹è°ƒåº¦å™¨æ—¥å¿—(kubeadm é›†ç¾¤)
   kubectl logs -n kube-system -l component=kube-scheduler --tail=100 | grep <pod-name>
   
   # æŸ¥çœ‹è¯¦ç»†è°ƒåº¦å†³ç­–(éœ€å¯ç”¨ --v=5 æ—¥å¿—çº§åˆ«)
   kubectl logs -n kube-system kube-scheduler-<node> --tail=500 | grep -A20 "Attempting to schedule pod"
   ```

#### è§£å†³å»ºè®®

| åŸå›  | è§£å†³æ–¹æ¡ˆ | ä¼˜å…ˆçº§ | å½±å“èŒƒå›´ |
|:----|:--------|:------|:--------|
| **èµ„æºä¸è¶³ - CPU** | 1. æ‰©å®¹èŠ‚ç‚¹å¢åŠ  CPU èµ„æº<br>2. é™ä½ Pod CPU requests<br>3. æ¸…ç†é—²ç½® Pod<br>4. å¯ç”¨ Cluster Autoscaler | P0 | é«˜ |
| **èµ„æºä¸è¶³ - å†…å­˜** | 1. æ‰©å®¹èŠ‚ç‚¹å¢åŠ å†…å­˜èµ„æº<br>2. é™ä½ Pod memory requests<br>3. ä¼˜åŒ–åº”ç”¨å†…å­˜ä½¿ç”¨<br>4. å¯ç”¨å‚ç›´æ‰©ç¼©å®¹(VPA) | P0 | é«˜ |
| **èµ„æºä¸è¶³ - Pod æ•°é‡** | 1. å¢åŠ èŠ‚ç‚¹ `--max-pods` å‚æ•°<br>2. æ‰©å®¹èŠ‚ç‚¹åˆ†æ•£ Pod<br>3. å‡å°‘ DaemonSet æ•°é‡ | P1 | ä¸­ |
| **èµ„æºä¸è¶³ - ä¸´æ—¶å­˜å‚¨** | 1. æ¸…ç†èŠ‚ç‚¹ä¸´æ—¶æ–‡ä»¶<br>2. å¢åŠ èŠ‚ç‚¹ç£ç›˜ç©ºé—´<br>3. é™ä½ Pod ephemeral-storage requests | P1 | ä¸­ |
| **èµ„æºä¸è¶³ - GPU** | 1. æ·»åŠ  GPU èŠ‚ç‚¹<br>2. ä¼˜åŒ– GPU å…±äº«ç­–ç•¥<br>3. ä½¿ç”¨ GPU åˆ‡ç‰‡æŠ€æœ¯ | P1 | ä¸­ |
| **æ±¡ç‚¹ä¸å®¹å¿** | 1. ä¸º Pod æ·»åŠ å¯¹åº”çš„ tolerations<br>2. ç§»é™¤èŠ‚ç‚¹ä¸å¿…è¦çš„æ±¡ç‚¹<br>3. è°ƒæ•´æ±¡ç‚¹æ•ˆæœ(NoSchedule -> PreferNoSchedule) | P1 | ä½ |
| **èŠ‚ç‚¹äº²å’Œæ€§ä¸åŒ¹é…** | 1. ä¿®æ”¹ Pod nodeSelector æˆ– nodeAffinity<br>2. ä¸ºèŠ‚ç‚¹æ·»åŠ åŒ¹é…çš„æ ‡ç­¾<br>3. ä½¿ç”¨ preferredDuringScheduling è½¯çº¦æŸ | P2 | ä½ |
| **Pod åäº²å’Œæ€§å†²çª** | 1. è°ƒæ•´åäº²å’Œæ€§æ‹“æ‰‘åŸŸ(node -> zone)<br>2. ä½¿ç”¨è½¯åäº²å’Œæ€§(preferred)<br>3. å¢åŠ èŠ‚ç‚¹ä»¥æ»¡è¶³åˆ†å¸ƒéœ€æ±‚ | P2 | ä¸­ |
| **æ‹“æ‰‘åˆ†å¸ƒçº¦æŸä¸æ»¡è¶³** | 1. è°ƒæ•´ maxSkew å®¹å¿åº¦<br>2. ä¿®æ”¹ whenUnsatisfiable ä¸º ScheduleAnyway<br>3. å¢åŠ æ‹“æ‰‘åŸŸ(zone/region)èŠ‚ç‚¹æ•°é‡ | P2 | ä¸­ |
| **å­˜å‚¨å·èŠ‚ç‚¹äº²å’Œæ€§å†²çª** | 1. æ£€æŸ¥ PV çš„ nodeAffinity é…ç½®<br>2. ä½¿ç”¨ WaitForFirstConsumer ç»‘å®šæ¨¡å¼<br>3. è¿ç§» PV åˆ°æ­£ç¡®çš„æ‹“æ‰‘åŸŸ | P1 | é«˜ |
| **PVC ä¸å­˜åœ¨æˆ–æœªç»‘å®š** | 1. åˆ›å»ºç¼ºå¤±çš„ PVC<br>2. æ£€æŸ¥ StorageClass æ˜¯å¦å­˜åœ¨<br>3. ç¡®è®¤ PV ä¾›åº”æ­£å¸¸ | P0 | é«˜ |
| **ç«¯å£å†²çª** | 1. ç§»é™¤ hostPort é…ç½®<br>2. ä½¿ç”¨ Service ä»£æ›¿ hostPort<br>3. è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹ | P2 | ä½ |
| **èŠ‚ç‚¹æœªå°±ç»ª** | 1. ä¿®å¤èŠ‚ç‚¹æ•…éšœ(kubelet/docker/ç½‘ç»œ)<br>2. æ ‡è®°èŠ‚ç‚¹ä¸º Unschedulable<br>3. ä»é›†ç¾¤ç§»é™¤æ•…éšœèŠ‚ç‚¹ | P0 | é«˜ |
| **èŠ‚ç‚¹ä¸å¯è°ƒåº¦** | 1. ç§»é™¤èŠ‚ç‚¹ Unschedulable æ ‡è®°<br>2. æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦åœ¨ç»´æŠ¤ä¸­<br>3. ç¡®è®¤èŠ‚ç‚¹æ˜¯å¦è¢« drain | P1 | ä¸­ |
| **æ— æ³•æŠ¢å ä½ä¼˜å…ˆçº§ Pod** | 1. æ£€æŸ¥ PriorityClass é…ç½®<br>2. ç¡®è®¤æ˜¯å¦æœ‰å¯æŠ¢å çš„ Pod<br>3. æ‰©å®¹èŠ‚ç‚¹é¿å…æŠ¢å  | P2 | ä¸­ |

---

### `Preempted` - è¢«æŠ¢å 

| å±æ€§ | è¯´æ˜ |
|:---|:---|
| **äº‹ä»¶ç±»å‹** | Normal |
| **æ¥æºç»„ä»¶** | default-scheduler |
| **å…³è”èµ„æº** | Pod |
| **é€‚ç”¨ç‰ˆæœ¬** | v1.11+ |
| **ç”Ÿäº§é¢‘ç‡** | ä½é¢‘ |

#### äº‹ä»¶å«ä¹‰
è¡¨ç¤ºå½“å‰ Pod å› é›†ç¾¤èµ„æºä¸è¶³,è¢«æ›´é«˜ä¼˜å…ˆçº§çš„ Pod æŠ¢å è€Œè¢«é©±é€ã€‚è¿™æ˜¯ Kubernetes ä¼˜å…ˆçº§è°ƒåº¦æœºåˆ¶çš„æ ¸å¿ƒä½“ç°,ç¡®ä¿é«˜ä¼˜å…ˆçº§å·¥ä½œè´Ÿè½½(å¦‚ç”Ÿäº§ä¸šåŠ¡)èƒ½å¤Ÿä¼˜å…ˆè·å¾—èµ„æº,ç‰ºç‰²ä½ä¼˜å…ˆçº§å·¥ä½œè´Ÿè½½(å¦‚æ‰¹å¤„ç†ä»»åŠ¡)ã€‚

æŠ¢å æµç¨‹:å½“é«˜ä¼˜å…ˆçº§ Pod æ— æ³•è°ƒåº¦æ—¶,è°ƒåº¦å™¨ä¼šè¯„ä¼°æ˜¯å¦å¯ä»¥é€šè¿‡åˆ é™¤ä½ä¼˜å…ˆçº§ Pod æ¥è…¾å‡ºèµ„æºã€‚å¦‚æœæŠ¢å å¯è¡Œ,è°ƒåº¦å™¨ä¼šé€‰æ‹©å½±å“æœ€å°çš„èŠ‚ç‚¹å’Œ Pod ç»„åˆ,ç„¶åå‘ API Server å‘é€åˆ é™¤è¯·æ±‚,è¢«æŠ¢å çš„ Pod ä¼šæ”¶åˆ° Preempted äº‹ä»¶å¹¶è¿›å…¥ Terminating çŠ¶æ€ã€‚

#### å…¸å‹äº‹ä»¶æ¶ˆæ¯
```yaml
Type:    Normal
Reason:  Preempted
Message: Preempted by default/high-priority-pod on node node-10-0-1-20

# è¯¦ç»†æ¶ˆæ¯åŒ…å«ä¼˜å…ˆçº§ä¿¡æ¯
Type:    Normal
Reason:  Preempted
Message: Preempted in order to admit pod "default/critical-app-abc123" on node "worker-node-5". This pod has priority 1000 while preempting pod has priority 10000.
```

#### å½±å“é¢è¯´æ˜
- **æœåŠ¡ä¸­æ–­**: è¢«æŠ¢å  Pod ç«‹å³ç»ˆæ­¢,å½±å“ä½ä¼˜å…ˆçº§æœåŠ¡
- **èµ„æºæµªè´¹**: é¢‘ç¹æŠ¢å å¯¼è‡´ Pod é‡å¯,æµªè´¹è®¡ç®—èµ„æº
- **è°ƒåº¦å»¶è¿Ÿ**: è¢«æŠ¢å  Pod é‡æ–°è°ƒåº¦éœ€è¦ç­‰å¾…èµ„æº
- **çº§è”å½±å“**: å¯èƒ½è§¦å‘å¤šä¸ª Pod è¢«æŠ¢å 

#### PriorityClass è¯´æ˜

Kubernetes é€šè¿‡ PriorityClass èµ„æºå®šä¹‰ Pod ä¼˜å…ˆçº§:

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 10000              # ä¼˜å…ˆçº§å€¼,è¶Šå¤§è¶Šé«˜
globalDefault: false       # æ˜¯å¦ä¸ºé»˜è®¤ä¼˜å…ˆçº§
description: "High priority for production workloads"
preemptionPolicy: PreemptLowerPriority  # æŠ¢å ç­–ç•¥
```

**ç³»ç»Ÿé¢„å®šä¹‰ PriorityClass**:
- `system-cluster-critical`: ä¼˜å…ˆçº§ 2000000000(é›†ç¾¤æ ¸å¿ƒç»„ä»¶,å¦‚ coredns)
- `system-node-critical`: ä¼˜å…ˆçº§ 2000001000(èŠ‚ç‚¹æ ¸å¿ƒç»„ä»¶,å¦‚ kube-proxy)

**æŠ¢å ç­–ç•¥ (preemptionPolicy)**:
- `PreemptLowerPriority` (é»˜è®¤): å¯ä»¥æŠ¢å ä½ä¼˜å…ˆçº§ Pod
- `Never` (v1.19+): é«˜ä¼˜å…ˆçº§ä½†ä¸æŠ¢å ,ä»…æ’é˜Ÿç­‰å¾…

**Pod ä½¿ç”¨ PriorityClass**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: critical-app
spec:
  priorityClassName: high-priority
  containers:
  - name: app
    image: nginx
```

#### æ’æŸ¥å»ºè®®

1. **ç¡®è®¤æŠ¢å è¯¦æƒ…**
   ```bash
   # æŸ¥çœ‹è¢«æŠ¢å  Pod çš„äº‹ä»¶
   kubectl describe pod <preempted-pod-name> | grep -A5 Preempted
   
   # æŸ¥çœ‹æŠ¢å è€… Pod ä¿¡æ¯
   kubectl get pod <preemptor-pod-name> -o yaml | grep priorityClassName
   kubectl get priorityclass <priority-class-name>
   ```

2. **æ£€æŸ¥ä¼˜å…ˆçº§é…ç½®**
   ```bash
   # æŸ¥çœ‹æ‰€æœ‰ PriorityClass
   kubectl get priorityclass
   kubectl describe priorityclass <priority-class-name>
   
   # æŸ¥çœ‹è¢«æŠ¢å  Pod çš„ä¼˜å…ˆçº§
   kubectl get pod <pod-name> -o json | jq '.spec.priority, .spec.priorityClassName'
   
   # æŸ¥çœ‹é›†ç¾¤ä¸­é«˜ä¼˜å…ˆçº§ Pod
   kubectl get pods -A -o json | jq '.items[] | select(.spec.priority > 1000) | {name: .metadata.name, namespace: .metadata.namespace, priority: .spec.priority}'
   ```

3. **åˆ†ææŠ¢å å†å²**
   ```bash
   # æŸ¥çœ‹é›†ç¾¤ä¸­æ‰€æœ‰æŠ¢å äº‹ä»¶
   kubectl get events -A --field-selector reason=Preempted --sort-by='.lastTimestamp'
   
   # ç»Ÿè®¡æŠ¢å é¢‘ç‡
   kubectl get events -A --field-selector reason=Preempted -o json | jq '[.items[] | {pod: .involvedObject.name, time: .lastTimestamp}]'
   ```

4. **è¯„ä¼°èµ„æºå‹åŠ›**
   ```bash
   # æ£€æŸ¥é›†ç¾¤èµ„æºä½¿ç”¨ç‡
   kubectl top nodes
   kubectl describe nodes | grep -A5 "Allocated resources"
   
   # æ£€æŸ¥é«˜ä¼˜å…ˆçº§ Pod çš„èµ„æºè¯·æ±‚
   kubectl get pods -A -o json | jq '.items[] | select(.spec.priority > 5000) | {name: .metadata.name, resources: .spec.containers[].resources.requests}'
   ```

#### è§£å†³å»ºè®®

| åŸå›  | è§£å†³æ–¹æ¡ˆ | ä¼˜å…ˆçº§ |
|:----|:--------|:------|
| é›†ç¾¤èµ„æºä¸è¶³å¯¼è‡´é¢‘ç¹æŠ¢å  | æ‰©å®¹é›†ç¾¤,å¢åŠ èŠ‚ç‚¹èµ„æº | P0 |
| ä¼˜å…ˆçº§é…ç½®ä¸åˆç† | é‡æ–°è®¾è®¡ PriorityClass ä½“ç³»,é¿å…è¿‡å¤§å·®è· | P1 |
| ä½ä¼˜å…ˆçº§ Pod è¢«é¢‘ç¹æŠ¢å  | æå‡å…³é”®ä¸šåŠ¡ä¼˜å…ˆçº§,é™ä½æ‰¹å¤„ç†ä¼˜å…ˆçº§ | P2 |
| æŠ¢å ç­–ç•¥è¿‡äºæ¿€è¿› | ä½¿ç”¨ PreemptionPolicy: Never ç¦ç”¨æŠ¢å  | P2 |
| èµ„æºé¢„ç•™ä¸è¶³ | ä¸ºå…³é”®ä¸šåŠ¡é¢„ç•™èŠ‚ç‚¹èµ„æº(taints + tolerations) | P1 |
| çªå‘é«˜ä¼˜å…ˆçº§ Pod è¿‡å¤š | é™åˆ¶é«˜ä¼˜å…ˆçº§ Pod çš„å¹¶å‘æ•°é‡(ResourceQuota) | P2 |

---

### `WaitingForGates` - ç­‰å¾…è°ƒåº¦é—¨

| å±æ€§ | è¯´æ˜ |
|:---|:---|
| **äº‹ä»¶ç±»å‹** | Normal |
| **æ¥æºç»„ä»¶** | default-scheduler |
| **å…³è”èµ„æº** | Pod |
| **é€‚ç”¨ç‰ˆæœ¬** | v1.26+ (Beta), v1.30 (GA) |
| **ç”Ÿäº§é¢‘ç‡** | ä½é¢‘ |

#### äº‹ä»¶å«ä¹‰
è¡¨ç¤º Pod å› é…ç½®äº† SchedulingGates è€Œæš‚åœè°ƒåº¦,ç­‰å¾…å¤–éƒ¨æ§åˆ¶å™¨ç§»é™¤è°ƒåº¦é—¨åæ‰ä¼šè¿›å…¥æ­£å¸¸è°ƒåº¦æµç¨‹ã€‚è¿™æ˜¯ Kubernetes v1.26 å¼•å…¥çš„æ–°ç‰¹æ€§,å…è®¸è‡ªå®šä¹‰è°ƒåº¦æ§åˆ¶é€»è¾‘,å®ç°å¤æ‚çš„ç¼–æ’åœºæ™¯,å¦‚æ‰¹é‡ä»»åŠ¡è°ƒåº¦ã€å¤šé›†ç¾¤åè°ƒã€é…é¢æ£€æŸ¥ç­‰ã€‚

SchedulingGates æä¾›äº†ä¸€ç§å£°æ˜å¼çš„è°ƒåº¦æš‚åœæœºåˆ¶,ç›¸æ¯” PodScheduled Condition æ›´åŠ ç®€æ´å’Œçµæ´»ã€‚Pod åˆ›å»ºæ—¶å¦‚æœ `spec.schedulingGates` éç©º,è°ƒåº¦å™¨ä¼šè·³è¿‡è¯¥ Pod,ç›´åˆ°æ‰€æœ‰ Gate è¢«ç§»é™¤ã€‚è¿™å…è®¸å¤–éƒ¨ç³»ç»Ÿåœ¨ Pod è°ƒåº¦å‰è¿›è¡Œé¢„å¤„ç†,å¦‚èµ„æºé¢„ç•™ã€é…é¢éªŒè¯ã€ä¾èµ–æ£€æŸ¥ç­‰ã€‚

#### SchedulingGates è¯´æ˜

**åŸºæœ¬ç”¨æ³•**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gated-pod
spec:
  schedulingGates:
  - name: "example.com/resource-quota-check"
  - name: "example.com/multi-cluster-placement"
  containers:
  - name: nginx
    image: nginx
```

**å…¸å‹åº”ç”¨åœºæ™¯**:
1. **æ‰¹é‡è°ƒåº¦**: ç­‰å¾…æ‰€æœ‰ Pod åˆ›å»ºå®Œæˆåç»Ÿä¸€è°ƒåº¦(Gang Scheduling)
2. **èµ„æºé…é¢**: å¤–éƒ¨ç³»ç»Ÿæ£€æŸ¥é…é¢åå†å…è®¸è°ƒåº¦
3. **å¤šé›†ç¾¤åè°ƒ**: è·¨é›†ç¾¤èµ„æºé¢„ç•™å’Œè°ƒåº¦åè°ƒ
4. **ä¾èµ–æ£€æŸ¥**: ç­‰å¾…ä¾èµ–æœåŠ¡å°±ç»ªåå†è°ƒåº¦
5. **å®¡æ‰¹æµç¨‹**: éœ€è¦äººå·¥æˆ–è‡ªåŠ¨å®¡æ‰¹åæ‰è°ƒåº¦

**ç§»é™¤ SchedulingGate**:
```bash
# ä½¿ç”¨ kubectl patch ç§»é™¤ç‰¹å®š gate
kubectl patch pod gated-pod --type=json -p='[{"op": "remove", "path": "/spec/schedulingGates/0"}]'

# ä½¿ç”¨ client-go æˆ–è‡ªå®šä¹‰ controller è‡ªåŠ¨åŒ–ç®¡ç†
```

#### å…¸å‹äº‹ä»¶æ¶ˆæ¯
```yaml
Type:    Normal
Reason:  WaitingForGates
Message: Scheduling is blocked on 2 gates: [example.com/resource-quota-check example.com/multi-cluster-placement]

# Gate è¢«ç§»é™¤åäº§ç”Ÿçš„äº‹ä»¶
Type:    Normal
Reason:  SchedulingGatesRemoved
Message: All scheduling gates have been removed, proceeding with scheduling.
```

#### å½±å“é¢è¯´æ˜
- **è°ƒåº¦å»¶è¿Ÿ**: Pod å¤„äº Pending çŠ¶æ€ç›´åˆ° Gate ç§»é™¤
- **æ§åˆ¶å™¨ä¾èµ–**: ä¾èµ–å¤–éƒ¨æ§åˆ¶å™¨æ­£ç¡®ç§»é™¤ Gate
- **æ•…éšœæ’æŸ¥**: Gate æœªç§»é™¤ä¼šå¯¼è‡´ Pod æ°¸ä¹… Pending

#### æ’æŸ¥å»ºè®®

1. **æ£€æŸ¥è°ƒåº¦é—¨çŠ¶æ€**
   ```bash
   # æŸ¥çœ‹ Pod çš„è°ƒåº¦é—¨é…ç½®
   kubectl get pod <pod-name> -o yaml | grep -A5 schedulingGates
   
   # æŸ¥çœ‹ç­‰å¾…è°ƒåº¦é—¨çš„æ‰€æœ‰ Pod
   kubectl get pods -A -o json | jq '.items[] | select(.spec.schedulingGates != null) | {name: .metadata.name, namespace: .metadata.namespace, gates: .spec.schedulingGates}'
   
   # æŸ¥çœ‹ WaitingForGates äº‹ä»¶
   kubectl get events --field-selector reason=WaitingForGates -A
   ```

2. **æ£€æŸ¥æ§åˆ¶å™¨çŠ¶æ€**
   ```bash
   # æŸ¥çœ‹è´Ÿè´£ç§»é™¤ Gate çš„æ§åˆ¶å™¨æ—¥å¿—
   kubectl logs -n <controller-namespace> <controller-pod> --tail=100
   
   # æ£€æŸ¥æ§åˆ¶å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œ
   kubectl get pods -n <controller-namespace>
   ```

3. **æ‰‹åŠ¨ç§»é™¤è°ƒåº¦é—¨**
   ```bash
   # æŸ¥çœ‹å½“å‰ Gate åˆ—è¡¨
   kubectl get pod <pod-name> -o json | jq '.spec.schedulingGates'
   
   # ç§»é™¤ç¬¬ä¸€ä¸ª Gate
   kubectl patch pod <pod-name> --type=json -p='[{"op": "remove", "path": "/spec/schedulingGates/0"}]'
   
   # ç§»é™¤æ‰€æœ‰ Gates
   kubectl patch pod <pod-name> --type=json -p='[{"op": "remove", "path": "/spec/schedulingGates"}]'
   ```

4. **æ£€æŸ¥ API Server ç‰ˆæœ¬æ”¯æŒ**
   ```bash
   # ç¡®è®¤é›†ç¾¤ç‰ˆæœ¬æ”¯æŒ SchedulingGates (v1.26+)
   kubectl version --short
   
   # æ£€æŸ¥ API ç‰ˆæœ¬
   kubectl api-resources | grep schedulinggates
   ```

#### è§£å†³å»ºè®®

| åŸå›  | è§£å†³æ–¹æ¡ˆ | ä¼˜å…ˆçº§ |
|:----|:--------|:------|
| æ§åˆ¶å™¨æœªæ­£å¸¸å·¥ä½œ | é‡å¯æ§åˆ¶å™¨,æ£€æŸ¥æ—¥å¿—æ’æŸ¥é—®é¢˜ | P0 |
| Gate åç§°æ‹¼å†™é”™è¯¯ | ä¿®æ­£ Gate åç§°,ç¡®ä¿ä¸æ§åˆ¶å™¨åŒ¹é… | P1 |
| æ§åˆ¶å™¨é€»è¾‘é”™è¯¯ | ä¿®å¤æ§åˆ¶å™¨é€»è¾‘,ç¡®ä¿æ­£ç¡®ç§»é™¤ Gate | P0 |
| Gate æ°¸ä¹…æœªç§»é™¤ | æ‰‹åŠ¨ç§»é™¤ Gate,å…è®¸è°ƒåº¦ç»§ç»­ | P1 |
| é›†ç¾¤ç‰ˆæœ¬ä¸æ”¯æŒ | å‡çº§é›†ç¾¤åˆ° v1.26+,æˆ–ç§»é™¤ SchedulingGates é…ç½® | P2 |
| é…é¢æ£€æŸ¥å¤±è´¥ | è°ƒæ•´èµ„æºé…é¢æˆ– Pod èµ„æºè¯·æ±‚ | P2 |

---

### `TaintManagerEviction` - æ±¡ç‚¹é©±é€

| å±æ€§ | è¯´æ˜ |
|:---|:---|
| **äº‹ä»¶ç±»å‹** | Normal |
| **æ¥æºç»„ä»¶** | node-controller |
| **å…³è”èµ„æº** | Pod |
| **é€‚ç”¨ç‰ˆæœ¬** | v1.13+ |
| **ç”Ÿäº§é¢‘ç‡** | ä½é¢‘ |

#### äº‹ä»¶å«ä¹‰
è¡¨ç¤º Pod å› èŠ‚ç‚¹æ±¡ç‚¹(Taint)å¯¼è‡´ä¸å†å®¹å¿(Tolerate)è€Œè¢« Taint Manager é©±é€ã€‚è¿™æ˜¯ Kubernetes èŠ‚ç‚¹çŠ¶æ€å¼‚å¸¸å¤„ç†æœºåˆ¶çš„å…³é”®éƒ¨åˆ†,å½“èŠ‚ç‚¹å‡ºç°æ•…éšœ(å¦‚ NotReadyã€ç£ç›˜å‹åŠ›ã€å†…å­˜å‹åŠ›)æ—¶,kube-controller-manager çš„ node-controller ç»„ä»¶ä¼šä¸ºèŠ‚ç‚¹æ·»åŠ æ±¡ç‚¹,å¹¶æ ¹æ® Pod çš„å®¹å¿é…ç½®å†³å®šæ˜¯å¦é©±é€ã€‚

Taint Manager æ˜¯ node-controller çš„ä¸€éƒ¨åˆ†,è´Ÿè´£ç›‘æ§èŠ‚ç‚¹æ±¡ç‚¹å˜åŒ–,è¯„ä¼°æ¯ä¸ª Pod çš„å®¹å¿æ—¶é—´(tolerationSeconds),å¹¶åœ¨è¶…æ—¶ååˆ é™¤ Podã€‚è¿™ç§æœºåˆ¶æ¯”ä¼ ç»Ÿçš„èŠ‚ç‚¹æ•…éšœæ£€æµ‹æ›´åŠ çµæ´»å’Œå¯æ§,å…è®¸ä¸åŒçš„ Pod æœ‰ä¸åŒçš„å®¹å¿ç­–ç•¥ã€‚

#### å…¸å‹äº‹ä»¶æ¶ˆæ¯
```yaml
# èŠ‚ç‚¹ NotReady é©±é€
Type:    Normal
Reason:  TaintManagerEviction
Message: Marking for deletion Pod default/nginx-7d4c8c6d9f-xkj2m due to NoExecute taint node.kubernetes.io/not-ready:NoExecute on node node-10-0-1-15

# èŠ‚ç‚¹ç£ç›˜å‹åŠ›é©±é€
Type:    Normal
Reason:  TaintManagerEviction
Message: Marking for deletion Pod default/app-5b9c7d-8xm9q due to NoExecute taint node.kubernetes.io/disk-pressure:NoExecute on node worker-3

# èŠ‚ç‚¹å†…å­˜å‹åŠ›é©±é€
Type:    Normal
Reason:  TaintManagerEviction
Message: Marking for deletion Pod default/web-6f8d5c-4nk2p due to NoExecute taint node.kubernetes.io/memory-pressure:NoExecute on node worker-5
```

#### å¸¸è§èŠ‚ç‚¹æ±¡ç‚¹ç±»å‹

| æ±¡ç‚¹é”®å | æ±¡ç‚¹æ•ˆæœ | è§¦å‘æ¡ä»¶ | é»˜è®¤å®¹å¿æ—¶é—´ | ç”Ÿäº§é¢‘ç‡ |
|:--------|:--------|:--------|:-----------|:--------|
| `node.kubernetes.io/not-ready` | NoExecute | èŠ‚ç‚¹ NotReady çŠ¶æ€ | 300s | é«˜é¢‘ |
| `node.kubernetes.io/unreachable` | NoExecute | èŠ‚ç‚¹å¤±è”(ç½‘ç»œä¸å¯è¾¾) | 300s | ä¸­é¢‘ |
| `node.kubernetes.io/disk-pressure` | NoSchedule/NoExecute | èŠ‚ç‚¹ç£ç›˜å‹åŠ› | æ— è‡ªåŠ¨é©±é€ | ä¸­é¢‘ |
| `node.kubernetes.io/memory-pressure` | NoSchedule/NoExecute | èŠ‚ç‚¹å†…å­˜å‹åŠ› | æ— è‡ªåŠ¨é©±é€ | ä¸­é¢‘ |
| `node.kubernetes.io/pid-pressure` | NoSchedule/NoExecute | èŠ‚ç‚¹ PID å‹åŠ› | æ— è‡ªåŠ¨é©±é€ | ä½é¢‘ |
| `node.kubernetes.io/network-unavailable` | NoSchedule/NoExecute | èŠ‚ç‚¹ç½‘ç»œä¸å¯ç”¨ | æ— è‡ªåŠ¨é©±é€ | ä½é¢‘ |
| `node.kubernetes.io/unschedulable` | NoSchedule | èŠ‚ç‚¹è¢«æ ‡è®°ä¸ºä¸å¯è°ƒåº¦ | N/A | ä¸­é¢‘ |

**Pod é»˜è®¤å®¹å¿é…ç½®**:
```yaml
# Kubernetes è‡ªåŠ¨ä¸º Pod æ·»åŠ çš„é»˜è®¤å®¹å¿
tolerations:
- key: node.kubernetes.io/not-ready
  operator: Exists
  effect: NoExecute
  tolerationSeconds: 300
- key: node.kubernetes.io/unreachable
  operator: Exists
  effect: NoExecute
  tolerationSeconds: 300
```

**è‡ªå®šä¹‰å®¹å¿é…ç½®**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tolerant-pod
spec:
  tolerations:
  - key: node.kubernetes.io/not-ready
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 600  # å®¹å¿ 10 åˆ†é’Ÿ
  - key: node.kubernetes.io/disk-pressure
    operator: Exists
    effect: NoExecute
    tolerationSeconds: 0    # ç«‹å³é©±é€
  - key: custom-taint
    operator: Equal
    value: "true"
    effect: NoExecute       # æ°¸ä¹…å®¹å¿(æ—  tolerationSeconds)
  containers:
  - name: nginx
    image: nginx
```

#### å½±å“é¢è¯´æ˜
- **æœåŠ¡ä¸­æ–­**: Pod è¢«é©±é€å¯¼è‡´æœåŠ¡ä¸­æ–­
- **è¿ç§»å»¶è¿Ÿ**: Pod é‡æ–°è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹éœ€è¦æ—¶é—´
- **çº§è”å½±å“**: èŠ‚ç‚¹æ•…éšœå¯èƒ½å¯¼è‡´å¤§é‡ Pod åŒæ—¶é©±é€
- **æ•°æ®ä¸¢å¤±**: StatefulSet Pod é©±é€å¯èƒ½å¯¼è‡´æ•°æ®è®¿é—®ä¸­æ–­

#### æ’æŸ¥å»ºè®®

1. **ç¡®è®¤é©±é€åŸå› **
   ```bash
   # æŸ¥çœ‹ Pod é©±é€äº‹ä»¶
   kubectl describe pod <pod-name> | grep -A5 TaintManagerEviction
   
   # æŸ¥çœ‹èŠ‚ç‚¹æ±¡ç‚¹çŠ¶æ€
   kubectl describe node <node-name> | grep -A5 Taints
   kubectl get node <node-name> -o json | jq '.spec.taints'
   ```

2. **æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€**
   ```bash
   # æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€å’Œæ¡ä»¶
   kubectl get nodes
   kubectl describe node <node-name> | grep -A10 Conditions
   
   # æ£€æŸ¥èŠ‚ç‚¹èµ„æºå‹åŠ›
   kubectl top node <node-name>
   kubectl describe node <node-name> | grep -A5 "Allocated resources"
   ```

3. **æ£€æŸ¥ Pod å®¹å¿é…ç½®**
   ```bash
   # æŸ¥çœ‹ Pod å®¹å¿é…ç½®
   kubectl get pod <pod-name> -o yaml | grep -A20 tolerations
   
   # æ£€æŸ¥é©±é€æ—¶é—´è®¡ç®—
   kubectl get events --field-selector involvedObject.name=<pod-name> --sort-by='.lastTimestamp'
   ```

4. **æŸ¥çœ‹èŠ‚ç‚¹æ±¡ç‚¹å†å²**
   ```bash
   # æŸ¥çœ‹èŠ‚ç‚¹äº‹ä»¶å†å²
   kubectl describe node <node-name> | grep -A20 Events
   
   # æŸ¥çœ‹æ‰€æœ‰ TaintManagerEviction äº‹ä»¶
   kubectl get events -A --field-selector reason=TaintManagerEviction --sort-by='.lastTimestamp'
   ```

5. **æ£€æŸ¥ node-controller é…ç½®**
   ```bash
   # æŸ¥çœ‹ kube-controller-manager é…ç½®
   kubectl get pods -n kube-system kube-controller-manager-<node> -o yaml | grep -A5 "pod-eviction-timeout\|node-monitor-grace-period"
   
   # é»˜è®¤é…ç½®:
   # --pod-eviction-timeout=5m0s (v1.13+ å·²åºŸå¼ƒ,ç”± taint å®¹å¿æ—¶é—´æ§åˆ¶)
   # --node-monitor-grace-period=40s
   ```

#### è§£å†³å»ºè®®

| åŸå›  | è§£å†³æ–¹æ¡ˆ | ä¼˜å…ˆçº§ |
|:----|:--------|:------|
| èŠ‚ç‚¹æ•…éšœ(NotReady/Unreachable) | ä¿®å¤èŠ‚ç‚¹é—®é¢˜(kubelet/ç½‘ç»œ/ç¡¬ä»¶),æ¢å¤èŠ‚ç‚¹å°±ç»ªçŠ¶æ€ | P0 |
| èŠ‚ç‚¹ç£ç›˜å‹åŠ› | æ¸…ç†èŠ‚ç‚¹ç£ç›˜ç©ºé—´,å¢åŠ ç£ç›˜å®¹é‡,é…ç½®æ—¥å¿—è½®è½¬ | P0 |
| èŠ‚ç‚¹å†…å­˜å‹åŠ› | é©±é€éå…³é”® Pod,å¢åŠ èŠ‚ç‚¹å†…å­˜,ä¼˜åŒ– Pod å†…å­˜ä½¿ç”¨ | P0 |
| å®¹å¿æ—¶é—´è¿‡çŸ­ | å¢åŠ  Pod tolerationSeconds,ç»™äºˆæ›´é•¿æ¢å¤æ—¶é—´ | P2 |
| å…³é”® Pod è¢«è¯¯é©±é€ | ä¸ºå…³é”® Pod é…ç½®æ°¸ä¹…å®¹å¿(æ—  tolerationSeconds) | P1 |
| èŠ‚ç‚¹é¢‘ç¹æŠ–åŠ¨ | è°ƒæ•´ node-monitor-grace-period,å¢åŠ èŠ‚ç‚¹ç¨³å®šæ€§æ£€æµ‹æ—¶é—´ | P2 |
| è‡ªå®šä¹‰æ±¡ç‚¹é©±é€ | æ£€æŸ¥è‡ªå®šä¹‰æ±¡ç‚¹é€»è¾‘,ç¡®ä¿ç¬¦åˆä¸šåŠ¡éœ€æ±‚ | P2 |

---

### `FailedBinding` - ç»‘å®šå¤±è´¥

| å±æ€§ | è¯´æ˜ |
|:---|:---|
| **äº‹ä»¶ç±»å‹** | Warning |
| **æ¥æºç»„ä»¶** | default-scheduler |
| **å…³è”èµ„æº** | Pod |
| **é€‚ç”¨ç‰ˆæœ¬** | v1.0+ |
| **ç”Ÿäº§é¢‘ç‡** | ç½•è§ |

#### äº‹ä»¶å«ä¹‰
è¡¨ç¤ºè°ƒåº¦å™¨å·²ä¸º Pod é€‰æ‹©äº†ç›®æ ‡èŠ‚ç‚¹,ä½†åœ¨æ‰§è¡Œç»‘å®šæ“ä½œ(Binding)æ—¶å¤±è´¥ã€‚è¿™æ˜¯ä¸€ä¸ªç½•è§ä½†ä¸¥é‡çš„é”™è¯¯,é€šå¸¸ç”±äºå¹¶å‘å†²çªã€API Server æ•…éšœã€æƒé™é—®é¢˜æˆ–è°ƒåº¦å™¨å†…éƒ¨é”™è¯¯å¯¼è‡´ã€‚ä¸ FailedScheduling ä¸åŒ,FailedBinding å‘ç”Ÿåœ¨è°ƒåº¦å†³ç­–å®Œæˆåçš„ç»‘å®šé˜¶æ®µã€‚

ç»‘å®šæµç¨‹:è°ƒåº¦å™¨é€šè¿‡ POST è¯·æ±‚å‘ API Server å‘é€ Binding å¯¹è±¡,å°† Pod.spec.nodeName è®¾ç½®ä¸ºç›®æ ‡èŠ‚ç‚¹ã€‚å¦‚æœç»‘å®šå¤±è´¥,Pod ä¼šå›é€€åˆ° Pending çŠ¶æ€,è°ƒåº¦å™¨ä¼šé‡æ–°è°ƒåº¦ã€‚å¸¸è§å¤±è´¥åŸå› åŒ…æ‹¬ Pod å·²è¢«å…¶ä»–è°ƒåº¦å™¨ç»‘å®šã€èŠ‚ç‚¹å·²è¢«åˆ é™¤ã€API Server ä¸å¯è¾¾ç­‰ã€‚

#### å…¸å‹äº‹ä»¶æ¶ˆæ¯
```yaml
# ç»‘å®šå†²çªç¤ºä¾‹
Type:    Warning
Reason:  FailedBinding
Message: Binding rejected: Pod "default/nginx-7d4c8c6d9f-xkj2m" is already bound to node "node-10-0-1-20"

# API Server ä¸å¯è¾¾ç¤ºä¾‹
Type:    Warning
Reason:  FailedBinding
Message: Failed to bind pod: Post "https://apiserver:6443/api/v1/namespaces/default/pods/nginx/binding": dial tcp: lookup apiserver: no such host

# èŠ‚ç‚¹ä¸å­˜åœ¨ç¤ºä¾‹
Type:    Warning
Reason:  FailedBinding
Message: Failed to bind pod: node "worker-node-99" not found
```

#### å½±å“é¢è¯´æ˜
- **è°ƒåº¦å¤±è´¥**: Pod æ— æ³•å¯åŠ¨,æŒç»­ Pending
- **èµ„æºæµªè´¹**: è°ƒåº¦å†³ç­–å®Œæˆä½†æ— æ³•ç»‘å®š,æµªè´¹è°ƒåº¦èµ„æº
- **å¹¶å‘å†²çª**: å¯èƒ½åæ˜ é›†ç¾¤å¹¶å‘æ§åˆ¶é—®é¢˜
- **API Server æ•…éšœ**: å¯èƒ½æ˜¯ API Server ä¸ç¨³å®šçš„ä¿¡å·

#### æ’æŸ¥å»ºè®®

1. **ç¡®è®¤ç»‘å®šå¤±è´¥è¯¦æƒ…**
   ```bash
   # æŸ¥çœ‹ Pod äº‹ä»¶è·å–è¯¦ç»†é”™è¯¯
   kubectl describe pod <pod-name> | grep -A10 FailedBinding
   
   # æŸ¥çœ‹ Pod å½“å‰ç»‘å®šçŠ¶æ€
   kubectl get pod <pod-name> -o yaml | grep nodeName
   kubectl get pod <pod-name> -o json | jq '.spec.nodeName, .status.phase'
   ```

2. **æ£€æŸ¥è°ƒåº¦å™¨çŠ¶æ€**
   ```bash
   # æŸ¥çœ‹è°ƒåº¦å™¨æ—¥å¿—
   kubectl logs -n kube-system -l component=kube-scheduler --tail=100 | grep -i "binding\|error"
   
   # æ£€æŸ¥è°ƒåº¦å™¨è¿è¡ŒçŠ¶æ€
   kubectl get pods -n kube-system -l component=kube-scheduler
   
   # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªè°ƒåº¦å™¨å®ä¾‹
   kubectl get pods -n kube-system --field-selector status.phase=Running | grep scheduler
   ```

3. **æ£€æŸ¥ API Server è¿é€šæ€§**
   ```bash
   # æµ‹è¯• API Server è¿æ¥
   kubectl cluster-info
   kubectl get --raw /healthz
   
   # æŸ¥çœ‹ API Server æ—¥å¿—
   kubectl logs -n kube-system kube-apiserver-<node> --tail=100 | grep -i "error\|binding"
   ```

4. **æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€**
   ```bash
   # ç¡®è®¤ç›®æ ‡èŠ‚ç‚¹å­˜åœ¨
   kubectl get node <node-name>
   
   # æŸ¥çœ‹èŠ‚ç‚¹æ˜¯å¦å¯è°ƒåº¦
   kubectl describe node <node-name> | grep "Unschedulable\|Taints"
   ```

5. **æ£€æŸ¥å¹¶å‘è°ƒåº¦å™¨**
   ```bash
   # æŸ¥çœ‹é›†ç¾¤ä¸­æ‰€æœ‰è°ƒåº¦å™¨
   kubectl get pods -A | grep scheduler
   
   # æ£€æŸ¥ Pod schedulerName é…ç½®
   kubectl get pod <pod-name> -o yaml | grep schedulerName
   ```

6. **æ£€æŸ¥ RBAC æƒé™**
   ```bash
   # æŸ¥çœ‹è°ƒåº¦å™¨ ServiceAccount æƒé™
   kubectl get clusterrolebinding | grep scheduler
   kubectl describe clusterrole system:kube-scheduler
   
   # æµ‹è¯•è°ƒåº¦å™¨æƒé™
   kubectl auth can-i create pods/binding --as=system:kube-scheduler -n default
   ```

#### è§£å†³å»ºè®®

| åŸå›  | è§£å†³æ–¹æ¡ˆ | ä¼˜å…ˆçº§ |
|:----|:--------|:------|
| Pod å·²ç»‘å®šåˆ°å…¶ä»–èŠ‚ç‚¹ | åˆ é™¤ Pod é‡å»º,æˆ–æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªè°ƒåº¦å™¨å†²çª | P1 |
| ç›®æ ‡èŠ‚ç‚¹ä¸å­˜åœ¨ | æ¸…ç†å·²åˆ é™¤èŠ‚ç‚¹çš„é—ç•™æ•°æ®,é‡æ–°è°ƒåº¦ | P1 |
| API Server ä¸å¯è¾¾ | æ£€æŸ¥ç½‘ç»œè¿æ¥,ä¿®å¤ API Server æ•…éšœ | P0 |
| è°ƒåº¦å™¨æƒé™ä¸è¶³ | ä¿®å¤ RBAC é…ç½®,ç¡®ä¿è°ƒåº¦å™¨æœ‰ pods/binding æƒé™ | P0 |
| å¹¶å‘è°ƒåº¦å†²çª | ç¡®ä¿åªæœ‰ä¸€ä¸ªé»˜è®¤è°ƒåº¦å™¨å®ä¾‹,æˆ–ä½¿ç”¨ Leader Election | P1 |
| è°ƒåº¦å™¨å†…éƒ¨é”™è¯¯ | é‡å¯è°ƒåº¦å™¨,å‡çº§è°ƒåº¦å™¨ç‰ˆæœ¬,æ£€æŸ¥æ—¥å¿—æ’æŸ¥ Bug | P1 |
| API Server é™æµ | è°ƒæ•´ API Priority and Fairness é…ç½®,å¢åŠ è°ƒåº¦å™¨ä¼˜å…ˆçº§ | P2 |

---

## ğŸ” è·¨åœºæ™¯æ’æŸ¥å»ºè®®

### 1. å¤§è§„æ¨¡ Pod è°ƒåº¦å¤±è´¥æ’æŸ¥
```bash
# ç»Ÿè®¡æ‰€æœ‰ Pending Pod çš„å¤±è´¥åŸå› åˆ†å¸ƒ
kubectl get pods -A --field-selector status.phase=Pending -o json | \
  jq -r '.items[] | [.metadata.namespace, .metadata.name] | @tsv' | \
  while read ns name; do
    kubectl describe pod -n $ns $name | grep "FailedScheduling" | tail -1
  done | sort | uniq -c | sort -rn

# å¿«é€Ÿè¯†åˆ«é›†ç¾¤ç“¶é¢ˆ
kubectl describe nodes | grep -A5 "Allocated resources" | grep -E "cpu|memory" | \
  awk '{print $2, $3}' | sed 's/[()]//g' | \
  awk '{sum+=$1; count++} END {print "å¹³å‡èµ„æºä½¿ç”¨ç‡:", sum/count "%"}'
```

### 2. è°ƒåº¦å»¶è¿Ÿåˆ†æ
```bash
# è®¡ç®— Pod è°ƒåº¦è€—æ—¶(åˆ›å»ºåˆ°è°ƒåº¦æˆåŠŸ)
kubectl get events --field-selector involvedObject.name=<pod-name> \
  --sort-by='.firstTimestamp' -o json | \
  jq -r '.items | map(select(.reason == "Scheduled" or .reason == "FailedScheduling")) | 
    .[0].firstTimestamp as $start | 
    .[-1].lastTimestamp as $end | 
    "\($start) -> \($end)"'
```

### 3. èŠ‚ç‚¹è°ƒåº¦èƒ½åŠ›è¯„ä¼°
```bash
# è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹è¿˜èƒ½è°ƒåº¦å¤šå°‘ Pod(åŸºäº CPU)
kubectl get nodes -o json | jq -r '.items[] | 
  .metadata.name as $name | 
  (.status.allocatable.cpu | tonumber) as $allocatable | 
  (.status.capacity.cpu | tonumber) as $capacity | 
  "\($name): å¯ç”¨ CPU \($allocatable) cores, å®¹é‡ \($capacity) cores"'
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£äº¤å‰å¼•ç”¨

### ç›¸å…³äº‹ä»¶æ–‡æ¡£
- **[01-pod-lifecycle-events.md](01-pod-lifecycle-events.md)** - Pod åˆ›å»ºã€å¯åŠ¨ã€åˆ é™¤äº‹ä»¶
- **[02-resource-management-events.md](02-resource-management-events.md)** - OOMKilledã€Evicted äº‹ä»¶
- **[03-volume-storage-events.md](03-volume-storage-events.md)** - PVC ç»‘å®šã€æŒ‚è½½å¤±è´¥äº‹ä»¶
- **[06-node-lifecycle-events.md](06-node-lifecycle-events.md)** - èŠ‚ç‚¹ NotReadyã€æ±¡ç‚¹ç®¡ç†äº‹ä»¶

### ç›¸å…³æŠ€æœ¯ä¸»é¢˜
- **[../domain-2-workload/10-pod-scheduling.md](../domain-2-workload/10-pod-scheduling.md)** - Pod è°ƒåº¦æœºåˆ¶è¯¦è§£
- **[../domain-4-storage/20-pv-pvc-dynamic-provisioning.md](../domain-4-storage/20-pv-pvc-dynamic-provisioning.md)** - å­˜å‚¨åŠ¨æ€ä¾›åº”ä¸ç»‘å®š
- **[../domain-3-cluster/15-node-management.md](../domain-3-cluster/15-node-management.md)** - èŠ‚ç‚¹ç®¡ç†ä¸æ±¡ç‚¹é…ç½®
- **[../topic-structural-trouble-shooting/01-control-plane/03-scheduler-troubleshooting.md](../topic-structural-trouble-shooting/01-control-plane/03-scheduler-troubleshooting.md)** - è°ƒåº¦å™¨æ•…éšœæ’æŸ¥

### ç›¸å…³æœ€ä½³å®è·µ
- **[../topic-dictionary/01-operations-best-practices.md](../topic-dictionary/01-operations-best-practices.md)** - è°ƒåº¦ç­–ç•¥æœ€ä½³å®è·µ
- **[../topic-dictionary/03-performance-tuning-expert.md](../topic-dictionary/03-performance-tuning-expert.md)** - è°ƒåº¦æ€§èƒ½ä¼˜åŒ–

---

> **KUDIG-DATABASE** | Domain-33: Kubernetes Events å…¨åŸŸäº‹ä»¶å¤§å…¨ | æ–‡æ¡£ 05/15

# 17 - ç”Ÿäº§ç¯å¢ƒè¿ç»´æœ€ä½³å®è·µ (Production Operations Best Practices)

> **é€‚ç”¨ç‰ˆæœ¬**: Kubernetes v1.25-v1.32 | **æœ€åæ›´æ–°**: 2026-02 | **ä¸“å®¶çº§åˆ«**: â­â­â­â­â­ | **å‚è€ƒ**: [Kubernetes Production Guide](https://kubernetes.io/docs/setup/production-environment/), CNCF Production Readiness

---

## ç›¸å…³æ–‡æ¡£äº¤å‰å¼•ç”¨

### ğŸ”— å…³è”æ¶æ„æ–‡æ¡£
- **[01-K8sæ¶æ„å…¨æ™¯å›¾](./01-kubernetes-architecture-overview.md)** - ç†è§£æ•´ä½“æ¶æ„æ˜¯è¿ç»´çš„åŸºç¡€
- **[02-æ§åˆ¶å¹³é¢è¯¦è§£](./02-core-components-deep-dive.md)** - æŒæ¡æ ¸å¿ƒç»„ä»¶å·¥ä½œåŸç†
- **[14-å®‰å…¨æ¶æ„](./14-security-architecture.md)** - å®‰å…¨è¿ç»´å®è·µåŸºç¡€
- **[15-å¯è§‚æµ‹æ€§ä½“ç³»](./15-observability-architecture.md)** - ç›‘æ§å‘Šè­¦ä½“ç³»å»ºè®¾

### ğŸ“š æ‰©å±•å­¦ä¹ èµ„æ–™
- **[Domain-12: æ•…éšœæ’æŸ¥](../domain-12-troubleshooting)** - ç³»ç»Ÿæ€§æ•…éšœè¯Šæ–­æ–¹æ³•è®º
- **[Domain-9: å¹³å°è¿ç»´](../domain-9-platform-ops)** - æ—¥å¸¸è¿ç»´æ“ä½œæŒ‡å—
- **[CNCF Production Readiness](https://www.cncf.io/blog/2020/08/12/kubernetes-production-readiness-checklist/)** - CNCFå®˜æ–¹ç”Ÿäº§å°±ç»ªæ£€æŸ¥æ¸…å•

---

## 1. ç”Ÿäº§ç¯å¢ƒæ¶æ„è®¾è®¡åŸåˆ™ (Production Architecture Principles)

### 1.1 é«˜å¯ç”¨æ€§è®¾è®¡ (High Availability Design)

#### æ§åˆ¶å¹³é¢é«˜å¯ç”¨éƒ¨ç½²
```yaml
# HAæ§åˆ¶å¹³é¢éƒ¨ç½²é…ç½®ç¤ºä¾‹
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
metadata:
  name: prod-cluster
spec:
  # etcdé«˜å¯ç”¨é…ç½®
  etcd:
    local:
      extraArgs:
        initial-cluster-state: new
        listen-client-urls: https://0.0.0.0:2379
        listen-peer-urls: https://0.0.0.0:2380
        advertise-client-urls: https://ETCD_IP:2379
        initial-advertise-peer-urls: https://ETCD_IP:2380
      serverCertSANs:
        - "etcd1.prod.internal"
        - "etcd2.prod.internal" 
        - "etcd3.prod.internal"
  
  # API Serveré«˜å¯ç”¨é…ç½®
  apiServer:
    certSANs:
      - "k8s-api.prod.internal"
      - "k8s-api-vip.prod.internal"
      - "10.100.0.100"  # VIPåœ°å€
    extraArgs:
      audit-log-path: "/var/log/kubernetes/audit.log"
      audit-policy-file: "/etc/kubernetes/policies/audit-policy.yaml"
      profiling: "false"
      service-account-issuer: "https://k8s-api.prod.internal"
      tls-cipher-suites: "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256"
      
  # Controller Manageré…ç½®
  controllerManager:
    extraArgs:
      cluster-signing-cert-file: "/etc/kubernetes/pki/ca.crt"
      cluster-signing-key-file: "/etc/kubernetes/pki/ca.key"
      concurrent-deployment-syncs: "10"
      concurrent-endpoint-syncs: "10"
      horizontal-pod-autoscaler-use-rest-clients: "true"
      
  # Scheduleré…ç½®
  scheduler:
    extraArgs:
      profiling: "false"
      bind-timeout: "600s"
```

#### è´Ÿè½½å‡è¡¡å’ŒVIPé…ç½®
```bash
# HAProxyé…ç½®ç¤ºä¾‹
cat > /etc/haproxy/haproxy.cfg << EOF
global
    log /dev/log local0
    log /dev/log local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    log global
    mode tcp
    option tcplog
    option dontlognull
    timeout connect 10s
    timeout client 30s
    timeout server 30s

frontend k8s-api
    bind *:6443
    default_backend k8s-api-backend

backend k8s-api-backend
    balance roundrobin
    server master1 10.100.1.10:6443 check fall 3 rise 2
    server master2 10.100.1.11:6443 check fall 3 rise 2  
    server master3 10.100.1.12:6443 check fall 3 rise 2
EOF
```

### 1.2 å®‰å…¨åŠ å›ºé…ç½® (Security Hardening)

#### Podå®‰å…¨ç­–ç•¥é…ç½®
```yaml
# PodSecurityPolicyç¤ºä¾‹ (å·²åºŸå¼ƒï¼Œæ¨èä½¿ç”¨Pod Security Admission)
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  readOnlyRootFilesystem: true
```

#### ç½‘ç»œç­–ç•¥å®æ–½
```yaml
# é»˜è®¤æ‹’ç»æ‰€æœ‰æµé‡çš„ç½‘ç»œç­–ç•¥
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# å…è®¸ç‰¹å®šæœåŠ¡é—´é€šä¿¡
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-app-traffic
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: my-app
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
```

### 1.3 èµ„æºç®¡ç†å’Œè°ƒåº¦ä¼˜åŒ– (Resource Management & Scheduling)

#### èŠ‚ç‚¹èµ„æºé¢„ç•™é…ç½®
```yaml
# kubeletèµ„æºé…ç½®
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
systemReserved:
  cpu: 500m
  memory: 1Gi
  ephemeral-storage: 10Gi
kubeReserved:
  cpu: 200m
  memory: 512Mi
  ephemeral-storage: 2Gi
evictionHard:
  memory.available: "500Mi"
  nodefs.available: "10%"
  nodefs.inodesFree: "5%"
  imagefs.available: "15%"
evictionSoft:
  memory.available: "1Gi"
  nodefs.available: "15%"
  nodefs.inodesFree: "10%"
  imagefs.available: "20%"
evictionSoftGracePeriod:
  memory.available: "1m30s"
  nodefs.available: "1m30s"
  nodefs.inodesFree: "1m30s"
  imagefs.available: "1m30s"
```

#### æœåŠ¡è´¨é‡ç­‰çº§é…ç½®
```yaml
# Pod QoSé…ç½®ç¤ºä¾‹
apiVersion: v1
kind: Pod
metadata:
  name: qos-example
spec:
  containers:
  - name: high-priority
    image: nginx
    resources:
      requests:
        memory: "256Mi"
        cpu: "250m"
      limits:
        memory: "512Mi" 
        cpu: "500m"
    # Guaranteed QoS - æœ€é«˜ä¼˜å…ˆçº§
    
  - name: burstable
    image: busybox
    resources:
      requests:
        memory: "128Mi"
        cpu: "100m"
      # Burstable QoS - å¯çªå‘
      
  - name: best-effort
    image: alpine
    # BestEffort QoS - æœ€ä½ä¼˜å…ˆçº§
```

## 2. ç›‘æ§å‘Šè­¦ä½“ç³»å»ºè®¾ (Monitoring & Alerting System)

### 2.1 æ ¸å¿ƒæŒ‡æ ‡ç›‘æ§é…ç½®

#### Prometheusç›‘æ§è§„åˆ™
```yaml
# æ ¸å¿ƒç»„ä»¶ç›‘æ§è§„åˆ™
groups:
- name: kubernetes.system.rules
  rules:
  # API Serverç›‘æ§
  - alert: APIServerDown
    expr: absent(up{job="apiserver"}) == 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "API Serverä¸å¯ç”¨"
      description: "Kubernetes API Serveråœ¨{{ $labels.instance }}ä¸Šå·²åœæ­¢å“åº”è¶…è¿‡5åˆ†é’Ÿ"
      
  - alert: APIServerLatencyHigh
    expr: histogram_quantile(0.99, apiserver_request_duration_seconds_bucket{verb=~"LIST|GET"}) > 1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "API Serverå“åº”å»¶è¿Ÿè¿‡é«˜"
      description: "{{ $labels.verb }}è¯·æ±‚99thç™¾åˆ†ä½å»¶è¿Ÿä¸º{{ $value }}ç§’ï¼Œè¶…è¿‡é˜ˆå€¼1ç§’"
      
  # etcdç›‘æ§
  - alert: EtcdMembersDown
    expr: (count(etcd_server_has_leader) - sum(etcd_server_has_leader)) > 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "etcdæˆå‘˜æ•…éšœ"
      description: "etcdé›†ç¾¤ä¸­æœ‰{{ $value }}ä¸ªæˆå‘˜å¤±å»é¢†å¯¼è€…ï¼Œå½±å“é›†ç¾¤å¯ç”¨æ€§"
      
  # èŠ‚ç‚¹ç›‘æ§
  - alert: NodeNotReady
    expr: kube_node_status_condition{condition="Ready",status!="true"} == 1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "èŠ‚ç‚¹NotReadyçŠ¶æ€"
      description: "èŠ‚ç‚¹{{ $labels.node }}å¤„äºNotReadyçŠ¶æ€è¶…è¿‡10åˆ†é’Ÿ"
      
  # Podç›‘æ§
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[5m]) * 60 * 5 > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "PodæŒç»­é‡å¯"
      description: "Pod {{ $labels.pod }} åœ¨å‘½åç©ºé—´ {{ $labels.namespace }} ä¸­åœ¨è¿‡å»5åˆ†é’Ÿå†…é‡å¯æ¬¡æ•°è¿‡å¤š"
```

#### Grafanaä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "title": "Kubernetes Production Overview",
    "panels": [
      {
        "title": "é›†ç¾¤å¥åº·çŠ¶æ€",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(kube_node_status_condition{condition=\"Ready\",status=\"true\"})",
            "legendFormat": "Ready Nodes"
          },
          {
            "expr": "count(kube_pod_status_ready{condition=\"true\"})",
            "legendFormat": "Ready Pods"
          }
        ]
      },
      {
        "title": "API Serveræ€§èƒ½",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(apiserver_request_total[5m])",
            "legendFormat": "{{ verb }} requests/sec"
          },
          {
            "expr": "histogram_quantile(0.99, apiserver_request_duration_seconds_bucket)",
            "legendFormat": "99th percentile latency"
          }
        ]
      }
    ]
  }
}
```

### 2.2 æ—¥å¿—æ”¶é›†å’Œåˆ†æç³»ç»Ÿ

#### Fluentdé…ç½®ç¤ºä¾‹
```xml
<!-- Fluentdé…ç½®æ–‡ä»¶ -->
<source>
  @type tail
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type json
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

<filter kubernetes.**>
  @type kubernetes_metadata
  @id filter_kube_metadata
  kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://kubernetes.default.svc:443/api'}"
  bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
  ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  skip_labels false
  skip_master_url false
  skip_container_metadata false
</filter>

<match kubernetes.var.log.containers.**_production_**.log>
  @type elasticsearch
  host elasticsearch.prod.svc.cluster.local
  port 9200
  logstash_format true
  logstash_prefix production-logs
  include_tag_key true
  type_name container_log
  <buffer>
    @type file
    path /var/log/fluentd-buffers/kubernetes.system.buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>
```

## 3. å¤‡ä»½æ¢å¤ç­–ç•¥ (Backup & Recovery Strategy)

### 3.1 etcdå¤‡ä»½é…ç½®

#### è‡ªåŠ¨å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# etcd-backup.sh - etcdè‡ªåŠ¨å¤‡ä»½è„šæœ¬

set -euo pipefail

BACKUP_DIR="/backup/etcd"
DATE=$(date +%Y%m%d_%H%M%S)
ETCDCTL_API=3

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p ${BACKUP_DIR}/${DATE}

# æ‰§è¡Œetcdå¿«ç…§
etcdctl snapshot save ${BACKUP_DIR}/${DATE}/snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# éªŒè¯å¿«ç…§å®Œæ•´æ€§
etcdctl snapshot status ${BACKUP_DIR}/${DATE}/snapshot.db \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
tar -czf ${BACKUP_DIR}/${DATE}.tar.gz -C ${BACKUP_DIR} ${DATE}

# åˆ é™¤æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
find ${BACKUP_DIR} -name "*.tar.gz" -mtime +7 -delete
find ${BACKUP_DIR} -mindepth 1 -maxdepth 1 -type d -mtime +7 -exec rm -rf {} \;

# ä¸Šä¼ åˆ°è¿œç¨‹å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
if [[ "${UPLOAD_TO_S3:-false}" == "true" ]]; then
  aws s3 cp ${BACKUP_DIR}/${DATE}.tar.gz s3://${S3_BUCKET}/etcd-backups/
fi

echo "etcd backup completed: ${BACKUP_DIR}/${DATE}.tar.gz"
```

#### å¤‡ä»½éªŒè¯è„šæœ¬
```bash
#!/bin/bash
# etcd-restore-test.sh - å¤‡ä»½æ¢å¤æµ‹è¯•è„šæœ¬

set -euo pipefail

BACKUP_FILE=$1
TEST_NAMESPACE="backup-test-$(date +%s)"

# æ¢å¤åˆ°ä¸´æ—¶é›†ç¾¤è¿›è¡ŒéªŒè¯
docker run --rm -d \
  --name etcd-restore-test \
  -p 2379:2379 \
  -v $(pwd)/${BACKUP_FILE}:/backup/snapshot.db \
  quay.io/coreos/etcd:v3.5.0 \
  etcd \
  --data-dir=/tmp/etcd-data \
  --listen-client-urls=http://0.0.0.0:2379 \
  --advertise-client-urls=http://0.0.0.0:2379

sleep 10

# éªŒè¯æ•°æ®å®Œæ•´æ€§
docker exec etcd-restore-test etcdctl snapshot status /backup/snapshot.db
docker exec etcd-restore-test etcdctl get / --prefix --keys-only | wc -l

# æ¸…ç†æµ‹è¯•ç¯å¢ƒ
docker stop etcd-restore-test

echo "Backup validation completed successfully"
```

### 3.2 åº”ç”¨é…ç½®å¤‡ä»½

#### Helm Releaseå¤‡ä»½
```bash
#!/bin/bash
# helm-backup.sh - Helm Releaseå¤‡ä»½è„šæœ¬

BACKUP_DIR="/backup/helm"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p ${BACKUP_DIR}/${DATE}

# å¤‡ä»½æ‰€æœ‰Helm Release
helm list --all-namespaces -o json | jq -r '.[].name + " " + .[].namespace' | while read release namespace; do
  mkdir -p ${BACKUP_DIR}/${DATE}/${namespace}
  helm get values ${release} --namespace ${namespace} > ${BACKUP_DIR}/${DATE}/${namespace}/${release}-values.yaml
  helm get manifest ${release} --namespace ${namespace} > ${BACKUP_DIR}/${DATE}/${namespace}/${release}-manifest.yaml
done

# å‹ç¼©å¤‡ä»½
tar -czf ${BACKUP_DIR}/${DATE}-helm.tar.gz -C ${BACKUP_DIR} ${DATE}
rm -rf ${BACKUP_DIR}/${DATE}

echo "Helm releases backup completed: ${BACKUP_DIR}/${DATE}-helm.tar.gz"
```

## 4. ç¾éš¾æ¢å¤è®¡åˆ’ (Disaster Recovery Plan)

### 4.1 DRæ¼”ç»ƒæµç¨‹

#### é›†ç¾¤é‡å»ºè„šæœ¬
```bash
#!/bin/bash
# cluster-restore.sh - é›†ç¾¤ç¾éš¾æ¢å¤è„šæœ¬

set -euo pipefail

BACKUP_FILE=${1:-"/backup/latest.tar.gz"}
NEW_CLUSTER_CIDR=${2:-"10.244.0.0/16"}

# 1. åˆå§‹åŒ–æ–°é›†ç¾¤
kubeadm init \
  --pod-network-cidr=${NEW_CLUSTER_CIDR} \
  --control-plane-endpoint="k8s-api.new-cluster.internal:6443" \
  --upload-certs

# 2. æ¢å¤etcdæ•°æ®
mkdir -p /tmp/etcd-restore
tar -xzf ${BACKUP_FILE} -C /tmp/etcd-restore

# åœæ­¢å½“å‰etcd
systemctl stop etcd

# æ¢å¤å¿«ç…§
ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcd-restore/snapshot.db \
  --data-dir=/var/lib/etcd \
  --initial-cluster="etcd1=https://10.100.1.10:2380" \
  --initial-cluster-token="etcd-cluster-1" \
  --initial-advertise-peer-urls="https://10.100.1.10:2380"

# å¯åŠ¨etcd
systemctl start etcd

# 3. é‡æ–°åº”ç”¨é…ç½®
kubectl apply -f /tmp/etcd-restore/manifests/

echo "Cluster restoration completed"
```

### 4.2 å¤šåŒºåŸŸéƒ¨ç½²ç­–ç•¥

#### è·¨åŒºåŸŸè”é‚¦é…ç½®
```yaml
# Kubefedé…ç½®ç¤ºä¾‹
apiVersion: types.kubefed.io/v1beta1
kind: KubeFedCluster
metadata:
  name: prod-us-east
  namespace: kube-federation-system
spec:
  apiEndpoint: https://k8s-api.us-east.prod.internal:6443
  secretRef:
    name: us-east-cluster-secret
    
---
apiVersion: types.kubefed.io/v1beta1
kind: KubeFedCluster
metadata:
  name: prod-us-west
  namespace: kube-federation-system
spec:
  apiEndpoint: https://k8s-api.us-west.prod.internal:6443
  secretRef:
    name: us-west-cluster-secret
```

## 5. æ€§èƒ½ä¼˜åŒ–å’Œå®¹é‡è§„åˆ’ (Performance Optimization & Capacity Planning)

### 5.1 é›†ç¾¤å®¹é‡è¯„ä¼°

#### å®¹é‡è§„åˆ’è®¡ç®—å™¨
```bash
#!/bin/bash
# capacity-planner.sh - é›†ç¾¤å®¹é‡è§„åˆ’å·¥å…·

NODE_COUNT=${1:-10}
POD_PER_NODE=${2:-110}
CPU_PER_NODE=${3:-8}
MEMORY_PER_NODE=${4:-32}

echo "=== Kubernetesé›†ç¾¤å®¹é‡è§„åˆ’æŠ¥å‘Š ==="
echo "èŠ‚ç‚¹æ•°é‡: ${NODE_COUNT}"
echo "æ¯èŠ‚ç‚¹Podæ•°: ${POD_PER_NODE}"
echo "æ¯èŠ‚ç‚¹CPU: ${CPU_PER_NODE}æ ¸"
echo "æ¯èŠ‚ç‚¹å†…å­˜: ${MEMORY_PER_NODE}GB"
echo ""

# è®¡ç®—æ€»å®¹é‡
TOTAL_PODS=$((NODE_COUNT * POD_PER_NODE))
TOTAL_CPU=$((NODE_COUNT * CPU_PER_NODE))
TOTAL_MEMORY=$((NODE_COUNT * MEMORY_PER_NODE))

echo "æ€»Podå®¹é‡: ${TOTAL_PODS}ä¸ª"
echo "æ€»CPUå®¹é‡: ${TOTAL_CPU}æ ¸"
echo "æ€»å†…å­˜å®¹é‡: ${TOTAL_MEMORY}GB"

# API Serverå‹åŠ›ä¼°ç®—
API_SERVER_QPS=$((TOTAL_PODS / 100))  # å‡è®¾æ¯ä¸ªPodäº§ç”Ÿ0.01 QPS
echo "é¢„ä¼°API Server QPS: ${API_SERVER_QPS}"

# etcdå­˜å‚¨éœ€æ±‚ä¼°ç®—
ETCD_STORAGE_GB=$((TOTAL_PODS * 2 / 1024))  # å‡è®¾æ¯ä¸ªå¯¹è±¡2KB
echo "é¢„ä¼°etcdå­˜å‚¨éœ€æ±‚: ${ETCD_STORAGE_GB}GB"

# å»ºè®®é…ç½®
echo ""
echo "=== å»ºè®®é…ç½® ==="
if [ $NODE_COUNT -gt 50 ]; then
  echo "âœ“ å»ºè®®å¯ç”¨API Serveræ°´å¹³æ‰©å±•"
  echo "âœ“ å»ºè®®etcdé›†ç¾¤ä½¿ç”¨SSDå­˜å‚¨"
  echo "âœ“ å»ºè®®å¢åŠ ç›‘æ§é‡‡æ ·é—´éš”"
fi
```

### 5.2 æ€§èƒ½è°ƒä¼˜å‚æ•°

#### å†…æ ¸å‚æ•°ä¼˜åŒ–
```bash
# /etc/sysctl.d/k8s.conf - Kuberneteså†…æ ¸ä¼˜åŒ–å‚æ•°
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 60
net.ipv4.tcp_keepalive_probes = 3
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 8096
net.ipv4.tcp_fin_timeout = 30
vm.max_map_count = 262144
fs.file-max = 1000000
fs.inotify.max_user_watches = 1048576
```

#### kubeletæ€§èƒ½è°ƒä¼˜
```yaml
# kubeletæ€§èƒ½ä¼˜åŒ–é…ç½®
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
maxPods: 250
podPidsLimit: 4096
serializeImagePulls: false
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
evictionPressureTransitionPeriod: "5m0s"
containerLogMaxSize: "100Mi"
containerLogMaxFiles: 10
cpuManagerPolicy: "static"
topologyManagerPolicy: "best-effort"
```

## 6. å®‰å…¨åˆè§„å’Œå®¡è®¡ (Security Compliance & Auditing)

### 6.1 CISåŸºå‡†ç¬¦åˆæ€§æ£€æŸ¥

#### è‡ªåŠ¨åŒ–åˆè§„æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# cis-benchmark-check.sh - CIS KubernetesåŸºå‡†æ£€æŸ¥

echo "=== CIS Kubernetes Benchmark Check ==="

# æ£€æŸ¥API Serveré…ç½®
echo "1. API Serverå®‰å…¨é…ç½®æ£€æŸ¥:"
if systemctl is-active kube-apiserver >/dev/null 2>&1; then
  ps aux | grep kube-apiserver | grep -q "profiling=false" && echo "âœ“ Profilingå·²ç¦ç”¨" || echo "âœ— Profilingæœªç¦ç”¨"
  ps aux | grep kube-apiserver | grep -q "anonymous-auth=false" && echo "âœ“ åŒ¿åè®¤è¯å·²ç¦ç”¨" || echo "âœ— åŒ¿åè®¤è¯æœªç¦ç”¨"
  ps aux | grep kube-apiserver | grep -q "authorization-mode.*RBAC" && echo "âœ“ RBACå·²å¯ç”¨" || echo "âœ— RBACæœªå¯ç”¨"
else
  echo "âœ— API Serveræœªè¿è¡Œ"
fi

# æ£€æŸ¥etcdé…ç½®
echo -e "\n2. etcdå®‰å…¨é…ç½®æ£€æŸ¥:"
if systemctl is-active etcd >/dev/null 2>&1; then
  ps aux | grep etcd | grep -q "client-cert-auth=true" && echo "âœ“ å®¢æˆ·ç«¯è¯ä¹¦è®¤è¯å·²å¯ç”¨" || echo "âœ— å®¢æˆ·ç«¯è¯ä¹¦è®¤è¯æœªå¯ç”¨"
  ps aux | grep etcd | grep -q "auto-tls=false" && echo "âœ“ è‡ªåŠ¨TLSå·²ç¦ç”¨" || echo "âœ— è‡ªåŠ¨TLSæœªç¦ç”¨"
else
  echo "âœ— etcdæœªè¿è¡Œ"
fi

# æ£€æŸ¥ç½‘ç»œç­–ç•¥
echo -e "\n3. ç½‘ç»œç­–ç•¥æ£€æŸ¥:"
default_deny_count=$(kubectl get networkpolicy --all-namespaces 2>/dev/null | grep -c "default-deny" || echo "0")
if [ "$default_deny_count" -gt 0 ]; then
  echo "âœ“ å‘ç°é»˜è®¤æ‹’ç»ç­–ç•¥"
else
  echo "âœ— æœªå‘ç°é»˜è®¤æ‹’ç»ç­–ç•¥"
fi

echo -e "\n=== æ£€æŸ¥å®Œæˆ ==="
```

### 6.2 å®¡è®¡æ—¥å¿—é…ç½®

#### å®¡è®¡ç­–ç•¥é…ç½®
```yaml
# /etc/kubernetes/policies/audit-policy.yaml
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# ä¸è®°å½•watchè¯·æ±‚
- level: None
  verbs: ["watch"]
  
# ä¸è®°å½•è¯»å–ç›¸å…³çš„è¯·æ±‚
- level: None
  resources:
  - group: ""
    resources: ["events"]
    
# è®°å½•Podå˜æ›´çš„å…ƒæ•°æ®
- level: Metadata
  resources:
  - group: ""
    resources: ["pods"]
  verbs: ["create", "update", "patch", "delete"]
  
# è®°å½•Secretå’ŒConfigMapçš„å˜æ›´
- level: RequestResponse
  resources:
  - group: ""
    resources: ["secrets", "configmaps"]
  verbs: ["create", "update", "patch", "delete"]
  
# è®°å½•è®¤è¯ç›¸å…³æ“ä½œ
- level: RequestResponse
  resources:
  - group: "rbac.authorization.k8s.io"
    resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]
  verbs: ["create", "update", "patch", "delete"]
  
# è®°å½•æ‰€æœ‰å…¶ä»–è¯·æ±‚çš„åŸºæœ¬ä¿¡æ¯
- level: Metadata
```

## 7. è¿ç»´è‡ªåŠ¨åŒ–å’ŒGitOps (Operations Automation & GitOps)

### 7.1 GitOpsæµæ°´çº¿é…ç½®

#### ArgoCDåº”ç”¨é…ç½®
```yaml
# argocd-application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: production-app
  namespace: argocd
spec:
  project: production
  source:
    repoURL: https://github.com/company/production-manifests.git
    targetRevision: HEAD
    path: apps/production
    helm:
      valueFiles:
      - values-prod.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - ApplyOutOfSyncOnly=true
  ignoreDifferences:
  - group: apps
    kind: Deployment
    jsonPointers:
    - /spec/replicas
  info:
  - name: url
    value: https://grafana.prod.company.com/d/cluster-overview
```

### 7.2 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

#### å¥åº·æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# health-check.sh - é›†ç¾¤å¥åº·æ£€æŸ¥è„šæœ¬

set -euo pipefail

echo "=== Kubernetesé›†ç¾¤å¥åº·æ£€æŸ¥ ==="
echo "æ£€æŸ¥æ—¶é—´: $(date)"
echo "é›†ç¾¤ç‰ˆæœ¬: $(kubectl version --short | grep Server | awk '{print $3}')"
echo ""

# 1. èŠ‚ç‚¹å¥åº·æ£€æŸ¥
echo "1. èŠ‚ç‚¹çŠ¶æ€æ£€æŸ¥:"
kubectl get nodes -o wide | grep -E "(NotReady|SchedulingDisabled)" && {
  echo "âš ï¸  å‘ç°ä¸å¥åº·çš„èŠ‚ç‚¹"
  exit 1
} || echo "âœ“ æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€æ­£å¸¸"

# 2. æ ¸å¿ƒç»„ä»¶æ£€æŸ¥
echo -e "\n2. æ ¸å¿ƒç»„ä»¶æ£€æŸ¥:"
for component in kube-apiserver kube-controller-manager kube-scheduler; do
  pod_count=$(kubectl get pods -n kube-system -l tier=control-plane,component=${component} --no-headers | wc -l)
  ready_count=$(kubectl get pods -n kube-system -l tier=control-plane,component=${component} -o jsonpath='{.items[*].status.containerStatuses[?(@.ready==true)].ready}' | wc -w)
  if [ "$pod_count" -eq "$ready_count" ] && [ "$pod_count" -gt 0 ]; then
    echo "âœ“ ${component}: ${ready_count}/${pod_count} å°±ç»ª"
  else
    echo "âœ— ${component}: ${ready_count}/${pod_count} å°±ç»ª"
    exit 1
  fi
done

# 3. ç³»ç»Ÿèµ„æºæ£€æŸ¥
echo -e "\n3. ç³»ç»Ÿèµ„æºæ£€æŸ¥:"
low_resource_pods=$(kubectl top pods --all-namespaces 2>/dev/null | awk '$3+0 > 80 || $4+0 > 80 {print $1"/"$2}' | head -10)
if [ -n "$low_resource_pods" ]; then
  echo "âš ï¸  å‘ç°èµ„æºä½¿ç”¨ç‡é«˜çš„Pod:"
  echo "$low_resource_pods"
else
  echo "âœ“ èµ„æºä½¿ç”¨ç‡æ­£å¸¸"
fi

echo -e "\n=== å¥åº·æ£€æŸ¥å®Œæˆ ==="
```

## 8. æˆæœ¬ä¼˜åŒ–å’ŒFinOps (Cost Optimization & FinOps)

### 8.1 èµ„æºä½¿ç”¨åˆ†æ

#### æˆæœ¬åˆ†æè„šæœ¬
```bash
#!/bin/bash
# cost-analyzer.sh - Kubernetesæˆæœ¬åˆ†æå·¥å…·

echo "=== Kubernetesæˆæœ¬åˆ†ææŠ¥å‘Š ==="
echo "åˆ†ææ—¶é—´: $(date)"
echo ""

# è·å–èŠ‚ç‚¹è§„æ ¼å’Œæˆæœ¬
echo "1. èŠ‚ç‚¹æˆæœ¬åˆ†æ:"
kubectl get nodes -o json | jq -r '
  .items[] | 
  {
    name: .metadata.name,
    instance_type: .metadata.labels."node.kubernetes.io/instance-type",
    capacity_cpu: .status.capacity.cpu,
    capacity_memory: .status.capacity.memory,
    allocatable_cpu: .status.allocatable.cpu,
    allocatable_memory: .status.allocatable.memory
  } | 
  "èŠ‚ç‚¹: \(.name)\nå®ä¾‹ç±»å‹: \(.instance_type)\nCPUå®¹é‡: \(.capacity_cpu)æ ¸\nå†…å­˜å®¹é‡: \(.capacity_memory)\nå¯åˆ†é…CPU: \(.allocatable_cpu)æ ¸\nå¯åˆ†é…å†…å­˜: \(.allocatable_memory)\n---"
'

# Podèµ„æºè¯·æ±‚åˆ†æ
echo -e "\n2. Podèµ„æºè¯·æ±‚åˆ†æ:"
kubectl get pods --all-namespaces -o json | jq -r '
  .items[] |
  .spec.containers[] |
  {
    namespace: .metadata.namespace,
    pod: .metadata.name,
    container: .name,
    cpu_request: .resources.requests.cpu,
    memory_request: .resources.requests.memory,
    cpu_limit: .resources.limits.cpu,
    memory_limit: .resources.limits.memory
  } |
  select(.cpu_request != null or .memory_request != null) |
  "NS:\(.namespace) Pod:\(.pod) Container:\(.container)\n  CPUè¯·æ±‚: \(.cpu_request) é™åˆ¶: \(.cpu_limit)\n  å†…å­˜è¯·æ±‚: \(.memory_request) é™åˆ¶: \(.memory_limit)\n---"
' | head -20

# èµ„æºåˆ©ç”¨ç‡ç»Ÿè®¡
echo -e "\n3. èµ„æºåˆ©ç”¨ç‡ç»Ÿè®¡:"
echo "CPUè¯·æ±‚æ€»é‡: $(kubectl get pods --all-namespaces -o json | jq '[.items[].spec.containers[].resources.requests.cpu | tonumber] | add') æ ¸"
echo "å†…å­˜è¯·æ±‚æ€»é‡: $(kubectl get pods --all-namespaces -o json | jq '[.items[].spec.containers[].resources.requests.memory | sub("Gi$"; "") | tonumber] | add') Gi"
```

### 8.2 è‡ªåŠ¨ä¼¸ç¼©é…ç½®

#### HPAé…ç½®ç¤ºä¾‹
```yaml
# HorizontalPodAutoscaleré…ç½®
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 4
        periodSeconds: 60
      selectPolicy: Max
```

---

## ğŸ’¡ ä¸“å®¶æç¤º (Expert Tips)

### å…³é”®æˆåŠŸå› ç´ 
1. **æ¸è¿›å¼éƒ¨ç½²** - é‡‡ç”¨è“ç»¿éƒ¨ç½²æˆ–é‡‘ä¸é›€å‘å¸ƒç­–ç•¥
2. **ç›‘æ§å…ˆè¡Œ** - åœ¨å˜æ›´å‰ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç›‘æ§è¦†ç›–
3. **æ–‡æ¡£é©±åŠ¨** - æ‰€æœ‰æ“ä½œéƒ½åº”è¯¥æœ‰è¯¦ç»†çš„æ–‡æ¡£è®°å½•
4. **å®šæœŸæ¼”ç»ƒ** - å®šæœŸè¿›è¡ŒDRæ¼”ç»ƒå’Œæ•…éšœæ¢å¤è®­ç»ƒ
5. **æŒç»­æ”¹è¿›** - åŸºäºç›‘æ§æ•°æ®å’Œæ•…éšœç»éªŒä¸æ–­ä¼˜åŒ–

### å¸¸è§é™·é˜±é¿å…
- âŒ å¿½è§†etcdæ€§èƒ½è°ƒä¼˜
- âŒ ç¼ºä¹é€‚å½“çš„å¤‡ä»½ç­–ç•¥  
- âŒ æ²¡æœ‰å®æ–½ç½‘ç»œç­–ç•¥
- âŒ å¿½ç•¥å®‰å…¨è¡¥ä¸æ›´æ–°
- âŒ ç¼ºä¹å®¹é‡è§„åˆ’

### æœ€ä½³å®è·µæ€»ç»“
- âœ… å®æ–½å¤šå±‚æ¬¡ç›‘æ§å‘Šè­¦ä½“ç³»
- âœ… å»ºç«‹å®Œå–„çš„å¤‡ä»½æ¢å¤æœºåˆ¶
- âœ… é‡‡ç”¨GitOpsè¿›è¡Œé…ç½®ç®¡ç†
- âœ… å®æ–½ä¸¥æ ¼çš„è®¿é—®æ§åˆ¶ç­–ç•¥
- âœ… å®šæœŸè¿›è¡Œå®‰å…¨åˆè§„æ£€æŸ¥

---
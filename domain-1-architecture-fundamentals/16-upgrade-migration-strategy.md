# Kubernetes å‡çº§å’Œè¿ç§»ç­–ç•¥

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†é˜è¿° Kubernetes é›†ç¾¤çš„å‡çº§ç­–ç•¥ã€è¿ç§»æ–¹æ¡ˆå’Œç‰ˆæœ¬ç®¡ç†æœ€ä½³å®è·µï¼Œæ¶µç›–ä»ç‰ˆæœ¬è§„åˆ’åˆ°å›æ»šæ¢å¤çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œç¡®ä¿é›†ç¾¤å‡çº§è¿‡ç¨‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚

---

## ä¸€ã€ç‰ˆæœ¬ç®¡ç†ç­–ç•¥

### 1.1 ç‰ˆæœ¬æ”¯æŒç”Ÿå‘½å‘¨æœŸ

#### Kubernetes ç‰ˆæœ¬æ”¯æŒç­–ç•¥
```yaml
version_support_policy:
  current_version: "v1.28.x"
  supported_versions:
    - "v1.28.x"  # å½“å‰ç¨³å®šç‰ˆ
    - "v1.27.x"  # ä¸Šä¸€ç¨³å®šç‰ˆ
    - "v1.26.x"  # å†ä¸Šä¸€ç¨³å®šç‰ˆ
  
  upgrade_windows:
    minor_version_upgrade: "æ¯å­£åº¦ä¸€æ¬¡"
    patch_version_upgrade: "æ¯æœˆä¸€æ¬¡"
    emergency_patches: "éšæ—¶å‘å¸ƒ"
  
  deprecation_timeline:
    alpha_features: "ä¸ç¨³å®šï¼Œéšæ—¶å¯èƒ½ç§»é™¤"
    beta_features: "è‡³å°‘æ”¯æŒ3ä¸ªç‰ˆæœ¬"
    stable_features: "é•¿æœŸæ”¯æŒï¼Œç§»é™¤å‰æå‰é€šçŸ¥"
```

#### ç‰ˆæœ¬å…¼å®¹æ€§çŸ©é˜µ
```bash
# Kubernetes ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥è„šæœ¬
#!/bin/bash

CURRENT_VERSION="v1.28.0"
TARGET_VERSION="v1.29.0"

# API ç‰ˆæœ¬å…¼å®¹æ€§æ£€æŸ¥
check_api_compatibility() {
    echo "æ£€æŸ¥ API ç‰ˆæœ¬å…¼å®¹æ€§..."
    
    # æ£€æŸ¥å·²å¼ƒç”¨çš„ API
    deprecated_apis=$(kubectl api-resources --verbs=list -o name | \
        xargs -I {} kubectl get {} --all-namespaces --ignore-not-found 2>/dev/null | \
        grep -E "(v1beta1|v1beta2)" | wc -l)
    
    if [ "$deprecated_apis" -gt 0 ]; then
        echo "âš ï¸  å‘ç° $deprecated_apis ä¸ªä½¿ç”¨å·²å¼ƒç”¨ API çš„èµ„æº"
        return 1
    fi
    
    echo "âœ… API å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡"
    return 0
}

# è‡ªå®šä¹‰èµ„æºå…¼å®¹æ€§æ£€æŸ¥
check_crds_compatibility() {
    echo "æ£€æŸ¥è‡ªå®šä¹‰èµ„æºå®šä¹‰å…¼å®¹æ€§..."
    
    crds=$(kubectl get crds -o jsonpath='{.items[*].metadata.name}')
    for crd in $crds; do
        version_supported=$(kubectl explain $crd --recursive | \
            grep -c "$TARGET_VERSION" || echo "0")
        if [ "$version_supported" -eq 0 ]; then
            echo "âŒ CRD $crd å¯èƒ½ä¸å…¼å®¹ç›®æ ‡ç‰ˆæœ¬"
        fi
    done
}
```

### 1.2 å‡çº§è·¯å¾„è§„åˆ’

#### ç‰ˆæœ¬è·³è·ƒé™åˆ¶
```yaml
upgrade_path_constraints:
  major_version_jumps: forbidden  # ç¦æ­¢è·¨å¤§ç‰ˆæœ¬å‡çº§
  minor_version_jumps: 
    maximum: 2                     # æœ€å¤šè·³è¿‡2ä¸ªå°ç‰ˆæœ¬
    recommended: 1                 # æ¨èé€ç‰ˆæœ¬å‡çº§
  
  patch_version_jumps: unlimited   # è¡¥ä¸ç‰ˆæœ¬å¯ç›´æ¥å‡çº§

safe_upgrade_paths:
  v1.26.x â†’ v1.27.x â†’ v1.28.x: âœ… æ¨èè·¯å¾„
  v1.26.x â†’ v1.28.x: âš ï¸  éœ€è°¨æ…è¯„ä¼°
  v1.25.x â†’ v1.28.x: âŒ ä¸æ¨èï¼Œé£é™©è¾ƒé«˜
```

---

## äºŒã€å‡çº§å‰å‡†å¤‡

### 2.1 ç¯å¢ƒè¯„ä¼°ä¸å¤‡ä»½

#### å‡çº§å‰å¥åº·æ£€æŸ¥æ¸…å•
```yaml
pre_upgrade_checklist:
  cluster_health:
    - control_plane_status: healthy
    - node_status: all_ready
    - pod_status: no_crashlooping
    - storage_status: healthy_pvcs
    
  resource_capacity:
    - cpu_reservation: "<80%"
    - memory_reservation: "<80%"
    - disk_space: ">20% free"
    - etcd_space: ">10% free"
    
  backup_requirements:
    - etcd_snapshot: complete
    - cluster_state: exported
    - application_data: backed_up
    - configuration_files: archived
```

#### è‡ªåŠ¨åŒ–å¥åº·æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# pre-upgrade-health-check.sh

CLUSTER_NAME="production-cluster"
BACKUP_DIR="/backup/${CLUSTER_NAME}/$(date +%Y%m%d_%H%M%S)"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

echo "=== Kubernetes å‡çº§å‰å¥åº·æ£€æŸ¥ ==="

# 1. é›†ç¾¤çŠ¶æ€æ£€æŸ¥
echo "1. æ£€æŸ¥é›†ç¾¤æ§åˆ¶å¹³é¢çŠ¶æ€"
control_plane_healthy=true
for component in kube-apiserver kube-controller-manager kube-scheduler; do
    ready_pods=$(kubectl get pods -n kube-system -l component=$component \
        -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | \
        tr ' ' '\n' | grep -c True)
    total_pods=$(kubectl get pods -n kube-system -l component=$component --no-headers | wc -l)
    
    if [ "$ready_pods" -ne "$total_pods" ]; then
        echo "âŒ $component ä¸å¥åº·: $ready_pods/$total_pods å‡†å¤‡å°±ç»ª"
        control_plane_healthy=false
    else
        echo "âœ… $component å¥åº·: $ready_pods/$total_pods å‡†å¤‡å°±ç»ª"
    fi
done

# 2. èŠ‚ç‚¹çŠ¶æ€æ£€æŸ¥
echo "2. æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€"
not_ready_nodes=$(kubectl get nodes --no-headers | awk '$2 != "Ready" {print $1}')
if [ -n "$not_ready_nodes" ]; then
    echo "âŒ ä»¥ä¸‹èŠ‚ç‚¹æœªå°±ç»ª:"
    echo "$not_ready_nodes"
    exit 1
else
    echo "âœ… æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€æ­£å¸¸"
fi

# 3. èµ„æºä½¿ç”¨ç‡æ£€æŸ¥
echo "3. æ£€æŸ¥èµ„æºä½¿ç”¨ç‡"
high_usage_resources=$(kubectl top nodes | awk 'NR>1 {if($2+0 > 80 || $4+0 > 80) print $1}')
if [ -n "$high_usage_resources" ]; then
    echo "âš ï¸  ä»¥ä¸‹èŠ‚ç‚¹èµ„æºä½¿ç”¨ç‡è¶…è¿‡80%:"
    echo "$high_usage_resources"
else
    echo "âœ… èµ„æºä½¿ç”¨ç‡æ­£å¸¸"
fi

# 4. æ‰§è¡Œ etcd å¤‡ä»½
echo "4. æ‰§è¡Œ etcd å¤‡ä»½"
ETCD_POD=$(kubectl get pod -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n kube-system $ETCD_POD -- \
    etcdctl snapshot save /tmp/etcd-snapshot.db \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key

kubectl cp kube-system/$ETCD_POD:/tmp/etcd-snapshot.db $BACKUP_DIR/etcd-snapshot.db

# 5. å¤‡ä»½å…³é”®é…ç½®
echo "5. å¤‡ä»½å…³é”®é…ç½®"
kubectl get -A all -o yaml > $BACKUP_DIR/all-resources.yaml
kubectl get -A secrets -o yaml > $BACKUP_DIR/all-secrets.yaml
kubectl get -A configmaps -o yaml > $BACKUP_DIR/all-configmaps.yaml

echo "âœ… å‡çº§å‰æ£€æŸ¥å®Œæˆï¼Œå¤‡ä»½ä¿å­˜åœ¨: $BACKUP_DIR"
```

### 2.2 åº”ç”¨å…¼å®¹æ€§éªŒè¯

#### åº”ç”¨ç‰ˆæœ¬å…¼å®¹æ€§æµ‹è¯•
```yaml
application_compatibility_testing:
  test_environment:
    infrastructure: separate_test_cluster
    configuration: mirror_production
    data: anonymized_subset
    
  compatibility_tests:
    - api_version_compatibility: check_deprecated_apis
    - resource_quota_validation: verify_limits_requests
    - network_policy_testing: validate_connectivity
    - storage_compatibility: test_pv_pvc_behavior
    - security_context_testing: verify_permissions
    
  rollback_preparation:
    - backup_restore_procedures: documented_and_tested
    - data_migration_scripts: validated
    - configuration_rollbacks: automated
```

#### å…¼å®¹æ€§æµ‹è¯•è„šæœ¬
```python
#!/usr/bin/env python3
# compatibility_tester.py

import subprocess
import json
import sys
from typing import List, Dict

class CompatibilityTester:
    def __init__(self, target_version: str):
        self.target_version = target_version
        self.results = {}
    
    def check_deprecated_apis(self) -> Dict:
        """æ£€æŸ¥å·²å¼ƒç”¨çš„ API ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ” æ£€æŸ¥å·²å¼ƒç”¨çš„ API...")
        
        # è·å–å½“å‰ä½¿ç”¨çš„ API ç‰ˆæœ¬
        cmd = ["kubectl", "api-resources", "--verbs=list", "-o=name"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        resources = result.stdout.strip().split('\n')
        
        deprecated_apis = []
        for resource in resources:
            try:
                # æ£€æŸ¥èµ„æºæ˜¯å¦ä½¿ç”¨å·²å¼ƒç”¨ç‰ˆæœ¬
                explain_cmd = ["kubectl", "explain", resource]
                explain_result = subprocess.run(explain_cmd, capture_output=True, text=True)
                
                if any(deprecated in explain_result.stdout.lower() 
                       for deprecated in ['deprecated', 'v1beta']):
                    deprecated_apis.append(resource)
            except Exception as e:
                print(f"è­¦å‘Š: æ£€æŸ¥ {resource} æ—¶å‡ºé”™: {e}")
        
        return {
            "deprecated_apis": deprecated_apis,
            "count": len(deprecated_apis),
            "status": "WARNING" if deprecated_apis else "PASS"
        }
    
    def check_resource_quotas(self) -> Dict:
        """æ£€æŸ¥èµ„æºé…é¢é…ç½®"""
        print("ğŸ“Š æ£€æŸ¥èµ„æºé…é¢...")
        
        cmd = ["kubectl", "get", "resourcequotas", "--all-namespaces", "-o=json"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        quotas = json.loads(result.stdout)
        issues = []
        
        for item in quotas.get("items", []):
            namespace = item["metadata"]["namespace"]
            spec = item["spec"]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆç†çš„é™åˆ¶
            if "hard" not in spec:
                issues.append(f"{namespace}: ç¼ºå°‘èµ„æºé™åˆ¶")
        
        return {
            "quota_issues": issues,
            "count": len(issues),
            "status": "FAIL" if issues else "PASS"
        }
    
    def run_all_tests(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰å…¼å®¹æ€§æµ‹è¯•"""
        self.results = {
            "deprecated_apis": self.check_deprecated_apis(),
            "resource_quotas": self.check_resource_quotas(),
            "overall_status": "PASS"
        }
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if (self.results["deprecated_apis"]["status"] == "WARNING" or 
            self.results["resource_quotas"]["status"] == "FAIL"):
            self.results["overall_status"] = "WARNING"
        
        return self.results

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python3 compatibility_tester.py <target_version>")
        sys.exit(1)
    
    target_version = sys.argv[1]
    tester = CompatibilityTester(target_version)
    results = tester.run_all_tests()
    
    print("\n=== å…¼å®¹æ€§æµ‹è¯•ç»“æœ ===")
    print(json.dumps(results, indent=2, ensure_ascii=False))
    
    if results["overall_status"] == "FAIL":
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## ä¸‰ã€å‡çº§å®æ–½ç­–ç•¥

### 3.1 æ¸è¿›å¼å‡çº§æ–¹æ³•

#### è“ç»¿éƒ¨ç½²å‡çº§
```yaml
blue_green_upgrade:
  blue_environment:
    version: v1.28.0
    traffic_weight: 100%
    status: active
    
  green_environment:
    version: v1.29.0
    traffic_weight: 0%
    status: preparing
    
  upgrade_phases:
    phase_1_preparation:
      - deploy_green_control_plane: v1.29.0
      - validate_green_cluster: health_checks
      - migrate_critical_workloads: testing_only
      
    phase_2_traffic_shift:
      - shift_10_percent_traffic: monitoring
      - shift_50_percent_traffic: validation
      - shift_100_percent_traffic: completion
      
    phase_3_cleanup:
      - decommission_blue: after_stable_period
      - promote_green: to_primary
```

#### æ»šåŠ¨å‡çº§é…ç½®
```yaml
# kubeadm æ»šåŠ¨å‡çº§é…ç½®
apiVersion: kubeadm.k8s.io/v1beta3
kind: UpgradeConfiguration
upgrade:
  # æ§åˆ¶å¹³é¢å‡çº§ç­–ç•¥
  controlPlane:
    strategy: RollingUpdate
    maxUnavailable: 1
    maxSurge: 0
    
  # èŠ‚ç‚¹å‡çº§ç­–ç•¥
  nodes:
    strategy: RollingUpdate
    maxUnavailable: 10%
    drainTimeout: 5m0s
    deleteLocalData: false
    force: false
    
  # ç»„ä»¶å‡çº§é¡ºåº
  componentOrder:
    - kube-apiserver
    - kube-controller-manager
    - kube-scheduler
    - kubelet
    - kube-proxy
```

### 3.2 åˆ†é˜¶æ®µå‡çº§æµç¨‹

#### æ§åˆ¶å¹³é¢å‡çº§
```bash
#!/bin/bash
# control-plane-upgrade.sh

set -euo pipefail

NEW_VERSION="v1.29.0"
CONTROL_PLANE_NODES=("master-1" "master-2" "master-3")

echo "ğŸš€ å¼€å§‹æ§åˆ¶å¹³é¢å‡çº§åˆ° $NEW_VERSION"

# 1. å‡çº§ç¬¬ä¸€ä¸ªæ§åˆ¶å¹³é¢èŠ‚ç‚¹
echo "ğŸ”§ å‡çº§ç¬¬ä¸€ä¸ªæ§åˆ¶å¹³é¢èŠ‚ç‚¹: ${CONTROL_PLANE_NODES[0]}"
ssh ${CONTROL_PLANE_NODES[0]} "
    # å¤‡ä»½å½“å‰é…ç½®
    sudo cp -r /etc/kubernetes /etc/kubernetes.backup.$(date +%Y%m%d_%H%M%S)
    
    # å‡çº§ kubeadm
    sudo apt-mark unhold kubeadm
    sudo apt-get update && sudo apt-get install -y kubeadm=$NEW_VERSION-00
    sudo apt-mark hold kubeadm
    
    # è®¡åˆ’å‡çº§
    sudo kubeadm upgrade plan
    
    # æ‰§è¡Œå‡çº§
    sudo kubeadm upgrade apply $NEW_VERSION --yes
"

# 2. éªŒè¯ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
echo "âœ… éªŒè¯ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å‡çº§çŠ¶æ€"
kubectl get nodes ${CONTROL_PLANE_NODES[0]} -o jsonpath='{.status.nodeInfo.kubeletVersion}'

# 3. å‡çº§å…¶ä½™æ§åˆ¶å¹³é¢èŠ‚ç‚¹
for node in "${CONTROL_PLANE_NODES[@]:1}"; do
    echo "ğŸ”§ å‡çº§æ§åˆ¶å¹³é¢èŠ‚ç‚¹: $node"
    ssh $node "
        sudo apt-mark unhold kubeadm
        sudo apt-get update && sudo apt-get install -y kubeadm=$NEW_VERSION-00
        sudo apt-mark hold kubeadm
        
        sudo kubeadm upgrade node
    "
done

# 4. å‡çº§æ§åˆ¶å¹³é¢ç»„ä»¶
echo "ğŸ”§ å‡çº§æ§åˆ¶å¹³é¢ç»„ä»¶"
for node in "${CONTROL_PLANE_NODES[@]}"; do
    ssh $node "
        sudo apt-mark unhold kubelet kubectl
        sudo apt-get update && sudo apt-get install -y kubelet=$NEW_VERSION-00 kubectl=$NEW_VERSION-00
        sudo apt-mark hold kubelet kubectl
        
        sudo systemctl daemon-reload
        sudo systemctl restart kubelet
    "
done

echo "ğŸ‰ æ§åˆ¶å¹³é¢å‡çº§å®Œæˆ"
```

#### Worker èŠ‚ç‚¹æ»šåŠ¨å‡çº§
```bash
#!/bin/bash
# worker-nodes-upgrade.sh

set -euo pipefail

NEW_VERSION="v1.29.0"
DRAIN_TIMEOUT="300s"
DELETE_LOCAL_DATA="false"

echo "ğŸš€ å¼€å§‹ Worker èŠ‚ç‚¹å‡çº§åˆ° $NEW_VERSION"

# è·å–æ‰€æœ‰ worker èŠ‚ç‚¹
WORKER_NODES=$(kubectl get nodes -l node-role.kubernetes.io/control-plane!=true -o jsonpath='{.items[*].metadata.name}')

# èŠ‚ç‚¹é€ä¸ªå‡çº§
for node in $WORKER_NODES; do
    echo "ğŸ”§ å¤„ç†èŠ‚ç‚¹: $node"
    
    # 1. æ ‡è®°èŠ‚ç‚¹ä¸å¯è°ƒåº¦
    echo "â¸ï¸  æ ‡è®°èŠ‚ç‚¹ $node ä¸ºä¸å¯è°ƒåº¦"
    kubectl cordon $node
    
    # 2. é©±é€èŠ‚ç‚¹ä¸Šçš„ Pod
    echo "ğŸ”„ é©±é€èŠ‚ç‚¹ $node ä¸Šçš„ Pod"
    kubectl drain $node \
        --ignore-daemonsets \
        --delete-emptydir-data=$DELETE_LOCAL_DATA \
        --timeout=$DRAIN_TIMEOUT \
        --force
    
    # 3. SSH åˆ°èŠ‚ç‚¹æ‰§è¡Œå‡çº§
    echo "âš¡ åœ¨èŠ‚ç‚¹ $node ä¸Šæ‰§è¡Œå‡çº§"
    ssh $node "
        # å¤‡ä»½é…ç½®
        sudo cp -r /etc/kubernetes /etc/kubernetes.backup.$(date +%Y%m%d_%H%M%S)
        
        # å‡çº§ç»„ä»¶
        sudo apt-mark unhold kubelet kubectl
        sudo apt-get update && sudo apt-get install -y kubelet=$NEW_VERSION-00 kubectl=$NEW_VERSION-00
        sudo apt-mark hold kubelet kubectl
        
        # é‡å¯æœåŠ¡
        sudo systemctl daemon-reload
        sudo systemctl restart kubelet
    "
    
    # 4. éªŒè¯èŠ‚ç‚¹çŠ¶æ€
    echo "âœ… éªŒè¯èŠ‚ç‚¹ $node çŠ¶æ€"
    sleep 30  # ç­‰å¾…èŠ‚ç‚¹é‡æ–°åŠ å…¥é›†ç¾¤
    kubectl uncordon $node
    
    # 5. éªŒè¯èŠ‚ç‚¹ç‰ˆæœ¬
    node_version=$(kubectl get node $node -o jsonpath='{.status.nodeInfo.kubeletVersion}')
    echo "ğŸ“‹ èŠ‚ç‚¹ $node ç‰ˆæœ¬: $node_version"
    
    echo "âœ… èŠ‚ç‚¹ $node å‡çº§å®Œæˆ"
    echo "---"
done

echo "ğŸ‰ æ‰€æœ‰ Worker èŠ‚ç‚¹å‡çº§å®Œæˆ"
```

### 3.3 ç»„ä»¶å‡çº§é¡ºåº

#### æ ¸å¿ƒç»„ä»¶ä¾èµ–å…³ç³»
```mermaid
graph TD
    A[etcd] --> B[kube-apiserver]
    B --> C[kube-controller-manager]
    B --> D[kube-scheduler]
    C --> E[kubelet]
    D --> E
    E --> F[kube-proxy]
    
    subgraph "å‡çº§é¡ºåº"
        A --> B --> C --> D --> E --> F
    end
    
    subgraph "å›æ»šé¡ºåº"
        F --> E --> D --> C --> B --> A
    end
```

#### ç»„ä»¶å‡çº§è¯¦ç»†æ­¥éª¤
```yaml
component_upgrade_sequence:
  1_etcd:
    prerequisites:
      - backup_existing_etcd: complete
      - version_compatibility: verified
    procedure:
      - upgrade_single_member: first_node
      - verify_cluster_health: after_each_member
      - upgrade_remaining_members: sequentially
      
  2_api_server:
    prerequisites:
      - etcd_upgrade_complete: true
      - certificate_validity: confirmed
    procedure:
      - update_api_server_manifest: one_instance
      - verify_api_connectivity: health_check
      - upgrade_remaining_instances: rolling_update
      
  3_controller_manager:
    prerequisites:
      - api_server_stable: 5_minutes
      - leader_election: verified
    procedure:
      - scale_down_controller_manager: to_zero
      - update_deployment: new_version
      - scale_up_controller_manager: to_original
      
  4_scheduler:
    prerequisites:
      - controller_manager_stable: 5_minutes
    procedure:
      - similar_to_controller_manager: rolling_update
      
  5_kubelet:
    prerequisites:
      - control_plane_stable: 10_minutes
    procedure:
      - node_by_node_upgrade: with_drain
      - verify_node_readiness: before_next
      - monitor_workload_health: continuously
      
  6_kube_proxy:
    prerequisites:
      - kubelet_upgrade_complete: true
    procedure:
      - daemonset_rolling_update: automatic
      - verify_network_connectivity: service_testing
```

---

## å››ã€è¿ç§»ç­–ç•¥ä¸æ–¹æ¡ˆ

### 4.1 è·¨äº‘å¹³å°è¿ç§»

#### å¤šäº‘è¿ç§»æ¶æ„
```yaml
multi_cloud_migration:
  source_environment:
    cloud_provider: aws
    region: us-east-1
    cluster_version: v1.28.0
    
  target_environment:
    cloud_provider: gcp
    region: us-central1
    cluster_version: v1.29.0
    
  migration_phases:
    phase_1_preparation:
      - infrastructure_provisioning: terraform_scripts
      - network_connectivity: vpn_tunnel_establishment
      - security_alignment: rbac_synchronization
      
    phase_2_data_migration:
      - etcd_data_migration: snapshot_transfer
      - persistent_volumes: csi_migration
      - container_images: registry_replication
      
    phase_3_workload_migration:
      - application_deployment: helm_charts
      - service_discovery: dns_synchronization
      - traffic_cutover: load_balancer_switch
```

#### æ•°æ®è¿ç§»è„šæœ¬
```bash
#!/bin/bash
# cross-cloud-migration.sh

SOURCE_CLUSTER="aws-cluster"
TARGET_CLUSTER="gcp-cluster"
MIGRATION_NAMESPACE="migration-system"

echo "ğŸšš å¼€å§‹è·¨äº‘å¹³å°è¿ç§»"

# 1. åˆ›å»ºè¿ç§»å‘½åç©ºé—´
kubectl create namespace $MIGRATION_NAMESPACE --context=$TARGET_CLUSTER

# 2. è¿ç§»å‘½åç©ºé—´å’Œèµ„æº
echo "ğŸ“¦ è¿ç§»å‘½åç©ºé—´å’Œèµ„æº"
NAMESPACES=$(kubectl get namespaces --context=$SOURCE_CLUSTER -o jsonpath='{.items[*].metadata.name}' | \
    grep -v -E "(kube-|default)")

for ns in $NAMESPACES; do
    echo "è¿ç§»å‘½åç©ºé—´: $ns"
    
    # å¯¼å‡ºå‘½åç©ºé—´é…ç½®
    kubectl get namespace $ns --context=$SOURCE_CLUSTER -o yaml | \
        grep -v "uid\|resourceVersion\|creationTimestamp" | \
        kubectl apply --context=$TARGET_CLUSTER -f -
    
    # è¿ç§»è¯¥å‘½åç©ºé—´ä¸­çš„èµ„æº
    kubectl get all,configmaps,secrets,ingresses,pv,pvc \
        -n $ns --context=$SOURCE_CLUSTER -o yaml | \
        sed '/uid:/d; /resourceVersion:/d; /creationTimestamp:/d' | \
        kubectl apply --context=$TARGET_CLUSTER -n $ns -f -
done

# 3. è¿ç§»æŒä¹…å·æ•°æ®
echo "ğŸ’¾ è¿ç§»æŒä¹…å·æ•°æ®"
PVCS=$(kubectl get pvc --all-namespaces --context=$SOURCE_CLUSTER -o jsonpath='{range .items[*]}{.metadata.namespace}/{.metadata.name} ')

for pvc_info in $PVCS; do
    namespace=$(echo $pvc_info | cut -d'/' -f1)
    pvc_name=$(echo $pvc_info | cut -d'/' -f2)
    
    echo "å¤„ç† PVC: $namespace/$pvc_name"
    
    # åˆ›å»ºæ•°æ®å¤åˆ¶ Job
    cat << EOF | kubectl apply --context=$TARGET_CLUSTER -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: migrate-$pvc_name
  namespace: $MIGRATION_NAMESPACE
spec:
  template:
    spec:
      containers:
      - name: data-migrator
        image: alpine:latest
        command:
        - sh
        - -c
        - |
          apk add --no-cache rsync
          # è¿™é‡Œæ·»åŠ å®é™…çš„æ•°æ®åŒæ­¥é€»è¾‘
          echo "æ¨¡æ‹Ÿæ•°æ®è¿ç§»: $namespace/$pvc_name"
      restartPolicy: Never
EOF
done

# 4. éªŒè¯è¿ç§»ç»“æœ
echo "âœ… éªŒè¯è¿ç§»ç»“æœ"
kubectl get all --all-namespaces --context=$TARGET_CLUSTER

echo "ğŸ‰ è·¨äº‘å¹³å°è¿ç§»å®Œæˆ"
```

### 4.2 å­˜å‚¨ç³»ç»Ÿè¿ç§»

#### CSI è¿ç§»ç­–ç•¥
```yaml
csi_migration_strategy:
  in_tree_to_csi:
    supported_drivers:
      - aws_ebs: migration_available_since_v1.17
      - gce_pd: migration_available_since_v1.17
      - azure_disk: migration_available_since_v1.19
      - cinder: migration_available_since_v1.18
      
    migration_phases:
      phase_1_enable_feature_gates:
        - CSIMigration: true
        - CSIMigrationAWS: true
        - AttachDetachReconciler: true
        
      phase_2_install_csi_driver:
        - deploy_csi_controller: ha_deployment
        - deploy_csi_node: daemonset
        - verify_driver_registration: node_testing
        
      phase_3_migrate_volumes:
        - in_tree_pv_migration: automatic_conversion
        - volume_attachment_migration: seamless_transition
        - cleanup_in_tree_components: after_stable_period
```

#### å­˜å‚¨è¿ç§»éªŒè¯è„šæœ¬
```python
#!/usr/bin/env python3
# storage_migration_validator.py

import subprocess
import json
import time
from typing import Dict, List

class StorageMigrationValidator:
    def __init__(self):
        self.source_context = "source-cluster"
        self.target_context = "target-cluster"
    
    def get_persistent_volumes(self, context: str) -> List[Dict]:
        """è·å–æŒ‡å®šé›†ç¾¤çš„ PV ä¿¡æ¯"""
        cmd = ["kubectl", "get", "pv", "-o", "json", "--context", context]
        result = subprocess.run(cmd, capture_output=True, text=True)
        data = json.loads(result.stdout)
        return data.get("items", [])
    
    def validate_pv_consistency(self) -> Dict:
        """éªŒè¯ PV æ•°æ®ä¸€è‡´æ€§"""
        print("ğŸ” éªŒè¯ PV ä¸€è‡´æ€§...")
        
        source_pvs = {pv["metadata"]["name"]: pv for pv in self.get_persistent_volumes(self.source_context)}
        target_pvs = {pv["metadata"]["name"]: pv for pv in self.get_persistent_volumes(self.target_context)}
        
        results = {
            "source_count": len(source_pvs),
            "target_count": len(target_pvs),
            "missing_pvs": [],
            "status_mismatches": [],
            "capacity_mismatches": []
        }
        
        # æ£€æŸ¥ç¼ºå¤±çš„ PV
        for pv_name in source_pvs:
            if pv_name not in target_pvs:
                results["missing_pvs"].append(pv_name)
        
        # æ£€æŸ¥çŠ¶æ€å’Œå®¹é‡
        for pv_name, source_pv in source_pvs.items():
            if pv_name in target_pvs:
                target_pv = target_pvs[pv_name]
                
                # çŠ¶æ€æ£€æŸ¥
                source_phase = source_pv["status"].get("phase", "")
                target_phase = target_pv["status"].get("phase", "")
                if source_phase != target_phase:
                    results["status_mismatches"].append({
                        "pv": pv_name,
                        "source": source_phase,
                        "target": target_phase
                    })
                
                # å®¹é‡æ£€æŸ¥
                source_capacity = source_pv["spec"]["capacity"]["storage"]
                target_capacity = target_pv["spec"]["capacity"]["storage"]
                if source_capacity != target_capacity:
                    results["capacity_mismatches"].append({
                        "pv": pv_name,
                        "source": source_capacity,
                        "target": target_capacity
                    })
        
        return results
    
    def validate_pvc_binding(self) -> Dict:
        """éªŒè¯ PVC ç»‘å®šçŠ¶æ€"""
        print("ğŸ” éªŒè¯ PVC ç»‘å®šçŠ¶æ€...")
        
        cmd = ["kubectl", "get", "pvc", "--all-namespaces", "-o", "json", "--context", self.target_context]
        result = subprocess.run(cmd, capture_output=True, text=True)
        data = json.loads(result.stdout)
        
        unbound_pvcs = []
        for pvc in data.get("items", []):
            if pvc["status"].get("phase") != "Bound":
                unbound_pvcs.append({
                    "namespace": pvc["metadata"]["namespace"],
                    "name": pvc["metadata"]["name"],
                    "phase": pvc["status"].get("phase", "Unknown")
                })
        
        return {
            "total_pvcs": len(data.get("items", [])),
            "unbound_pvcs": unbound_pvcs,
            "unbound_count": len(unbound_pvcs)
        }
    
    def run_validation(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„å­˜å‚¨è¿ç§»éªŒè¯"""
        print("ğŸš€ å¼€å§‹å­˜å‚¨è¿ç§»éªŒè¯...")
        
        results = {
            "pv_consistency": self.validate_pv_consistency(),
            "pvc_binding": self.validate_pvc_binding(),
            "overall_status": "PASS"
        }
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if (results["pv_consistency"]["missing_pvs"] or 
            results["pv_consistency"]["status_mismatches"] or
            results["pvc_binding"]["unbound_count"] > 0):
            results["overall_status"] = "FAIL"
        
        return results

def main():
    validator = StorageMigrationValidator()
    results = validator.run_validation()
    
    print("\n=== å­˜å‚¨è¿ç§»éªŒè¯ç»“æœ ===")
    print(json.dumps(results, indent=2, ensure_ascii=False))
    
    if results["overall_status"] == "FAIL":
        exit(1)

if __name__ == "__main__":
    main()
```

---

## äº”ã€å›æ»šä¸æ¢å¤ç­–ç•¥

### 5.1 å¿«é€Ÿå›æ»šæœºåˆ¶

#### å›æ»šè§¦å‘æ¡ä»¶
```yaml
rollback_triggers:
  critical_failures:
    - control_plane_unavailable: ">5åˆ†é’Ÿ"
    - node_failure_rate: ">10%"
    - api_server_errors: ">5% of requests"
    - etcd_cluster_unhealthy: any_member_down
    
  performance_degradation:
    - api_latency_increase: ">100%"
    - pod_startup_time: ">200% increase"
    - resource_consumption: ">50% increase"
    
  application_impact:
    - application_unavailable: ">5% of services"
    - user_facing_errors: significant_increase
    - sla_violations: detected
```

#### è‡ªåŠ¨åŒ–å›æ»šè„šæœ¬
```bash
#!/bin/bash
# automated-rollback.sh

set -euo pipefail

ORIGINAL_VERSION="v1.28.0"
ROLLBACK_REASON="$1"

echo "ğŸš¨ æ‰§è¡Œè‡ªåŠ¨åŒ–å›æ»š - åŸå› : $ROLLBACK_REASON"
echo "ğŸ•’ å›æ»šå¼€å§‹æ—¶é—´: $(date)"

# 1. åœæ­¢æ–°çš„å‡çº§æ“ä½œ
echo "â¹ï¸  åœæ­¢æ­£åœ¨è¿›è¡Œçš„å‡çº§æ“ä½œ"
kubectl annotate nodes --all kubeadm.alpha.kubernetes.io/cri-socket-

# 2. å›æ»šæ§åˆ¶å¹³é¢
echo "ğŸ”§ å›æ»šæ§åˆ¶å¹³é¢åˆ° $ORIGINAL_VERSION"

CONTROL_PLANE_NODES=("master-1" "master-2" "master-3")
for node in "${CONTROL_PLANE_NODES[@]}"; do
    echo "ğŸ”„ å›æ»šæ§åˆ¶å¹³é¢èŠ‚ç‚¹: $node"
    ssh $node "
        # æ¢å¤å¤‡ä»½é…ç½®
        sudo rm -rf /etc/kubernetes
        sudo cp -r /etc/kubernetes.backup.* /etc/kubernetes
        
        # é™çº§ç»„ä»¶ç‰ˆæœ¬
        sudo apt-mark unhold kubeadm kubelet kubectl
        sudo apt-get install -y \
            kubeadm=$ORIGINAL_VERSION-00 \
            kubelet=$ORIGINAL_VERSION-00 \
            kubectl=$ORIGINAL_VERSION-00
        sudo apt-mark hold kubeadm kubelet kubectl
        
        # é‡å¯æœåŠ¡
        sudo systemctl daemon-reload
        sudo systemctl restart kubelet
    "
done

# 3. éªŒè¯æ§åˆ¶å¹³é¢çŠ¶æ€
echo "âœ… éªŒè¯æ§åˆ¶å¹³é¢æ¢å¤çŠ¶æ€"
sleep 60  # ç­‰å¾…æœåŠ¡ç¨³å®š
kubectl get nodes -l node-role.kubernetes.io/control-plane

# 4. å›æ»š Worker èŠ‚ç‚¹
echo "ğŸ”§ å›æ»š Worker èŠ‚ç‚¹"

WORKER_NODES=$(kubectl get nodes -l node-role.kubernetes.io/control-plane!=true -o jsonpath='{.items[*].metadata.name}')
for node in $WORKER_NODES; do
    echo "ğŸ”„ å›æ»š Worker èŠ‚ç‚¹: $node"
    
    # æ ‡è®°èŠ‚ç‚¹ä¸å¯è°ƒåº¦
    kubectl cordon $node
    
    # é©±é€ Pod
    kubectl drain $node --ignore-daemonsets --delete-emptydir-data --force
    
    # æ‰§è¡Œå›æ»š
    ssh $node "
        sudo rm -rf /etc/kubernetes
        sudo cp -r /etc/kubernetes.backup.* /etc/kubernetes
        
        sudo apt-mark unhold kubelet kubectl
        sudo apt-get install -y kubelet=$ORIGINAL_VERSION-00 kubectl=$ORIGINAL_VERSION-00
        sudo apt-mark hold kubelet kubectl
        
        sudo systemctl daemon-reload
        sudo systemctl restart kubelet
    "
    
    # æ¢å¤èŠ‚ç‚¹è°ƒåº¦
    sleep 30
    kubectl uncordon $node
done

# 5. æ¢å¤ etcd æ•°æ®ï¼ˆå¦‚éœ€è¦ï¼‰
echo "ğŸ’¾ æ¢å¤ etcd æ•°æ®"
ETCD_POD=$(kubectl get pod -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')
kubectl cp /backup/etcd-snapshot.db kube-system/$ETCD_POD:/tmp/etcd-snapshot.db

kubectl exec -n kube-system $ETCD_POD -- \
    etcdctl snapshot restore /tmp/etcd-snapshot.db \
    --data-dir=/var/lib/etcd-restored \
    --initial-cluster-token=etcd-cluster-1

# 6. éªŒè¯é›†ç¾¤çŠ¶æ€
echo "âœ… éªŒè¯é›†ç¾¤å®Œå…¨æ¢å¤"
kubectl get nodes
kubectl get pods --all-namespaces

echo "ğŸ‰ å›æ»šå®Œæˆ - ç»“æŸæ—¶é—´: $(date)"
echo "ğŸ“ å›æ»šåŸå› : $ROLLBACK_REASON"
```

### 5.2 ç¾éš¾æ¢å¤é¢„æ¡ˆ

#### å®Œå…¨é‡å»ºæµç¨‹
```yaml
disaster_recovery_plan:
  scenario_detection:
    complete_cluster_failure: 
      detection_time: "<15åˆ†é’Ÿ"
      notification: immediate_alerting
      
  recovery_phases:
    phase_1_assessment:
      - damage_assessment: inventory_lost_resources
      - root_cause_analysis: incident_investigation
      - recovery_planning: strategy_development
      
    phase_2_infrastructure_rebuild:
      - iaas_provisioning: terraform_deployment
      - network_setup: connectivity_restoration
      - security_baseline: hardening_implementation
      
    phase_3_cluster_restoration:
      - kubernetes_deployment: kubeadm_init
      - control_plane_setup: ha_configuration
      - worker_node_join: scaling_restoration
      
    phase_4_data_recovery:
      - etcd_restoration: snapshot_recovery
      - persistent_volumes: backup_restoration
      - application_data: database_restores
      
    phase_5_service_validation:
      - functionality_testing: end_to_end_validation
      - performance_benchmarking: baseline_comparison
      - user_acceptance_testing: business_validation
```

#### ç¾éš¾æ¢å¤æ‰§è¡Œè„šæœ¬
```bash
#!/bin/bash
# disaster-recovery.sh

set -euo pipefail

BACKUP_LOCATION="/backup/disaster-recovery"
CLUSTER_CONFIG="$BACKUP_LOCATION/cluster-config.tar.gz"
ETCD_SNAPSHOT="$BACKUP_LOCATION/etcd-snapshot.db"
APPLICATION_BACKUPS="$BACKUP_LOCATION/application-backups"

echo "ğŸŒªï¸  å¯åŠ¨ç¾éš¾æ¢å¤æµç¨‹"
echo "ğŸ• å¼€å§‹æ—¶é—´: $(date)"

# 1. åŸºç¡€è®¾æ–½é‡å»º
echo "ğŸ—ï¸  é‡å»ºåŸºç¡€è®¾æ–½"
cd $BACKUP_LOCATION/terraform
terraform init
terraform apply -auto-approve

# 2. Kubernetes é›†ç¾¤åˆå§‹åŒ–
echo "ğŸ³ åˆå§‹åŒ– Kubernetes é›†ç¾¤"
MASTER_IP=$(terraform output -raw master_public_ip)

ssh ubuntu@$MASTER_IP "
    sudo kubeadm init \
        --pod-network-cidr=10.244.0.0/16 \
        --upload-certs \
        --control-plane-endpoint=$(terraform output -raw lb_dns_name):6443
"

# 3. æ¢å¤ etcd æ•°æ®
echo "ğŸ’¾ æ¢å¤ etcd æ•°æ®"
scp $ETCD_SNAPSHOT ubuntu@$MASTER_IP:/tmp/
ssh ubuntu@$MASTER_IP "
    sudo mkdir -p /var/lib/etcd-restored
    sudo etcdctl snapshot restore /tmp/etcd-snapshot.db \
        --data-dir=/var/lib/etcd-restored \
        --initial-cluster=default=https://127.0.0.1:2380 \
        --initial-advertise-peer-urls=https://127.0.0.1:2380
"

# 4. æ¢å¤é›†ç¾¤é…ç½®
echo "âš™ï¸  æ¢å¤é›†ç¾¤é…ç½®"
scp $CLUSTER_CONFIG ubuntu@$MASTER_IP:/tmp/
ssh ubuntu@$MASTER_IP "
    sudo tar -xzf /tmp/cluster-config.tar.gz -C /
    sudo systemctl restart kubelet
"

# 5. åŠ å…¥ Worker èŠ‚ç‚¹
echo "ğŸ‘¨â€ğŸ’» åŠ å…¥ Worker èŠ‚ç‚¹"
JOIN_COMMAND=$(ssh ubuntu@$MASTER_IP "sudo kubeadm token create --print-join-command")

WORKER_IPS=$(terraform output -raw worker_private_ips | tr ',' ' ')
for worker_ip in $WORKER_IPS; do
    ssh ubuntu@$worker_ip "$JOIN_COMMAND"
done

# 6. æ¢å¤åº”ç”¨ç¨‹åº
echo "åº”ç”¨æŸ¥çœ‹åº”ç”¨ç¨‹åº"
kubectl apply -f $APPLICATION_BACKUPS/namespaces.yaml
kubectl apply -f $APPLICATION_BACKUPS/resources.yaml

echo "âœ… ç¾éš¾æ¢å¤å®Œæˆ"
echo "ğŸ• ç»“æŸæ—¶é—´: $(date)"
echo "ğŸ“Š æ¢å¤ç»Ÿè®¡:"
echo "  - æ¢å¤çš„å‘½åç©ºé—´: $(kubectl get namespaces --no-headers | wc -l)"
echo "  - æ¢å¤çš„ Pod: $(kubectl get pods --all-namespaces --no-headers | wc -l)"
```

---

## å…­ã€å‡çº§åéªŒè¯ä¸ç›‘æ§

### 6.1 åŠŸèƒ½éªŒè¯æ¸…å•

#### å‡çº§åéªŒè¯è„šæœ¬
```bash
#!/bin/bash
# post-upgrade-validation.sh

NEW_VERSION="v1.29.0"
VALIDATION_TIMEOUT=300

echo "âœ… å¼€å§‹å‡çº§åéªŒè¯ ($NEW_VERSION)"

# 1. é›†ç¾¤ç»„ä»¶éªŒè¯
echo "ğŸ”§ éªŒè¯é›†ç¾¤ç»„ä»¶ç‰ˆæœ¬"
components=("kube-apiserver" "kube-controller-manager" "kube-scheduler" "kubelet" "kube-proxy")

for component in "${components[@]}"; do
    version_match=$(kubectl get pods -n kube-system -l component=$component \
        -o jsonpath='{.items[*].spec.containers[*].image}' | \
        grep -c "$NEW_VERSION" || echo "0")
    
    total_instances=$(kubectl get pods -n kube-system -l component=$component --no-headers | wc -l)
    
    if [ "$version_match" -eq "$total_instances" ] && [ "$total_instances" -gt 0 ]; then
        echo "âœ… $component: $version_match/$total_instances å®ä¾‹å·²å‡çº§"
    else
        echo "âŒ $component: $version_match/$total_instances å®ä¾‹å·²å‡çº§"
        exit 1
    fi
done

# 2. æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
echo "ğŸ§ª æ‰§è¡Œæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•"

# API æœåŠ¡å™¨è¿é€šæ€§æµ‹è¯•
echo "  ğŸ”Œ æµ‹è¯• API æœåŠ¡å™¨è¿é€šæ€§"
for i in {1..10}; do
    if kubectl get nodes >/dev/null 2>&1; then
        echo "  âœ… API æœåŠ¡å™¨å“åº”æ­£å¸¸"
        break
    else
        echo "  â³ ç­‰å¾… API æœåŠ¡å™¨å°±ç»ª... ($i/10)"
        sleep 10
    fi
done

# DNS è§£ææµ‹è¯•
echo "  ğŸŒ æµ‹è¯• DNS è§£æ"
kubectl run dns-test --image=busybox:1.28 --rm -it --restart=Never \
    -- sh -c "nslookup kubernetes.default >/dev/null && echo 'DNS è§£ææ­£å¸¸'"

# ç½‘ç»œè¿é€šæ€§æµ‹è¯•
echo "  ğŸ“¡ æµ‹è¯•ç½‘ç»œè¿é€šæ€§"
kubectl run network-test --image=nginx:alpine --port=80 --expose
sleep 10

kubectl run curl-test --image=curlimages/curl --rm -it --restart=Never \
    -- curl -s --connect-timeout 5 http://network-test:80 >/dev/null && \
    echo "  âœ… æœåŠ¡é—´ç½‘ç»œè¿é€šæ€§æ­£å¸¸"

# 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
echo "ğŸ“Š æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"
echo "  â±ï¸  API æœåŠ¡å™¨å»¶è¿Ÿæµ‹è¯•"
API_LATENCY=$(kubectl get --raw /metrics | \
    grep apiserver_request_duration_seconds_sum | \
    head -1 | awk '{print $2}' | cut -d'=' -f2)

echo "  ğŸ“ˆ API å»¶è¿Ÿ: ${API_LATENCY}s"

# 4. åº”ç”¨å…¼å®¹æ€§éªŒè¯
echo "ğŸ“± éªŒè¯åº”ç”¨å…¼å®¹æ€§"
kubectl get pods --all-namespaces -o jsonpath='{.items[*].status.phase}' | \
    tr ' ' '\n' | sort | uniq -c

echo "ğŸ‰ å‡çº§éªŒè¯å®Œæˆ"
```

### 6.2 æŒç»­ç›‘æ§é…ç½®

#### å‡çº§åç›‘æ§å‘Šè­¦
```yaml
post_upgrade_monitoring:
  enhanced_alerting:
    - name: version_consistency
      expr: count by (component) (kube_pod_info{version!="v1.29.0"}) > 0
      severity: critical
      description: "æ£€æµ‹åˆ°æœªå‡çº§çš„ç»„ä»¶å®ä¾‹"
      
    - name: upgrade_performance_regression
      expr: rate(apiserver_request_duration_seconds_sum[5m]) > 2 * 
            rate(apiserver_request_duration_seconds_sum[5m] offset 1d)
      severity: warning
      description: "API æœåŠ¡å™¨æ€§èƒ½æ˜¾è‘—ä¸‹é™"
      
    - name: node_stability
      expr: count(kube_node_status_condition{condition="Ready",status!="true"}) > 0
      severity: critical
      description: "èŠ‚ç‚¹çŠ¶æ€å¼‚å¸¸"
      
  health_dashboard:
    cluster_version_status:
      panels:
        - component_versions: gauge_charts
        - upgrade_progress: progress_bars
        - health_indicators: status_lights
        
    performance_metrics:
      panels:
        - api_server_latency: time_series
        - node_resource_usage: stacked_areas
        - pod_startup_times: histograms
```

---

## ä¸ƒã€ç‰ˆæœ¬ç®¡ç†æœ€ä½³å®è·µ

### 7.1 è‡ªåŠ¨åŒ–å‡çº§æµæ°´çº¿

#### CI/CD é›†æˆå‡çº§æµç¨‹
```yaml
# GitHub Actions å‡çº§æµæ°´çº¿
name: Kubernetes Upgrade Pipeline

on:
  schedule:
    - cron: '0 2 * * 0'  # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
  workflow_dispatch:
    inputs:
      target_version:
        description: 'ç›®æ ‡ Kubernetes ç‰ˆæœ¬'
        required: true

jobs:
  pre_upgrade_check:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      
    - name: Run compatibility checks
      run: |
        python3 scripts/compatibility_tester.py ${{ github.event.inputs.target_version }}
        
    - name: Health assessment
      run: |
        bash scripts/pre-upgrade-health-check.sh

  approve_upgrade:
    needs: pre_upgrade_check
    runs-on: ubuntu-latest
    steps:
    - name: Manual approval
      uses: trstringer/manual-approval@v1
      with:
        secret: ${{ secrets.UPGRADE_APPROVAL_TOKEN }}
        approvers: kubernetes-admins

  execute_upgrade:
    needs: approve_upgrade
    runs-on: ubuntu-latest
    steps:
    - name: Execute control plane upgrade
      run: |
        bash scripts/control-plane-upgrade.sh
      
    - name: Execute worker nodes upgrade
      run: |
        bash scripts/worker-nodes-upgrade.sh

  post_upgrade_validation:
    needs: execute_upgrade
    runs-on: ubuntu-latest
    steps:
    - name: Run validation tests
      run: |
        bash scripts/post-upgrade-validation.sh
        
    - name: Performance benchmarking
      run: |
        bash scripts/performance-benchmark.sh
        
    - name: Send notification
      uses: slackapi/slack-github-action@v1.23.0
      with:
        payload: |
          {
            "text": "Kubernetes å‡çº§å®Œæˆ",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "âœ… Kubernetes é›†ç¾¤å·²æˆåŠŸå‡çº§åˆ°ç‰ˆæœ¬ ${{ github.event.inputs.target_version }}"
                }
              }
            ]
          }
```

### 7.2 ç‰ˆæœ¬æ²»ç†ç­–ç•¥

#### ç‰ˆæœ¬ç®¡ç†æ”¿ç­–
```yaml
version_governance:
  version_standardization:
    standard_versions:
      development: "æœ€æ–°ç¨³å®šç‰ˆæœ¬ - 1"
      staging: "æœ€æ–°ç¨³å®šç‰ˆæœ¬"
      production: "æœ€æ–°ç¨³å®šç‰ˆæœ¬ - 1"
      
    update_frequency:
      patch_updates: "å®æ—¶åº”ç”¨"
      minor_updates: "æ¯å­£åº¦è¯„ä¼°"
      major_updates: "å¹´åº¦è§„åˆ’"
      
  risk_management:
    canary_deployments: "5% æµé‡å…ˆè¡Œ"
    rollback_windows: "å‡çº§å 24 å°æ—¶å†…å¯å›æ»š"
    business_hour_restrictions: "é¿å…åœ¨ä¸šåŠ¡é«˜å³°æœŸå‡çº§"
    
  compliance_requirements:
    security_patches: "å¿…é¡»åœ¨ 30 å¤©å†…åº”ç”¨"
    audit_trail: "å®Œæ•´è®°å½•æ‰€æœ‰å˜æ›´"
    stakeholder_notification: "é‡å¤§å‡çº§æå‰ 7 å¤©é€šçŸ¥"
```

---
# Kubernetes è°ƒåº¦å™¨æ·±åº¦ä¼˜åŒ–ä¸è‡ªå®šä¹‰è°ƒåº¦ (Scheduler Deep Optimization and Custom Scheduling)

> **ä½œè€…**: Kubernetesè°ƒåº¦ä¸“å®¶ | **ç‰ˆæœ¬**: v1.5 | **æ›´æ–°æ—¶é—´**: 2026-02-07
> **é€‚ç”¨åœºæ™¯**: é«˜æ€§èƒ½è°ƒåº¦ä¸èµ„æºä¼˜åŒ– | **å¤æ‚åº¦**: â­â­â­â­â­

## ğŸ¯ æ‘˜è¦

æœ¬æ–‡æ¡£æ·±å…¥æ¢è®¨äº†Kubernetesè°ƒåº¦å™¨çš„æ¶æ„åŸç†ã€è°ƒåº¦ç®—æ³•å’Œæ‰©å±•æœºåˆ¶ï¼ŒåŸºäºå¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒçš„è°ƒåº¦ä¼˜åŒ–å®è·µç»éªŒï¼Œæä¾›ä»åŸºç¡€è°ƒåº¦ç­–ç•¥åˆ°è‡ªå®šä¹‰è°ƒåº¦å™¨å¼€å‘çš„å®Œæ•´æŠ€æœ¯æŒ‡å—ï¼Œå¸®åŠ©ä¼ä¸šå®ç°é«˜æ•ˆçš„èµ„æºè°ƒåº¦å’Œåº”ç”¨éƒ¨ç½²ã€‚

## 1. è°ƒåº¦å™¨æ¶æ„åŸç†

### 1.1 è°ƒåº¦å™¨æ ¸å¿ƒç»„ä»¶

```yaml
è°ƒåº¦å™¨æ¶æ„ç»„ä»¶:
  æ ¸å¿ƒè°ƒåº¦æµç¨‹:
    - Informer: ç›‘å¬Podå’ŒNodeäº‹ä»¶
    - Queue: å¾…è°ƒåº¦Podé˜Ÿåˆ—ç®¡ç†
    - Scheduler Cache: é›†ç¾¤çŠ¶æ€ç¼“å­˜
    - Framework: è°ƒåº¦æ¡†æ¶æ‰©å±•
    - Algorithm: é¢„é€‰å’Œä¼˜é€‰ç®—æ³•
  
  è°ƒåº¦ç®—æ³•:
    - PreFilter: é¢„è¿‡æ»¤é˜¶æ®µ
    - Filter: è¿‡æ»¤é˜¶æ®µ
    - Score: è¯„åˆ†é˜¶æ®µ
    - Reserve: é¢„ç•™é˜¶æ®µ
    - Permit: è®¸å¯é˜¶æ®µ
    - PreBind: é¢„ç»‘å®šé˜¶æ®µ
    - Bind: ç»‘å®šé˜¶æ®µ
```

### 1.2 è°ƒåº¦æµç¨‹è¯¦è§£

```mermaid
graph TD
    A[Pod Created] --> B[Queue Manager]
    B --> C[Informer Sync]
    C --> D[PreFilter Phase]
    D --> E[Filter Phase]
    E --> F[Score Phase]
    F --> G[Reserve Phase]
    G --> H[Permit Phase]
    H --> I[PreBind Phase]
    I --> J[Bind Phase]
    J --> K[Pod Bound]
    
    D --> L[Node Affinity Check]
    D --> M[Resource Check]
    D --> N[Taint/Toleration Check]
    E --> O[Resource Scoring]
    E --> P[Affinity Scoring]
    E --> Q[Taint Scoring]
    G --> R[Reservation Confirmation]
    H --> S[Wait for Permit]
    H --> T[Reject if Timeout]
```

## 2. è°ƒåº¦ç­–ç•¥ä¼˜åŒ–

### 2.1 é«˜æ€§èƒ½è°ƒåº¦å™¨é…ç½®

```yaml
# é«˜æ€§èƒ½è°ƒåº¦å™¨é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-config
  namespace: kube-system
data:
  scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta3
    kind: KubeSchedulerConfiguration
    leaderElection:
      leaderElect: true
      resourceName: kube-scheduler
      resourceNamespace: kube-system
    profiles:
    - schedulerName: default-scheduler
      plugins:
        queueSort:
          enabled:
          - name: PrioritySort
          disabled:
          - name: ""
        preFilter:
          enabled:
          - name: NodeResourcesFit
          - name: NodePorts
          - name: PodTopologySpread
          - name: InterPodAffinity
          - name: VolumeBinding
        filter:
          enabled:
          - name: NodeUnschedulable
          - name: NodeResourcesFit
          - name: NodePorts
          - name: NodeAffinity
          - name: VolumeRestrictions
          - name: TaintToleration
          - name: CheckVolumeBinding
          - name: InterPodAffinity
          - name: NodeVolumeLimits
          - name: EvenPodsSpread
          - name: PodTopologySpread
        score:
          enabled:
          - name: NodeResourcesBalancedAllocation
            weight: 1
          - name: ImageLocality
            weight: 1
          - name: InterPodAffinity
            weight: 2
          - name: NodeResourcesLeastAllocated
            weight: 1
          - name: NodeAffinity
            weight: 2
          - name: PodTopologySpread
            weight: 2
          - name: TaintToleration
            weight: 1
        reserve:
          enabled:
          - name: VolumeBinding
        permit:
          enabled:
          - name: DefaultBinder
        bind:
          enabled:
          - name: DefaultBinder
      pluginConfig:
      - name: NodeResourcesFit
        args:
          apiVersion: kubescheduler.config.k8s.io/v1beta3
          kind: NodeResourcesFitArgs
          ignoredResources:
          - "example.com/dummy-resource"
          scoringStrategy:
            type: LeastAllocated
            resources:
            - name: cpu
              weight: 1
            - name: memory
              weight: 1
      - name: PodTopologySpread
        args:
          apiVersion: kubescheduler.config.k8s.io/v1beta3
          kind: PodTopologySpreadArgs
          defaultConstraints:
          - maxSkew: 1
            topologyKey: topology.kubernetes.io/zone
            whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
      - name: NodeResourcesBalancedAllocation
        args:
          apiVersion: kubescheduler.config.k8s.io/v1beta3
          kind: NodeResourcesBalancedAllocationArgs
          resources:
          - name: cpu
            weight: 1
          - name: memory
            weight: 1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  replicas: 2
  selector:
    matchLabels:
      component: kube-scheduler
  template:
    metadata:
      labels:
        component: kube-scheduler
    spec:
      hostNetwork: true
      priorityClassName: system-node-critical
      containers:
      - name: kube-scheduler
        image: k8s.gcr.io/kube-scheduler:v1.28.5
        command:
        - kube-scheduler
        - --config=/etc/kubernetes/scheduler-config.yaml
        - --leader-elect=true
        - --bind-address=0.0.0.0
        - --secure-port=10259
        - --profiling=false
        - --v=2
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10259
            scheme: HTTPS
          initialDelaySeconds: 15
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10259
            scheme: HTTPS
          initialDelaySeconds: 5
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        volumeMounts:
        - name: scheduler-config
          mountPath: /etc/kubernetes
          readOnly: true
      volumes:
      - name: scheduler-config
        configMap:
          name: scheduler-config
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
```

### 2.2 è°ƒåº¦ç®—æ³•ä¼˜åŒ–

```go
// è‡ªå®šä¹‰è°ƒåº¦å™¨æ’ä»¶ç¤ºä¾‹
package main

import (
    "context"
    "fmt"
    "math"

    v1 "k8s.io/api/core/v1"
    "k8s.io/apimachinery/pkg/runtime"
    "k8s.io/kubernetes/pkg/scheduler/framework"
)

const (
    Name = "CustomResourceOptimizer"
)

type CustomResourceOptimizer struct {
    handle framework.Handle
}

var _ framework.FilterPlugin = &CustomResourceOptimizer{}
var _ framework.ScorePlugin = &CustomResourceOptimizer{}

func (c *CustomResourceOptimizer) Name() string {
    return Name
}

// Filteræ–¹æ³•å®ç°èµ„æºè¿‡æ»¤é€»è¾‘
func (c *CustomResourceOptimizer) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
    // æ£€æŸ¥èŠ‚ç‚¹èµ„æºæ˜¯å¦æ»¡è¶³Podè¯·æ±‚
    node := nodeInfo.Node()
    if node == nil {
        return framework.NewStatus(framework.Error, "node not found")
    }

    // è‡ªå®šä¹‰èµ„æºæ£€æŸ¥é€»è¾‘
    if !c.checkCustomResources(pod, nodeInfo) {
        return framework.NewStatus(framework.Unschedulable, "insufficient custom resources")
    }

    return framework.NewStatus(framework.Success, "")
}

// Scoreæ–¹æ³•å®ç°è¯„åˆ†é€»è¾‘
func (c *CustomResourceOptimizer) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
    nodeInfo, err := c.handle.SnapshotSharedLister().NodeInfos().Get(nodeName)
    if err != nil {
        return 0, framework.AsStatus(err)
    }

    score := c.calculateScore(pod, nodeInfo)
    return score, framework.NewStatus(framework.Success, "")
}

// ScoreExtensionsè¿”å›è¯„åˆ†æ‰©å±•
func (c *CustomResourceOptimizer) ScoreExtensions() framework.ScoreExtensions {
    return c
}

// NormalizeScoreæ ‡å‡†åŒ–è¯„åˆ†
func (c *CustomResourceOptimizer) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
    var highestScore int64 = 0
    var lowestScore int64 = math.MaxInt64

    for _, nodeScore := range scores {
        if nodeScore.Score > highestScore {
            highestScore = nodeScore.Score
        }
        if nodeScore.Score < lowestScore {
            lowestScore = nodeScore.Score
        }
    }

    if highestScore == lowestScore {
        return framework.NewStatus(framework.Success, "")
    }

    for i := range scores {
        if highestScore == lowestScore {
            scores[i].Score = 0
        } else {
            scores[i].Score = ((scores[i].Score - lowestScore) * framework.MaxNodeScore) / (highestScore - lowestScore)
        }
    }

    return framework.NewStatus(framework.Success, "")
}

// æ£€æŸ¥è‡ªå®šä¹‰èµ„æº
func (c *CustomResourceOptimizer) checkCustomResources(pod *v1.Pod, nodeInfo *framework.NodeInfo) bool {
    // å®ç°è‡ªå®šä¹‰èµ„æºæ£€æŸ¥é€»è¾‘
    // ä¾‹å¦‚GPUã€FPGAç­‰ç‰¹æ®Šèµ„æº
    for _, container := range pod.Spec.Containers {
        if container.Resources.Requests != nil {
            // æ£€æŸ¥ç‰¹å®šèµ„æºéœ€æ±‚
            if gpuReq, ok := container.Resources.Requests["nvidia.com/gpu"]; ok {
                if nodeInfo.Allocatable.GPU <= 0 || gpuReq.Value() > nodeInfo.Allocatable.GPU {
                    return false
                }
            }
        }
    }
    return true
}

// è®¡ç®—è¯„åˆ†
func (c *CustomResourceOptimizer) calculateScore(pod *v1.Pod, nodeInfo *framework.NodeInfo) int64 {
    score := int64(0)

    // èµ„æºåˆ†é…è¯„åˆ†
    cpuFraction := c.resourceFraction(nodeInfo.Requested.Pods, nodeInfo.Allocatable.Pods)
    memoryFraction := c.resourceFraction(nodeInfo.Requested.Memory, nodeInfo.Allocatable.Memory)
    
    // èµ„æºå‡è¡¡è¯„åˆ†
    resourceBalance := (1 - (cpuFraction + memoryFraction) / 2) * 100
    score += int64(resourceBalance)

    // äº²å’Œæ€§è¯„åˆ†
    if c.hasNodeAffinity(pod, nodeInfo.Node()) {
        score += 50
    }

    // æ‹“æ‰‘è¯„åˆ†
    if c.hasPodSpreadConstraints(pod, nodeInfo) {
        score += 30
    }

    // æœ€ç»ˆè¯„åˆ†é™åˆ¶åœ¨0-100èŒƒå›´å†…
    if score < 0 {
        score = 0
    } else if score > 100 {
        score = 100
    }

    return score
}

// è®¡ç®—èµ„æºåˆ†æ•°
func (c *CustomResourceOptimizer) resourceFraction(requested, capacity int64) float64 {
    if capacity == 0 {
        return 1
    }
    return float64(requested) / float64(capacity)
}

// æ£€æŸ¥èŠ‚ç‚¹äº²å’Œæ€§
func (c *CustomResourceOptimizer) hasNodeAffinity(pod *v1.Pod, node *v1.Node) bool {
    if pod.Spec.Affinity == nil || pod.Spec.Affinity.NodeAffinity == nil {
        return true
    }
    // å®ç°èŠ‚ç‚¹äº²å’Œæ€§æ£€æŸ¥é€»è¾‘
    return true
}

// æ£€æŸ¥Podåˆ†å¸ƒçº¦æŸ
func (c *CustomResourceOptimizer) hasPodSpreadConstraints(pod *v1.Pod, nodeInfo *framework.NodeInfo) bool {
    // å®ç°Podåˆ†å¸ƒçº¦æŸæ£€æŸ¥é€»è¾‘
    return true
}

// æ’ä»¶å·¥å‚å‡½æ•°
func New(obj runtime.Object, handle framework.Handle) (framework.Plugin, error) {
    return &CustomResourceOptimizer{handle: handle}, nil
}
```

## 3. é«˜çº§è°ƒåº¦ç­–ç•¥

### 3.1 æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦

```yaml
# æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦é…ç½®
apiVersion: scheduling.k8s.io/v1
kind: PodTopologySpreadConstraint
metadata:
  name: topology-aware-scheduling
spec:
  topologyKey: topology.kubernetes.io/zone
  maxSkew: 1
  whenUnsatisfiable: ScheduleAnyway
  labelSelector:
    matchLabels:
      app: distributed-app
---
# å¤šç»´åº¦æ‹“æ‰‘è°ƒåº¦
apiVersion: v1
kind: Pod
metadata:
  name: topology-aware-pod
  labels:
    app: distributed-app
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: distributed-app
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app: distributed-app
  containers:
  - name: app
    image: myapp:latest
    resources:
      requests:
        cpu: "1"
        memory: "2Gi"
      limits:
        cpu: "2"
        memory: "4Gi"
```

### 3.2 äº²å’Œæ€§ä¸åäº²å’Œæ€§è°ƒåº¦

```yaml
# èŠ‚ç‚¹äº²å’Œæ€§è°ƒåº¦
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/arch
            operator: In
            values:
            - amd64
          - key: node-type
            operator: In
            values:
            - high-performance
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 10
        preference:
          matchExpressions:
          - key: node-type
            operator: In
            values:
            - gpu-node
  containers:
  - name: app
    image: myapp:latest
---
# Podäº²å’Œæ€§è°ƒåº¦
apiVersion: v1
kind: Pod
metadata:
  name: pod-affinity-pod
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - database
        topologyKey: kubernetes.io/hostname
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: database
          topologyKey: topology.kubernetes.io/zone
  containers:
  - name: app
    image: myapp:latest
```

## 4. èµ„æºä¼˜åŒ–è°ƒåº¦

### 4.1 èµ„æºå‡è¡¡è°ƒåº¦

```yaml
# èµ„æºå‡è¡¡è°ƒåº¦é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: balanced-scheduler-config
  namespace: kube-system
data:
  scheduler-config.yaml: |
    apiVersion: kubescheduler.config.k8s.io/v1beta3
    kind: KubeSchedulerConfiguration
    profiles:
    - schedulerName: balanced-scheduler
      plugins:
        score:
          enabled:
          - name: NodeResourcesBalancedAllocation
            weight: 2
          - name: NodeResourcesLeastAllocated
            weight: 1
          - name: ImageLocality
            weight: 1
          - name: InterPodAffinity
            weight: 1
      pluginConfig:
      - name: NodeResourcesBalancedAllocation
        args:
          apiVersion: kubescheduler.config.k8s.io/v1beta3
          kind: NodeResourcesBalancedAllocationArgs
          resources:
          - name: cpu
            weight: 1
          - name: memory
            weight: 1
          - name: nvidia.com/gpu
            weight: 2
```

### 4.2 ä¼˜å…ˆçº§å’ŒæŠ¢å è°ƒåº¦

```yaml
# ä¼˜å…ˆçº§ç±»å®šä¹‰
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "High priority class for critical applications"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority
value: 500000
globalDefault: false
description: "Medium priority class for regular applications"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 100000
globalDefault: true
description: "Low priority class for batch jobs"
---
# é«˜ä¼˜å…ˆçº§Podé…ç½®
apiVersion: v1
kind: Pod
metadata:
  name: high-priority-pod
spec:
  priorityClassName: high-priority
  containers:
  - name: app
    image: myapp:latest
    resources:
      requests:
        cpu: "1"
        memory: "1Gi"
      limits:
        cpu: "2"
        memory: "2Gi"
  preemptionPolicy: PreemptLowerPriority
```

## 5. è‡ªå®šä¹‰è°ƒåº¦å™¨å¼€å‘

### 5.1 è‡ªå®šä¹‰è°ƒåº¦å™¨å®ç°

```go
// è‡ªå®šä¹‰è°ƒåº¦å™¨å®ç°
package main

import (
    "context"
    "fmt"
    "time"

    v1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/apimachinery/pkg/fields"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
    "k8s.io/client-go/tools/cache"
    "k8s.io/client-go/tools/clientcmd"
    "k8s.io/klog/v2"
)

type CustomScheduler struct {
    clientset kubernetes.Interface
}

func NewCustomScheduler(kubeconfig string) (*CustomScheduler, error) {
    var config *rest.Config
    var err error

    if kubeconfig != "" {
        config, err = clientcmd.BuildConfigFromFlags("", kubeconfig)
    } else {
        config, err = rest.InClusterConfig()
    }

    if err != nil {
        return nil, err
    }

    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        return nil, err
    }

    return &CustomScheduler{
        clientset: clientset,
    }, nil
}

func (cs *CustomScheduler) Run() {
    // åˆ›å»ºPodç›‘å¬å™¨
    podListWatch := cache.NewListWatchFromClient(
        cs.clientset.CoreV1().RESTClient(),
        "pods",
        v1.NamespaceAll,
        fields.OneTermEqualSelector("spec.schedulerName", "custom-scheduler"),
    )

    _, controller := cache.NewInformer(
        podListWatch,
        &v1.Pod{},
        time.Second*30,
        cache.ResourceEventHandlerFuncs{
            AddFunc: cs.schedulePod,
        },
    )

    stopCh := make(chan struct{})
    go controller.Run(stopCh)

    // ç­‰å¾…ç¼“å­˜åŒæ­¥
    for !controller.HasSynced() {
        time.Sleep(time.Second)
    }

    <-stopCh
}

func (cs *CustomScheduler) schedulePod(obj interface{}) {
    pod := obj.(*v1.Pod)

    if pod.Spec.NodeName != "" {
        // Podå·²ç»è¢«è°ƒåº¦
        return
    }

    // å®ç°è°ƒåº¦é€»è¾‘
    nodeName, err := cs.selectBestNode(pod)
    if err != nil {
        klog.Errorf("Failed to select node for pod %s/%s: %v", pod.Namespace, pod.Name, err)
        return
    }

    // ç»‘å®šPodåˆ°èŠ‚ç‚¹
    err = cs.bindPod(pod, nodeName)
    if err != nil {
        klog.Errorf("Failed to bind pod %s/%s to node %s: %v", pod.Namespace, pod.Name, nodeName, err)
        return
    }

    klog.Infof("Successfully scheduled pod %s/%s to node %s", pod.Namespace, pod.Name, nodeName)
}

func (cs *CustomScheduler) selectBestNode(pod *v1.Pod) (string, error) {
    // è·å–æ‰€æœ‰èŠ‚ç‚¹
    nodeList, err := cs.clientset.CoreV1().Nodes().List(context.TODO(), metav1.ListOptions{})
    if err != nil {
        return "", err
    }

    var bestNode string
    var bestScore int64 = -1

    for _, node := range nodeList.Items {
        // æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å‡†å¤‡å¥½
        if !cs.isNodeReady(&node) {
            continue
        }

        // æ£€æŸ¥èŠ‚ç‚¹èµ„æºæ˜¯å¦æ»¡è¶³Podéœ€æ±‚
        if !cs.checkNodeResources(&node, pod) {
            continue
        }

        // è®¡ç®—èŠ‚ç‚¹è¯„åˆ†
        score := cs.calculateNodeScore(&node, pod)
        if score > bestScore {
            bestScore = score
            bestNode = node.Name
        }
    }

    if bestNode == "" {
        return "", fmt.Errorf("no suitable node found for pod %s/%s", pod.Namespace, pod.Name)
    }

    return bestNode, nil
}

func (cs *CustomScheduler) isNodeReady(node *v1.Node) bool {
    for _, condition := range node.Status.Conditions {
        if condition.Type == v1.NodeReady && condition.Status == v1.ConditionTrue {
            return true
        }
    }
    return false
}

func (cs *CustomScheduler) checkNodeResources(node *v1.Node, pod *v1.Pod) bool {
    // æ£€æŸ¥CPUèµ„æº
    allocatableCPU := node.Status.Allocatable.Cpu().MilliValue()
    requestedCPU := int64(0)

    for _, container := range pod.Spec.Containers {
        if cpu := container.Resources.Requests.Cpu(); cpu != nil {
            requestedCPU += cpu.MilliValue()
        }
    }

    if requestedCPU > allocatableCPU {
        return false
    }

    // æ£€æŸ¥å†…å­˜èµ„æº
    allocatableMemory := node.Status.Allocatable.Memory().Value()
    requestedMemory := int64(0)

    for _, container := range pod.Spec.Containers {
        if memory := container.Resources.Requests.Memory(); memory != nil {
            requestedMemory += memory.Value()
        }
    }

    if requestedMemory > allocatableMemory {
        return false
    }

    return true
}

func (cs *CustomScheduler) calculateNodeScore(node *v1.Node, pod *v1.Pod) int64 {
    score := int64(0)

    // CPUåˆ©ç”¨ç‡è¯„åˆ†
    cpuAllocatable := node.Status.Allocatable.Cpu().MilliValue()
    cpuRequested := int64(0)

    // è·å–èŠ‚ç‚¹ä¸Šå·²æœ‰Podçš„èµ„æºè¯·æ±‚
    pods, err := cs.clientset.CoreV1().Pods("").List(context.TODO(), metav1.ListOptions{
        FieldSelector: fmt.Sprintf("spec.nodeName=%s", node.Name),
    })
    if err == nil {
        for _, pod := range pods.Items {
            for _, container := range pod.Spec.Containers {
                if cpu := container.Resources.Requests.Cpu(); cpu != nil {
                    cpuRequested += cpu.MilliValue()
                }
            }
        }
    }

    cpuUtilization := float64(cpuRequested) / float64(cpuAllocatable)
    cpuScore := int64((1 - cpuUtilization) * 50) // CPUè¶Šç©ºé—²å¾—åˆ†è¶Šé«˜

    // å†…å­˜åˆ©ç”¨ç‡è¯„åˆ†
    memoryAllocatable := node.Status.Allocatable.Memory().Value()
    memoryRequested := int64(0)

    if err == nil {
        for _, pod := range pods.Items {
            for _, container := range pod.Spec.Containers {
                if memory := container.Resources.Requests.Memory(); memory != nil {
                    memoryRequested += memory.Value()
                }
            }
        }
    }

    memoryUtilization := float64(memoryRequested) / float64(memoryAllocatable)
    memoryScore := int64((1 - memoryUtilization) * 30) // å†…å­˜è¶Šç©ºé—²å¾—åˆ†è¶Šé«˜

    // äº²å’Œæ€§è¯„åˆ†
    affinityScore := cs.calculateAffinityScore(node, pod)

    score = cpuScore + memoryScore + affinityScore

    // ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
    if score < 0 {
        score = 0
    } else if score > 100 {
        score = 100
    }

    return score
}

func (cs *CustomScheduler) calculateAffinityScore(node *v1.Node, pod *v1.Pod) int64 {
    score := int64(0)

    // å®ç°äº²å’Œæ€§è¯„åˆ†é€»è¾‘
    if pod.Spec.Affinity != nil {
        // èŠ‚ç‚¹äº²å’Œæ€§è¯„åˆ†
        if nodeAffinity := pod.Spec.Affinity.NodeAffinity; nodeAffinity != nil {
            if required := nodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution; required != nil {
                for _, term := range required.NodeSelectorTerms {
                    for _, expr := range term.MatchExpressions {
                        if cs.matchesNodeLabel(node, expr) {
                            score += 20
                        }
                    }
                }
            }
        }

        // Podäº²å’Œæ€§è¯„åˆ†
        if podAffinity := pod.Spec.Affinity.PodAffinity; podAffinity != nil {
            for _, term := range podAffinity.RequiredDuringSchedulingIgnoredDuringExecution {
                if cs.matchesPodAffinity(node, pod, term.LabelSelector) {
                    score += 15
                }
            }
        }
    }

    return score
}

func (cs *CustomScheduler) matchesNodeLabel(node *v1.Node, expr v1.NodeSelectorRequirement) bool {
    for key, value := range node.Labels {
        if key == expr.Key {
            switch expr.Operator {
            case v1.NodeSelectorOpIn:
                for _, val := range expr.Values {
                    if val == value {
                        return true
                    }
                }
            case v1.NodeSelectorOpNotIn:
                for _, val := range expr.Values {
                    if val == value {
                        return false
                    }
                }
                return true
            }
        }
    }
    return false
}

func (cs *CustomScheduler) matchesPodAffinity(node *v1.Node, pod *v1.Pod, selector *metav1.LabelSelector) bool {
    // å®ç°Podäº²å’Œæ€§åŒ¹é…é€»è¾‘
    return true
}

func (cs *CustomScheduler) bindPod(pod *v1.Pod, nodeName string) error {
    binding := &v1.Binding{
        ObjectMeta: metav1.ObjectMeta{
            Name:      pod.Name,
            Namespace: pod.Namespace,
        },
        Target: v1.ObjectReference{
            Kind: "Node",
            Name: nodeName,
        },
    }

    return cs.clientset.CoreV1().Pods(pod.Namespace).Bind(context.TODO(), binding, metav1.CreateOptions{})
}

func main() {
    scheduler, err := NewCustomScheduler("")
    if err != nil {
        klog.Fatal("Failed to create scheduler: ", err)
    }

    klog.Info("Starting custom scheduler...")
    scheduler.Run()
}
```

### 5.2 è°ƒåº¦å™¨æ‰©å±•æ’ä»¶

```yaml
# è°ƒåº¦å™¨æ‰©å±•é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-extender-config
  namespace: kube-system
data:
  scheduler-extender.json: |
    {
      "kind": "SchedulerExtender",
      "apiVersion": "v1",
      "extenders": [
        {
          "urlPrefix": "http://scheduler-extender-service:8000",
          "filterVerb": "filter",
          "prioritizeVerb": "prioritize",
          "weight": 5,
          "bindVerb": "bind",
          "enableHttps": false,
          "nodeCacheCapable": false,
          "managedResources": [
            {
              "name": "example.com/custom-resource",
              "ignoredByScheduler": false
            }
          ],
          "ignorable": false
        }
      ],
      "predicates": [
        {"name": "GeneralPredicates"},
        {"name": "PodToleratesNodeTaints"},
        {"name": "CheckNodeUnschedulablePredicate"},
        {"name": "CheckNodeLabelPresence"},
        {"name": "CheckServiceAffinity"}
      ],
      "priorities": [
        {"name": "LeastRequestedPriority", "weight": 1},
        {"name": "BalancedResourceAllocation", "weight": 1},
        {"name": "NodePreferAvoidPodsPriority", "weight": 10000},
        {"name": "NodeAffinityPriority", "weight": 1},
        {"name": "TaintTolerationPriority", "weight": 1},
        {"name": "ImageLocalityPriority", "weight": 1}
      ]
    }
```

## 6. ç›‘æ§ä¸æ€§èƒ½åˆ†æ

### 6.1 è°ƒåº¦æ€§èƒ½ç›‘æ§

```yaml
# è°ƒåº¦å™¨ç›‘æ§é…ç½®
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-scheduler-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      component: kube-scheduler
  endpoints:
  - port: https
    path: /metrics
    scheme: https
    interval: 30s
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecureSkipVerify: true
    bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    metricRelabelings:
    - sourceLabels: [__name__]
      regex: 'scheduler_(.*)'
      targetLabel: __name__
    - sourceLabels: [__name__]
      regex: 'workqueue_(.*)'
      targetLabel: __name__
    - sourceLabels: [__name__]
      regex: 'rest_client_(.*)'
      targetLabel: __name__
```

### 6.2 è°ƒåº¦æ€§èƒ½æŒ‡æ ‡

```prometheus
# è°ƒåº¦å™¨å…³é”®æ€§èƒ½æŒ‡æ ‡
# è°ƒåº¦å»¶è¿Ÿ
histogram_quantile(0.99, rate(scheduler_e2e_scheduling_duration_seconds_bucket[5m])) > 5
histogram_quantile(0.95, rate(scheduler_e2e_scheduling_duration_seconds_bucket[5m])) > 2

# é˜Ÿåˆ—ç§¯å‹
scheduler_pending_pods > 100
scheduler_queue_incoming_pods_rate > 50

# è°ƒåº¦è¿‡æ»¤å¤±è´¥
increase(scheduler_framework_extension_point_duration_seconds_count{extension_point="Filter",result="Error"}[5m]) > 0

# è°ƒåº¦è¯„åˆ†åˆ†å¸ƒ
histogram_quantile(0.95, rate(scheduler_node_score_duration_seconds_bucket[5m])) > 0.1

# èŠ‚ç‚¹ç¼“å­˜åŒæ­¥
increase(scheduler_cache_adds_total[5m]) > 1000
increase(scheduler_cache_updates_total[5m]) > 500

# ç»‘å®šæ€§èƒ½
histogram_quantile(0.99, rate(scheduler_binding_duration_seconds_bucket[5m])) > 1
```

### 6.3 è°ƒåº¦åˆ†æå·¥å…·

```bash
#!/bin/bash
# scheduler-performance-analyzer.sh

# è°ƒåº¦å™¨æ€§èƒ½åˆ†æè„šæœ¬
echo "=== è°ƒåº¦å™¨æ€§èƒ½åˆ†æ ==="

# 1. æ£€æŸ¥è°ƒåº¦å™¨çŠ¶æ€
echo "1. æ£€æŸ¥è°ƒåº¦å™¨çŠ¶æ€:"
kubectl get pods -n kube-system -l component=kube-scheduler

# 2. è·å–è°ƒåº¦å™¨æŒ‡æ ‡
echo "2. è·å–è°ƒåº¦å™¨æŒ‡æ ‡:"
kubectl get --raw /metrics | grep scheduler | head -20

# 3. åˆ†æè°ƒåº¦å»¶è¿Ÿ
echo "3. åˆ†æè°ƒåº¦å»¶è¿Ÿ:"
kubectl get --raw /metrics | grep "scheduler_e2e_scheduling_duration_seconds_bucket" | sort

# 4. æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
echo "4. æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€:"
kubectl get --raw /metrics | grep "scheduler_pending_pods"

# 5. åˆ†æè°ƒåº¦å¤±è´¥åŸå› 
echo "5. åˆ†æè°ƒåº¦å¤±è´¥åŸå› :"
kubectl get --raw /metrics | grep "scheduler_framework_extension_point_duration_seconds_count" | grep "Error"

# 6. æ£€æŸ¥èŠ‚ç‚¹ç¼“å­˜çŠ¶æ€
echo "6. æ£€æŸ¥èŠ‚ç‚¹ç¼“å­˜çŠ¶æ€:"
kubectl get --raw /metrics | grep "scheduler_cache_"

# 7. ç”Ÿæˆè°ƒåº¦æ€§èƒ½æŠ¥å‘Š
echo "7. ç”Ÿæˆè°ƒåº¦æ€§èƒ½æŠ¥å‘Š:"
cat << EOF > /tmp/scheduler-performance-report.txt
Scheduler Performance Report - $(date)
====================================

Scheduling Latency (99th percentile): $(kubectl get --raw /metrics | grep 'scheduler_e2e_scheduling_duration_seconds_bucket' | grep 'le="+Inf"}' | cut -d' ' -f2)

Pending Pods: $(kubectl get --raw /metrics | grep 'scheduler_pending_pods ' | cut -d' ' -f2)

Filter Error Count (last 5m): $(kubectl get --raw /metrics | grep 'scheduler_framework_extension_point_duration_seconds_count{extension_point="Filter",result="Error"}' | cut -d' ' -f2)

Cache Adds (last 5m): $(kubectl get --raw /metrics | grep 'scheduler_cache_adds_total' | cut -d' ' -f2)

Binding Duration (99th percentile): $(kubectl get --raw /metrics | grep 'scheduler_binding_duration_seconds_bucket' | grep 'le="+Inf"}' | cut -d' ' -f2)

EOF

echo "è°ƒåº¦æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: /tmp/scheduler-performance-report.txt"

# 8. è°ƒåº¦äº‹ä»¶åˆ†æ
echo "8. è°ƒåº¦ç›¸å…³äº‹ä»¶åˆ†æ:"
kubectl get events --all-namespaces --field-selector involvedObject.kind=Pod -o json | \
    jq -r '.items[] | select(.reason == "Scheduled") | "\(.firstTimestamp) \(.involvedObject.namespace)/\(.involvedObject.name) \(.message)"' | \
    tail -20
```

## 7. æœ€ä½³å®è·µä¸ä¼˜åŒ–ç­–ç•¥

### 7.1 è°ƒåº¦ä¼˜åŒ–åŸåˆ™

```markdown
## âš¡ è°ƒåº¦ä¼˜åŒ–åŸåˆ™

### 1. èµ„æºè§„åˆ’
- åˆç†è®¾ç½®èµ„æºè¯·æ±‚å’Œé™åˆ¶
- ä½¿ç”¨å‚ç›´Podè‡ªåŠ¨æ‰©ç¼©å®¹(VPA)
- å®æ–½èµ„æºé…é¢ç®¡ç†

### 2. è°ƒåº¦ç­–ç•¥
- å¯ç”¨Podæ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
- é…ç½®èŠ‚ç‚¹äº²å’Œæ€§å’Œåäº²å’Œæ€§
- ä½¿ç”¨ä¼˜å…ˆçº§å’ŒæŠ¢å æœºåˆ¶

### 3. æ€§èƒ½è°ƒä¼˜
- ä¼˜åŒ–è°ƒåº¦å™¨é…ç½®å‚æ•°
- è°ƒæ•´è°ƒåº¦å™¨å¹¶å‘åº¦
- å¯ç”¨è°ƒåº¦å™¨ç¼“å­˜ä¼˜åŒ–

### 4. ç›‘æ§å‘Šè­¦
- å»ºç«‹è°ƒåº¦å»¶è¿Ÿç›‘æ§
- è®¾ç½®é˜Ÿåˆ—ç§¯å‹å‘Šè­¦
- ç›‘æ§è°ƒåº¦å¤±è´¥ç‡
```

### 7.2 å®æ–½æ£€æŸ¥æ¸…å•

```yaml
è°ƒåº¦ä¼˜åŒ–å®æ–½æ¸…å•:
  èµ„æºé…ç½®:
    â˜ è®¾ç½®åˆç†çš„èµ„æºè¯·æ±‚/é™åˆ¶
    â˜ å¯ç”¨VPAæ¨èå™¨
    â˜ é…ç½®èµ„æºé…é¢
    â˜ å®æ–½QoSç­‰çº§
  
  è°ƒåº¦ç­–ç•¥:
    â˜ é…ç½®Podæ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
    â˜ è®¾ç½®èŠ‚ç‚¹äº²å’Œæ€§è§„åˆ™
    â˜ å®šä¹‰ä¼˜å…ˆçº§ç±»
    â˜ å¯ç”¨æŠ¢å ç­–ç•¥
  
  æ€§èƒ½ä¼˜åŒ–:
    â˜ è°ƒåº¦å™¨å‚æ•°è°ƒä¼˜
    â˜ å¹¶å‘åº¦é…ç½®ä¼˜åŒ–
    â˜ ç¼“å­˜ç­–ç•¥è°ƒæ•´
    â˜ è°ƒåº¦ç®—æ³•ä¼˜åŒ–
  
  ç›‘æ§å‘Šè­¦:
    â˜ è°ƒåº¦å»¶è¿Ÿç›‘æ§
    â˜ é˜Ÿåˆ—çŠ¶æ€ç›‘æ§
    â˜ è°ƒåº¦å¤±è´¥å‘Šè­¦
    â˜ èŠ‚ç‚¹èµ„æºç›‘æ§
```

## 8. æœªæ¥å‘å±•è¶‹åŠ¿

### 8.1 æ™ºèƒ½è°ƒåº¦

```yaml
æ™ºèƒ½è°ƒåº¦å‘å±•è¶‹åŠ¿:
  1. AIé©±åŠ¨çš„è°ƒåº¦ä¼˜åŒ–
     - æœºå™¨å­¦ä¹ èµ„æºé¢„æµ‹
     - æ™ºèƒ½è°ƒåº¦ç­–ç•¥
     - è‡ªé€‚åº”è°ƒåº¦ç®—æ³•
  
  2. å¤šç»´åº¦èµ„æºè°ƒåº¦
     - GPU/FPGAç­‰å¼‚æ„èµ„æº
     - ç½‘ç»œå¸¦å®½èµ„æº
     - å­˜å‚¨æ€§èƒ½èµ„æº
  
  3. æ··åˆäº‘è°ƒåº¦
     - è·¨äº‘èµ„æºè°ƒåº¦
     - è¾¹ç¼˜è®¡ç®—è°ƒåº¦
     - æ··åˆéƒ¨ç½²ä¼˜åŒ–
```

---
*æœ¬æ–‡æ¡£åŸºäºä¼ä¸šçº§è°ƒåº¦ä¼˜åŒ–å®è·µç»éªŒç¼–å†™ï¼ŒæŒç»­æ›´æ–°æœ€æ–°æŠ€æœ¯å’Œæœ€ä½³å®è·µã€‚*